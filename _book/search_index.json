[
["index.html", "Applied Longitudinal Data Analysis in brms and the tidyverse version 0.0.1 What and why Caution: Work in progress", " Applied Longitudinal Data Analysis in brms and the tidyverse version 0.0.1 A Solomon Kurz 2020-04-21 What and why This project is based on Singer and Willett’s classic (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence. You can download the data used in the text at http://www.bristol.ac.uk/cmm/learning/support/singer-willett.html and find a wealth of ideas on how to fit the models in the text at https://stats.idre.ucla.edu/other/examples/alda/. My contributions show how to fit these models and others like them within a Bayesian framework. I make extensive use of Paul Bürkner’s brms package, which makes it easy to fit Bayesian regression models in R using Hamiltonian Monte Carlo (HMC) via the Stan probabilistic programming language. Much of the data wrangling and plotting code is done with packages connected to the tidyverse. Caution: Work in progress This inaugural 0.0.1 release contains first drafts of Chapters 1 through 5 and 9 through 12. Chapters 1 through 5 provide the motivation and foundational principles for fitting longitudinal multilevel models. Chapters 9 through 12 motivation and foundational principles for fitting discrete-time survival analyses. A few of the remaining chapters have partially completed drafts and will be added sometime soon. In addition to fleshing out more of the chapters, I plan to add more goodies like introductions to multivariate longitudinal models and mixed-effect location and scale models. But there is no time-table for this project. To keep up with the latest changes, check in at the GitHub repository, https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse, or follow my announcements on twitter at https://twitter.com/SolomonKurz. "],
["a-framework-for-investigating-change-over-time.html", "1 A Framework for Investigating Change over Time 1.1 When might you study change over time? 1.2 Distinguishing between two types of questions about change 1.3 Three important features of a study of change Reference Session info", " 1 A Framework for Investigating Change over Time It is possible to measure change, and to do it well, if you have longitudinal data (Rogosa, Brandt, &amp; Zimowski, 1982; Willett, 1989). Cross-sectional data–so easy to collect and so widely available–will not suffice. In this chapter, we describe why longitudinal data are necessary for studying change. (p. 3, emphasis in the original) 1.1 When might you study change over time? Perhaps a better question is When wouldn’t you? 1.2 Distinguishing between two types of questions about change On page 8, Singer and Willett proposed there are two fundamental questions for longitudinal data analysis: “How does the outcome change over time?” and “Can we predict differences in these changes?” Within the hierarchical framework, we often speak about two levels of change. We address within-individual change at level-1. The goal of a level-1 analysis is to describe the shape of each person’s individual growth trajectory. In the second stage of an analysis of change, known as level-2, we ask about interindividual differences in change… The goal of a level-2 analysis is to detect heterogeneity in change across individuals and to determine the relationship between predictors and the shape of each person’s individual growth trajectory. (p. 8, emphasis in the original) 1.3 Three important features of a study of change Three or more waves of data An outcome whose values change systematically over time A sensible metric for clocking time 1.3.1 Multiple waves of data. Singer and Willett criticized two-waves data on two grounds. First, it cannot tell us about the shape of each person’s individual growth trajectory, the focus of our level-1 question. Did all the change occur immediately after the first assessment? Was progress steady or delayed? Second, it cannot distinguish true change from measurement error. If measurement error renders pretest scores too low and posttest scores too high, you might conclude erroneously that scores increase over time when a longer temporal view would suggest the opposite. In statistical terms, two-waves studies cannot describe individual trajectories of change and they confound true change with measurement error (see Rogosa, Brandt, &amp; Zimowski, 1982). (p. 10, emphasis in the original) I am not a fan of this ‘true change/measurement error’ way of speaking and would rather speak in terms of systemic and [seemingly] un-systemic changes among means and variances. Otherwise put, I’d rather speak in terms of trait and state. Two waves of data do not allow us to disentangle systemic mean differences from stable means and substantial variances for those means. Two waves of data do not allow us to disentangle changes in traits from stable traits but important differences in states. For an introduction to this way of thinking, check out Nezlek’s (2007) A multilevel framework for understanding relationships among traits, states, situations and behaviors. 1.3.2 A sensible metric for time. Choice of a time metric affects several interrelated decisions about the number and spacing of data collection waves…. Our overarching point is that there is no single answer to the seemingly simple question about the most sensible metric for time. You should adopt whatever scale makes most sense for your outcomes and your research question…. Our point is simple: choose a metric for the that reflect the cadence you expect to be most useful for your outcome. (p. 11) Data collection waves can be evenly spaced or not. E.g., if you anticipate a time period of rapid nonlinear change, it might be helpful to increase the density of assessments during that period. Everyone does not have to have the same assessment schedule. If all are assessed on the same schedule, we describe the data as time-structured. When assessment schedules vary across participants, the data are termed time-unstructured. The data are balanced if all participants have the same number of waves. Issues like attrition and so on lead to unbalanced data. Though they may have some pedagogical use, I have not found these terms useful in practice. 1.3.3 A continuous outcome that changes systematically over time. To my eye, the most interesting part of this section is the discussion of measurement validity over time. E.g., When we say the metric in which the outcome is measured must be preserved across time, we mean that the outcome scores must be equatable over time–a given value of the outcome on any occasion must represent the same “amount” of the outcome on every occasion. Outcome equatability is easiest to ensure when you use the identical instrument for measurement repeatedly over time. (p. 13) This isn’t as simple as is sounds. Though it’s beyond the scope of this project, you might learn more about this from a study of the longitudinal measurement invariance literature. To dive in, see the first couple chapters in Newsom’s (2015) text, Longitudinal Structural Equation Modeling: A Comprehensive Introduction. Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_3.6.3 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.5 evaluate_0.14 "],
["exploring-longitudinal-data-on-change.html", "2 Exploring Longitudinal Data on Change 2.1 Creating a longitudinal data set 2.2 Descriptive analysis of individual change over time 2.3 Exploring differences in change across people 2.4 Improving the precision and reliability of OLS single-level-Bayesian-estimated rates of change: Lessons for research design Reference Session info", " 2 Exploring Longitudinal Data on Change Wise researchers conduct descriptive exploratory analyses of their data before fitting statistical models. As when working with cross-sectional data, exploratory analyses of longitudinal data con reveal general patterns, provide insight into functional form, and identify individuals whose data do not conform to the general pattern. The exploratory analyses presented in this chapter are based on numerical and graphical strategies already familiar from cross-sectional work. Owing to the nature of longitudinal data, however, they are inevitably more complex in this new setting. (p. 16) 2.1 Creating a longitudinal data set In longitudinal work, data-set organization is less straightforward because you can use two very different arrangements: A person-level data set, in which each person has one record and multiple variables contain the data from each measurement occasion A person-period data set, in which each person has multiple records—one for each measurement occasion (p. 17, emphasis in the original) These are also sometimes referred to as the wide and long data formats, respectively. As you will see, we will use two primary functions from the tidyverse to convert data from one format to another. 2.1.1 The person-level data set. Here we load the person-level data from this UCLA web site. These are the NLY data (see Raudenbush &amp; Chan, 1992) shown in the top of Figure 2.1. library(tidyverse) tolerance &lt;- read_csv(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/tolerance1.txt&quot;, col_names = T) head(tolerance, n = 16) ## # A tibble: 16 x 8 ## id tol11 tol12 tol13 tol14 tol15 male exposure ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 2.23 1.79 1.9 2.12 2.66 0 1.54 ## 2 45 1.12 1.45 1.45 1.45 1.99 1 1.16 ## 3 268 1.45 1.34 1.99 1.79 1.34 1 0.9 ## 4 314 1.22 1.22 1.55 1.12 1.12 0 0.81 ## 5 442 1.45 1.99 1.45 1.67 1.9 0 1.13 ## 6 514 1.34 1.67 2.23 2.12 2.44 1 0.9 ## 7 569 1.79 1.9 1.9 1.99 1.99 0 1.99 ## 8 624 1.12 1.12 1.22 1.12 1.22 1 0.98 ## 9 723 1.22 1.34 1.12 1 1.12 0 0.81 ## 10 918 1 1 1.22 1.99 1.22 0 1.21 ## 11 949 1.99 1.55 1.12 1.45 1.55 1 0.93 ## 12 978 1.22 1.34 2.12 3.46 3.32 1 1.59 ## 13 1105 1.34 1.9 1.99 1.9 2.12 1 1.38 ## 14 1542 1.22 1.22 1.99 1.79 2.12 0 1.44 ## 15 1552 1 1.12 2.23 1.55 1.55 0 1.04 ## 16 1653 1.11 1.11 1.34 1.55 2.12 0 1.25 With person-level data, each participant has a single row. In these data, participants are indexed by their id number. To see how many participants are in these data, just count() the rows. tolerance %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 16 The nrow() function will work, too. tolerance %&gt;% nrow() ## [1] 16 With the base R cor() function, you can get the Pearson’s correlation matrix shown in Table 2.1. cor(tolerance[ , 2:6]) %&gt;% round(digits = 2) ## tol11 tol12 tol13 tol14 tol15 ## tol11 1.00 0.66 0.06 0.14 0.26 ## tol12 0.66 1.00 0.25 0.21 0.39 ## tol13 0.06 0.25 1.00 0.59 0.57 ## tol14 0.14 0.21 0.59 1.00 0.83 ## tol15 0.26 0.39 0.57 0.83 1.00 We used round() to limit the number of decimal places in the output. Leave it off and you’ll see cor() returns up to seven decimal places instead. It can be hard to see the patters within a matrix of numerals. It might be easier in a plot. cor(tolerance[ , 2:6]) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;row&quot;) %&gt;% pivot_longer(-row, names_to = &quot;column&quot;, values_to = &quot;correlation&quot;) %&gt;% mutate(row = factor(row) %&gt;% fct_rev(.)) %&gt;% ggplot(aes(x = column, y = row)) + geom_raster(aes(fill = correlation)) + geom_text(aes(label = round(correlation, digits = 2)), size = 3.5) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red4&quot;, limits = c(0, 1)) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme(axis.ticks = element_blank()) If all you wanted was the lower diagonal, you could use the lowerCor() function from the psych package. psych::lowerCor(tolerance[ , 2:6]) ## tol11 tol12 tol13 tol14 tol15 ## tol11 1.00 ## tol12 0.66 1.00 ## tol13 0.06 0.25 1.00 ## tol14 0.14 0.21 0.59 1.00 ## tol15 0.26 0.39 0.57 0.83 1.00 For more ways to compute, organize, and visualize correlations within the tidyverse paradigm, check out the corr package. 2.1.2 The person-period data set. Here are the person-period data (i.e., those shown in the bottom of Figure 2.1). tolerance_pp &lt;- read_csv(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/tolerance1_pp.txt&quot;, col_names = T) tolerance_pp %&gt;% slice(c(1:9, 76:80)) ## # A tibble: 14 x 6 ## id age tolerance male exposure time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 11 2.23 0 1.54 0 ## 2 9 12 1.79 0 1.54 1 ## 3 9 13 1.9 0 1.54 2 ## 4 9 14 2.12 0 1.54 3 ## 5 9 15 2.66 0 1.54 4 ## 6 45 11 1.12 1 1.16 0 ## 7 45 12 1.45 1 1.16 1 ## 8 45 13 1.45 1 1.16 2 ## 9 45 14 1.45 1 1.16 3 ## 10 1653 11 1.11 0 1.25 0 ## 11 1653 12 1.11 0 1.25 1 ## 12 1653 13 1.34 0 1.25 2 ## 13 1653 14 1.55 0 1.25 3 ## 14 1653 15 2.12 0 1.25 4 With data like these, the simple use of count() or nrow() won’t help us discover how many participants there are in the tolerance_pp data. One quick way is to count() the number of distinct() id values. tolerance_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 16 A fundamental skill is knowing how to convert longitudinal data in one format to the other. If you’re using packages within the tidyverse, the pivot_longer() function will get you from the person-level format to the person-period format. tolerance %&gt;% # this is the main event pivot_longer(-c(id, male, exposure), names_to = &quot;age&quot;, values_to = &quot;tolerance&quot;) %&gt;% # here we remove the `tol` prefix from the `age` values and then save the numbers as integers mutate(age = str_remove(age, &quot;tol&quot;) %&gt;% as.integer()) %&gt;% # these last two lines just make the results look more like those in the last code chunk arrange(id, age) %&gt;% slice(c(1:9, 76:80)) ## # A tibble: 14 x 5 ## id male exposure age tolerance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 0 1.54 11 2.23 ## 2 9 0 1.54 12 1.79 ## 3 9 0 1.54 13 1.9 ## 4 9 0 1.54 14 2.12 ## 5 9 0 1.54 15 2.66 ## 6 45 1 1.16 11 1.12 ## 7 45 1 1.16 12 1.45 ## 8 45 1 1.16 13 1.45 ## 9 45 1 1.16 14 1.45 ## 10 1653 0 1.25 11 1.11 ## 11 1653 0 1.25 12 1.11 ## 12 1653 0 1.25 13 1.34 ## 13 1653 0 1.25 14 1.55 ## 14 1653 0 1.25 15 2.12 You can learn more about the pivot_longer() function here and here. As hinted at in the above hyperlinks, the opposite of the pivot_longer() function is pivot_wider(). We can use pivot_wider() to convert the person-period tolerance_pp data to the same format as the person-level tolerance data. tolerance_pp %&gt;% # we&#39;ll want to add that `tol` prefix back to the `age` values mutate(age = str_c(&quot;tol&quot;, age)) %&gt;% # this variable is just in the way. we&#39;ll drop it select(-time) %&gt;% # here&#39;s the main action pivot_wider(names_from = age, values_from = tolerance) ## # A tibble: 16 x 8 ## id male exposure tol11 tol12 tol13 tol14 tol15 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0 1.54 2.23 1.79 1.9 2.12 2.66 ## 2 45 1 1.16 1.12 1.45 1.45 1.45 1.99 ## 3 268 1 0.9 1.45 1.34 1.99 1.79 1.34 ## 4 314 0 0.81 1.22 1.22 1.55 1.12 1.12 ## 5 442 0 1.13 1.45 1.99 1.45 1.67 1.9 ## 6 514 1 0.9 1.34 1.67 2.23 2.12 2.44 ## 7 569 0 1.99 1.79 1.9 1.9 1.99 1.99 ## 8 624 1 0.98 1.12 1.12 1.22 1.12 1.22 ## 9 723 0 0.81 1.22 1.34 1.12 1 1.12 ## 10 918 0 1.21 1 1 1.22 1.99 1.22 ## 11 949 1 0.93 1.99 1.55 1.12 1.45 1.55 ## 12 978 1 1.59 1.22 1.34 2.12 3.46 3.32 ## 13 1105 1 1.38 1.34 1.9 1.99 1.9 2.12 ## 14 1542 0 1.44 1.22 1.22 1.99 1.79 2.12 ## 15 1552 0 1.04 1 1.12 2.23 1.55 1.55 ## 16 1653 0 1.25 1.11 1.11 1.34 1.55 2.12 2.2 Descriptive analysis of individual change over time The following “descriptive analyses [are intended to] reveal the nature and idiosyncrasies of each person’s temporal pattern of growth, addressing the question: How does each person change over time” (p. 23)? 2.2.1 Empirical growth plots. Empirical growth plots show individual-level sequence in a variable of interest over time. We’ll put age on the x-axis, tolerance on the y-axis, and make our variant of Figure 2.2 with geom_point(). It’s the facet_wrap() part of the code that splits the plot up by id. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + geom_point() + coord_cartesian(ylim = c(1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~id) By default, ggplot2 sets the scales of the x- and y-axes to the same values across subpanels. If you’d like to free that constraint, play around with the scales argument within facet_wrap(). 2.2.2 Using a trajectory to summarize each person’s empirical growth record. If we wanted to connect the dots, we might just add a geom_line() line. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + geom_point() + geom_line() + coord_cartesian(ylim = c(1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~id) However, Singer and Willett recommend two other approaches: nonparametric smoothing parametric functions 2.2.2.1 Smoothing the empirical growth trajectory nonparametrically. For our version of Figure 2.3, we’ll use a loess smoother. When using the stat_smooth() function in ggplot2, you can control how smooth or wiggly the line is with the span argument. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + geom_point() + stat_smooth(method = &quot;loess&quot;, se = F, span = .9) + coord_cartesian(ylim = c(1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~id) 2.2.2.2 Smoothing the empirical growth trajectory using OLS single-level Bayesian regression. Although “fitting person-specific regression models, one individual at a time, is hardly the most efficient use of longitudinal data” (p. 28), we may as well play along with the text. It’ll have pedagogical utility. You’ll see. For this section, we’ll take a cue from Hadley Wickham and use group_by() and nest() to make a tibble composed of tibbles (i.e., a nested tibble). by_id &lt;- tolerance_pp %&gt;% group_by(id) %&gt;% nest() You can get a sense of what we did with head(). by_id %&gt;% head() ## # A tibble: 6 x 2 ## # Groups: id [6] ## id data ## &lt;dbl&gt; &lt;list&gt; ## 1 9 &lt;tibble [5 × 5]&gt; ## 2 45 &lt;tibble [5 × 5]&gt; ## 3 268 &lt;tibble [5 × 5]&gt; ## 4 314 &lt;tibble [5 × 5]&gt; ## 5 442 &lt;tibble [5 × 5]&gt; ## 6 514 &lt;tibble [5 × 5]&gt; As indexed by id, each participant now has their own data set stored in the data column. To get a better sense, we’ll use our double-bracket subsetting skills to open up the first data set, the one for id == 9. If you’re not familiar with this skill, you can learn more from Chapter 9 of Roger Peng’s great online book, R Programming for Data Science, or Jenny Bryan’s fun and useful talk, Behind every great plot there’s a great deal of wrangling. by_id$data[[1]] ## # A tibble: 5 x 5 ## age tolerance male exposure time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 2.23 0 1.54 0 ## 2 12 1.79 0 1.54 1 ## 3 13 1.9 0 1.54 2 ## 4 14 2.12 0 1.54 3 ## 5 15 2.66 0 1.54 4 Our by_id data object has many data sets stored in a higher-level data set. The code we used is verbose, but that’s what made it human-readable. Now we have our nested tibble, we can make a function that will fit the simple linear model tolerance ~ 1 + time to each id-level data set. Why use time as the predictor? you ask. On page 29 in the text, Singer and Willett clarified they fit their individual models with \\((\\text{age} - 11)\\) in order to have the model intercepts centered at 11 years old rather than 0. If we wanted to, we could make an \\((\\text{age} - 11)\\) variable like so. by_id$data[[1]] %&gt;% mutate(age_minus_11 = age - 11) ## # A tibble: 5 x 6 ## age tolerance male exposure time age_minus_11 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 2.23 0 1.54 0 0 ## 2 12 1.79 0 1.54 1 1 ## 3 13 1.9 0 1.54 2 2 ## 4 14 2.12 0 1.54 3 3 ## 5 15 2.66 0 1.54 4 4 Did you notice how our age_minus_11 variable is the same as the time variable already in the data set? Yep, that’s why we’ll be using time in the model. In our data, \\((\\text{age} - 11)\\) is encoded as time. Singer and Willett used OLS to fit their exploratory models. We could do that to with the lm() function and we will do a little of that in this project. But let’s get frisky and fit the models as Bayesians, instead. Our primary statistical package for fitting Bayesian models will be Paul Bürkner’s brms. Let’s open it up. library(brms) Since this is our first Bayesian model, we should start slow. The primary model-fitting function in brms is brm(). The function is astonishingly general and includes numerous arguments, most of which have sensible defaults. The primary two arguments are data and formula. I’m guessing they’re self-explanatory. I’m not going to go into detail on the three arguments at the bottom of the code. We’ll go over them later. For simple models like these, I would have omitted them entirely, but given the sparsity of the data (i.e., 5 data points per model), I wanted to make sure we gave the algorithm a good chance to arrive at reasonable estimates. fit2.1 &lt;- brm(data = by_id$data[[1]], formula = tolerance ~ 1 + time, prior = prior(normal(0, 2), class = b), iter = 4000, chains = 4, cores = 4, seed = 2, file = &quot;fits/fit02.01&quot;) We just fit a single-level Bayesian regression model for our first participant. We saved the results as an object named fit2.1. We can return a useful summary of fit2.1 with either print() or summary(). Since it’s less typing, we’ll use print(). print(fit2.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ 1 + time ## Data: by_id$data[[1]] (Number of observations: 5) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.90 0.54 0.84 2.96 1.00 3925 2667 ## time 0.12 0.22 -0.31 0.57 1.00 3394 2367 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.61 0.50 0.21 1.92 1.00 1529 2288 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The ‘Intercept’ and ‘time’ coefficients are the primary regression parameters. Also notice ‘sigma’, which is our variant of the residual standard error you might get from an OLS output (e.g., from base R lm()). Since we’re Bayesians, the output summaries do not contain \\(p\\)-values. But we do get posterior standard deviations (i.e., the ‘Est.Error’ column) and the upper- and lower-levels of the percentile-based 95% intervals. You probably heard somewhere that Bayesian statistics require priors. We can see what those were by pulling them out of our fit2.1 object. fit2.1$prior ## prior class coef group resp dpar nlpar bound ## 1 normal(0, 2) b ## 2 b time ## 3 student_t(3, 2, 10) Intercept ## 4 student_t(3, 0, 10) sigma The prior in the top line, normal(0, 2), is for all parameters of class = b. We actually specified this in our brm() code, above, with the code snip: prior = prior(normal(0, 2), class = b). At this stage in the project, my initial impulse was to leave this line blank and save the discussion of how to set priors by hand for later. However, the difficulty is that the first several models we’re fitting are all of \\(n = 5\\). Bayesian statistics handle small-\\(n\\) models just fine. However, when your \\(n\\) gets small, the algorithms we use to implement our Bayesian models benefit from priors that are at least modestly informative. As it turns out, the brms default priors are flat for parameters of class = b. They offer no information beyond that contained in the likelihood. To stave off algorithm problems with our extremely-small-\\(n\\) data subsets, we used normal(0, 2) instead. In our model, the only parameter of class = b is the regression slope for time. On the scale of the data, normal(0, 2) is a vary-permissive prior for our time slope. In addition to our time slope parameter, our model contained an intercept and a residual variance. From the fit2.1$prior output, we can see those were student_t(3, 2, 10) and student_t(3, 0, 10), respectively. brms default priors are designed to be weakly informative. Given the data and the model, these priors have a minimal influence on the results. We’ll focus more on priors later in the project. For now just recognize that even if you don’t specify your priors, you can’t escape using some priors when using brm(). This is a good thing. Okay, so that was the model for just one participant. We want to do that for all 16. Instead of repeating that code 15 times, we can work in bulk. With brms, you can reuse a model with the update() function. Here’s how to do that with the data from our second participant. fit2.2 &lt;- update(fit2.1, newdata = by_id$data[[2]], file = &quot;fits/fit02.02&quot;) Peek at the results. print(fit2.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ 1 + time ## Data: by_id$data[[2]] (Number of observations: 5) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.14 0.49 0.32 1.92 1.00 3104 1618 ## time 0.18 0.20 -0.14 0.50 1.00 3070 1515 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.39 0.53 0.11 1.51 1.00 1001 1086 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Different participants yield different model results. Looking ahead a bit, we’ll need to know how to get the \\(R^2\\) for a single-level Gaussian model. With brms, you do that with the bayes_R2() function. bayes_R2(fit2.2) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.6235052 0.2489466 0.01109299 0.8148395 Though the default spits out summary statistics, you can get the full posterior distribution for the \\(R^2\\) by specifying summary = F. bayes_R2(fit2.2, summary = F) %&gt;% str() ## num [1:8000, 1] 0.639 0.67 0.809 0.812 0.78 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;R2&quot; Our code returned a numeric vector. If you’d like to plot the results with ggplot2, you’ll need to convert the vector to a data frame. bayes_R2(fit2.2, summary = F) %&gt;% data.frame() %&gt;% ggplot(aes(x = R2)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(expression(Bayesian~italic(R)^2), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) You’ll note how non-Gaussian the Bayesian \\(R^2\\) can be. Also, with the combination of default minimally-informative priors and only 5 data points, there’ massive uncertainty in the shape. As such, the value of central tendency will vary widely based on which statistic you use. bayes_R2(fit2.2, summary = F) %&gt;% data.frame() %&gt;% summarise(mean = mean(R2), median = median(R2), mode = tidybayes::Mode(R2)) ## mean median mode ## 1 0.6235052 0.7477635 0.796846 By default, bayes_R2() returns the mean. You can get the median with the robust = T argument. To pull the mode, you’ll need to use summary = F and feed the results into a mode function, like tidybayes::Mode(). I should also point out the brms package did not get these \\(R^2\\) values by traditional method used in, say, OLS estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out Gelman, Goodrich, Gabry, and Vehtari’s (2017) paper, R-squared for Bayesian regression models. With a little tricky programing, we can use the purrr::map() function to serially fit this model to each of our participant-level data sets. We’ll save the results as models. models &lt;- by_id %&gt;% mutate(model = map(data, ~update(fit2.1, newdata = ., seed = 2))) Let’s walk through what we did. The map() function takes two primary arguments, .x and .f, respectively. We set .x = data, which meant we wanted to iterate over the contents in our data vector. Recall that each row of data itself contained an entire data set–one for each of the 16 participants. It’s with the second argument .f that we indicated what we wanted to do with our rows of data. We set that to .f = ~update(fit2.1, newdata = ., seed = 2). With the ~ syntax, we entered in a formula, which was update(fit2.1, newdata = ., seed = 2). Just like we did with fit2.2, above, we reused the model formula and other technical specs from fit2.1. Now notice the middle part of the formula, newdata = .. That little . refers to the element we specified in the .x argument. What this combination means is that for each of the 16 rows of our nested by_id tibble, we plugged in the id-specific data set into update(fit, newdata[[i]]) where i is simply meant as a row index. The new column, model, contains the output of each of the 16 iterations. print(models) ## # A tibble: 16 x 3 ## # Groups: id [16] ## id data model ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 9 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 2 45 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 3 268 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 4 314 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 5 442 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 6 514 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 7 569 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 8 624 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 9 723 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 10 918 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 11 949 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 12 978 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 13 1105 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 14 1542 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 15 1552 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 16 1653 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; Next, we’ll want to extract the necessary summary information from our models to remake our version of Table 2.2. There’s a lot of info in that table, so let’s take it step by step. First, we’ll extract the posterior means (i.e., “Estimate”) and standard deviations (i.e., “se”) for the initial status and rate of change of each model. We’ll also do the same for sigma (i.e., the square of the “Residual variance”). mean_structure &lt;- models %&gt;% mutate(coefs = map(model, ~ posterior_summary(.)[1:2, 1:2] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;coefficients&quot;))) %&gt;% unnest(coefs) %&gt;% select(-data, -model) %&gt;% unite(temp, Estimate, Est.Error) %&gt;% pivot_wider(names_from = coefficients, values_from = temp) %&gt;% separate(b_Intercept, into = c(&quot;init_stat_est&quot;, &quot;init_stat_sd&quot;), sep = &quot;_&quot;) %&gt;% separate(b_time, into = c(&quot;rate_change_est&quot;, &quot;rate_change_sd&quot;), sep = &quot;_&quot;) %&gt;% mutate_if(is.character, ~ as.double(.) %&gt;% round(digits = 2)) %&gt;% ungroup() head(mean_structure) ## # A tibble: 6 x 5 ## id init_stat_est init_stat_sd rate_change_est rate_change_sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 1.9 0.54 0.12 0.22 ## 2 45 1.15 0.37 0.17 0.15 ## 3 268 1.55 0.69 0.02 0.28 ## 4 314 1.32 0.38 -0.03 0.16 ## 5 442 1.58 0.49 0.06 0.2 ## 6 514 1.43 0.34 0.27 0.14 It’s simpler to extract the residual variance. Recall that because brms gives that in the standard deviation metric (i.e., \\(\\sigma\\)), you need to square it to return it in a variance metric (i.e., \\(\\sigma^2\\)). residual_variance &lt;- models %&gt;% mutate(residual_variance = map_dbl(model, ~ posterior_summary(.)[3, 1])^2) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% select(id, residual_variance) head(residual_variance) ## # A tibble: 6 x 2 ## # Groups: id [6] ## id residual_variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0.37 ## 2 45 0.13 ## 3 268 0.45 ## 4 314 0.15 ## 5 442 0.290 ## 6 514 0.12 We’ll extract our Bayesian \\(R^2\\) summaries, next. Given how nonnormal these are, we’ll use the posterior median rather than the mean. We get that by using the robust = T argument within the bayes_R2() function. r2 &lt;- models %&gt;% mutate(r2 = map_dbl(model, ~ bayes_R2(., robust = T)[1])) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% select(id, r2) head(r2) ## # A tibble: 6 x 2 ## # Groups: id [6] ## id r2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0.33 ## 2 45 0.75 ## 3 268 0.2 ## 4 314 0.23 ## 5 442 0.25 ## 6 514 0.86 Here we combine all the components with a series of left_join() statements and present it in a knitr-type table. table &lt;- models %&gt;% unnest(data) %&gt;% group_by(id) %&gt;% slice(1) %&gt;% select(id, male, exposure) %&gt;% left_join(mean_structure, by = &quot;id&quot;) %&gt;% left_join(residual_variance, by = &quot;id&quot;) %&gt;% left_join(r2, by = &quot;id&quot;) %&gt;% rename(residual_var = residual_variance) %&gt;% select(id, init_stat_est:r2, everything()) %&gt;% ungroup() table %&gt;% knitr::kable() id init_stat_est init_stat_sd rate_change_est rate_change_sd residual_var r2 male exposure 9 1.90 0.54 0.12 0.22 0.37 0.33 0 1.54 45 1.15 0.37 0.17 0.15 0.13 0.75 1 1.16 268 1.55 0.69 0.02 0.28 0.45 0.20 1 0.90 314 1.32 0.38 -0.03 0.16 0.15 0.23 0 0.81 442 1.58 0.49 0.06 0.20 0.29 0.25 0 1.13 514 1.43 0.34 0.27 0.14 0.12 0.86 1 0.90 569 1.82 0.08 0.05 0.03 0.00 0.86 0 1.99 624 1.12 0.12 0.02 0.05 0.01 0.36 1 0.98 723 1.27 0.28 -0.06 0.11 0.06 0.47 0 0.81 918 1.01 0.66 0.13 0.26 0.54 0.32 0 1.21 949 1.74 0.70 -0.10 0.29 0.43 0.32 1 0.93 978 1.07 0.77 0.61 0.30 0.67 0.86 1 1.59 1105 1.54 0.36 0.15 0.15 0.15 0.66 1 1.38 1542 1.20 0.53 0.23 0.19 0.23 0.76 0 1.44 1552 1.22 0.90 0.14 0.38 0.89 0.30 0 1.04 1653 0.96 0.38 0.24 0.15 0.13 0.84 0 1.25 We can make the four stem-and-leaf plots of Figure 2.4 with serial combinations of pull() and stem(). # fitted initial status table %&gt;% pull(init_stat_est) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 9 | 6 ## 10 | 17 ## 11 | 25 ## 12 | 027 ## 13 | 2 ## 14 | 3 ## 15 | 458 ## 16 | ## 17 | 4 ## 18 | 2 ## 19 | 0 # fitted rate of change table %&gt;% pull(rate_change_est) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## -1 | 0 ## -0 | 63 ## 0 | 2256 ## 1 | 23457 ## 2 | 347 ## 3 | ## 4 | ## 5 | ## 6 | 1 # residual variance table %&gt;% pull(residual_var) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 0 | 016 ## 1 | 23355 ## 2 | 39 ## 3 | 7 ## 4 | 35 ## 5 | 4 ## 6 | 7 ## 7 | ## 8 | 9 # r2 statistic table %&gt;% pull(r2) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 2 | 035 ## 3 | 02236 ## 4 | 7 ## 5 | ## 6 | 6 ## 7 | 56 ## 8 | 4666 To make Figure 2.5, we’ll combine information from the original data and the ‘Estimates’ (i.e., posterior means) from our Bayesian models we’ve encoded in mean_structure. by_id %&gt;% unnest(data) %&gt;% ggplot(aes(x = time, y = tolerance, group = id)) + geom_point() + geom_abline(data = mean_structure, aes(intercept = init_stat_est, slope = rate_change_est, group = id), color = &quot;blue&quot;) + scale_x_continuous(breaks = 0:4, labels = 0:4 + 11) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~id) 2.3 Exploring differences in change across people “Having summarized how each individual changes over time, we now examine similarities and differences in these changes across people” (p. 33). 2.3.1 Examining the entire set of smooth trajectories. The key to making our version of the left-hand side of Figure 2.6 is two stat_smooth() lines. The first one will produce the overall smooth. The second one, the one including the aes(group = id) argument, will give the id-specific smooths. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + stat_smooth(method = &quot;loess&quot;, se = F, span = .9, size = 2) + stat_smooth(aes(group = id), method = &quot;loess&quot;, se = F, span = .9, size = 1/4) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) To get the linear OLS trajectories, just switch method = &quot;loess&quot; to method = &quot;lm&quot;. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + stat_smooth(method = &quot;lm&quot;, se = F, span = .9, size = 2) + stat_smooth(aes(group = id), method = &quot;lm&quot;, se = F, span = .9, size = 1/4) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) But we wanted to be Bayesians. We already have the id-specific trajectories. All we need now is one based on all the data. fit2.3 &lt;- update(fit2.1, newdata = tolerance_pp, file = &quot;fits/fit02.03&quot;) Here’s the model summary. summary(fit2.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ 1 + time ## Data: tolerance_pp (Number of observations: 80) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.36 0.09 1.18 1.53 1.00 6951 5710 ## time 0.13 0.04 0.06 0.21 1.00 7128 5355 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.47 0.04 0.40 0.55 1.00 6938 5381 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before, we used posterior_summary() to isolate the posterior means and \\(SD\\)s. We can also use the fixef() function for that. fixef(fit2.3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 1.355090 0.09115680 1.17625722 1.5308531 ## time 0.131688 0.03707848 0.06014904 0.2053063 With a little subsetting, we can extract just the means from each. fixef(fit2.3)[1, 1] ## [1] 1.35509 fixef(fit2.3)[2, 1] ## [1] 0.131688 For this plot, we’ll work more directly with the model formulas to plot the trajectories. We can use init_stat_est and rate_change_est from the mean_structure object as stand-ins for \\(\\beta_{0i}\\) and \\(\\beta_{1i}\\) from our model equation, \\[\\text{tolerance}_{ij} = \\beta_{0i} + \\beta_{1i} \\cdot \\text{time}_{ij} + \\epsilon_{ij},\\] where \\(i\\) indexes children and \\(j\\) indexes time points. All we need to do is plug in the appropriate values for time and we’ll have the fitted tolerance values for each level of id. After a little wrangling, the data will be in good shape for plotting. tol_fitted &lt;- mean_structure %&gt;% mutate(`11` = init_stat_est + rate_change_est * 0, `15` = init_stat_est + rate_change_est * 4) %&gt;% select(id, `11`, `15`) %&gt;% pivot_longer(-id, names_to = &quot;age&quot;, values_to = &quot;tolerance&quot;) %&gt;% mutate(age = as.integer(age)) head(tol_fitted) ## # A tibble: 6 x 3 ## id age tolerance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 11 1.9 ## 2 9 15 2.38 ## 3 45 11 1.15 ## 4 45 15 1.83 ## 5 268 11 1.55 ## 6 268 15 1.63 We’ll plot the id-level trajectories with those values and geom_line(). To get the overall trajectory, we’ll get tricky with fixef(fit2.3) and geom_abline(). tol_fitted %&gt;% ggplot(aes(x = age, y = tolerance, group = id)) + geom_line(color = &quot;blue&quot;, size = 1/4) + geom_abline(intercept = fixef(fit2.3)[1, 1] + fixef(fit2.3)[2, 1] * -11, slope = fixef(fit2.3)[2, 1], color = &quot;blue&quot;, size = 2) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) 2.3.2 Using the results of model fitting to frame questions about change. If you’re new to the multilevel model, the ideas in this section are foundational. To learn about the observed average pattern of change, we examine the sample averages of the fitted intercepts and slopes; these tell us about the average initial status and the average annual rate of change in the sample as a whole. To learn about the observed individual differences in change, we examine the sample variances and standard deviations of the intercepts and slopes; these tell us about the observed variability in initial status. And to learn about the observed relationship between initial status and the rate of change, we can examine the sample covariance or correlation between intercepts and slopes. Formal answers to these questions require the multilevel model for change of chapter 3. But we can presage this work by conducting simple descriptive analyses of the estimated intercepts and slopes. (p. 36, emphasis in the original) Here are the means and standard deviations presented in Table 2.3. mean_structure %&gt;% pivot_longer(ends_with(&quot;est&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 3 ## name mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 init_stat_est 1.37 0.290 ## 2 rate_change_est 0.13 0.17 Here’s how to get the Pearson’s correlation coefficient. mean_structure %&gt;% select(init_stat_est, rate_change_est) %&gt;% cor() %&gt;% round(digits = 2) ## init_stat_est rate_change_est ## init_stat_est 1.00 -0.42 ## rate_change_est -0.42 1.00 2.3.3 Exploring the relationship between change and time-invariant predictors. “Evaluating the impact of predictors helps you uncover systematic patterns in the individual change trajectories corresponding to interindividual variation in personal characteristics” (p. 37). 2.3.3.1 Graphically examining groups of smoothed individual growth trajectories. If we’d like Bayesian estimates differing by male, we’ll need to fit an interaction model. fit2.4 &lt;- update(fit2.1, newdata = tolerance_pp, tolerance ~ 1 + time + male + time:male, file = &quot;fits/fit02.04&quot;) Check the model summary. print(fit2.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ time + male + time:male ## Data: tolerance_pp (Number of observations: 80) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.35 0.12 1.12 1.59 1.00 4411 5364 ## time 0.10 0.05 0.01 0.20 1.00 4277 4949 ## male 0.01 0.19 -0.36 0.38 1.00 3714 4226 ## time:male 0.07 0.08 -0.09 0.21 1.00 3468 3867 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.47 0.04 0.40 0.55 1.00 5993 5064 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s how to use fixef() and the model equation to get fitted values for tolerance based on specific values for time and male. tol_fitted_male &lt;- tibble(male = rep(0:1, each = 2), age = rep(c(11, 15), times = 2)) %&gt;% mutate(time = age - 11) %&gt;% mutate(tolerance = fixef(fit2.4)[1, 1] + fixef(fit2.4)[2, 1] * time + fixef(fit2.4)[3, 1] * male + fixef(fit2.4)[4, 1] * time * male) tol_fitted_male ## # A tibble: 4 x 4 ## male age time tolerance ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 11 0 1.35 ## 2 0 15 4 1.76 ## 3 1 11 0 1.36 ## 4 1 15 4 2.03 Now we’re ready to make our Bayesian version of the top panels of Figure 2.7. tol_fitted %&gt;% # we need to add `male` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, male), by = &quot;id&quot;) %&gt;% ggplot(aes(x = age, y = tolerance, color = factor(male))) + geom_line(aes(group = id), size = 1/4) + geom_line(data = tol_fitted_male, size = 2) + scale_color_viridis_d(end = .75) + coord_cartesian(ylim = c(0, 4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~male) Before we can do the same thing with exposure, we’ll need to dichotomize it by its median. A simple way is with a conditional statement within the if_else() function. tolerance_pp &lt;- tolerance_pp %&gt;% mutate(exposure_01 = if_else(exposure &gt; median(exposure), 1, 0)) Now fit the second interaction model. fit2.5 &lt;- update(fit2.4, newdata = tolerance_pp, tolerance ~ 1 + time + exposure_01 + time:exposure_01, file = &quot;fits/fit02.05&quot;) Here’s the summary. print(fit2.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ time + exposure_01 + time:exposure_01 ## Data: tolerance_pp (Number of observations: 80) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.39 0.12 1.16 1.63 1.00 4132 5646 ## time 0.04 0.05 -0.05 0.14 1.00 3879 4757 ## exposure_01 -0.07 0.17 -0.40 0.25 1.00 3594 4929 ## time:exposure_01 0.18 0.07 0.04 0.31 1.00 3283 4207 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.43 0.04 0.37 0.51 1.00 5907 4869 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now use fixef() and the model equation to get fitted values for tolerance based on specific values for time and exposure_01. tol_fitted_exposure &lt;- crossing(exposure_01 = 0:1, age = c(11, 15)) %&gt;% mutate(time = age - 11) %&gt;% mutate(tolerance = fixef(fit2.5)[1, 1] + fixef(fit2.5)[2, 1] * time + fixef(fit2.5)[3, 1] * exposure_01 + fixef(fit2.5)[4, 1] * time * exposure_01, exposure = if_else(exposure_01 == 1, &quot;high exposure&quot;, &quot;low exposure&quot;) %&gt;% factor(., levels = c(&quot;low exposure&quot;, &quot;high exposure&quot;))) tol_fitted_exposure ## # A tibble: 4 x 5 ## exposure_01 age time tolerance exposure ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 11 0 1.39 low exposure ## 2 0 15 4 1.56 low exposure ## 3 1 11 0 1.32 high exposure ## 4 1 15 4 2.20 high exposure Did you notice in the last lines in the second mutate() how we made a version of exposure that is a factor? That will come in handy for labeling and ordering the subplots. Now make our Bayesian version of the bottom panels of Figure 2.7. tol_fitted %&gt;% # we need to add `exposure_01` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, exposure_01), by = &quot;id&quot;) %&gt;% mutate(exposure = if_else(exposure_01 == 1, &quot;high exposure&quot;, &quot;low exposure&quot;) %&gt;% factor(., levels = c(&quot;low exposure&quot;, &quot;high exposure&quot;))) %&gt;% ggplot(aes(x = age, y = tolerance, color = exposure)) + geom_line(aes(group = id), size = 1/4) + geom_line(data = tol_fitted_exposure, size = 2) + scale_color_viridis_d(option = &quot;A&quot;, end = .75) + coord_cartesian(ylim = c(0, 4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~exposure) 2.3.3.2 The relationship between OLS-Estimated single-level Bayesian trajectories and substantive predictors “To investigate whether fitted trajectories vary systematically with predictors, we can treat the estimated intercepts and slopes as outcomes and explore the relationship between them and predictors” (p. 39). Here are the left panels of Figure 2.8. p1 &lt;- mean_structure %&gt;% pivot_longer(ends_with(&quot;est&quot;)) %&gt;% mutate(name = factor(name, labels = c(&quot;Fitted inital status&quot;, &quot;Fitted rate of change&quot;))) %&gt;% # we need to add `male` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, male), by = &quot;id&quot;) %&gt;% ggplot(aes(x = factor(male), y = value, color = name)) + geom_point(alpha = 1/2) + scale_color_viridis_d(option = &quot;B&quot;, begin = .2, end = .7) + labs(x = &quot;male&quot;, y = NULL) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~name, scale = &quot;free_y&quot;, ncol = 1) p1 Here are the right panels. p2 &lt;- mean_structure %&gt;% pivot_longer(ends_with(&quot;est&quot;)) %&gt;% mutate(name = factor(name, labels = c(&quot;Fitted inital status&quot;, &quot;Fitted rate of change&quot;))) %&gt;% # we need to add `male` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, exposure), by = &quot;id&quot;) %&gt;% ggplot(aes(x = exposure, y = value, color = name)) + geom_point(alpha = 1/2) + scale_color_viridis_d(option = &quot;B&quot;, begin = .2, end = .7) + scale_x_continuous(breaks = 0:2, limits = c(0, 2.4)) + labs(y = NULL) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~name, scale = &quot;free_y&quot;, ncol = 1) p2 Did you notice how we saved those last two plots as p1 and p2? We can use syntax from the patchwork package to combine them into one compound plot. library(patchwork) p1 + p2 + scale_y_continuous(breaks = NULL) As interesting as these plots are, do remember that “the need for ad hoc correlations has been effectively replaced by the widespread availability of computer software for fitting the multilevel model for change directly” (pp. 41–42). As you’ll see, Bürkner’s brms package is one of the foremost in that regard. 2.4 Improving the precision and reliability of OLS single-level-Bayesian-estimated rates of change: Lessons for research design Statisticians assess the precision of a parameter estimate in terms of its sampling variation, a measure of the variability that would be found across infinite resamplings from the same population. The most common measure of sampling variability is an estimate’s standard error, the square root of its estimated sampling variance. Precision and standard error have an inverse relationship; the smaller the standard error, the more precise the estimate. (p. 41, emphasis in the original) So here’s the deal: When Singer and Willett wrote “Statisticians assess…” a more complete expression would have been ‘Frequentist statisticians assess…’ Bayesian statistics are not based on asymptotic theory. They do not presume an idealized infinite distribution of replications. Rather, Bayesian statistics use Bayes theorem to estimate the probability of the parameters given the data. That probability has a distribution. Analogous to frequentist statistics, we often summarize that distribution (i.e., the posterior distribution) in terms of central tendency (e.g., posterior mean, posterior median, posterior mode) and spread. Spread? you say. We typically express spread in one or both of two ways. One typical expression of spread is the 95% intervals. In the Bayesian world, these are often called credible or probability intervals. The other typical expression of spread is the posterior standard deviation. In brms, this of typically summarized in the ‘Est.error’ column of the output of functions like print() and posterior_summary() and so on. The posterior standard deviation is analogous to the frequentist standard error. Philosophically and mechanically, they are not the same. But in practice, they are often quite similar. Later we read: Unlike precision which describes how well an individual slope estimate measures that person’s true rate of change, reliability describes how much the rate of change varies across people. Precision has meaning for the individual; reliability has meaning for the group. (p. 42) I have to protest. True, if we were working within a Classical Test Theory paradigm, this would be correct. But this places reliability with the context of group-based cross-sectional design. Though this is a popular design, it is not the whole story (i.e., see this book!). For introductions to more expansive and person-specific notions of reliability, check out Cronbach’s Generalizability Theory (Cronbach, Gleser, Nanda, &amp; Rajaratnam, 1972; Brennan, 2001; also Cranford, Shrout, Iida, Rafaeli, Yip, &amp; Bolger, 2006; LoPilato, Carter, &amp; Wang , 2015; Shrout &amp; Lane, 2012). Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 brms_2.12.0 Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.3 ## [8] readr_1.3.1 tidyr_1.0.2 tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [6] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [11] svUnit_0.7-12 DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 splines_3.6.3 mnormt_1.5-6 knitr_1.28 ## [21] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 ## [26] shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [31] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 later_1.0.0 prettyunits_1.1.1 ## [36] htmltools_0.4.0 tools_3.6.3 igraph_1.2.5 coda_0.19-3 gtable_0.3.0 ## [41] glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 vctrs_0.2.4 nlme_3.1-144 ## [46] crosstalk_1.1.0.1 psych_1.9.12.31 xfun_0.13 ps_1.3.2 rvest_0.3.5 ## [51] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 tidybayes_2.0.3 gtools_3.8.2 ## [56] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [61] Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [66] curl_4.3 gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [71] highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 pkgconfig_2.0.3 ## [76] matrixStats_0.56.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [81] labeling_0.3 processx_3.4.2 tidyselect_1.0.0 plyr_1.8.6 magrittr_1.5 ## [86] bookdown_0.18 R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.3 ## [91] haven_2.2.0 withr_2.1.2 mgcv_1.8-31 xts_0.12-0 abind_1.4-5 ## [96] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [101] grid_3.6.3 readxl_1.3.1 callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [106] digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [111] viridisLite_0.3.0 shinyjs_1.1 "],
["introducing-the-multilevel-model-for-change.html", "3 Introducing the Multilevel Model for Change 3.1 What is the purpose of the multilevel model for change? 3.2 The level-1 submodel for individual change 3.3 The level-2 submodel for systematic interindividual differences in change 3.4 Fitting the multilevel model for change to data 3.5 Examining estimated fixed effects 3.6 Examining estimated variance components 3.7 Bonus: How did you simulate that data? Reference Session info", " 3 Introducing the Multilevel Model for Change In this chapter [Singer and Willett introduced] the multilevel model for change, demonstrating how it allows us to address within-person and between-person questions about change simultaneously. Although there are several ways of writing the statistical model, here we adopt a simple and common approach that has much substantive appeal. We specify the multilevel model for change by simultaneously postulating a pair of subsidiary models—a level-1 submodel that describes how each person changes over time, and a level-2 model that describes how these changes differ across people (Bryk &amp; Raudenbush, 1987; Rogosa &amp; Willett, 1985). (p. 3) 3.1 What is the purpose of the multilevel model for change? Unfortunately, we do not have access to the full data set Singer and Willett used in this chapter. For details, go here. However, I was able to use the data provided in Table 3.1 and the model results in Table 3.3 to simulate data with similar characteristics as the original. To see how I did it, look at the section at the end of the chapter. Anyway, here are the data in Table 3.1. library(tidyverse) early_int &lt;- tibble(id = rep(c(68, 70:72, 902, 904, 906, 908), each = 3), age = rep(c(1, 1.5, 2), times = 8), cog = c(103, 119, 96, 106, 107, 96, 112, 86, 73, 100, 93, 87, 119, 93, 99, 112, 98, 79, 89, 66, 81, 117, 90, 76), program = rep(1:0, each = 12)) print(early_int) ## # A tibble: 24 x 4 ## id age cog program ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 68 1 103 1 ## 2 68 1.5 119 1 ## 3 68 2 96 1 ## 4 70 1 106 1 ## 5 70 1.5 107 1 ## 6 70 2 96 1 ## 7 71 1 112 1 ## 8 71 1.5 86 1 ## 9 71 2 73 1 ## 10 72 1 100 1 ## # … with 14 more rows Later on, we also fit models using \\(age - 1\\). Here we’ll compute that and save it as age_c early_int &lt;- early_int %&gt;% mutate(age_c = age - 1) head(early_int) ## # A tibble: 6 x 5 ## id age cog program age_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 68 1 103 1 0 ## 2 68 1.5 119 1 0.5 ## 3 68 2 96 1 1 ## 4 70 1 106 1 0 ## 5 70 1.5 107 1 0.5 ## 6 70 2 96 1 1 Here we’ll load our simulation of the full \\(n = 103\\) data set. load(&quot;data/early_int_sim.rda&quot;) 3.2 The level-1 submodel for individual change This part of the model is also called the individual growth model. Remember how in last chapter we fit a series of participant-specific models? That’s the essence of this part of the model. Here’s our version of Figure 3.1. Note that here we’re being lazy and just using OLS estimates. early_int %&gt;% ggplot(aes(x = age, y = cog)) + stat_smooth(method = &quot;lm&quot;, se = F) + geom_point() + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(panel.grid = element_blank()) + facet_wrap(~id, ncol = 4) Based on these data, we postulate our level-1 submodel to be \\[ \\text{cog}_{ij} = [ \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1) ] + [\\epsilon_{ij}]. \\] 3.2.1 The structural part of the level-1 submodel. As far as I can tell, the data for Figure 3.2 are something like this. d &lt;- tibble(id = &quot;i&quot;, age = c(1, 1.5, 2), cog = c(95, 100, 135)) d ## # A tibble: 3 x 3 ## id age cog ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 i 1 95 ## 2 i 1.5 100 ## 3 i 2 135 To add in the horizontal dashed lines in Figure 3.2, we’ll need to fit a model. Let’s be lazy and use OLS. Don’t worry, we’ll use Bayes in a bit. fit3.1 &lt;- lm(data = d, cog ~ age) summary(fit3.1) ## ## Call: ## lm(formula = cog ~ age, data = d) ## ## Residuals: ## 1 2 3 ## 5 -10 5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.00 26.93 1.857 0.314 ## age 40.00 17.32 2.309 0.260 ## ## Residual standard error: 12.25 on 1 degrees of freedom ## Multiple R-squared: 0.8421, Adjusted R-squared: 0.6842 ## F-statistic: 5.333 on 1 and 1 DF, p-value: 0.2601 We can use the fitted() function to compute the model-implied fitted values for cog based on the age values in the data. We’ll then save those in a sensibly-named vector which we’ll attach to the rest of the data. f &lt;- fitted(fit3.1) %&gt;% data.frame() %&gt;% set_names(&quot;fitted&quot;) %&gt;% bind_cols(d) print(f) ## fitted id age cog ## 1 90 i 1.0 95 ## 2 110 i 1.5 100 ## 3 130 i 2.0 135 To make all the dashed lines and arrows in the figure, we’ll want a few specialty tibbles. path &lt;- tibble(age = c(1, 2, 2), cog = c(90, 90, 130)) text &lt;- tibble(age = c(1.2, 1.65, 2.15, 1.125, 2.075), cog = c(105, 101, 137, 75, 110), label = c(&quot;epsilon[italic(i)][1]&quot;, &quot;epsilon[italic(i)][2]&quot;, &quot;epsilon[italic(i)][3]&quot;, &quot;pi[0][italic(i)]&quot;, &quot;pi[1][italic(i)]&quot;)) arrow &lt;- tibble(age = c(1.15, 1.6, 2.1, 1.1), xend = c(1.01, 1.51, 2.01, 1.01), cog = c(103, 101, 137, 78), yend = c(92.5, 105, 132.5, 89)) # we&#39;re finally ready to plot! f %&gt;% ggplot(aes(x = age, y = cog)) + geom_point() + # the main fitted trajectory geom_line(aes(y = fitted)) + # the thick dashed line bending upward at age == 2 geom_path(data = path, linetype = 2, size = 1/2) + # the thin dashed vertical lines extending from the data dots to the fitted line geom_segment(aes(xend = age, y = cog, yend = fitted), linetype = 3, size = 1/4) + # the arrows geom_segment(data = arrow, aes(xend = xend, yend = yend), arrow = arrow(length = unit(0.1, &quot;cm&quot;)), size = 1/4) + # the statistical notation geom_text(data = text, aes(label = label), size = c(4, 4, 4, 5, 5), parse = T) + # &quot;1 year&quot; annotate(geom = &quot;text&quot;, x = 1.5, y = 86, label = &quot;1 year&quot;) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(panel.grid = element_blank()) In-specifying a level-1 submodel that attempts to describe everyone (all the \\(i\\)’s) in the population, we implicitly assume that all the true individual change trajectories have a common algebraic form. But we do not assume that everyone has the same exact trajectory. Because each person has his or her own individual growth parameters (intercepts and slopes), different people can have their own distinct change trajectories. (pp. 53–54) In this way, the multilevel model’s level-1 submodel is much like an interaction/moderation model with interaction terms for each level of \\(i\\). 3.2.2 The stochastic part of the level-1 submodel. The last term in our level-1 equation from above was \\([\\epsilon_{ij}]\\). This is the residual variance left in the criterion after accounting for the predictor(s) in the model. It is a mixture of systemic variation that could be accounted for by adding covariates to the model as well as measurement error. The typical assumption is \\[ \\epsilon_{ij} \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2). \\] We should point out that another way to express this is \\[\\begin{align*} \\text{cog} &amp; \\sim \\operatorname{Normal} (\\mu_{ij}, \\sigma_\\epsilon^2) \\\\ \\mu_{ij} &amp; = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1). \\end{align*}\\] This won’t be a huge deal in the context of the models Singer and Willett presented in the initial chapters of the text but expressing the models this way can help one think in terms of likelihood functions. That’s a major advantage when you start working with data which are natural to model using other distributions (e.g., count data and the Poisson, binary data and the binomial). For more on this approach, check out McElreath’s Statistical Rethinking and my companion project translating his work into brms and tidyverse code. Also, thinking in terms of likelihoods will pay off starting around Chapter 10 when we start fitting discrete-time survival models. 3.2.3 Relating the level-1 submodel to the OLS exploratory methods of chapter 2. To get the top panel in Figure 3.3, we’ll use stat_smooth() to get the OLS trajectories. early_int_sim %&gt;% ggplot(aes(x = age, y = cog)) + stat_smooth(aes(group = id), method = &quot;lm&quot;, se = F, size = 1/6) + stat_smooth(method = &quot;lm&quot;, se = F, size = 2) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(panel.grid = element_blank()) Note that now we’re working with our early_int_sim data, the one where we added the data of 95 simulated individuals to the real data of 8 id levels from Table 3.1. As such, our results will deviate a bit from those in the text. But anyways, here we go on to fit 103 individual OLS models, one for each of the id levels. Don’t worry; we’ll depart from this madness shortly. by_id &lt;- early_int_sim %&gt;% mutate(age_c = age - 1) %&gt;% group_by(id) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(data = ., cog ~ age_c))) head(by_id) ## # A tibble: 6 x 3 ## # Groups: id [6] ## id data model ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 2 2 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 3 3 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 4 4 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 5 5 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 6 6 &lt;tibble [3 × 4]&gt; &lt;lm&gt; Now we’ll use the great helper functions from the broom package, tidy() and glance(), to store the coefficient information and model fit information, respectively, in a tidy data format (see here and also here). # install.packages(&quot;broom&quot;) library(broom) by_id &lt;- by_id %&gt;% mutate(tidy = map(model, tidy), glance = map(model, glance)) Here’s what our by_id object now looks like: by_id %&gt;% head() ## # A tibble: 6 x 5 ## # Groups: id [6] ## id data model tidy glance ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 11]&gt; ## 2 2 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 11]&gt; ## 3 3 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 11]&gt; ## 4 4 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 11]&gt; ## 5 5 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 11]&gt; ## 6 6 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 11]&gt; If you want to extract the intercepts from the tidy column, you might execute code like this. unnest(by_id, tidy) %&gt;% filter(term == &quot;(Intercept)&quot;) ## # A tibble: 103 x 9 ## # Groups: id [103] ## id data model term estimate std.error statistic p.value glance ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 1 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 117. 2.43e-14 4.81e15 1.32e-16 &lt;tibble [1 × 11]&gt; ## 2 2 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 110. 5.22e+ 0 2.11e 1 3.01e- 2 &lt;tibble [1 × 11]&gt; ## 3 3 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 117. 1.08e+ 1 1.08e 1 5.87e- 2 &lt;tibble [1 × 11]&gt; ## 4 4 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 136. 5.59e+ 0 2.42e 1 2.62e- 2 &lt;tibble [1 × 11]&gt; ## 5 5 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 108. 3.35e+ 0 3.21e 1 1.99e- 2 &lt;tibble [1 × 11]&gt; ## 6 6 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 110. 2.61e+ 0 4.21e 1 1.51e- 2 &lt;tibble [1 × 11]&gt; ## 7 7 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 124. 7.45e- 1 1.66e 2 3.84e- 3 &lt;tibble [1 × 11]&gt; ## 8 8 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 97 6.71e+ 0 1.45e 1 4.40e- 2 &lt;tibble [1 × 11]&gt; ## 9 9 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 121. 7.08e+ 0 1.71e 1 3.73e- 2 &lt;tibble [1 × 11]&gt; ## 10 10 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 109. 3.73e- 1 2.92e 2 2.18e- 3 &lt;tibble [1 × 11]&gt; ## # … with 93 more rows This first line took the model coefficients and their respective statistics (e.g., standard errors) and unnested them (i.e., took them out of the list of data frames and converted the data to a longer structure). The second line filtered out any coefficients that were not the intercept. In this case, there are just two coefficients, the intercept and the slope for age_c. With that, we can make the leftmost stem and leaf plot from Figure 3.3. unnest(by_id, tidy) %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) %&gt;% stem() ## ## The decimal point is 1 digit(s) to the right of the | ## ## 8 | 3 ## 8 | 5 ## 9 | 000124 ## 9 | 5666677779999 ## 10 | 000111223334444 ## 10 | 555678888889 ## 11 | 00000223444 ## 11 | 55667777788 ## 12 | 001112222344 ## 12 | 556666889 ## 13 | 011223444 ## 13 | 56 ## 14 | ## 14 | 8 Here’s the stem and leaf plot in the middle. unnest(by_id, tidy) %&gt;% filter(term == &quot;age_c&quot;) %&gt;% pull(estimate) %&gt;% stem() ## ## The decimal point is 1 digit(s) to the right of the | ## ## -4 | 7 ## -4 | 111 ## -3 | 9886666655 ## -3 | 322111000 ## -2 | 9877776666555 ## -2 | 44443321111110000 ## -1 | 999999888665 ## -1 | 433322211100000 ## -0 | 988877776 ## -0 | 33211 ## 0 | 124 ## 0 | 57 ## 1 | 1113 If you want the residual variances (i.e., \\(\\sigma_\\epsilon^2\\)), you’d unnest() the glance column. They’ll be listed in the sigma column. unnest(by_id, glance) %&gt;% pull(sigma) %&gt;% stem(scale = 1) ## ## The decimal point is at the | ## ## 0 | 0044444488888 ## 1 | 22 ## 2 | 000000444499999 ## 3 | 3377777 ## 4 | 11111599999 ## 5 | 33377777 ## 6 | 111111559 ## 7 | 3333888 ## 8 | 26 ## 9 | 0004488 ## 10 | 22 ## 11 | 0088 ## 12 | 777 ## 13 | 115 ## 14 | 3 ## 15 | 59 ## 16 | 3 ## 17 | 16 ## 18 | 0 ## 19 | ## 20 | ## 21 | 266 3.3 The level-2 submodel for systematic interindividual differences in change Here are the top panels of Figure 3.4. early_int_sim &lt;- early_int_sim %&gt;% mutate(label = str_c(&quot;program = &quot;, program)) early_int_sim %&gt;% ggplot(aes(x = age, y = cog, color = label)) + stat_smooth(aes(group = id), method = &quot;lm&quot;, se = F, size = 1/6) + stat_smooth(method = &quot;lm&quot;, se = F, size = 2) + scale_color_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~label) Given the simplicity of the shapes, the bottom panels of Figure 3.4 will take a bit of preparatory work. First, we’ll need to wrangle the data a bit to get the necessary points. If we were working with a Bayesian model fit with brms, we’d use the fitted() function. But since we’re working with models fit with base R’s OLS estimator, lm(), we’ll use predict(), which accommodates a newdata argument. That’ll be crucial because in order to get the shapes correct, we’ll need to evaluate the minimum and maximum values across the fitted lines across a densely-packed sequence of age_c values. # how may `age_c` values do we need? n &lt;- 30 # define the specific `age_c` values nd &lt;- tibble(age_c = seq(from = 0, to = 1, length.out = n)) # wrangle p &lt;- by_id %&gt;% mutate(fitted = map(model, ~predict(., newdata = nd))) %&gt;% unnest(fitted) %&gt;% mutate(age = seq(from = 1, to = 2, length.out = n), program = ifelse(id &lt; 900, 1, 0)) %&gt;% group_by(program, age) %&gt;% summarise(min = min(fitted), max = max(fitted)) %&gt;% mutate(label = str_c(&quot;program = &quot;, program)) # what did we do? head(p) ## # A tibble: 6 x 5 ## # Groups: program [1] ## program age min max label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0 1 82.7 133. program = 0 ## 2 0 1.03 82.4 132. program = 0 ## 3 0 1.07 82.1 131. program = 0 ## 4 0 1.10 81.8 131. program = 0 ## 5 0 1.14 81.6 130. program = 0 ## 6 0 1.17 81.3 129. program = 0 Before we plot, we’ll need a couple tibbles for the annotation. text &lt;- tibble(age = 1.01, cog = c(101.5, 110), label = c(&quot;program = 0&quot;, &quot;program = 1&quot;), text = c(&quot;Average population trajectory,&quot;, &quot;Average population trajectory,&quot;), angle = c(345.7, 349)) math &lt;- tibble(age = 1.01, cog = c(94.5, 103), label = c(&quot;program = 0&quot;, &quot;program = 1&quot;), text = c(&quot;gamma[0][0] + gamma[10](italic(age) - 1)&quot;, &quot;(gamma[0][0] + gamma[10]) + (gamma[10] + gamma[11]) (italic(age) - 1)&quot;), angle = c(345.7, 349)) Finally, we’re ready for the bottom panels of Figure 3.4. p %&gt;% ggplot(aes(x = age)) + geom_ribbon(aes(ymin = min, ymax = max, fill = label), alpha = 1/3) + stat_smooth(data = early_int_sim, aes(y = cog, color = label), method = &quot;lm&quot;, se = F, size = 1) + geom_text(data = text, aes(y = cog, label = text, angle = angle), hjust = 0) + geom_text(data = math, aes(y = cog, label = text, angle = angle), hjust = 0, parse = T) + scale_fill_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) + scale_color_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) + scale_x_continuous(breaks = c(1, 1.5, 2)) + scale_y_continuous(&quot;cog&quot;, limits = c(50, 150)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~label) Returning to the text (p. 58), Singer and Willett asked: “What kind [of] population model might have given rise to these patterns?” Their answer is the level-2 model should have 4 specific features: “Its outcomes must be the individual growth parameters.” “The level-2 submodel must be written in separate parts, one for each level-1 growth parameter.” “Each part must specify a relationship between an individual growth parameter and the predictor.” “Each model must allow individuals who share common predictor values to vary in their individual change trajectories.” Given the current model, the level-2 submodel of change may be expressed as \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{program} + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{program} + \\zeta_{1i}. \\end{align*}\\] We’ll discuss the details about the \\(\\zeta\\) terms in a bit. 3.3.1 Structural components of the level-2 submodel. The structural parts of the level-2 submodel contain four level-2 parameters–\\(\\gamma_{00}\\), \\(\\gamma_{01}\\), \\(\\gamma_{10}\\), and \\(\\gamma_{11}\\)–known collectively as the fixed effects. The fixed effects capture systematic interindividual differences in change trajectory according to values of the level-2 predictors. (p. 60, emphasis in the original) 3.3.2 Stochastic components of the level-2 submodel. Each part of the level-2 submodel contains a residual that allows the value of each person’s growth parameters to be scattered around the relevant population averages. These residuals, \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) in [the] equation [above], represent those portions of the level-2 outcomes–the individual growth parameters–that remain unexplained by the level-2 predictor(s). (p. 61) We often summarize the \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) deviations as \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\), respectively. And importantly, these variance parameters have a covariance \\(\\sigma_{01}\\). However, and this next part is quite important, brms users should know that unlike the convention in many frequentist software packages (e.g., lme4) and in the text, brms parameterizes these in the standard-deviation metric. That is, in brms, these are expressed as \\(\\sigma_0\\) and \\(\\sigma_1\\). Similarly, the \\(\\sigma_{01}\\) presented in brms output is in a correlation metric, rather than a covariance. There are technical reasons for this are outside of the scope of the present situation (see Bürkner, 2017). The consequences is that we’ll make frequent use of squares and square roots in this project when comparing our brms::brm() results to those in the text. As on page 63 of the text, the typical way to express the multivariate distribution of the \\(\\zeta\\) parameters would be \\[\\begin{align*} \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{N} \\bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01}\\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\bigg ), \\end{align*}\\] where the bracketed matrix on the right part of the equation is the variance/covariance matrix. If we summarize the vector of \\(\\zeta\\) terms as \\(u\\) and so on, we can re-express the above equation as \\[ u \\sim \\operatorname{N} (\\mathbf{0}, \\mathbf{\\Sigma}), \\] where \\(\\mathbf{0}\\) is the vector of 0 means and \\(\\mathbf{\\Sigma}\\) is the variance/covariance matrix. In Stan, and thus brms, we typically decompose \\(\\mathbf{\\Sigma}\\) as \\[\\begin{align*} \\mathbf{\\Sigma} &amp; = \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}, \\text{where} \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}. \\end{align*}\\] Thus \\(\\mathbf{D}\\) is the diagonal matrix of standard deviations and \\(\\mathbf{\\Omega}\\) is the correlation matrix. 3.4 Fitting the multilevel model for change to data Singer and Willett discussed how in the 90s, we saw a bloom of software for fitting multilevel models. Very notably, they mentioned BUGS (Gilks, Richardson, &amp; Spiegelhalter, 1996) which stands for ‘Bayesian inference using Gibbs sampling’ and was a major advance in Bayesian software. As we learn in Kruschke (2015): In 1997, BUGS had a Windows-only version called WinBUGS, and later it was reimplemented in OpenBUGS which also runs best on Windows operating systems. JAGS (Plummer, 2003, 2012) retained many of the design features of BUGS, but with different samplers under the hood and better usability across different computer-operating systems (Windows, MacOS, and other forms of Linux/Unix). (pp. 193–194) There’s also Stan. From their homepage, we read “Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation.” Stan is free and open-source and you can find links to various documentation resources, such as the current User’s Guide and Stan Language Reference Manual, at https://mc-stan.org/users/documentation/. Unlike BUGS and JAGS, Stan samples from the posterior via Hamiltonian Monte Carlo, which tends to scale particularly well for complex multilevel models. However, in this project we won’t be working with Stan directly. Rather, we’ll interface with it indirectly through brms. To my knowledge, brms is the most flexible and user-friendly interface for Stan within the R ecosystem. Talking about the various software options, Singer and Willett wrote: All have their strengths, and we use many of them in our research and in this book. At their core, each program does the same job; it fits the multilevel model for change to data and provides parameter estimates, measures of precision, diagnostics, and so on. There is also some evidence that all the different packages produce the same, or similar, answers to a given problem (Kreft &amp; de Leeuw, 1990). So, in one sense, it does not matter which program you choose. (p. 64) But importantly, in the next paragraph the authors clarified their text focused on “one particular method of estimation–maximum likelihood” (p. 64, emphasis in the original). This is quite important because, whereas we might expect various maximum-likelihood-based packages to yield the same or similar results, this will not necessarily hold when working with Bayesian software which, in addition to point estimates and expressions of uncertainty, yields an entire posterior distribution as a consequence of Bayes’ theorem, \\[ p(\\theta | d) = \\frac{p(d | \\theta) \\; p(\\theta)}{p(d)}, \\] where \\(p(\\theta | d)\\) is the posterior distribution, \\(p(d | \\theta)\\) is the likelihood (i.e., the star of maximum likelihood), \\(p(\\theta)\\) are the priors, and \\(p(d)\\) is the normalizing constant, the probability of the data which transforms the numerator to a probability metric. Given that multilevel models are a fairly advanced topic, it’s not my goal in this project to offer a tutorial on the foundations of applied Bayesian statistics. I’m taking it for granted you’re familiar with the basics. But if you’re very ambitious and this is new or if you’re just rusty, I recommend you back up and lean the ropes with Richard McElreath’s excellent introductory text, Statistical Rethinking. He has great freely-available lectures that augment the text and I also have a ebook translating his book in to brms and tidyverse-style code. 3.4.1 The advantages of maximum likelihood Bayesian estimation. I just don’t think I have the strength to evangelize Bayes, at the moment. McElreath covered that a little bit in Statistical Rethinking, but he mainly took it for granted. Kruschke has been more of a Bayesian evangelist, examples of which are his 2015 text or his coauthored paper with Torrin Liddell, The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. My assumption is if you’re reading this, you’re already interested. 3.4.2 Using maximum likelihood modern Bayesian methods to fit a multilevel model. Just as with maximum likelihood, you have to specify a likelihood function with Bayes, too. The likelihood, \\(p(d | \\theta)\\), is half of the numerator of Bayes’s theorem and its meaning in Bayes is that same as with maximum likelihood estimation. The likelihood “describes the probability of observing the sample data as a function of the model’s unknown parameters” (p. 66). But unlike with maximum likelihood, we multiply the likelihood with the prior(s) and normalize the results so they’re in a probability metric. 3.5 Examining estimated fixed effects Singer and Willett discussed hypothesis testing in this section. The Bayesian paradigm can be used for hypothesis testing. For an introduction to this approach, check out Chapters 11 and 12 of Kruschke’s (2015) text. This will not be our approach in this project. My perspective on Bayesian modeling is more influenced by McElreath’s text and by Andrew Gelman’s various works. I like fitting models, inspecting their parameters, interpreting them from an effect-size perspective, and considering posterior predictions. You’ll see plenty of examples of this approach in the examples to come. 3.5.1 Interpreting estimated fixed effects. Singer and Willett presented the maximum likelihood results of our multilevel model in Table 3.3. Before we present ours, we’ll need to fit our corresponding Bayesian model. Let’s fire up brms. library(brms) Just like we did with the single-level models in the last chapter, we’ll fit our Bayesian multilevel models with the brm() function. fit3.2 &lt;- brm(data = early_int_sim, family = gaussian, formula = cog ~ 0 + Intercept + age_c + program + age_c:program + (1 + age_c | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.9), seed = 3, file = &quot;fits/fit03.02&quot;) Compared to last chapter, we’ve added a few arguments. Notice the second line, family = gaussian. Remember all that likelihood talk from the last few sections? Well, with family = gaussian we’ve indicated we want to use the Gaussian likelihood function. As this is the brms default, we didn’t actually need to type out the argument. But we’ll follow this convention for the remainder of the text for two reasons. First, I hope it’s pedagogically useful to remind you of what likelihood you’re working with. Second, I think it’s generally a good idea to explicate your likelihood. In the context of the initial chapters of this text, this might seem unnecessary. We’ll constantly be using the Gaussian. But that’s largely a pedagogical decision made by the authors. There are lots of every-day applications for multilevel models with other likelihood functions, such as those suited for discrete data. And when the day comes you’ll need to fit a multilevel logistic regression model, you’ll need to use a different setting in the family argument. Plus, we will have some practice using other likelihood functions in the survival chapters later in the text. Here’s the big new thing: our formula line specified a multilevel model! Check out the (1 + age_c | id) syntax on the right. This syntax is designed to be similar to the that of the widely-used frequentist lme4 package. In the brms reference manual, we lean this syntax follows the generic form (gterms | group) where “the optional gterms part may contain effects that are assumed to vary across grouping variables specified in group. We call them ‘group-level’ effects or (adopting frequentist vocabulary) ‘random’ effects, although the latter name is misleading in a Bayesian context” (p. 35). And like with base R’s lm() function or with lme4, the 1 portion is a stand-in for the intercept. Thus, with 1 + age_c, we indicated we wanted the intercept and age_c slope to vary across groups. On the right side of the |, we defined our grouping variable as id. Another important part of the formula syntax concerns the intercept for the fixed effects. See the 0 + Intercept part? Here’s the deal: If we were using default behavior, we’d have coded either 1 + ... or just left that part out entirely. Both would have estimated the fixed intercept according to brms default behavior. But that’s the issue. By default, brms::brm() presumes your predictors are mean centered. This is critical because the default priors set by brms::brm() are also set based on this assumption. As it turns out, neither our age_c nor program variables are centered that way. program is a dummy variable and age_c is centered on 1, not the mean. Now since the brm() default priors are rather broad and uninformative, This probably wouldn’t have made much of a difference, here. However, we may as well address this issue now and avoid bad practices. So, with our 0 + Intercept solution, we told brm() to suppress the default intercept and replace it with our smartly-named Intercept parameter. This is our fixed effect for the population intercept and, importantly, brms() will assign default priors to it based on the data themselves without assumptions about centering. I’d like to acknowledge at this point that if brms and/or the multilevel model are new to you, this can be disorienting and confusing. I’m sorry. The world is unfair and Singer and Willett didn’t write their text with brms in mind. Had they done so, they’d have used mean-centered predictors for the first few models to put off this technical issue until later chapters. Yet here we are. So if you’re feeling frustrated, that mainly just means you’re paying attention. Good job! Forge on, friends. It’ll get better. The line starting with iter largely explicates brms::brm() default settings. The only change from the defaults is cores = 4, which allow you to sample from all four chains simultaneously. The control line opens up a can of worms I just don’t want to address at this point in the text. It’s a technical setting that helped us do a better job sampling form the posterior. We’ll have more opportunities to talk about it later. For now, know that the default setting for adapt_delta is 0.8. That parameter ranges from 0 to 1. The last line of interest is seed = 3. Markov chain Monte Carlo methods use pseudo-random number generators to sample from the posterior. To make the results of a pseudo-random process reproducible, you set the seed. There’s nothing special about setting it to 3. I just did so because that’s the chapter we’re on. Play around with other values and see what happens. Okay, that’s a lot of boring technical talk. Let’s use print() to see what we did! print(fit3.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cog ~ 0 + Intercept + age_c + program + age_c:program + (1 + age_c | id) ## Data: early_int_sim (Number of observations: 309) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 103) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9.63 1.18 7.56 12.01 1.00 704 1823 ## sd(age_c) 3.66 2.39 0.17 8.79 1.01 304 879 ## cor(Intercept,age_c) -0.46 0.39 -0.96 0.66 1.00 1966 1837 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 106.61 1.85 103.03 110.35 1.00 1096 1635 ## age_c -20.59 1.86 -24.25 -16.98 1.00 2160 2731 ## program 9.11 2.47 4.23 14.03 1.00 1183 1816 ## age_c:program 3.32 2.55 -1.77 8.30 1.00 2335 2676 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.61 0.51 7.61 9.62 1.00 833 1362 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’re in multilevel-model land, we have three main sections in the output. Let’s start in the middle. The ‘Population-Level Effects:’ section contains our brms analogues to the “Fixed Effects” portion of Singer and Willett’s Table 3.3. These are our \\(\\gamma\\) parameter summaries. Like we briefly covered in the last chapter, brms does not give \\(z\\)- or \\(p\\)-values. But we do get high-quality percentile-based Bayesian 95% intervals. Perhaps somewhat frustratingly, our ‘Estimate’ values are a little different from those in the text. In this instance, this is more due to us not having access to the original data than differences between Bayesian and maximum likelihood estimation. They’ll be closer in many other examples. The bottom section, ‘Family Specific Parameters:’, is our analogue to the first line in Table 3.3’s “Variance Components” section. What Singer and Willett referred to as \\(\\epsilon_{ij}\\), the brms package calls sigma. But importantly, do recall that Stan and brms parameterize variance components in the standard-deviation metric. So you’d have to square our sigma to put it in a similar metric to the estimate in the text. This will be the case throughout this project. But why call this section ‘Family Specific Parameters’? Well, not all likelihoods have a \\(\\sigma\\) parameter. In the Poisson likelihood, for example, the mean and variance scale together as one parameter called \\(\\lambda\\). Since brms is designed to handle a whole slew of likelihood functions (see the Parameterization of Response Distributions in brms vignette), it behooved Bürkner to give this section a generic name. Now we’re ready to draw our attention to the topmost section. The ‘Group-Level Effects:’ are our brms variants of the Level 2 section of Singer and Willett’s “Variance Components” section in Table 3.3. Our sd(Intercept) corresponds to their \\(\\sigma_0^2\\) and our sd(age_c) corresponds to their \\(\\sigma_1^2\\). But recall, ours are in a standard-deviation metric. The estimates in the book are expressed as variances. Finally, our cor(Intercept,age_c) parameter is a correlation among the varying-effects, whereas Singer and Willett’s \\(\\sigma_{01}\\) is a covariance. In addition to print(), a handy way to pull the fixed effects from a brm() model is with the fixef() function. fixef(fit3.2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 106.610627 1.847049 103.030250 110.353080 ## age_c -20.591229 1.863751 -24.250559 -16.982688 ## program 9.113610 2.470954 4.225454 14.029559 ## age_c:program 3.317418 2.546378 -1.769252 8.300471 You can subset its components with [] syntax. E.g., here we’ll pull the posterior mean for the overall intercept and round to two decimal places. fixef(fit3.2)[1, 1] %&gt;% round(digits = 2) ## [1] 106.61 Thus, we can write our version of the equations atop page 70 as \\(\\hat{\\pi}_{0i} =\\) 106.61 \\(+\\) 9.11\\(\\text{program}_i\\) and \\(\\hat{\\pi}_{1i} =\\) -20.59 \\(+\\) 3.32\\(\\text{program}_i\\). Again, our results differ largely because they’re based on simulated data rather than the real data in the text. We’ll be able to work with the original data in the later chapters. Anyway, here are the results of those two equations. fixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] ## [1] 115.7242 fixef(fit3.2)[2, 1] + fixef(fit3.2)[4, 1] ## [1] -17.27381 Here’s how to get our estimates corresponding to the values at the bottom of page 70. # when `program` is 0 fixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] * 0 ## [1] 106.6106 fixef(fit3.2)[2, 1] * 1 + fixef(fit3.2)[4, 1] * 1 * 0 ## [1] -20.59123 # when `program` is 1 fixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] * 1 ## [1] 115.7242 fixef(fit3.2)[2, 1] * 1 + fixef(fit3.2)[4, 1] * 1 * 1 ## [1] -17.27381 To make our version of Figure 3.5, we’ll pump the necessary age_c and program values into the full formula of the fixed effects. # specify the values for our covariates `age_c` and `program` crossing(age_c = 0:1, program = 0:1) %&gt;% # push those values through the fixed effects mutate(cog = fixef(fit3.2)[1, 1] + fixef(fit3.2)[2, 1] * age_c + fixef(fit3.2)[3, 1] * program + fixef(fit3.2)[4, 1] * age_c * program, # wrangle a bit age = age_c + 1, size = ifelse(program == 1, 1/5, 3), program = factor(program, levels = c(&quot;0&quot;, &quot;1&quot;))) %&gt;% # plot! ggplot(aes(x = age, y = cog, group = program)) + geom_line(aes(size = program)) + scale_size_manual(values = c(1, 1/2)) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) 3.5.2 Single parameter tests for the fixed effects. As in regular regression, you can conduct a hypothesis test on each fixed effect (each \\(\\gamma\\)) using a single parameter text. Although you can equate the parameter value to any pre-specified value in your hypothesis text, most commonly you examine the null hypothesis that, controlling for all other predictors in the model, the population value of the parameter is 0, \\(H_0: \\gamma = 0\\), against the two-sided alternative that it is not, \\(H_1: \\gamma \\neq 0\\). (p. 71) You can do this with brms with the hypothesis() function. I’m not a fan of this method and am not going to showcase it in this project. If you insist on the NHST paradigm, you’ll have to go that alone. Within the Bayesian paradigm, we have an entire posterior distribution. So let’s just look at that. Recall that in the print() and fixef() outputs, we get the parameter estimates for our \\(\\gamma\\)’s summarized in terms of the posterior mean (i.e., ‘Estimate’), the posterior standard deviation (i.e., ‘Est.error’), and the percentile-based 95% credible intervals (i.e., ‘Q2.5’ and ‘Q97.5’). But we can get much richer output with the posterior_samples() function. post &lt;- posterior_samples(fit3.2) Here’s a look at the first 10 columns. post[, 1:10] %&gt;% glimpse() ## Rows: 4,000 ## Columns: 10 ## $ b_Intercept &lt;dbl&gt; 102.8892, 105.2727, 103.8568, 107.7953, 106.4047, 107.5565, 106.6523, 106.… ## $ b_age_c &lt;dbl&gt; -18.29395, -22.22785, -21.75086, -21.99438, -22.47966, -19.22179, -19.1981… ## $ b_program &lt;dbl&gt; 13.108392, 10.818055, 10.914435, 7.008055, 9.500380, 5.725332, 11.002178, … ## $ `b_age_c:program` &lt;dbl&gt; 2.1247692, 3.1293869, 6.4368342, 5.4919968, 4.4228299, 3.7876163, -0.23135… ## $ sd_id__Intercept &lt;dbl&gt; 9.546093, 7.611304, 10.323331, 9.057213, 8.866942, 9.016007, 8.900583, 8.9… ## $ sd_id__age_c &lt;dbl&gt; 0.2167529, 0.7048712, 1.5680315, 0.4272036, 0.4169315, 3.3179110, 1.519087… ## $ cor_id__Intercept__age_c &lt;dbl&gt; 0.73881953, -0.01906430, -0.81995102, -0.65061021, 0.38164788, -0.87725497… ## $ sigma &lt;dbl&gt; 8.908052, 8.469657, 8.535338, 8.486847, 9.015498, 8.255877, 9.075255, 8.18… ## $ `r_id[1,Intercept]` &lt;dbl&gt; 5.97206700, -1.75574914, 4.07787355, 9.07237406, 9.63738880, -0.89131378, … ## $ `r_id[2,Intercept]` &lt;dbl&gt; 1.007895, 1.052926, -0.279572, 6.142621, -3.572201, 5.760308, -3.382944, 5… Here are the dimensions. post %&gt;% dim() ## [1] 4000 215 We saved our results as post, which is a data frame with 4,000 rows (i.e., 1,000 post-warmup iterations times 4 chains) and 215 columns, each depicting one of the model parameters. With brms, the \\(\\gamma\\) parameters (i.e., the fixed effects or population parameters) get b_ prefixes in the posterior_samples() output. So we can isolate them like so. post %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% head() ## b_Intercept b_age_c b_program b_age_c:program ## 1 102.8892 -18.29395 13.108392 2.124769 ## 2 105.2727 -22.22785 10.818055 3.129387 ## 3 103.8568 -21.75086 10.914435 6.436834 ## 4 107.7953 -21.99438 7.008055 5.491997 ## 5 106.4047 -22.47966 9.500380 4.422830 ## 6 107.5565 -19.22179 5.725332 3.787616 Just a little more data wrangling will put post in a format suitable for plotting. post %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% ggplot(aes(x = value)) + geom_density(color = &quot;transparent&quot;, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Sure, you could fixate on zero if you wanted to. But of more interest is the overall shape of each parameter’s posterior distribution. Look at each’s central tendency and spread. Look at where each is in the parameter space. To my mind, that story is so much richer than fixating on zero. We’ll have more to say along these lines in subsequent chapters. 3.6 Examining estimated variance components Estimated variance and covariance components are trickier to interpret as their numeric values have little absolute meaning and there are no graphic aids to fall back on. Interpretation for a single fitted model is especially difficult as you lack benchmarks for evaluating the components’ magnitudes. This increases the utility of hypothesis testing, for at least the tests provide some benchmark (against the null value of 0) for comparison. (p. 72) No, no, no! I do protest. No! As I hope to demonstrate, our Bayesian brms paradigm offers rich and informative alternatives to the glib picture Singer and Willett painted back in 2003. Nowadays, we have full Bayesian estimation with Stan. Rejoice, friends. Rejoice. 3.6.1 Interpreting the estimated variance components. To extract just the variance components of a brm() model, use the VarCorr() function. VarCorr(fit3.2) ## $id ## $id$sd ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 9.634276 1.176707 7.5580325 12.012222 ## age_c 3.657434 2.391918 0.1711413 8.791714 ## ## $id$cor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 1.0000000 0.0000000 1.0000000 1.0000000 ## age_c -0.4556416 0.3903946 -0.9597077 0.6584062 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.4556416 0.3903946 -0.9597077 0.6584062 ## age_c 1.0000000 0.0000000 1.0000000 1.0000000 ## ## ## $id$cov ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 94.20357 23.15539 57.12386 144.293488 ## age_c -20.07028 19.21041 -64.25712 5.370382 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -20.07028 19.21041 -64.25711883 5.370382 ## age_c 19.09667 21.47643 0.02928937 77.294236 ## ## ## ## $residual__ ## $residual__$sd ## Estimate Est.Error Q2.5 Q97.5 ## 8.608017 0.513173 7.607589 9.623975 In case that output is confusing, VarCorr() returned a 2-element list of lists. We can use the [[]] subsetting syntax to isolate the first list of lists. VarCorr(fit3.2)[[1]] %&gt;% str() ## List of 3 ## $ sd : num [1:2, 1:4] 9.63 3.66 1.18 2.39 7.56 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## $ cor: num [1:2, 1:4, 1:2] 1 -0.456 0 0.39 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## $ cov: num [1:2, 1:4, 1:2] 94.2 -20.1 23.2 19.2 57.1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; If you just want the \\(\\zeta\\)’s, subset the first list of the first list. VarCorr(fit3.2)[[1]][[1]] ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 9.634276 1.176707 7.5580325 12.012222 ## age_c 3.657434 2.391918 0.1711413 8.791714 Here’s how to get their correlation matrix. VarCorr(fit3.2)[[1]][[2]] ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 1.0000000 0.0000000 1.0000000 1.0000000 ## age_c -0.4556416 0.3903946 -0.9597077 0.6584062 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.4556416 0.3903946 -0.9597077 0.6584062 ## age_c 1.0000000 0.0000000 1.0000000 1.0000000 And perhaps of great interest, here’s how to get their variance/covariance matrix. VarCorr(fit3.2)[[1]][[3]] ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 94.20357 23.15539 57.12386 144.293488 ## age_c -20.07028 19.21041 -64.25712 5.370382 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -20.07028 19.21041 -64.25711883 5.370382 ## age_c 19.09667 21.47643 0.02928937 77.294236 You can also use the appropriate algebraic operations to transform some of the columns in the posterior_samples() output into the variance metric used in the text. Here we’ll do so for the elements in the variance/covariance matrix and \\(\\sigma_\\epsilon^2\\), too. posterior_samples(fit3.2) %&gt;% mutate(`sigma[0]^2` = sd_id__Intercept^2, `sigma[1]^2` = sd_id__age_c^2, `sigma[0][1]` = sd_id__Intercept * cor_id__Intercept__age_c * sd_id__age_c, `sigma[epsilon]^2` = sigma^2) %&gt;% pivot_longer(starts_with(&quot;sigma[&quot;), values_to = &quot;posterior&quot;) %&gt;% ggplot(aes(x = posterior)) + geom_density(color = &quot;transparent&quot;, fill = &quot;grey33&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank(), strip.text = element_text(size = 12)) + facet_wrap(~name, scales = &quot;free&quot;, labeller = label_parsed) As it turns out, the multilevel variance components are often markedly non-Gaussian. This is important for the next section. 3.6.2 Single parameter tests for the variance components. Statisticians disagree as to the nature, form, and effectiveness of these tests. Miller (1986), Raudenbush and Bryk (2002), and others have long questioned their utility because of their sensitivity to departures from normality. Longford (1999) describes their sensitivity to sample size and imbalance (unequal numbers of observations per person) and argues that they are so misleading that they should be abandoned completely. (p. 73) This reminds me of parts from Gelman and Hill’s (2007) text on multilevel models. In section 12.7 on the topic of model building and statistical significance, they wrote: It is not appropriate to use statistical significance as a criterion for including particular group indicators in a multilevel model…. [They go on to discuss a particular example from the text, regarding radon levels in housed in various counties.] However, we should include all 85 counties in the model, and nothing is lost by doing so. The purpose of the multilevel model is not to use whether radon levels in county 1 are statistically significantly different from those in county 2, or from the Minnesota average. Rather, we seek the best possible estimate in each county, with appropriate accounting for uncertainty. Rather that make some significance threshold, we allow all the intercepts to vary and recognize that we may not have much precision in many of the individual groups…. The same principle holds for the models discussed in the following chapters, which include varying slopes, non-nested levels, discrete data, and other complexities. Once we have included a source of variation, we do not use statistical significance to pick and choose indicators to include or exclude from the model. In practice, our biggest constraints–the main reasons we do not use extremely elaborate models in which all coefficients can vary with respect to all grouping factors–are fitting and understanding complex models. The lmer() function works well when it works, but it can break down for models with many groping factors. (p. 272, emphasis in the original) For context, lmer() is the primary function in the frequentist lme4 package. After pointing out difficulties with lmer(), they went on to point out how the Bayesian Bugs software can often overcome limitations in frequentist packages. We now have the benefit of Stan and brms. My general recommendation is if your theory suggests there should be group-level variability and you’ve collected the necessary data to fit that model, fit the full model. 3.7 Bonus: How did you simulate that data? What makes our task difficult is the multilevel model we’d like to simulate our data for has both varying intercepts and slopes. And worst yet, those varying intercepts and slopes have a correlation structure. Also of note, Singer and Willett presented their summary statistics in the form of a variance/covariance matrix in Table 3.3. As it turns out, the mvnorm() function from the MASS package will allow us to simulate multivariate normal data from a given mean structure and variance/covariance matrix. So our first step in simulating our data is to simulate the \\(103 – 8 = 95\\) \\(\\zeta\\) values. We’ll name the results z. # how many people are we simulating? n &lt;- 103 - 8 # what&#39;s the variance/covariance matrix? sigma &lt;- matrix(c(124.64, -36.41, -36.41, 12.29), ncol = 2) # what&#39;s our mean structure? mu &lt;- c(0, 0) # set the seed and simulate! set.seed(3) z &lt;- MASS::mvrnorm(n = n, mu = mu, Sigma = sigma) %&gt;% data.frame() %&gt;% set_names(&quot;zeta_0&quot;, &quot;zeta_1&quot;) head(z) ## zeta_0 zeta_1 ## 1 10.7586672 -3.0908765 ## 2 3.4258938 -0.4186497 ## 3 -3.0770183 0.2140130 ## 4 12.5303603 -4.9043416 ## 5 -2.1114641 0.8936950 ## 6 -0.5521597 -0.6310265 For our next step, we’ll define our \\(\\gamma\\) parameters. These are also taken from Table 3.3. g &lt;- tibble(id = 1:n, gamma_00 = 107.84, gamma_01 = 6.85, gamma_10 = -21.13, gamma_11 = 5.27) head(g) ## # A tibble: 6 x 5 ## id gamma_00 gamma_01 gamma_10 gamma_11 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 108. 6.85 -21.1 5.27 ## 2 2 108. 6.85 -21.1 5.27 ## 3 3 108. 6.85 -21.1 5.27 ## 4 4 108. 6.85 -21.1 5.27 ## 5 5 108. 6.85 -21.1 5.27 ## 6 6 108. 6.85 -21.1 5.27 Note how they’re the same for each row. That’s the essence of the meaning of a fixed effect. Anyway, this next block is a big one. After we combine g and z, we add in the appropriate program and age_c values. You can figure out those from pages 46 and 47. We then insert our final model parameter, \\(\\epsilon\\), and combine the \\(\\gamma\\)’s and \\(\\zeta\\)’s to make our two \\(\\pi\\) parameters (see page 60). Once that’s all in place, we’re ready to use the model formula to calculate the expected cog values from the \\(\\pi\\)’s, age_c, and \\(\\epsilon\\). # set the seed for the second `mutate()` line set.seed(3) early_int_sim &lt;- bind_cols(g, z) %&gt;% mutate(program = rep(1:0, times = c(54, 41))) %&gt;% expand(nesting(id, gamma_00, gamma_01, gamma_10, gamma_11, zeta_0, zeta_1, program), age_c = c(0, 0.5, 1)) %&gt;% mutate(epsilon = rnorm(n(), mean = 0, sd = sqrt(74.24))) %&gt;% mutate(pi_0 = gamma_00 + gamma_01 * program + zeta_0, pi_1 = gamma_10 + gamma_11 * program + zeta_1) %&gt;% mutate(cog = pi_0 + pi_1 * age_c + epsilon) head(early_int_sim) ## # A tibble: 6 x 13 ## id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c epsilon pi_0 pi_1 cog ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0 -8.29 125. -19.0 117. ## 2 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0.5 -2.52 125. -19.0 113. ## 3 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 1 2.23 125. -19.0 109. ## 4 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0 -9.93 118. -16.3 108. ## 5 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0.5 1.69 118. -16.3 112. ## 6 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 1 0.260 118. -16.3 102. But before we do, we’ll want to wrangle a little. We need an age column. If you look closely at Table 3.3, you’ll see all the cog values are integers. So we’ll round ours to match. Finally, we’ll want to renumber our id values to match up better with those in Table 3.3. early_int_sim &lt;- early_int_sim %&gt;% mutate(age = age_c + 1, cog = round(cog, digits = 0), id = ifelse(id &gt; 54, id + 900, id)) head(early_int_sim) ## # A tibble: 6 x 14 ## id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c epsilon pi_0 pi_1 cog age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0 -8.29 125. -19.0 117 1 ## 2 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0.5 -2.52 125. -19.0 113 1.5 ## 3 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 1 2.23 125. -19.0 109 2 ## 4 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0 -9.93 118. -16.3 108 1 ## 5 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0.5 1.69 118. -16.3 112 1.5 ## 6 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 1 0.260 118. -16.3 102 2 Finally, now we just need to prune the columns with the model parameters, rearrange the order of the columns we’d like to keep, and join these data with those from Table 3.3. early_int_sim &lt;- early_int_sim %&gt;% select(id, age, cog, program, age_c) %&gt;% full_join(early_int, by = c(&quot;id&quot;, &quot;age&quot;, &quot;cog&quot;, &quot;program&quot;, &quot;age_c&quot;)) %&gt;% arrange(id, age) glimpse(early_int_sim) ## Rows: 309 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10… ## $ age &lt;dbl&gt; 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1… ## $ cog &lt;dbl&gt; 117, 113, 109, 108, 112, 102, 112, 113, 85, 138, 110, 97, 106, 107, 99, 111, 98, 92, 124, 1… ## $ program &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ age_c &lt;dbl&gt; 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0… Here we save our results in an external file for use later. save(early_int_sim, file = &quot;data/early_int_sim.rda&quot;) Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 Rcpp_1.0.4.6 broom_0.5.5 forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [7] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [6] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [11] DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [16] bridgesampling_1.0-0 splines_3.6.3 knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 ## [26] backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 ## [31] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 igraph_1.2.5 ## [36] coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 cellranger_1.1.0 ## [41] vctrs_0.2.4 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ps_1.3.2 ## [46] rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [51] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [56] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [66] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 pkgconfig_2.0.3 matrixStats_0.56.0 ## [71] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [76] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 magrittr_1.5 bookdown_0.18 ## [81] R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [86] withr_2.1.2 mgcv_1.8-31 xts_0.12-0 abind_1.4-5 modelr_0.1.6 ## [91] crayon_1.3.4 utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [96] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [101] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 "],
["doing-data-analysis-with-the-multilevel-model-for-change.html", "4 Doing Data Analysis with the Multilevel Model for Change 4.1 Example: Changes in adolescent alcohol use 4.2 The composite specification of the multilevel model for change 4.3 Methods of estimation, revisited 4.4 First steps: Fitting two unconditional multilevel models for change 4.5 Practical data analytic strategies for model building 4.6 Comparing models using deviance statistics 4.7 Using Wald statistics to test composite hypotheses about fixed effects 4.8 Evaluating the tenability of a model’s assumptions 4.9 Model-based (Empirical Bayes) estimates of the individual growth parameters Reference Session info", " 4 Doing Data Analysis with the Multilevel Model for Change “We now delve deeper into the specification, estimation, and interpretation of the multilevel model for change” (p. 75). 4.1 Example: Changes in adolescent alcohol use Load the data. library(tidyverse) alcohol1_pp &lt;- read_csv(&quot;data/alcohol1_pp.csv&quot;) head(alcohol1_pp) ## # A tibble: 6 x 9 ## id age coa male age_14 alcuse peer cpeer ccoa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 14 1 0 0 1.73 1.26 0.247 0.549 ## 2 1 15 1 0 1 2 1.26 0.247 0.549 ## 3 1 16 1 0 2 2 1.26 0.247 0.549 ## 4 2 14 1 1 0 0 0.894 -0.124 0.549 ## 5 2 15 1 1 1 0 0.894 -0.124 0.549 ## 6 2 16 1 1 2 1 0.894 -0.124 0.549 Do note we already have an \\((\\text{age} - 14)\\) variable in the data, age_14. Here’s our version of Figure 4.1, using stat_smooth() to get the exploratory OLS trajectories. alcohol1_pp %&gt;% filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% ggplot(aes(x = age, y = alcuse)) + stat_smooth(method = &quot;lm&quot;, se = F) + geom_point() + coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~id, ncol = 4) By this figure, Singer and Willett suggested the simple linear level-1 submodel following the form \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 14) + \\epsilon_{ij}\\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2), \\end{align*}\\] where \\(\\pi_{0i}\\) is the initial status of participant \\(i\\), \\(\\pi_{1i}\\) is participant \\(i\\)’s rate of change, and \\(\\epsilon_{ij}\\) is the variation in participant \\(i\\)’s data not accounted for in the model. Singer and Willett made their Figure 4.2 “with a random sample of 32 of the adolescents” (p. 78). If we just wanted a random sample of rows, the sample_n() function would do the job. But since we’re working with long data, we’ll need some group_by() + nest() mojo. I got the trick from Jenny Bryan’s Sample from groups, n varies by group. Setting the seed makes the results from sample_n() reproducible. Here are the top panels. set.seed(4) alcohol1_pp %&gt;% group_by(id) %&gt;% nest() %&gt;% sample_n(size = 32, replace = T) %&gt;% unnest(data) %&gt;% mutate(coa = ifelse(coa == 0, &quot;coa = 0&quot;, &quot;coa = 1&quot;)) %&gt;% ggplot(aes(x = age, y = alcuse, group = id)) + stat_smooth(method = &quot;lm&quot;, se = F, size = 1/4) + coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~coa) We have similar data wrangling needs for the bottom panels. set.seed(4) alcohol1_pp %&gt;% group_by(id) %&gt;% nest() %&gt;% ungroup() %&gt;% sample_n(size = 32, replace = T) %&gt;% unnest(data) %&gt;% mutate(hp = ifelse(peer &lt; mean(peer), &quot;low peer&quot;, &quot;high peer&quot;)) %&gt;% mutate(hp = factor(hp, levels = c(&quot;low peer&quot;, &quot;high peer&quot;))) %&gt;% ggplot(aes(x = age, y = alcuse, group = id)) + stat_smooth(method = &quot;lm&quot;, se = F, size = 1/4) + coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~hp) Based on the exploratory analyses, Singer and Willett posited the initial level-2 submodel might take the form \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\zeta_{0i}\\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{coa}_i + \\zeta_{1i} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01}\\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\Bigg ), \\end{align*}\\] where \\(\\gamma_{00}\\) and \\(\\gamma_{10}\\) are the level-2 intercepts, the population averages when \\(\\text{coa} = 0\\), \\(\\gamma_{10}\\) and \\(\\gamma_{11}\\) are the level-2 slopes expressing the difference when \\(\\text{coa} = 1\\) and \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) are the unexplained variation across the \\(\\text{id}\\)-level intercepts and slopes. Since we’ll be fitting the model with brms::brm(), the \\(\\Sigma\\) matrix will be parameterized in the \\(\\sigma\\) metric. So we might reexpress the model as \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\zeta_{0i}\\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{coa}_i + \\zeta_{1i} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0 &amp; \\rho_{01}\\\\ \\rho_{01} &amp; \\sigma_1 \\end{bmatrix} \\Bigg ). \\end{align*}\\] 4.2 The composite specification of the multilevel model for change With a little algebra, we can combine the level-1 and level-2 submodels into the composite multilevel model for change, which follows the form \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{10} \\text{age_14}_{ij} + \\gamma_{01} \\text{coa}_i + \\gamma_{11} (\\text{coa}_i \\times \\text{age_14}_{ij}) \\big ] \\\\ &amp; \\;\\;\\;\\;\\; + [ \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\Bigg ), \\end{align*}\\] where the brackets in the first line partition the structural model (i.e., the model for \\(\\mu\\)) and the stochastic components (i.e., the \\(\\sigma\\) terms). We should note that this is the format that most closely mirrors what we use in the formula argument in brms::brm(). As long as age is not centered on the mean, our brms syntax would be: formula = alcuse ~ 0 + Intercept + age_c + coa + age_c:coa + (1 + age_c | id). 4.2.1 The structural component of the composite model. Although their interpretation is identical, the \\(\\gamma\\)s in the composite model describe patterns of change in a different way. Rather than postulating first how ALCUSE is related to TIME and the individual growth parameters, and second how the individual growth parameters are related to COA, the composite specification in equation 4.3 postulates that ALCUSE depends simultaneously on: (1) the level-1 predictor, TIME; (2) the level-2 predictor, COA; and (3) the cross-level interaction, COA by TIME. From this perspective, the composite model’s structural portion strongly resembles a regular regression model with predictors, TIME and COA, appearing as main effects (associated with \\(\\gamma_{10}\\) and \\(\\gamma_{01}\\), respectively) and in a cross-level interaction (associated with \\(\\gamma_{11}\\)). (p. 82, emphasis in the original) 4.2.2 The stochastic component of the composite model. A distinctive feature of the composite multilevel model is its composite residual, the three terms in the second set of brackets on the right of equation 4.3 that combine together the level-1 residual and the two level-2 residuals: \\[\\text{Composite residual: } [ \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} ].\\] The composite residual is not a simple sum. Instead, the second level-2 residual, \\(\\zeta_{1i}\\), is multiplied by the level-1 predictor, \\([\\text{age_14}_{ij}]\\), before joining its siblings. Despite its unusual construction, the interpretation of the composite residual is straightforward: it describes the difference between the observed and expected value of \\([\\text{alcuse}]\\) for individual \\(i\\) on occasion \\(j\\). The mathematical form of the composite residual reveals two important properties about the occasion-specific residuals not readily apparent in the level-1/level-2 specification: they can be both autocorrelated and heteroscedastic within person. (p. 84, emphasis in the original) 4.3 Methods of estimation, revisited In this section, the authors introduced generalized least squares (GLS) estimation and iterative generalized least squares (IGLS) estimation and then distinguished between full and restricted maximum likelihood estimation. Since our goal is to fit these models as Bayesians, we won’t be using or discussing any of these in this project. There are, of course, different ways to approach Bayesian estimation. Though we’re using Hamiltonian Monte Carlo, we could use other algorithms, such as the Gibbs sampler. However, all that is outside of the scope of this project. I suppose the only thing to add is that whereas GLS estimates come from mimimizing a weighted function of the residuals and maximum likelihood estimates come from maximizing the log-likelihood function, the results of our Bayesian analyses (i.e., the posterior distribution) come from the consequences of Bayes theorem, \\[ p(\\theta | d) = \\frac{p(d | \\theta) p(\\theta)}{p(d)}. \\] If you really want to dive into the details of this, I suggest referencing a proper introductory Bayesian textbook, such as McElreath (2015), Kruschke (2015), or Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (2013). I haven’t had time to check it out, but I’ve heard Labmert’s (2018) text is good, too. And for details specific to Stan, and thus brms, you might check out the documentation resources at https://mc-stan.org/users/documentation/. 4.4 First steps: Fitting two unconditional multilevel models for change Singer and Willett recommended that before you fit your full theoretical multilevel model of change–the one with all the interesting covariates–you should fit two simpler preliminary models. The first is the unconditional means model. The second is the unconditional growth model. I agree. In addition to the reasons they cover in the text, this is just good pragmatic data analysis. Start simple and build up to the more complicated models only after you’re confident you understand what’s going on with the simpler ones. And if you’re new to them, you’ll discover this is especially so with Bayesian methods. 4.4.1 The unconditional means model. The likelihood for the unconditional means model follows the formula \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\zeta_{0i} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\zeta_{0i} &amp; \\sim \\operatorname{Normal} (0, \\sigma_0^2). \\end{align*}\\] Let’s open brms. library(brms) Up till this point, we haven’t focused on priors. It would have been reasonable to wonder if we’d been using them at all. Yes, we have. Even if you don’t specify priors in the brm() function, it’ll compute default weakly-informative priors for you. You might be wondering, What might these default priors look like? The get_prior() function let us take a look. get_prior(data = alcohol1_pp, family = gaussian, alcuse ~ 1 + (1 | id)) ## prior class coef group resp dpar nlpar bound ## 1 student_t(3, 1, 10) Intercept ## 2 student_t(3, 0, 10) sd ## 3 sd id ## 4 sd Intercept id ## 5 student_t(3, 0, 10) sigma For this model, all three priors are based on Student’s \\(t\\)-distribution. In case you’re rusty, the Normal distribution is just a special case of Student’s \\(t\\)-distribution. Whereas the Normal is defined by two parameters (\\(\\mu\\) and \\(\\sigma\\)), the \\(t\\) distribution is defined by \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\). In frequentist circles, \\(\\nu\\) is often called the degrees of freedom. More generally, it’s also referred to as a normality parameter. We’ll examine the prior more closely in a bit. For now, let’s practice setting our priors by manually specifying them within brm(). You do with the prior argument. There are actually several ways to do this. To explore all the options, check out the set_prior section of the reference manual. I typically define my individual priors with the prior() function. When there are more than one priors to define, I typically bind them together within c(...). Other than the addition of our fancy prior statement, the rest of the settings within brm() are much like those in prior chapters. Let’s fit the model. fit4.1 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 1 + (1 | id), prior = c(prior(student_t(3, 1, 10), class = Intercept), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.01&quot;) Here are the results. print(fit4.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 1 + (1 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.77 0.08 0.62 0.94 1.00 1662 2274 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.92 0.10 0.72 1.12 1.00 2602 2734 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.76 0.04 0.68 0.84 1.00 3219 3327 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Compare the results to those listed under “Model A” in Table 4.1. It’s important to keep in mind that brms returns ‘sigma’ and ‘sd(Intercept)’ in the standard deviation metric rather than the variance metric. “But I want them in the variance metric like in the text!”, you say. Okay fine. The best way to do the transformations is after saving the results from posterior_samples(). post &lt;- posterior_samples(fit4.1) glimpse(post[, 1:12]) ## Rows: 4,000 ## Columns: 12 ## $ b_Intercept &lt;dbl&gt; 1.0327617, 1.0176357, 0.8999551, 1.0666116, 0.9471380, 1.0461035, 0.9… ## $ sd_id__Intercept &lt;dbl&gt; 0.9317019, 0.9259266, 0.7670859, 0.6670863, 0.7075109, 0.7324726, 0.8… ## $ sigma &lt;dbl&gt; 0.7657679, 0.7630844, 0.8073889, 0.6964495, 0.7029354, 0.7410712, 0.7… ## $ `r_id[1,Intercept]` &lt;dbl&gt; 0.7525755, 1.1180550, 0.4892602, 0.9660265, 0.9259051, 0.8063631, 0.7… ## $ `r_id[2,Intercept]` &lt;dbl&gt; 0.07598890, -0.13078777, -0.26457278, -0.64529871, -0.72063809, -0.55… ## $ `r_id[3,Intercept]` &lt;dbl&gt; 1.3955514, 0.8532742, 0.8310599, 1.3219625, 1.2018000, 0.3310101, 1.5… ## $ `r_id[4,Intercept]` &lt;dbl&gt; 0.41462311, 0.40985772, -0.48227528, 0.57693850, 0.74470742, -0.04842… ## $ `r_id[5,Intercept]` &lt;dbl&gt; -0.94622094, -1.39462489, -0.47880633, -0.55613003, -0.61670702, -0.5… ## $ `r_id[6,Intercept]` &lt;dbl&gt; 1.125702, 1.736155, 1.708224, 1.919763, 1.945292, 1.292308, 1.628131,… ## $ `r_id[7,Intercept]` &lt;dbl&gt; 0.8668786, 0.9029424, 0.4616220, 0.5419488, 0.9775908, 0.1501572, 1.2… ## $ `r_id[8,Intercept]` &lt;dbl&gt; 0.06167846, -0.36815820, -1.12403522, -0.27823892, -0.99175538, -0.47… ## $ `r_id[9,Intercept]` &lt;dbl&gt; 0.30049368, 0.45626836, 0.09886899, 1.03263516, 0.97242471, -0.417528… Since all we’re interested in are the variance components, we’ll select() out the relevant columns from post, compute the squared versions, and save the results in a mini data frame, v. v &lt;- post %&gt;% select(sigma, sd_id__Intercept) %&gt;% mutate(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2) head(v) ## sigma sd_id__Intercept sigma_2_epsilon sigma_2_0 ## 1 0.7657679 0.9317019 0.5864005 0.8680685 ## 2 0.7630844 0.9259266 0.5822978 0.8573400 ## 3 0.8073889 0.7670859 0.6518768 0.5884208 ## 4 0.6964495 0.6670863 0.4850419 0.4450042 ## 5 0.7029354 0.7075109 0.4941182 0.5005717 ## 6 0.7410712 0.7324726 0.5491865 0.5365161 We can view their distributions like this. v %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_vline(xintercept = c(.25, .5, .75, 1), color = &quot;white&quot;) + geom_density(size = 0, fill = &quot;black&quot;) + scale_x_continuous(NULL, limits = c(0, 1.25), breaks = seq(from = 0, to = 1.25, by = .25)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free_y&quot;) In case it’s hard to follow what just happened, the estimates in the brms-default standard-deviation metric are the two panels on the top. Those on the bottom are in the Singer-and-Willett style variance metric. Like we discussed toward the end of last chapter, the variance parameters won’t often be Gaussian. In my experience, they’re typically skewed to the right. There’s nothing wrong with that. This is a recurrent pattern among distributions that are constrained to be zero and above. If you’re interested, you can summarize those posteriors like so. v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% # this last bit just rounds the output mutate_if(is.double, round, digits = 3) ## # A tibble: 4 x 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sd_id__Intercept 0.767 0.762 0.083 0.616 0.939 ## 2 sigma 0.756 0.755 0.041 0.68 0.841 ## 3 sigma_2_0 0.596 0.580 0.13 0.38 0.882 ## 4 sigma_2_epsilon 0.574 0.570 0.062 0.463 0.707 For this model, our posterior medians are closer to the estimates in the text (Table 4.1) than the means. However, our posterior standard deviations are pretty close to the standard errors in the text. One of the advantages of our Bayesian method is that when we compute something like the intraclass correlation coefficient \\(\\rho\\), we get an entire distribution for the parameter rather than a measly point estimates. This is always the case with Bayes. The algebraic transformations of the posterior distribution are themselves distributions. Before we compute \\(\\rho\\), do pay close attention to the formulia, \\[ \\rho = \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_\\epsilon^2}. \\] Even though our brms output yields the variance parameters in the standard-deviation metric, the formula for \\(\\rho\\) demands we use variances. That’s nothing a little squaring can’t fix. Here’s what our \\(\\rho\\) looks like. v %&gt;% transmute(rho = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% ggplot(aes(x = rho)) + geom_density(size = 0, fill = &quot;black&quot;) + scale_x_continuous(expression(rho), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Though the posterior for \\(\\rho\\) is indeed centered around .5, look at how wide and uncertain that distribution is. The bulk of the posterior mass takes up almost half of the parameter space. If you wanted the summary statistics, you might do what we did for the variance parameters, above. v %&gt;% transmute(rho = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% summarise(mean = mean(rho), median = median(rho), sd = sd(rho), ll = quantile(rho, prob = .025), ul = quantile(rho, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## mean median sd ll ul ## 1 0.505 0.505 0.063 0.376 0.625 Concerning \\(\\rho\\), Singer and Willett pointed out it summarizes the size of the residual autocorrelation in the composite unconditional means mode… Each person has a different composite residual on each occasion of measurement. But notice the difference in the subscripts of the pieces of the composite residual: while the level-1 residual, \\(\\epsilon_{ij}\\) has two subscripts (\\(i\\) and \\(j\\)), the level-2 residual, \\(\\zeta_{0i}\\), has only one (\\(i\\)). Each person can have a different \\(\\epsilon_{ij}\\) on each occasion, but has only one \\(\\zeta_{0i}\\) across every occasion. The repeated presence of \\(\\zeta_{0i}\\) in individual \\(i\\)’s composite residual links his or her composite residuals across occasions. The error autocorrelation coefficient quantifies the magnitude of this linkage; in the unconditional means model, the error autocorrelation coefficient is the intraclass correlation coefficient. Thus, we estimate that, for each person, the average correlation between any pair of composite residuals–between occasions 1 and 2, or 2 and 3, or 1 and 3–is [.5]. (pp. 96–97, emphasis in the original) Because of the differences in how they’re estimated with and presented by brm(), we focused right on the variance components. But before we move on to the next section, we should back up a bit. On page 93, Singer and Willett discussed their estimate for \\(\\gamma_{00}\\). Here’s ours. fixef(fit4.1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.9183551 0.1003442 0.722879 1.118637 They talked about how squaring that value puts it back to the natural metric the data were originally collected in. [Recall that as discussed earlier in the text the alcuse variable was square-root transformed because of excessive skew.] If you want a quick and dirty look, you can square our results, too. fixef(fit4.1)^2 ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.8433761 0.01006895 0.522554 1.251349 However, I do not recommend this method. Though it did okay at transforming the posterior mean (i.e., Estimate), it’s not a great way to get the summary statistics correct. To do that, you’ll need to work with the posterior samples themselves. Remember how we saved them as post? Let’s refresh ourselves and look at the first few columns. post[1:6, 1:3] ## b_Intercept sd_id__Intercept sigma ## 1 1.0327617 0.9317019 0.7657679 ## 2 1.0176357 0.9259266 0.7630844 ## 3 0.8999551 0.7670859 0.8073889 ## 4 1.0666116 0.6670863 0.6964495 ## 5 0.9471380 0.7075109 0.7029354 ## 6 1.0461035 0.7324726 0.7410712 See that b_Intercept column there? That contains our posterior draws from \\(\\gamma_{00}\\). If you want proper summary statistics from the transformed estimate, get them after transforming that column. post %&gt;% transmute(gamma_00_squared = b_Intercept^2) %&gt;% summarise(mean = mean(gamma_00_squared), median = median(gamma_00_squared), sd = sd(gamma_00_squared), ll = quantile(gamma_00_squared, prob = .025), ul = quantile(gamma_00_squared, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) %&gt;% pivot_longer(everything()) ## # A tibble: 5 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 mean 0.853 ## 2 median 0.84 ## 3 sd 0.185 ## 4 ll 0.523 ## 5 ul 1.25 And one last bit before we move on to the next section. Remember how we discovered what the brm() default priors were for our model with the handy get_prior() function? Let’s refresh ourselves on how that worked. get_prior(data = alcohol1_pp, family = gaussian, alcuse ~ 1 + (1 | id)) ## prior class coef group resp dpar nlpar bound ## 1 student_t(3, 1, 10) Intercept ## 2 student_t(3, 0, 10) sd ## 3 sd id ## 4 sd Intercept id ## 5 student_t(3, 0, 10) sigma We inserted the data and the model and get_prior() returned the default priors. Especially for new Bayesians, or even for experienced Bayesians working with unfamiliar models, it can be handy to plot your priors to get a sense of them. Base R has an array of functions based on the \\(t\\) distribution (e.g., rt(), dt()). These functions are limited in that while they allow users to select the desired \\(\\nu\\) values (i.e., degrees of freedom), they fix \\(\\mu = 0\\) and \\(\\sigma = 1\\). If you want to stick with the base R functions, you can find tricky ways around this. To avoid overwhelming anyone new to Bayes or the multilevel model or R or some exasperating combination, let’s just make things simpler and use a different function. As it turns out, the metrology package contains a dt.scaled() function that allows users to define all three parameters for Student’s \\(t\\). We’ll start with the default intercept prior, \\(t(\\nu = 3, \\mu = 1, \\sigma = 10)\\). Here’s the density in the range \\([-100, 100]\\). tibble(x = seq(from = -100, to = 100, length.out = 1e3)) %&gt;% mutate(density = metRology::dt.scaled(x, df = 3, mean = 1, sd = 10)) %&gt;% ggplot(aes(x = x, y = density)) + geom_vline(xintercept = 1, color = &quot;white&quot;) + geom_line() + labs(title = expression(paste(&quot;prior for &quot;, gamma[0][0])), x = &quot;parameter space&quot;) + theme(panel.grid = element_blank()) Though it’s centered on 1, the bulk of the mass seems to range from -40 to 40. Given the model estimate ended up about 0.9, it looks like that was a pretty broad and minimally-informative prior. However, the prior isn’t flat and it does help guard against wasting time and HMC iterations sampling from ridiculous regions of the parameter space such as -10,000 or +500,000,000. No adolescent is drinking that much (or that little–how does one drink a negative value?). Here’s the shape of the variance priors. tibble(x = seq(from = 0, to = 100, length.out = 1e3)) %&gt;% mutate(density = metRology::dt.scaled(x, df = 3, mean = 0, sd = 10)) %&gt;% ggplot(aes(x = x, y = density)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_line() + labs(title = expression(paste(&quot;prior for both &quot;, sigma[0], &quot; and &quot;, sigma[epsilon])), x = &quot;parameter space&quot;) + theme(panel.grid = element_blank()) Recall that by brms default, the variance parameters have a lower-limit of 0. So specifying a Student’s \\(t\\) or other Gaussian-like prior on them ends up cutting the distribution off at 0. Given that our estimates were both below 1, it appears that these priors were minimally informative. But again, they did help prevent brm() from sampling from negative values or from obscenely-large values. These priors look kinda silly, you might say. Anyone with a little common sense can do better. Well, sure. Probably. Maybe. But keep in mind we’re still getting the layout of the land. And plus, this was a pretty simple model. Selecting high-quality priors gets tricky as the models get more complicated. In other chapters, we’ll explore other ways to specify priors for our multilevel models. But to keep things simple for now, let’s keep practicing inspecting and using the defaults with get_prior() and so on. 4.4.2 The unconditional growth model. Using the composite formula, our next model, the unconditional growth model, follows the form \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{age_14}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\Bigg ). \\end{align*}\\] With it, we now have a full composite stochastic model. Let’s query the brms::brm() default priors when we apply this model to our data. get_prior(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id)) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b age_14 ## 3 b Intercept ## 4 lkj(1) cor ## 5 cor id ## 6 student_t(3, 0, 10) sd ## 7 sd id ## 8 sd age_14 id ## 9 sd Intercept id ## 10 student_t(3, 0, 10) sigma Several things of note: First, notice how we continue to use the student_t(3, 0, 10) for all three of our standard-deviation-metric variance parameters. Since we’re now estimating \\(\\sigma_0\\) and \\(\\sigma_1\\), which themselves have a correlation, \\(\\rho_{01}\\), we have a prior of class = cor. I’m going to put off what is meant by the name lkj, but for the moment just realize that this prior is essentially noninformative within this context. There’s a major odd development with this output. Notice how there’s the prior column is empty for the rows for our two coefficients of class b. And if you’re a little confused, recall that because our predictor age_14 is not mean-centered, we’ve used the 0 + Intercept syntax, which switches the model intercept parameter to the class of b. Anyway, it might seem odd that the prior values for those rows are blank. From the set_prior section of the reference manual for brms version 2.12.0, we read: “The default prior for population-level effects (including monotonic and category specific effects) is an improper flat prior over the reals” (p. 179). At present, these priors are uniform across the entire parameter space. They’re not just weak, their entirely noninformative. That is, the likelihood dominates the posterior for those parameters. Here’s how to fit the model with these priors. fit4.2 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.02&quot;) How did we do? print(fit4.2, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.786 0.099 0.598 0.995 1.003 1148 1952 ## sd(age_14) 0.358 0.094 0.141 0.519 1.004 405 464 ## cor(Intercept,age_14) -0.106 0.266 -0.493 0.592 1.004 675 795 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.648 0.108 0.438 0.870 1.003 1806 2103 ## age_14 0.273 0.064 0.149 0.399 1.001 3237 2825 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.609 0.052 0.518 0.718 1.003 521 1239 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If your compare our results with those in the “Model B” column in Table 4.1, you’ll see our summary results match well with those in the text. Our \\(\\gamma\\)s (i.e., ‘Population-Level Effects:’) are near identical. The leftmost panel in Figure 4.3 shows the prototypical trajectory, based on the \\(\\gamma\\)s. A quick way to get that within our brms framework is with the conditional_effects() function. Here’s the default output. conditional_effects(fit4.2) Staying with conditional_effects() allows users some flexibility for customizing the plot(s). For example, the default behavior is to depict the trajectory in terms of its 95% intervals and posterior median. If you’d prefer the 80% intervals and the posterior mean, customize it like so. conditional_effects(fit4.2, robust = F, probs = c(.1, .9)) We’ll explore more options with brms::conditional_effects() with Model C. For now, let’s turn our focus on the stochastic elements in the model. Here we extract the posterior samples and do the conversions to see how they compare with Singer and Willett’s. post &lt;- posterior_samples(fit4.2) v &lt;- post %&gt;% transmute(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__age_14^2, sigma_01 = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14) head(v) ## sigma_2_epsilon sigma_2_0 sigma_2_1 sigma_01 ## 1 0.4641867 0.4946951 0.1548531 -0.08353620 ## 2 0.3582816 0.4775434 0.1404647 -0.01830842 ## 3 0.4210113 0.7406254 0.1261567 -0.07041419 ## 4 0.3491970 0.5348618 0.1102675 -0.04012958 ## 5 0.3573036 0.7199475 0.1797023 -0.11079723 ## 6 0.3713001 0.7513729 0.1304161 -0.11221189 This time, our v object only contains the stochastic components in the variance metric. Let’s plot. v %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) For each, their posterior mass is centered near the point estimates Singer and Willet reported in the text. Here are the summary statistics. v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 4 x 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sigma_01 -0.047 -0.042 0.074 -0.207 0.081 ## 2 sigma_2_0 0.628 0.619 0.159 0.357 0.989 ## 3 sigma_2_1 0.137 0.135 0.063 0.02 0.269 ## 4 sigma_2_epsilon 0.374 0.367 0.064 0.268 0.516 Happily, they’re quite comparable to those in the text. We’ve been pulling the posterior samples for all parameters with posterior_samples() and subsetting to a few variables of interest, such as the variance parameters. But it our primary interest is just the iterations for the variance parameters, we can extract them in a more focused way with the VarCorr() function. Here’s how we’d do so for fit4.2. VarCorr(fit4.2, summary = F) %&gt;% str() ## List of 2 ## $ id :List of 3 ## ..$ sd : num [1:4000, 1:2] 0.703 0.691 0.861 0.731 0.848 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ iterations: NULL ## .. .. ..$ parameters: chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## ..$ cor: num [1:4000, 1:2, 1:2] 1 1 1 1 1 1 1 1 1 1 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## ..$ cov: num [1:4000, 1:2, 1:2] 0.495 0.478 0.741 0.535 0.72 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## $ residual__:List of 1 ## ..$ sd: num [1:4000, 1] 0.681 0.599 0.649 0.591 0.598 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ iterations: NULL ## .. .. ..$ parameters: chr &quot;&quot; That last part, the contents of the second higher-level list indexed by $ residual, contains the contents for \\(\\sigma_\\epsilon\\). On page 100 in the text, Singer and Willett compared \\(\\sigma_\\epsilon^2\\) from the first model to that from the second. We might do that like so. cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~.^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(`fit4.1 - fit4.2` = fit4.1 - fit4.2) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;fit4.1&quot;, &quot;fit4.2&quot;, &quot;fit4.1 - fit4.2&quot;))) %&gt;% ggplot(aes(x = value)) + geom_vline(xintercept = .5, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[epsilon]^2)) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free_y&quot;, ncol = 3) To compute a formal summary of the decline in \\(\\sigma_\\epsilon^2\\) after adding time to the model, we might summarize like before. cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~.^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(proportion_decline = (fit4.1 - fit4.2) / fit4.1) %&gt;% summarise(mean = mean(proportion_decline), median = median(proportion_decline), sd = sd(proportion_decline), ll = quantile(proportion_decline, prob = .025), ul = quantile(proportion_decline, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## mean median sd ll ul ## 1 0.341 0.356 0.136 0.036 0.561 In case it wasn’t clear, when we presented fit4.1 – fit4.2 in the density plot, that was a simple difference score. However, we computed proportion_decline above by dividing that difference score by fit4.1; that’s what put the difference in a proportion metric. Anyway, Singer and Willett’s method led them to summarize the decline as .40. Our method was a more conservative .34-ish. And very happily, our method allows us to describe the proportion decline with summary statistics for the full posterior, such as with the \\(SD\\) and the 95% intervals. post %&gt;% ggplot(aes(x = cor_id__Intercept__age_14)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(expression(rho[0][1]), limits = c(-1, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) The estimate Singer and Willett hand-computed in the text, -.22, is near the mean of our posterior distribution for \\(\\rho_{01}\\). However, our distribution provides a full expression of the uncertainty in the parameter. As are many other values within the parameter space, zero is indeed a credible value for \\(\\rho_{01}\\). On page 101, we get the generic formula for computing the residual variance for a given occasion \\(j\\), \\[ \\sigma_{\\text{Residual}_j}^2 = \\sigma_0^2 + \\sigma_1^2 \\text{time}_j + 2 \\sigma_{01} \\text{time}_j + \\sigma_\\epsilon^2. \\] If we were just interested in applying it to one of our age values, say 14, we might apply the formula to the posterior like this. post %&gt;% transmute(sigma_2_residual_j = sd_id__Intercept^2 + sd_id__age_14^2 * 0 + 2 * sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * 0 + sigma^2) %&gt;% head() ## sigma_2_residual_j ## 1 0.9588818 ## 2 0.8358250 ## 3 1.1616367 ## 4 0.8840587 ## 5 1.0772511 ## 6 1.1226730 But given we’d like to do so over several values of age, it might be better to wrap the equation in a custom function. Let’s call it make_s2rj(). make_s2rj &lt;- function(x) { post %&gt;% transmute(sigma_2_residual_j = sd_id__Intercept^2 + sd_id__age_14^2 * x + 2 * sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * x + sigma^2) %&gt;% pull() } Now we can put our custom make_s2rj() function to work within the purrr::map() paradigm. We’ll plot the results. tibble(age = 14:16) %&gt;% mutate(age_c = age - 14) %&gt;% mutate(s2rj = map(age_c, make_s2rj)) %&gt;% unnest(s2rj) %&gt;% mutate(label = str_c(&quot;age = &quot;, age)) %&gt;% ggplot(aes(x = s2rj)) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + # just for reference geom_vline(xintercept = 1, color = &quot;grey92&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Behold the shape of longitudinal heteroscedasticity.&quot;, x = expression(sigma[italic(Residual[j])]^2)) + theme(panel.grid = element_blank()) + facet_wrap(~label, scales = &quot;free_y&quot;, ncol = 1) We see a subtle increase over time, particularly from age = 15 to age = 16. Yep, that’s heteroscedasticity. It is indeed “beyond the bland homoscedasticity we assume of residuals in cross-sectional data” (p. 101). We might also be interested in computing the autocorrelation between the composite residuals on occasions \\(j\\) and \\(j’\\), which follows the formula \\[ \\rho_{\\text{Residual}_j, \\text{Residual}_{j&#39;}} = \\frac{\\sigma_0^2 + \\sigma_{01} (\\text{time}_j + \\text{time}_{j&#39;}) + \\sigma_1^2 \\text{time}_j \\text{time}_{j&#39;}} {\\sqrt{\\sigma_{\\text{Residual}_j}^2 \\sigma_{\\text{Residual}_{j&#39;}}^2 }}. \\] We only want to do that by hand once. Let’s make a custom function following the formula. make_rho_rj_rjp &lt;- function(j, jp) { # define the elements in the denominator s2rj_j &lt;- make_s2rj(j) s2rj_jp &lt;- make_s2rj(jp) # compute post %&gt;% transmute(r = (sd_id__Intercept^2 + sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * (j + jp) + sd_id__age_14^2 * j * jp) / sqrt(s2rj_j * s2rj_jp)) %&gt;% pull() } If you only cared about measures of central tendency, such as the posterior median, you could use the funciton like this. make_rho_rj_rjp(0, 1) %&gt;% median() ## [1] 0.5666644 make_rho_rj_rjp(1, 2) %&gt;% median() ## [1] 0.7174044 make_rho_rj_rjp(0, 2) %&gt;% median() ## [1] 0.5140162 Here are the full posteriors. tibble(occasion = 1:3) %&gt;% mutate(age_c = occasion - 1, j = c(1, 2, 1) - 1, jp = c(2, 3, 3) - 1) %&gt;% mutate(r = map2(j, jp, make_rho_rj_rjp)) %&gt;% unnest(r) %&gt;% mutate(label = str_c(&quot;occasions &quot;, j + 1, &quot; and &quot;, jp + 1)) %&gt;% ggplot(aes(x = r)) + # just for reference geom_vline(xintercept = c(.5, .75), color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(expression(rho[Residual[italic(j)]][Residual[italic(j*minute)]]), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Behold the shapes of our autocorrelations!&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~label, scales = &quot;free_y&quot;, ncol = 1) 4.4.3 Quantifying the proportion of outcome variation “explained.” Because of the way the multilevel model partitions off variance into different sources (e.g., \\(\\sigma_0^2\\), \\(\\sigma_1^2\\), and \\(\\sigma_\\epsilon^2\\) in the unconditional growth model), the conventional \\(R^2\\) is not applicable for evaluating models in the traditional OLS sense of percent of variance explained. Several pseudo \\(R^2\\) statistics are frequently used instead. Be warned, “statisticians have yet to agree on appropriate summaries (Kreft &amp; deLeeuw, 1998; Snidjers &amp; Bosker, 1994[; Jaeger, Edwards, Das, &amp; Sen, 2017; Rights &amp; Cole; 2018; Rights &amp; Sterba, 2019])” (p. 102) and none of the solutions presented in this section are magic bullets. 4.4.3.1 An overall summary of total outcome variability explained. In multiple regression, one simple way of computing a summary \\(R^2\\) statistic is to square the sample correlation between observed and predicted values of the outcome. The same approach can be used in the multilevel model for change. All you need to do is: (1) compute the predicted outcome value for each person on each occasion of measurement; and (2) square the sample correlation between observed and predicted values. The resultant pseudo-\\(R^2\\) statistic assesses the proportion of total outcome variation “explained” by the multilevel model’s specific contribution of predictors. (p. 102, emphasis added) Singer and Willett called this \\(R_{y, \\hat y}^2\\). They then walked through an example with their Model B (fit4.2), the unconditional growth model. Within our brms paradigm, we typically use the fitted() function to return predicted outcome values for cases within the data. The default option for the fitted() function is to return these predictions after accounting for the level-2 clustering. As we will see, Singer and Willett’s \\(R_{y, \\hat y}^2\\) statistic only accounts for predictors (i.e., age_14, in this case), not clustering variables (i.e., id, in this case). To follow Singer and Willett’s specification, we need to set re_formula = NA, which will instruct fitted() to return the expected values without reference to the level-2 clustering. Here’s a look at the first six rows of that output. fitted(fit4.2, re_formula = NA) %&gt;% head() ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.6482574 0.10815896 0.4376040 0.8698159 ## [2,] 0.9211839 0.09978151 0.7282534 1.1251522 ## [3,] 1.1941103 0.12844911 0.9395229 1.4535722 ## [4,] 0.6482574 0.10815896 0.4376040 0.8698159 ## [5,] 0.9211839 0.09978151 0.7282534 1.1251522 ## [6,] 1.1941103 0.12844911 0.9395229 1.4535722 Within our Bayesian/brms paradigm, out expected values come with expressions of uncertainty in terms of the posterior standard deviation and percentile-based 95% intervals. If we followed Singer and Willett’s method in the text, we’d only work with the posterior means as presented within the Estimate column. But since we’re Bayesians, we should attempt to work with the model uncertainty. One approach is to set summary = F. f &lt;- fitted(fit4.2, summary = F, re_formula = NA) %&gt;% as_tibble() %&gt;% set_names(1:ncol(.)) %&gt;% rownames_to_column(&quot;iter&quot;) head(f) ## # A tibble: 6 x 247 ## iter `1` `2` `3` `4` `5` `6` `7` `8` `9` `10` `11` `12` `13` `14` `15` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.653 1.02 1.38 0.653 1.02 1.38 0.653 1.02 1.38 0.653 1.02 1.38 0.653 1.02 1.38 ## 2 2 0.620 0.956 1.29 0.620 0.956 1.29 0.620 0.956 1.29 0.620 0.956 1.29 0.620 0.956 1.29 ## 3 3 0.825 1.01 1.19 0.825 1.01 1.19 0.825 1.01 1.19 0.825 1.01 1.19 0.825 1.01 1.19 ## 4 4 0.766 0.976 1.19 0.766 0.976 1.19 0.766 0.976 1.19 0.766 0.976 1.19 0.766 0.976 1.19 ## 5 5 0.949 1.14 1.33 0.949 1.14 1.33 0.949 1.14 1.33 0.949 1.14 1.33 0.949 1.14 1.33 ## 6 6 0.904 1.12 1.34 0.904 1.12 1.34 0.904 1.12 1.34 0.904 1.12 1.34 0.904 1.12 1.34 ## # … with 231 more variables: `16` &lt;dbl&gt;, `17` &lt;dbl&gt;, `18` &lt;dbl&gt;, `19` &lt;dbl&gt;, `20` &lt;dbl&gt;, ## # `21` &lt;dbl&gt;, `22` &lt;dbl&gt;, `23` &lt;dbl&gt;, `24` &lt;dbl&gt;, `25` &lt;dbl&gt;, `26` &lt;dbl&gt;, `27` &lt;dbl&gt;, `28` &lt;dbl&gt;, ## # `29` &lt;dbl&gt;, `30` &lt;dbl&gt;, `31` &lt;dbl&gt;, `32` &lt;dbl&gt;, `33` &lt;dbl&gt;, `34` &lt;dbl&gt;, `35` &lt;dbl&gt;, `36` &lt;dbl&gt;, ## # `37` &lt;dbl&gt;, `38` &lt;dbl&gt;, `39` &lt;dbl&gt;, `40` &lt;dbl&gt;, `41` &lt;dbl&gt;, `42` &lt;dbl&gt;, `43` &lt;dbl&gt;, `44` &lt;dbl&gt;, ## # `45` &lt;dbl&gt;, `46` &lt;dbl&gt;, `47` &lt;dbl&gt;, `48` &lt;dbl&gt;, `49` &lt;dbl&gt;, `50` &lt;dbl&gt;, `51` &lt;dbl&gt;, `52` &lt;dbl&gt;, ## # `53` &lt;dbl&gt;, `54` &lt;dbl&gt;, `55` &lt;dbl&gt;, `56` &lt;dbl&gt;, `57` &lt;dbl&gt;, `58` &lt;dbl&gt;, `59` &lt;dbl&gt;, `60` &lt;dbl&gt;, ## # `61` &lt;dbl&gt;, `62` &lt;dbl&gt;, `63` &lt;dbl&gt;, `64` &lt;dbl&gt;, `65` &lt;dbl&gt;, `66` &lt;dbl&gt;, `67` &lt;dbl&gt;, `68` &lt;dbl&gt;, ## # `69` &lt;dbl&gt;, `70` &lt;dbl&gt;, `71` &lt;dbl&gt;, `72` &lt;dbl&gt;, `73` &lt;dbl&gt;, `74` &lt;dbl&gt;, `75` &lt;dbl&gt;, `76` &lt;dbl&gt;, ## # `77` &lt;dbl&gt;, `78` &lt;dbl&gt;, `79` &lt;dbl&gt;, `80` &lt;dbl&gt;, `81` &lt;dbl&gt;, `82` &lt;dbl&gt;, `83` &lt;dbl&gt;, `84` &lt;dbl&gt;, ## # `85` &lt;dbl&gt;, `86` &lt;dbl&gt;, `87` &lt;dbl&gt;, `88` &lt;dbl&gt;, `89` &lt;dbl&gt;, `90` &lt;dbl&gt;, `91` &lt;dbl&gt;, `92` &lt;dbl&gt;, ## # `93` &lt;dbl&gt;, `94` &lt;dbl&gt;, `95` &lt;dbl&gt;, `96` &lt;dbl&gt;, `97` &lt;dbl&gt;, `98` &lt;dbl&gt;, `99` &lt;dbl&gt;, ## # `100` &lt;dbl&gt;, `101` &lt;dbl&gt;, `102` &lt;dbl&gt;, `103` &lt;dbl&gt;, `104` &lt;dbl&gt;, `105` &lt;dbl&gt;, `106` &lt;dbl&gt;, ## # `107` &lt;dbl&gt;, `108` &lt;dbl&gt;, `109` &lt;dbl&gt;, `110` &lt;dbl&gt;, `111` &lt;dbl&gt;, `112` &lt;dbl&gt;, `113` &lt;dbl&gt;, ## # `114` &lt;dbl&gt;, `115` &lt;dbl&gt;, … With those settings, fitted() returned a \\(4,000 \\times 246\\) numeric array. The 4,000 rows corresponded to the 4,000 post-warmup HMC draws. Each of the 246 columns corresponded to one of the 246 rows in the original alcohol1_pp data. To make the output more useful, we converted it to a data frame, named the columns by the row numbers corresponding to the original alcohol1_pp data, and converted the row names to an iter column. In the next code block, we’ll convert f to the long format and use left_join() to join it with the relevant subset of the alcohol1_pp data. f &lt;- f %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;, values_to = &quot;fitted&quot;, names_ptypes = list(row = integer())) %&gt;% left_join( alcohol1_pp %&gt;% mutate(row = 1:n()) %&gt;% select(row, alcuse), by = &quot;row&quot; ) f ## # A tibble: 984,000 x 4 ## iter row fitted alcuse ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.653 1.73 ## 2 1 2 1.02 2 ## 3 1 3 1.38 2 ## 4 1 4 0.653 0 ## 5 1 5 1.02 0 ## 6 1 6 1.38 1 ## 7 1 7 0.653 1 ## 8 1 8 1.02 2 ## 9 1 9 1.38 3.32 ## 10 1 10 0.653 0 ## # … with 983,990 more rows If we collapse the distinction across the 4,000 HMC draws, here is the squared correlation between fitted and alcuse. f %&gt;% summarise(r = cor(fitted, alcuse), r2 = cor(fitted, alcuse)^2) ## # A tibble: 1 x 2 ## r r2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.186 0.0345 This is close to the \\(R_{y, \\hat y}^2 = .043\\) Singer and Willett reported in the text. It might seem unsatisfying how this seemingly ignores model uncertainty by collapsing across HMC iterations. Here’s a look at what happens is we compute the \\(R_{y, \\hat y}^2\\) separately for each iteration. f %&gt;% mutate(iter = iter %&gt;% as.double()) %&gt;% group_by(iter) %&gt;% summarise(r = cor(fitted, alcuse), r2 = cor(fitted, alcuse)^2) ## # A tibble: 4,000 x 3 ## iter r r2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.208 0.0434 ## 2 2 0.208 0.0434 ## 3 3 0.208 0.0434 ## 4 4 0.208 0.0434 ## 5 5 0.208 0.0434 ## 6 6 0.208 0.0434 ## 7 7 0.208 0.0434 ## 8 8 0.208 0.0434 ## 9 9 0.208 0.0434 ## 10 10 0.208 0.0434 ## # … with 3,990 more rows Now for every level of iter, \\(R_{y, \\hat y}^2 = .0434\\), which matches up nicely with the text. But it seems odd that the value should be the same for each of the 4,000 HMC draws. Sadly, my efforts to debug my workflow have been unsuccessful. If you see a flaw in this method, please share on GitHub. Just for kicks, here’s a more compact alternative to our fitted() + left_join() approach that more closely resembles the work flow Singer and Willett showed on pages 102 and 103. tibble(age_14 = 0:2) %&gt;% mutate(fitted = map(age_14, ~ post$b_Intercept + post$b_age_14 * .)) %&gt;% full_join(alcohol1_pp %&gt;% select(id, age_14, alcuse), by = &quot;age_14&quot;) %&gt;% mutate(row = 1:n()) %&gt;% unnest(fitted) %&gt;% mutate(iter = rep(1:4000, times = alcohol1_pp %&gt;% nrow())) %&gt;% group_by(iter) %&gt;% summarise(r = cor(fitted, alcuse), r2 = cor(fitted, alcuse)^2) ## # A tibble: 4,000 x 3 ## iter r r2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.208 0.0434 ## 2 2 0.208 0.0434 ## 3 3 0.208 0.0434 ## 4 4 0.208 0.0434 ## 5 5 0.208 0.0434 ## 6 6 0.208 0.0434 ## 7 7 0.208 0.0434 ## 8 8 0.208 0.0434 ## 9 9 0.208 0.0434 ## 10 10 0.208 0.0434 ## # … with 3,990 more rows Either way, our results agree with those in the text: about “4.3% of the total variability in ALCUSE is associated with linear time” (p. 103, emphasis in the original). 4.4.3.2 Pseudo-\\(R^2\\) statistics computed from the variance components. Residual variation–that portion of the outcome variation unexplained by a model’s predictors–provides another criterion for comparison. When you fit a series of models, you hope that added predictors further explain unexplained outcome variation, causing residual variation to decline. The magnitude of this decline quantifies the improvement in fit. A large decline suggests that the predictors make a big difference; a small, or zero, decline suggests that they do not. To assess these declines on a common scale, we compute the proportional reduction in residual variance as we add predictors. Each unconditional model yields residual variances that serve as yardsticks for comparison. The unconditional means model provides a baseline estimate of \\(\\sigma_\\epsilon^2\\); the unconditional growth model provides baseline estimates of \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\). Each leads to its own pseudo-\\(R^2\\) statistic. (p. 103, emphasis in the original) This provides three more pseudo-\\(R^2\\) statistics: \\(R_\\epsilon^2\\), \\(R_0^2\\), and \\(R_1^2\\). The formula for the first is \\[ R_\\epsilon^2 = \\frac{\\sigma_\\epsilon^2 (\\text{unconditional means model}) - \\sigma_\\epsilon^2 (\\text{unconditional growth model})}{\\sigma_\\epsilon^2 (\\text{unconditional means model})}. \\] We’ve actually already computed this one, above, under the name where we referred to it as the decline in \\(\\sigma_\\epsilon^2\\) after adding time to the model. Here it is again. cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~.^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(r_2_epsilon = (fit4.1 - fit4.2) / fit4.1) %&gt;% summarise(mean = mean(r_2_epsilon), median = median(r_2_epsilon), sd = sd(r_2_epsilon), ll = quantile(r_2_epsilon, prob = .025), ul = quantile(r_2_epsilon, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## mean median sd ll ul ## 1 0.341 0.356 0.136 0.036 0.561 Here’s a look at the full distribution for our \\(\\sigma_\\epsilon^2\\). cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~.^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(r_2_epsilon = (fit4.1 - fit4.2) / fit4.1) %&gt;% ggplot(aes(x = r_2_epsilon)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(expression(Pseudo~italic(R)[epsilon]^2), limits = c(-1, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) When we use the full posteriors of our two \\(\\epsilon_\\epsilon^2\\) parameters, we end up with a slightly smaller statistic than the one in the text. So our conclusion is about 35% of the intraindividual variance is accounted for by time. If we consider additional models with predictors for the \\(\\zeta\\)s, we can examine similar pseudo \\(R^2\\) statistics following the generic form \\[ R_\\zeta^2 = \\frac{\\sigma_\\zeta^2 (\\text{unconditional growth model}) - \\sigma_\\zeta^2 (\\text{subsequent model})}{\\sigma_\\zeta^2 (\\text{unconditional growth model})}, \\] where \\(\\zeta\\) could refer to \\(\\zeta_{0i}\\), \\(\\zeta_{1i}\\), and so on. If you look back up at the shape of the full posterior of \\(R_\\epsilon^2\\), you’ll notice part of the left tail crosses zero. “Unlike traditional \\(R^2\\) statistics, which will always be positive (or zero), some of these statistics can be negative” (p. 104)! If you compute them, interpret pseudo-\\(R^2\\) statistics with a grain of salt. 4.5 Practical data analytic strategies for model building A sound statistical model includes all necessary predictors and no unnecessary ones. But how do you separate the wheat from the chaff? We suggest you rely on a combination of substantive theory, research questions, and statistical evidence. Never let a computer select predictors mechanically. (pp. 104–105, emphasis in the original) 4.5.1 A taxonomy of statistical models. We suggest that you base decisions to enter, retain, and remove predictors on a combination of logic, theory, and prior research, supplemented by judicious [parameter evaluation] and comparison of model fit. At the outset, you might examine the effect of each predictor individually. You might then focus on predictors of primary interest (while including others whose effects you want to control). As in regular regression, you can add predictors singly or in groups and you can address issues of functional form using interactions and transformations. As you develop the taxonomy, you will progress toward a “final model” whose interpretation addresses your research questions. We place quotes around this term to emphasize that we believe no statistical model is ever final; it is simply a placeholder until a better model is found. (p. 105, emphasis in the original) 4.5.2 Interpreting fitted models. You need not interpret every model you fit, especially those designed to guide interim decision making. When writing up findings for presentation and publication, we suggest that you identify a manageable subset of models that, taken together, tells a persuasive story parsimoniously. At a minimum, this includes the unconditional means model, the unconditional growth model, and a “final model”. You may also want to present intermediate models that either provide important building blocks or tell interesting stories in their own right. (p. 106) In the dawn of the post-replication crisis era, it’s astonishing to reread and transcribe this section and the one above. I like a lot of what the authors had to say. Much of it seems like good pragmatic advice. But if they were to rewrite these sections again, I wonder what changes they’d make. Would they recommend researchers preregister their primary hypothesis, variables of interest, and perhaps their model building strategy? Would they be interested in a multiverse analysis? Would the still recommend sharing only a subset of one’s analyses in the era of sharing platforms like GitHub and the Open Science Framework? Would they weigh in on developments in causal inference? 4.5.2.1 Model C: The uncontrolled effects of COA. The default priors for Model C are the same as for the unconditional growth model. All we’ve done is add parameters of class = b. As these default to improper flat priors, we have nothing to add to the prior argument to include them. Feel free to check with get_prior(). For the sake of practice, this model follows the form \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\gamma_{10} \\text{age_14}_{ij} + \\gamma_{11} \\text{coa}_i \\times \\text{age_14}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\text{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\text{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\Bigg ). \\end{align*}\\] Fit the model. fit4.3 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + age_14:coa + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.03&quot;) Check the summary. print(fit4.3, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + coa + age_14:coa + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.700 0.101 0.500 0.898 1.003 823 1429 ## sd(age_14) 0.367 0.094 0.142 0.529 1.008 337 286 ## cor(Intercept,age_14) -0.105 0.287 -0.513 0.679 1.008 481 370 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.314 0.134 0.054 0.574 1.002 1998 2417 ## age_14 0.294 0.085 0.128 0.460 1.001 2775 2957 ## coa 0.739 0.201 0.334 1.132 1.002 2028 2141 ## age_14:coa -0.050 0.126 -0.303 0.197 1.001 2956 2987 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.608 0.052 0.517 0.720 1.007 425 674 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our \\(\\gamma\\)s are quite similar to those presented in the text. Our \\(\\sigma_\\epsilon\\) for this model is about the same as with fit4.2. Let’s practice with conditional_effects() to plot the consequences of this model. conditional_effects(fit4.3) This time we got back three plots. The first two were of the lower-order parameters \\(\\gamma_{10}\\) and \\(\\gamma_{01}\\). Note how the plot for coa treated it as a continuous variable. This is because the variable was saved as an integer in the original data set. fit4.3$data %&gt;% glimpse() ## Rows: 246 ## Columns: 5 ## $ alcuse &lt;dbl&gt; 1.732051, 2.000000, 2.000000, 0.000000, 0.000000, 1.000000, 1.000000, 2.000000,… ## $ Intercept &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9… Coding it as an integer further complicated things for the third plot returned by conditional_effects(), the one for the interaction of age_14 and coa, \\(\\gamma_{11}\\). Since coa is binary, the natural way to express its interaction with age_14 would be with age_14 on the \\(x\\)-axis and two separate trajectories, one for each value of coa. That’s what Singer and Willett very sensibly did with the middle panel of Figure 4.3. However, the conditional_effects()function defaults to expressing interactions such that the first variable in the term–in this case,age_14–is on the \\(x\\)-axis and the second variable in the term–coa, treated as an integer–is depicted in three lines corresponding its mean and its mean \\(\\pm\\) one standard deviation. This is great for continuous variables, but incoherent for categorical ones. The fix is to adjust the data and refit the model. fit4.4 &lt;- update(fit4.3, newdata = alcohol1_pp %&gt;% mutate(coa = factor(coa)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.04&quot;) We might compare the updated model with its predecessor. To get a focused look, we can use the posterior_summary() function with a little subsetting. posterior_summary(fit4.3)[1:4, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.314 0.134 0.054 0.574 ## b_age_14 0.294 0.085 0.128 0.460 ## b_coa 0.739 0.201 0.334 1.132 ## b_age_14:coa -0.050 0.126 -0.303 0.197 posterior_summary(fit4.4)[1:4, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.314 0.134 0.054 0.574 ## b_age_14 0.294 0.085 0.128 0.460 ## b_coa1 0.739 0.201 0.334 1.132 ## b_age_14:coa1 -0.050 0.126 -0.303 0.197 The results are about the same. The payoff comes when we try again with conditional_effects(). conditional_effects(fit4.4) Much better. Now the plot for \\(\\gamma_{01}\\) treats coa as binary and our plot for the interaction between age_14 and coa is much close to the one in Figure 4.3. Since we’re already on a conditional_effects() tangent, we may as well go further. When working with models like fit4.3 where you have multiple fixed effects, sometimes you only want the plots for a subset of those effects. For example, if our main goal is to do a good job tastefully reproducing the middle plot in Figure 4.3, we only need the interaction plot. In such a case, use the effects argument. conditional_effects(fit4.4, effects = &quot;age_14:coa&quot;) Earlier we discussed how conditional_effects() lets users adjust some of the output. But if you want an extensive overhaul, it’s better to save the output of conditional_effects() as an object and manipulate that object with the plot() function. ce &lt;- conditional_effects(fit4.4, effects = &quot;age_14:coa&quot;) str(ce) ## List of 1 ## $ age_14:coa:&#39;data.frame&#39;: 200 obs. of 13 variables: ## ..$ age_14 : num [1:200] 0 0 0.0202 0.0202 0.0404 ... ## ..$ coa : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## ..$ alcuse : num [1:200] 0.922 0.922 0.922 0.922 0.922 ... ## ..$ Intercept : num [1:200] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ intercept : num [1:200] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ id : logi [1:200] NA NA NA NA NA NA ... ## ..$ cond__ : Factor w/ 1 level &quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## ..$ effect1__ : num [1:200] 0 0 0.0202 0.0202 0.0404 ... ## ..$ effect2__ : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## ..$ estimate__: num [1:200] 0.318 1.055 0.324 1.059 0.33 ... ## ..$ se__ : num [1:200] 0.135 0.146 0.134 0.145 0.133 ... ## ..$ lower__ : num [1:200] 0.0545 0.7652 0.0612 0.7713 0.0689 ... ## ..$ upper__ : num [1:200] 0.574 1.344 0.578 1.347 0.583 ... ## ..- attr(*, &quot;effects&quot;)= chr [1:2] &quot;age_14&quot; &quot;coa&quot; ## ..- attr(*, &quot;response&quot;)= chr &quot;alcuse&quot; ## ..- attr(*, &quot;surface&quot;)= logi FALSE ## ..- attr(*, &quot;categorical&quot;)= logi FALSE ## ..- attr(*, &quot;ordinal&quot;)= logi FALSE ## ..- attr(*, &quot;points&quot;)=&#39;data.frame&#39;: 246 obs. of 6 variables: ## .. ..$ age_14 : num [1:246] 0 1 2 0 1 2 0 1 2 0 ... ## .. ..$ coa : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## .. ..$ resp__ : num [1:246] 1.73 2 2 0 0 ... ## .. ..$ cond__ : Factor w/ 1 level &quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## .. ..$ effect1__: num [1:246] 0 1 2 0 1 2 0 1 2 0 ... ## .. ..$ effect2__: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## - attr(*, &quot;class&quot;)= chr &quot;brms_conditional_effects&quot; Our ce is an object of class “’brms_conditional_effects” which contains a list of a single data frame. Had we omitted our effects argument, above, we’d have a list of 3 instead. Anyway, these data frames contain the necessary information to produce the plot. The advantage of saving ce this way is we can now insert it into the plot() function. The simple output is the same as before. plot(ce) The plot() function will allow us to do other things, like add in the original data or omit the white grid lines. ce %&gt;% plot(points = T, point_args = list(size = 1/4, alpha = 1/4, width = .05, height = .05, color = &quot;black&quot;), theme = theme(panel.grid = element_blank())) And for even more control, you can tack on typical ggplot2 functions. But when you want to do so, make sure to set the plot = FALSE argument and then subset after the right parenthesis of the plot() function. plot(ce, theme = theme(legend.position = &quot;none&quot;, panel.grid = element_blank()), plot = FALSE)[[1]] + annotate(geom = &quot;text&quot;, x = 2.1, y = c(.95, 1.55), label = str_c(&quot;coa = &quot;, 0:1), hjust = 0, size = 3.5) + scale_fill_brewer(type = &quot;qual&quot;) + scale_color_brewer(type = &quot;qual&quot;) + scale_x_continuous(&quot;age&quot;, limits = c(-1, 3), labels = 13:17) + scale_y_continuous(limits = c(0, 2), breaks = 0:2) But anyway, let’s get back on track and talk about the variance components. Singer and Willett contrasted \\(\\sigma_\\epsilon^2\\) from Model B to the new one from Model C. We might use VarCorr() to do the same. VarCorr(fit4.2)[[2]] ## $sd ## Estimate Est.Error Q2.5 Q97.5 ## 0.6091221 0.05178938 0.5178184 0.7180072 VarCorr(fit4.3)[[2]] ## $sd ## Estimate Est.Error Q2.5 Q97.5 ## 0.6075042 0.05160899 0.5173927 0.7195604 We could have also extracted that information by subsetting posterior_summary(). posterior_summary(fit4.2)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.60912214 0.05178938 0.51781844 0.71800716 posterior_summary(fit4.3)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.60750423 0.05160899 0.51739275 0.71956038 Anyway, to get these in a variance metric, just square their posterior samples and summarize. Our next task is to formally compare fit4.2 and fit4.3 in terms of declines in \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\). bind_cols( posterior_samples(fit4.2) %&gt;% transmute(fit2_sigma_2_0 = sd_id__Intercept^2, fit2_sigma_2_1 = sd_id__age_14^2), posterior_samples(fit4.3) %&gt;% transmute(fit3_sigma_2_0 = sd_id__Intercept^2, fit3_sigma_2_1 = sd_id__age_14^2) ) %&gt;% mutate(`decline~&#39;in&#39;~sigma[0]^2` = (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0, `decline~&#39;in&#39;~sigma[1]^2` = (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) %&gt;% pivot_longer(contains(&quot;decline&quot;)) %&gt;% ggplot(aes(x = value)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(NULL, limits = c(-5, 2)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, labeller = label_parsed, ncol = 1) Here are the percents of variance declined from fit4.2 to fit4.3. bind_cols( posterior_samples(fit4.2) %&gt;% transmute(fit2_sigma_2_0 = sd_id__Intercept^2, fit2_sigma_2_1 = sd_id__age_14^2), posterior_samples(fit4.3) %&gt;% transmute(fit3_sigma_2_0 = sd_id__Intercept^2, fit3_sigma_2_1 = sd_id__age_14^2) ) %&gt;% mutate(`decline~&#39;in&#39;~sigma[0]^2` = (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0, `decline~&#39;in&#39;~sigma[1]^2` = (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) %&gt;% pivot_longer(contains(&quot;decline&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) ## # A tibble: 2 x 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 decline~&#39;in&#39;~sigma[0]^2 0.152 0.207 0.335 -0.659 0.650 ## 2 decline~&#39;in&#39;~sigma[1]^2 -1.96 -0.0549 27.2 -6.91 0.873 In this case, we end up with massive uncertainty when working with the full posteriors. This is particularly the case with the difference in \\(\\sigma_1^2\\), which is left skewed for days. Here are the results when we only use point estimates. bind_cols( posterior_samples(fit4.2) %&gt;% transmute(fit2_sigma_2_0 = sd_id__Intercept^2, fit2_sigma_2_1 = sd_id__age_14^2), posterior_samples(fit4.3) %&gt;% transmute(fit3_sigma_2_0 = sd_id__Intercept^2, fit3_sigma_2_1 = sd_id__age_14^2) ) %&gt;% summarise_all(median) %&gt;% transmute(`% decline in sigma_2_0` = 100 * (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0, `% decline in sigma_2_1` = 100 * (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) ## % decline in sigma_2_0 % decline in sigma_2_1 ## 1 20.73695 -4.800244 “These variance components are now called partial or conditional variances because they quantify the interindividual differences in change that remain unexplained by the model’s predictors” (p. 108, emphasis in the original). 4.5.2.2 Model D: The controlled effects of COA. This model follows the form \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\gamma_{02} \\text{peer}_i + \\gamma_{10} \\text{age_14}_{ij} \\\\ &amp; \\;\\;\\; + \\gamma_{11} \\text{coa}_i \\times \\text{age_14}_{ij} + \\gamma_{12} \\text{peer}_i \\times \\text{age_14}_{ij} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\text{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\text{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\Bigg ). \\end{align*}\\] Fit that joint. fit4.5 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:coa + age_14:peer + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.05&quot;) print(fit4.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:coa + age_14:peer + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.481 0.099 0.291 0.677 1.001 687 1661 ## sd(age_14) 0.363 0.080 0.205 0.519 1.008 296 649 ## cor(Intercept,age_14) 0.122 0.334 -0.415 0.866 1.004 307 457 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.315 0.149 -0.612 -0.009 1.001 1573 2265 ## age_14 0.434 0.118 0.201 0.667 1.004 1421 2066 ## coa 0.578 0.168 0.239 0.911 1.000 2305 2525 ## peer 0.694 0.116 0.464 0.920 1.001 1550 1976 ## age_14:coa -0.017 0.129 -0.268 0.231 1.003 2230 2567 ## age_14:peer -0.151 0.089 -0.329 0.025 1.002 1551 2126 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.607 0.048 0.519 0.703 1.002 500 1300 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). All our \\(\\gamma\\) estimates are similar to those presented in Table 4.1. Let’s compute the variances and the covariance, \\(\\sigma_{01}^2\\). Here are the plots. v &lt;- posterior_samples(fit4.5) %&gt;% transmute(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__age_14^2, sigma_01 = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14) v %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) And now we compute the summary statistics. v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 4 x 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sigma_01 0.006 0.013 0.054 -0.12 0.091 ## 2 sigma_2_0 0.242 0.233 0.097 0.084 0.458 ## 3 sigma_2_1 0.138 0.132 0.059 0.042 0.269 ## 4 sigma_2_epsilon 0.371 0.368 0.058 0.269 0.494 Like the \\(\\gamma\\)’s, our variance components are all similar to those in the text. bind_cols( posterior_samples(fit4.2) %&gt;% transmute(fit4.2_sigma_2_epsilon = sigma^2, fit4.2_sigma_2_0 = sd_id__Intercept^2, fit4.2_sigma_2_1 = sd_id__age_14^2), posterior_samples(fit4.5) %&gt;% transmute(fit4.5_sigma_2_epsilon = sigma^2, fit4.5_sigma_2_0 = sd_id__Intercept^2, fit4.5_sigma_2_1 = sd_id__age_14^2) ) %&gt;% summarise_all(median) %&gt;% mutate(`% decline in sigma_2_epsilon` = 100 * (fit4.2_sigma_2_epsilon - fit4.5_sigma_2_epsilon) / fit4.2_sigma_2_epsilon, `% decline in sigma_2_0` = 100 * (fit4.2_sigma_2_0 - fit4.5_sigma_2_0) / fit4.2_sigma_2_0, `% decline in sigma_2_1` = 100 * (fit4.2_sigma_2_1 - fit4.5_sigma_2_1) / fit4.2_sigma_2_1) %&gt;% pivot_longer(contains(&quot;%&quot;)) %&gt;% select(name, value) ## # A tibble: 3 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 % decline in sigma_2_epsilon -0.299 ## 2 % decline in sigma_2_0 62.4 ## 3 % decline in sigma_2_1 2.47 The percentages in which our variance componence declined relative to the unconditional growth model are of similar orders of magnitude as those presented in the text. 4.5.2.3 Model E: A tentative “final model” for the controlled effects of coa. This model is just like the last, but with the simple omission of the \\(\\gamma_{12}\\) parameter. fit4.6 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.06&quot;) print(fit4.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.482 0.102 0.274 0.676 1.002 574 1118 ## sd(age_14) 0.362 0.083 0.200 0.521 1.008 303 687 ## cor(Intercept,age_14) 0.120 0.343 -0.419 0.879 1.006 267 346 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.313 0.156 -0.626 -0.006 1.000 1732 2244 ## age_14 0.423 0.109 0.206 0.640 1.001 1821 2419 ## coa 0.572 0.151 0.279 0.879 1.003 1939 2531 ## peer 0.695 0.116 0.464 0.917 1.002 1857 2474 ## age_14:peer -0.150 0.087 -0.318 0.024 1.000 1754 2224 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.606 0.048 0.516 0.705 1.005 361 1112 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The \\(\\gamma\\)’s all look well behaved. Here are the variance component summaries. v &lt;- posterior_samples(fit4.6) %&gt;% transmute(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__age_14^2, sigma_01 = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14) v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 4 x 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sigma_01 0.005 0.01 0.054 -0.116 0.093 ## 2 sigma_2_0 0.243 0.233 0.098 0.075 0.457 ## 3 sigma_2_1 0.138 0.132 0.06 0.04 0.271 ## 4 sigma_2_epsilon 0.37 0.365 0.059 0.267 0.497 4.5.3 Displaying prototypical change trajectories. On page 111, Singer and Willett computed the various levels of the \\(\\pi\\) coefficients when coa == 0 or coa == 1. To follow along, we’ll want to work directly with the posterior samples of fit4.3. post &lt;- posterior_samples(fit4.3) post %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% head() ## b_Intercept b_age_14 b_coa b_age_14:coa ## 1 0.2618439 0.2495084 0.8431245 -0.08459538 ## 2 0.2911188 0.3038611 0.9213667 -0.10428155 ## 3 0.3188864 0.2983608 0.8543070 -0.09359624 ## 4 0.1661855 0.3694042 0.9406988 -0.20639550 ## 5 0.3612977 0.2752802 0.6327648 0.04498002 ## 6 0.2614384 0.2797979 0.7827562 -0.07184190 Here we apply the formulas to the posterior samples and then summarize with posterior means. post %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% transmute(pi_0_coa0 = b_Intercept + b_coa * 0, pi_1_coa0 = b_age_14 + `b_age_14:coa` * 0, pi_0_coa1 = b_Intercept + b_coa * 1, pi_1_coa1 = b_age_14 + `b_age_14:coa` * 1) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(posterior_mean = mean(value) %&gt;% round(digits = 3)) ## # A tibble: 4 x 2 ## name posterior_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 pi_0_coa0 0.314 ## 2 pi_0_coa1 1.05 ## 3 pi_1_coa0 0.294 ## 4 pi_1_coa1 0.245 We already plotted these trajectories and their 95% intervals a few sections up. If we want to work with the full composite model to predict \\(Y_{ij}\\) (i.e., alcuse) directly, we multiply the b_coa, b_age_14, and b_age_14:coa vectors by the appropriate values of coa and peer. For example, here’s what you’d code if you wanted the initial alcuse status for when coa == 1. post %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(y = b_Intercept + b_coa * 1 + b_age_14 * 0 + `b_age_14:coa` * 0 * 1) %&gt;% head() ## b_Intercept b_age_14 b_coa b_age_14:coa y ## 1 0.2618439 0.2495084 0.8431245 -0.08459538 1.1049684 ## 2 0.2911188 0.3038611 0.9213667 -0.10428155 1.2124855 ## 3 0.3188864 0.2983608 0.8543070 -0.09359624 1.1731934 ## 4 0.1661855 0.3694042 0.9406988 -0.20639550 1.1068843 ## 5 0.3612977 0.2752802 0.6327648 0.04498002 0.9940625 ## 6 0.2614384 0.2797979 0.7827562 -0.07184190 1.0441946 If you were to take the mean of that new y column, you’d discover it’s the same as the mean of our pi_0_coa1, above. post %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(y = b_Intercept + b_coa * 1 + b_age_14 * 0 + `b_age_14:coa` * 0 * 1) %&gt;% summarise(pi_0_coa1 = mean(y)) ## pi_0_coa1 ## 1 1.053244 Singer and Willett suggested four strategies to help researchers pick the prototypical values of the predictors to focus on: Substantively interesting values (e.g., typical ages, values corresponding to transition points) A range of percentiles (e.g., 25th, 50th, and 75th) Just the sample mean The sample mean \\(\\pm\\) something like 1 standard deviation They next discuss the right panel of Figure 4.3. We could continue to work directly with the posterior_samples() to make our version of that figure. But it you want to accompany the posterior mean trajectories with their 95% intervals, and I hope you do, the posterior_samples() method will get tedious. Happily, brms offers users and alternative with the fitted() function. Since the right panel is somewhat complicated, it’ll behove us to practice with the simpler left panel, first. In fit4.2 (i.e., Model C), age_14 is the only predictor. Here we’ll specify the values along the range in the original data, ranging from 0 to 2. However, we end up specifying a bunch of values within that range in addition to the two endpoints. This is because the 95% intervals typically have a bowtie shape. To depict that shape well, we need more than a couple values. We save those values as a tibble called nd (i.e., new data). We make use of them within fitted with the newdata = nd argument. Since we’re only interested in the general trajectory, the consequence of the \\(\\gamma\\)’s, we end up coding re_formula = NA. In so doing, we ask fitted() to ignore the group-level effects. In this example, that means we are ignoring the id-level deviations from the overall trajectories. If you’re confused by that that means, don’t worry. That part of the model should become more clear as we go along in the text. Since fitted() returns an array, we then convert the results into a data frame for use within the tidyverse framework. For plotting, it’s handy to bind those results together with the nd, the predictor values we used to compute the fitted values with. In the final wrangling step, we use our age_14 values to compute the age values. nd &lt;- tibble(age_14 = seq(from = 0, to = 2, length.out = 30)) f &lt;- fitted(fit4.2, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(age = age_14 + 14) head(f) ## Estimate Est.Error Q2.5 Q97.5 age_14 age ## 1 0.6482574 0.1081590 0.4376040 0.8698159 0.00000000 14.00000 ## 2 0.6670799 0.1063590 0.4620157 0.8842208 0.06896552 14.06897 ## 3 0.6859024 0.1047164 0.4842801 0.8978622 0.13793103 14.13793 ## 4 0.7047250 0.1032386 0.5068118 0.9121371 0.20689655 14.20690 ## 5 0.7235475 0.1019329 0.5299203 0.9271172 0.27586207 14.27586 ## 6 0.7423700 0.1008058 0.5503669 0.9437117 0.34482759 14.34483 Since we only had one predictor, age_14, for which we specified 30 specific values, we ended up with 30 rows in our output. By default, fitted() summarized the fitted values with posterior means (Estimate), standard deviations (Est.Error), and percentile-based 95% intervals (Q2.5 and Q97.5). The other columns are the values we bound to them. Here’s how we might use these to make our fitted() version of the leftmost panel of Figure 4.3. f %&gt;% ggplot(aes(x = age)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;, alpha = 3/4) + geom_line(aes(y = Estimate)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2, limits = c(0, 2)) + coord_cartesian(xlim = c(13, 17)) + theme(panel.grid = element_blank()) With fit4.6 (i.e., Model E), we now have three predictors. We’d like to see the full range across age_14 for four combinations of coa and peer values. To my mind, the easiest way to get those values right is with a little crossing() and expand(). nd &lt;- crossing(coa = 0:1, peer = c(.655, 1.381)) %&gt;% expand(nesting(coa, peer), age_14 = seq(from = 0, to = 2, length.out = 30)) head(nd, n = 10) ## # A tibble: 10 x 3 ## coa peer age_14 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.655 0 ## 2 0 0.655 0.0690 ## 3 0 0.655 0.138 ## 4 0 0.655 0.207 ## 5 0 0.655 0.276 ## 6 0 0.655 0.345 ## 7 0 0.655 0.414 ## 8 0 0.655 0.483 ## 9 0 0.655 0.552 ## 10 0 0.655 0.621 Now we use fitted() much like before. f &lt;- fitted(fit4.6, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # a little wrangling will make plotting much easier mutate(age = age_14 + 14, coa = ifelse(coa == 0, &quot;coa = 0&quot;, &quot;coa = 1&quot;), peer = factor(peer)) glimpse(f) ## Rows: 120 ## Columns: 8 ## $ Estimate &lt;dbl&gt; 0.1425897, 0.1649825, 0.1873752, 0.2097680, 0.2321607, 0.2545535, 0.2769462, 0.… ## $ Est.Error &lt;dbl&gt; 0.1134501, 0.1117202, 0.1101835, 0.1088480, 0.1077213, 0.1068100, 0.1061196, 0.… ## $ Q2.5 &lt;dbl&gt; -0.078688764, -0.053886847, -0.028335511, -0.004650132, 0.020322470, 0.04483448… ## $ Q97.5 &lt;dbl&gt; 0.3669462, 0.3856548, 0.4047552, 0.4244724, 0.4453436, 0.4642707, 0.4854083, 0.… ## $ coa &lt;chr&gt; &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;c… ## $ peer &lt;fct&gt; 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.… ## $ age_14 &lt;dbl&gt; 0.00000000, 0.06896552, 0.13793103, 0.20689655, 0.27586207, 0.34482759, 0.41379… ## $ age &lt;dbl&gt; 14.00000, 14.06897, 14.13793, 14.20690, 14.27586, 14.34483, 14.41379, 14.48276,… For our version of the right panel of Figure 4.3, most of the action is in ggplot(), geom_ribbon(), geom_line(), and facet_wrap(). All the rest is cosmetic. f %&gt;% ggplot(aes(x = age, color = peer, fill = peer)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate, size = peer)) + scale_size_manual(values = c(1/2, 1)) + scale_fill_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + scale_color_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2) + labs(subtitle = &quot;High peer values are in red; low ones are in blue.&quot;) + coord_cartesian(xlim = c(13, 17)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~coa) In my opinion, it works better to split the plot into two when you include the 95% intervals. 4.5.4 Recentering predictors to improve interpretation. The easiest strategy for recentering a time-invariant predictor is to subtract its sample mean from each observed value. When we center a predictor on its sample mean, the level-2 fitted intercepts represent the average fitted values of initial status (or rate of change). We can also recenter a time-invariant predictor by subtracting another meaningful value… Recentering works best when the centering constant is substantively meaningful. (pp. 113–114) As we’ll see later, centering can also make it easier to select meaningful priors on the model intercept. If you look at our alcohol1_pp data, you’ll see we already have centered versions of our time-invariant predictors. They’re the last two columns, cpeer and ccoa. alcohol1_pp %&gt;% glimpse() ## Rows: 246 ## Columns: 9 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 1… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ male &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0… ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0… ## $ alcuse &lt;dbl&gt; 1.732051, 2.000000, 2.000000, 0.000000, 0.000000, 1.000000, 1.000000, 2.000000, 3.… ## $ peer &lt;dbl&gt; 1.2649111, 1.2649111, 1.2649111, 0.8944272, 0.8944272, 0.8944272, 0.8944272, 0.894… ## $ cpeer &lt;dbl&gt; 0.2469111, 0.2469111, 0.2469111, -0.1235728, -0.1235728, -0.1235728, -0.1235728, -… ## $ ccoa &lt;dbl&gt; 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549… If you wanted to center them by hand, you’d just execute something like this. alcohol1_pp %&gt;% mutate(peer_c = peer - mean(peer)) ## # A tibble: 246 x 10 ## id age coa male age_14 alcuse peer cpeer ccoa peer_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 14 1 0 0 1.73 1.26 0.247 0.549 0.247 ## 2 1 15 1 0 1 2 1.26 0.247 0.549 0.247 ## 3 1 16 1 0 2 2 1.26 0.247 0.549 0.247 ## 4 2 14 1 1 0 0 0.894 -0.124 0.549 -0.123 ## 5 2 15 1 1 1 0 0.894 -0.124 0.549 -0.123 ## 6 2 16 1 1 2 1 0.894 -0.124 0.549 -0.123 ## 7 3 14 1 1 0 1 0.894 -0.124 0.549 -0.123 ## 8 3 15 1 1 1 2 0.894 -0.124 0.549 -0.123 ## 9 3 16 1 1 2 3.32 0.894 -0.124 0.549 -0.123 ## 10 4 14 1 1 0 0 1.79 0.771 0.549 0.771 ## # … with 236 more rows Did you notice how our peer_c values, above, deviated slightly from those in cpeer? That’s because peer_c was based on the exact sample mean. Those in cpeer are based on the sample mean as provided in the text, 1.018, which is introduces rounding error. For the sake of simplicity, we’ll go with centered variables matching up with the text. Here we’ll hastily fit the models with help from the update() function. fit4.7 &lt;- update(fit4.6, newdata = alcohol1_pp, alcuse ~ 0 + Intercept + age_14 + coa + cpeer + age_14:cpeer + (1 + age_14 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.07&quot;) fit4.8 &lt;- update(fit4.6, newdata = alcohol1_pp, alcuse ~ 0 + Intercept + age_14 + ccoa + peer + age_14:peer + (1 + age_14 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.08&quot;) Here we reproduce the \\(\\gamma\\)s from fit4.6 and compare the to the updates from fit4.7 and fit4.8. fixef(fit4.6) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.313 0.156 -0.626 -0.006 ## age_14 0.423 0.109 0.206 0.640 ## coa 0.572 0.151 0.279 0.879 ## peer 0.695 0.116 0.464 0.917 ## age_14:peer -0.150 0.087 -0.318 0.024 fixef(fit4.7) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.393 0.107 0.177 0.602 ## age_14 0.271 0.065 0.146 0.397 ## coa 0.572 0.149 0.287 0.870 ## cpeer 0.692 0.113 0.465 0.907 ## age_14:cpeer -0.150 0.087 -0.323 0.016 fixef(fit4.8) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.059 0.140 -0.327 0.224 ## age_14 0.424 0.111 0.209 0.642 ## ccoa 0.572 0.148 0.285 0.863 ## peer 0.697 0.113 0.478 0.921 ## age_14:peer -0.150 0.087 -0.321 0.023 4.6 Comparing models using deviance statistics As you will see, we will also make use of deviance within our Bayesian Stan-based paradigm. But we’ll do so a little differently than what Singer and Willett presented. 4.6.1 The deviance statistic. As it turns out, we Bayesians use the log-likelihood (LL), too. Recall how the numerator in the right-hand side of Bayes’ Theorem was \\(p(\\text{data} | \\theta) p(\\theta)\\)? That first part, \\(p(\\text{data} | \\theta)\\), is the likelihood. In words, the likelihood is the probability of the data given the parameters. We generally work with the log of the likelihood rather than the likelihood itself because it’s easier to work with statistically. When you’re working with brms, you can extract the LL with the log_lik() function. Here’s an example with fit4.1, our unconditional means model. log_lik(fit4.1) %&gt;% str() ## num [1:4000, 1:246] -0.654 -0.788 -0.795 -0.65 -0.587 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL You may have noticed we didn’t just get a single value back. Rather, we got an array of 4,000 rows and 246 columns. The reason we got 4,000 rows is because that’s how many post-warmup iterations we drew from the posterior. I.e., we set brm(..., iter = 2000, warmup = 1000, chains = 4). With respect to the 246 columns, that’s how many rows there are in the alcohol1_pp data. So for each case in the data, we get an entire posterior distribution of LL values. With the multilevel model, we can define deviance for a given model as its LL times -2, \\[ \\text{Deviance} = -2 LL_\\text{current model}. \\] Here that is in code for fit4.1. ll &lt;- log_lik(fit4.1) %&gt;% data.frame() %&gt;% mutate(ll = rowSums(.)) %&gt;% mutate(deviance = -2 * ll) %&gt;% select(ll, deviance, everything()) dim(ll) ## [1] 4000 248 Because we used HMC, deviance is a distribution rather than a single number. Here’s what it looks like for fit4.1. ll %&gt;% ggplot(aes(x = deviance)) + geom_density(fill = &quot;grey25&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Much like the frequentists, we Bayesian generally prefer models with smaller deviance distributions. The reasons frequentists multiply the LL by -2 is because after doing so, the difference in deviances between two models follows a \\(\\chi^2\\) distribution and the old \\(\\chi^2\\)-difference test is widely-used in frequentist statistics. Bayesians often just go ahead and use the -2 multiplication, too. It’s largely out of tradition. But as we’ll see, some contemporary Bayesians are challenging that tradition. 4.6.2 When and how can you compare deviance statistics? As for the frequentists, deviance values/distributions in the Bayesian context are only meaningful in the relative sense. You cannot directly interpret a single models deviance distribution by the magnitude or sign of its central tendency. But you can compare two or more models by the relative locations of their deviance distributions. When doing so, they must have been computed using the same data (i.e., no differences in missingness in the predictors) and the models must be nested. However, in contemporary Bayesian practice we don’t tend to compare models with deviance. For details on why, check out Chapter 6 in McElreath’s Statistical Rethinking. McElreath also covered the topic in several online lectures (e.g., here and here). 4.6.3 Implementing deviance-based hypothesis tests. In this project, we are not going to practice comparing deviances using frequentist \\(\\chi^2\\) tests. We will, however, cover Bayesian information criteria. 4.6.4 AIC and BIC WAIC and LOO statistics: Comparing nonnested models using information criteria [and cross validation]. We do not use the AIC or the BIC within the Stan ecosystem. The AIC is frequentist and cannot handle models with priors. The BIC is interesting in it’s a double misnomer. It is neither Bayesian nor is it a proper information criterion–though it does scale like one. However, it might be useful for our purposes to walk out the AIC a bit. It’ll ground our discussion of the WAIC and LOO. From Spiegelhalter, Best, Carlin and van der Linde (2014), we read: Suppose that we have a given set of candidate models, and we would like a criterion to assess which is ‘better’ in a defined sense. Assume that a model for observed data \\(y\\) postulates a density \\(p(y | \\theta)\\) (which may include covariates etc.), and call \\(D(\\theta) = -2 \\log {p(y | \\theta)}\\) the deviance, here considered as a function of \\(\\theta\\). Classical model choice uses hypothesis testing for comparing nested models, e.g. the deviance (likelihood ratio) test in generalized linear models. For non nested models, alternatives include the Akaike information criterion \\[AIC = -2 \\log {p(y | \\hat{\\theta})} + 2k\\] where \\(\\hat{\\theta}\\) is the maximum likelihood estimate and \\(k\\) is the number of parameters in the model (dimension of \\(\\Theta\\)). AIC is built with the aim of favouring models that are likely to make good predictions. Since we generally do not have independent validation data, we can assess which model best predicts the observed data by using the deviance, but if parameters have been estimated we need some penalty for this double use of the data. AIC’s penalty of 2k has been shown to be asymptotically equivalent to leave-one-out cross-validation. However, AIC does not work in models with informative prior information, such as hierarchical models, since the prior effectively acts to ‘restrict’ the freedom of the model parameters, so the appropriate ‘number of parameters’ is generally unclear. (pp. 485–486, emphasis in the original) For the past two decades, the Deviance Information Criterion (DIC; Spiegelhalter, Best, Carlin, &amp; van der Linde, 2002) has been a popular information criterion among Bayesians. Let’s define \\(D\\) as the posterior distribution of deviance values and \\(\\bar D\\) as its mean. If you compute deviance based on the posterior mean, you have \\(\\hat D\\). Within a multi-parameter model, this would be the deviance based on the collection of the posterior mean of each parameter. With these, we define the DIC as \\[\\text{DIC} = \\bar D + (\\bar D + \\hat D) + \\bar D + p_D,\\] where \\(p_D\\) is the number of effective parameters in the model, which is also sometimes referred to as the penalty term. As McElreath pointed out in Statistical Rethinking, the \\(p_D\\) is just the expected distance between the deviance in-sample and the deviance out-of-sample. In the case of flat priors, DIC reduces directly to AIC, because the expected distance is just the number of parameters. But more generally, \\(p_D\\) will be some fraction of the number of parameters, because regularizing priors constrain a model’s flexibility. (p. 191) As you’ll see, you can get the \\(p_D\\) for brms::brm() models. However, the DIC is limited in that it requires a multivariate Gaussian posterior and I’m not aware of a convenience function within brms that will compute the DIC. Which is fine. The DIC has been overshadowed in recent years by newer methods. But for a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. 4.6.4.1 The Widely Applicable Information Criterion (WAIC). The main information criterion within our Stan ecosystem paradigm is the Widely Applicable Information Criterion (WAIC; Watanabe, 2010). From McElreath, again, we read: It does not require a multivariate Gaussian posterior, and it is often more accurate than DIC. There are types of models for which it is hard to define at all, however. We’ll discuss that issue more, after defining WAIC. The distinguishing feature of WAIC is that it is pointwise. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data. This is useful, because some observations are much harder to predict than others and may also have different uncertainty… You can think of WAIC as handling uncertainty where it actually matters: for each independent observation. Define \\(\\text{Pr} (y_i)\\) as the average likelihood of observation \\(i\\) in the training sample. This means we compute the likelihood of \\(y_i\\) for each set of parameters samples from the posterior distribution. Then we average the likelihoods for each observation \\(i\\) and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density, \\[\\text{lppd} = \\sum_{i = 1}^N \\text{log Pr} (y_i)\\] You might say this out loud as: The log-pointwise-predictive-density is the total across observations of the logarithm of the average likelihood of each observation. The lppd is just a pointwise analog of deviance, averaged over the posterior distribution. If you multiplied it by -2, it’d be similar to the deviance, in fact. The second piece of WAIC is the effective number of parameters \\(p_\\text{WAIC}\\). Define \\(V(y_i)\\) as the variance in log-likelihood for observation \\(i\\) in the training sample. This means we compute the log-likelihood of \\(y_i\\) for each sample from the posterior distribution. Then we take the variance of those values. This is \\(V(y_i)\\). Now \\(p_\\text{WAIC}\\) is defined as: \\[p_\\text{WAIC} = \\sum_{i=1}^N V(y_i)\\] Now WAIC is defined as: \\[\\text{WAIC} = -2(\\text{lppd} - p_\\text{WAIC})\\] And this value is yet another estimate of out-of-sample deviance. (pp. 191–192, emphasis in the original) In Chapter 6 of my project translating McElreath’s Statistical Rethinking into brms and tidyverse code, I walk out how to hand compute the WAIC for a brm() fit. I’m not going to repeat the exercise, here. But do see the project and McElreath’s text if you’re interested. Rather, I’d like to get down to business. In brms, you can get a model’s WAIC with the waic() function. waic(fit4.1) ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_waic -312.2 12.0 ## p_waic 54.8 4.7 ## waic 624.5 24.0 ## ## 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying loo instead. We’ll come back to that warning message, later. For now, notice the main output is a \\(3 \\times 2\\) data frame with named rows. For the statistic in each row, you get a point estimate and a standard error. The WAIC is on the bottom. The effective number of parameters, the \\(p_\\text{WAIC}\\), is in the middle. Notice the elpd_waic on the top. That’s what you get without the \\(-2 \\times ...\\) in the formula. Remember how that part is just to put things in a metric amenable to \\(\\chi^2\\)-difference testing? Well, not all Bayesians like that and within the Stan ecosystem you’ll also see the WAIC expressed instead as the \\(\\text{elpd}_\\text{WAIC}\\). The current recommended workflow within brms is to attach the WAIC information to the model fit. You do it with the add_criterion() function. fit4.1 &lt;- add_criterion(fit4.1, &quot;waic&quot;) And now you can access that information directly with good-old $ indexing. fit4.1$criteria$waic ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_waic -312.2 12.0 ## p_waic 54.8 4.7 ## waic 624.5 24.0 ## ## 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying loo instead. You might notice how that value is similar to the AIC and BIC values for Model A in Table 4.1. But it’s not identical and we shouldn’t expect it to be. It was computed by a different formula that accounts for priors. For our purposes, this is much better than the frequentist AIC and BIC. We need statistics that can handle priors. 4.6.4.2 Leave-one-out cross-validation (LOO-CV). We have another big option for model comparison within the Stan ecosystem. It involves leave-one-out cross-validation (LOO-CV). It’s often the case that we aren’t just interested in modeling the data we have in hand. The hope is our findings would generalize to other data we could have collected or may collect in the future. We’d like our findings to tell us something more general about the world at large. But unless you’re studying something highly uniform like the weights of hydrogen atoms, chances are your data have idiosyncrasies that won’t generalize well to other data. Sure, if we had all the information on all the relevant variables, we could explain the discrepancies across samples with hidden moderators and such. But we don’t have all the data and we typically don’t even know what all the relevant variables are. Welcome to science. To address this problem, you might recommend we collect data from two samples for each project. Starting with sample A, we’d fit a series of models and settle on one or a small subset that both speak to our scientific hypothesis and seem to fit the sample A data well. Then we’d switch to sample B and rerun our primary model(s) from A to make sure our findings generalize. In this paradigm, we might call the A data in sample and the B data out of sample–or out of sample A, anyways. The problem is we often have time and funding constraints. We only have sample A and we may never collect sample B. So we’ll need to make the most out of A. Happily, tricky statisticians have our back. Instead, what we might do is divide our data into \\(k\\) equally-sized subsets. Call those subsets folds. If we leave one of the folds out, we can fit the model with the remaining data and then see how well that model speaks to the left-out fold. After doing this for every fold, we can get an average performance across folds. Note how as \\(k\\) increases, the number of cases with a fold get smaller. In the extreme, \\(k = N\\), the number of cases within the data. At that point, \\(k\\)-fold cross-validation turns into leave-one-out cross-validation (LOO-CV). But there’s a practical difficulty with LOO-CV: it’s costly. As you may have noticed, it takes some time to fit a Bayesian multilevel model. For large data and/or complicated models, sometimes it takes hours or days. Most of us just don’t have enough time or computational resources to fit that many models. Happily, we have an approximation to pure LOO-CV. Vehtari, Gelman, and Gabry (2017) proposed Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO) as an efficient way to approximate true LOO-CV. At this point, it’s probably best to let the statisticians speak for themselves: To maintain comparability with the given dataset and to get easier interpretation of the differences in scale of effective number of parameters, we define a measure of predictive accuracy for the \\(n\\) data points taken one at a time: \\[\\begin{align*} \\text{elpd} &amp; = \\text{expected log pointwise predictive density for a new dataset} \\\\ &amp; = \\sum_{i = 1}^n \\int p_t (\\tilde{y}_i) \\log p (\\tilde{y}_i | y) d \\tilde{y}_i, \\end{align*}\\] where \\(p_t (\\tilde{y}_i)\\) is the distribution representing the true data-generating process for \\(\\tilde{y}_i\\). The \\(p_t (\\tilde{y}_i)\\)’s are unknown, and we will use cross-validation or WAIC to approximate. In a regression, these distributions are also implicitly conditioned on any predictors in the model… The Bayesian LOO estimate of out-of-sample predictive fit is \\[\\text{elpd}_{\\text{loo}} = \\sum_{i = 1}^n \\log p (y_i | y - _i),\\] where \\[p (y_i | y - _i) = \\int p (y_i | \\theta) p (\\theta | y - _i) d \\theta\\] is the leave-one-out predictive density given the data without the ith data point. (pp. 2–3) For the rest of the details, check out the original paper. Our goal is to practice using the PSIS-LOO. Since this is the only version of the LOO we’ll be using in this project, I’m just going to refer to it as the LOO from here on. To use the LOO to evaluate a brm() fit, you just use the loo() function. Though you don’t have to save the results as an object, we’ll be forward thinking and do so here. l_fit4.1 &lt;- loo(fit4.1) ## Warning: Found 3 observations with a pareto_k &gt; 0.7 in model &#39;fit4.1&#39;. It is recommended to set ## &#39;reloo = TRUE&#39; in order to calculate the ELPD without the assumption that these observations are ## negligible. This will refit the model 3 times to compute the ELPDs for the problematic observations ## directly. print(l_fit4.1) ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_loo -315.5 12.3 ## p_loo 58.0 4.9 ## looic 630.9 24.6 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 220 89.4% 706 ## (0.5, 0.7] (ok) 23 9.3% 380 ## (0.7, 1] (bad) 3 1.2% 99 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## See help(&#39;pareto-k-diagnostic&#39;) for details. Remember that warning message we got from the waic() a while back? We get more information along those lines from the loo(). As it turns out, a few of the cases in the data were unduly influential in the model fit. Within the loo() paradigm, those are indexed by the pareto_k values. As it turns out, the Pareto \\(k\\) can be used as a diagnostic tool. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)’s are low. We typically get worried when those \\(k\\)’s exceed 0.7 and the loo() function spits out a warning message when they do. If you didn’t know, the brms functions like the waic() and loo() actually come from the loo package. Explicitly loading loo will buy us some handy convenience functions. library(loo) We’ll be leveraging those \\(k\\) values with the pareto_k_table() and pareto_k_ids() functions. Both functions take objects created by the loo() or psis() functions. Let’s take a look at the pareto_k_table() function first. pareto_k_table(l_fit4.1) ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 220 89.4% 706 ## (0.5, 0.7] (ok) 23 9.3% 380 ## (0.7, 1] (bad) 3 1.2% 99 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; This is the same table that popped out earlier after using the loo(). Recall that this data set has 246 observations (i.e., execute count(alcohol1_pp)). With pareto_k_table(), we see how the Pareto \\(k\\) values have been categorized into bins ranging from “good” to “very bad”. Clearly, we like nice and low \\(k\\)’s. In this example, most of our observations are “good” or “ok.” Three are in the “bad” \\(k\\) range. We can take a closer look by placing our loo() object into plot(). plot(l_fit4.1) We got back a nice diagnostic plot for those \\(k\\) values, ordered by row number. We can see that our three observations with the “bad” \\(k\\) values were earlier in the data and it appears their \\(k\\) values are just a smidge above the recommended threshold. If we wanted to further verify to ourselves which observations those were, we’d use the pareto_k_ids() function. pareto_k_ids(l_fit4.1, threshold = .7) ## [1] 27 33 112 Note our use of the threshold argument. Play around with it to see how it works. In case you’re curious, here are those rows. alcohol1_pp[c(27, 33, 112), ] ## # A tibble: 3 x 9 ## id age coa male age_14 alcuse peer cpeer ccoa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 16 1 1 2 3.46 0 -1.02 0.549 ## 2 11 16 1 1 2 3.16 0 -1.02 0.549 ## 3 38 14 0 0 0 0 1.26 0.247 -0.451 If you want an explicit look at those \\(k\\) values, execute l_fit4.1$diagnostics$pareto_k. For the sake of space, I’m going to omit the output. l_fit4.1$diagnostics$pareto_k The pareto_k values can be used to examine cases that are overly-influential on the model parameters, something like a Cook’s \\(D_{i}\\). See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in this paper and in this presentation by Aki Vehtari. Anyway, the implication of all this is these values suggest fit4.1 (i.e., Model A) might not be the best model of the data. Happily, we have other models to compare it to. That leads into the next section: 4.6.4.3 You can compare Bayesian models with the WAIC and LOO. Remember how we used the add_criterion() function, above. That’ll work for both WAIC and the LOO. Let’s do that for Models A through E. fit4.1 &lt;- add_criterion(fit4.1, c(&quot;loo&quot;, &quot;waic&quot;)) fit4.2 &lt;- add_criterion(fit4.2, c(&quot;loo&quot;, &quot;waic&quot;)) fit4.3 &lt;- add_criterion(fit4.3, c(&quot;loo&quot;, &quot;waic&quot;)) fit4.5 &lt;- add_criterion(fit4.5, c(&quot;loo&quot;, &quot;waic&quot;)) And to refresh, we can pull the WAIC and LOO information with $ indexing. Here’s how to get the LOO info for fit4.2. fit4.2$criteria$loo ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_loo -291.5 12.9 ## p_loo 95.9 7.7 ## looic 583.0 25.8 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 126 51.2% 341 ## (0.5, 0.7] (ok) 92 37.4% 81 ## (0.7, 1] (bad) 25 10.2% 16 ## (1, Inf) (very bad) 3 1.2% 8 ## See help(&#39;pareto-k-diagnostic&#39;) for details. Sigh. Turns out there are even more overly-influential cases in the unconditional growth model. In the case of a real data analysis, this might suggest we need a more robust model. One possible solution might be switching out our Gaussian likelihood for the robust Student’s \\(t\\)-distribution. For an introduction, you might check out my blog post on the topic, Robust Linear Regression with Student’s \\(t\\)-Distribution. But that’ll take us farther afield than I want to go, right now. The point to focus on, here, is we can use the loo_compare() function to compare fits by their WAIC or LOO. Let’s practice with the WAIC. ws &lt;- loo_compare(fit4.1, fit4.2, fit4.3, fit4.5, criterion = &quot;waic&quot;) print(ws) ## elpd_diff se_diff ## fit4.5 0.0 0.0 ## fit4.3 -4.9 3.6 ## fit4.2 -6.7 4.2 ## fit4.1 -40.9 8.0 Remember how we said that some contemporary Bayesians aren’t fans of putting Bayesian information criteria in a \\(\\chi^2\\) metric? Well, it turns out Aki Vehtari, of the Stan team and loo package fame–and also the primary author in that PSIS-LOO paper from before–, is one of those Bayesians. So instead of getting difference scores in the WAIC metric, we get them in the \\(\\text{elpd}_\\text{WAIC}\\) metric instead. But remember, if you prefer these estimates in the traditional metric, just multiply by -2. cbind(waic_diff = ws[, 1] * -2, se = ws[, 2] * 2) ## waic_diff se ## fit4.5 0.000000 0.000000 ## fit4.3 9.851001 7.267540 ## fit4.2 13.444486 8.488034 ## fit4.1 81.723541 16.097508 The reason we multiplied the se_diff column (i.e., the standard errors for the difference estimates) by 2 is because you can’t have negative standard errors. That’d be silly. But anyway, notice that the brm() fits have been rank ordered with the smallest differences at the top. Each row in the output is the difference of one of the fits compared to the best-fitting fit. Since fit4.5 apparently had the lowest WAIC value, it was ranked at the top. And notice how its waic_diff is 0. That, of course, is because \\(x - x = 0\\). So all the other difference scores are follow the formula \\(\\text{Difference}_x = \\text{WAIC}_\\text{fit_x} - \\text{WAIC}_\\text{fit_4.5}\\). Concerning our ws object, we can get more information on our models’ WAIC information if we include a simplify = F argument within print(). print(ws, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit4.5 0.0 0.0 -271.4 11.2 73.9 5.7 542.7 22.4 ## fit4.3 -4.9 3.6 -276.3 11.5 80.9 6.3 552.6 23.0 ## fit4.2 -6.7 4.2 -278.1 11.8 82.4 6.5 556.2 23.5 ## fit4.1 -40.9 8.0 -312.2 12.0 54.8 4.7 624.5 24.0 Their WAIC estimates and the associated standard errors are in the final two columns. In the two before that, we get the \\(p_\\text{WAIC}\\) estimates and their standard errors. We can get similar information for the LOO. loo_compare(fit4.1, fit4.2, fit4.3, fit4.5, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit4.5 0.0 0.0 -282.3 12.0 84.9 6.7 564.6 24.0 ## fit4.3 -6.2 3.8 -288.5 12.4 93.1 7.2 577.0 24.7 ## fit4.2 -9.2 4.6 -291.5 12.9 95.9 7.7 583.0 25.8 ## fit4.1 -33.2 8.3 -315.5 12.3 58.0 4.9 630.9 24.6 If you wanted a more focused comparison, say between fit1 and fit2, you’d just simplify your input. loo_compare(fit4.1, fit4.2, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit4.2 0.0 0.0 -291.5 12.9 95.9 7.7 583.0 25.8 ## fit4.1 -24.0 7.7 -315.5 12.3 58.0 4.9 630.9 24.6 We’ll get more practice with these methods as we go along. But for your own edification, you might check out the vignettes put out by the loo team. 4.7 Using Wald statistics to test composite hypotheses about fixed effects I’m not going to address issues of composite null-hypothesis tests using the Wald statistic. However, we can address some of these issues from a different more estimation-based perspective. Consider the initial question posed on page 123: Suppose, for example, you wanted to test whether the entire true change trajectory for a particular type of adolescent–say, a child of non-alcoholic parents with an average value of PEER–differs from a “null” trajectory (one with zero intercept and zero slope). This is tantamount to asking whether the average child of non-alcoholic parents drinks no alcohol at age 14 and remains abstinent over time. Singer and Willett then expressed their joint null hypothesis as \\[H_0: \\gamma_{00} = 0 \\; \\text{and} \\; \\gamma_{10} = 0.\\] This is a substantive question we can address more informatively with fitted(). First, let’s provide the necessary values for our predictor variables, coa, peer, and age_14. mu_peer &lt;- mean(alcohol1_pp$peer) nd &lt;- tibble(coa = 0, peer = mu_peer, age_14 = seq(from = 0, to = 2, length.out = 30)) head(nd) ## # A tibble: 6 x 3 ## coa peer age_14 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1.02 0 ## 2 0 1.02 0.0690 ## 3 0 1.02 0.138 ## 4 0 1.02 0.207 ## 5 0 1.02 0.276 ## 6 0 1.02 0.345 Now we use fitted() to examine the model-implied trajectory for a child of non-alcoholic parents and average peer values. f &lt;- fitted(fit4.6, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(age = age_14 + 14) f %&gt;% ggplot(aes(x = age)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2, limits = c(0, 2)) + labs(subtitle = &quot;Zero is credible for neither\\nthe intercept nor the slope.&quot;) + coord_cartesian(xlim = c(13, 17)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Recall that the result of our Bayesian analyses are the probability of the parameters given the data, \\(p(\\theta | d)\\). Based on our plot, there is much less than a .05 probability either intercept or slope for teens of this demographic are zero. If you really wanted to fixate on zero, you could even use geom_hline() to insert a horizontal line at zero in the figure. But all that fixating on zero detracts from what to my mind are the more important parts of the model. The intercept at age = 14 is about 1/3 and the endpoint when age = 16 is almost at \\(1\\). Those are our effect sizes. If you wanted to quantify those effect sizes more precisely, just query our fitted() object, f. f %&gt;% select(age, Estimate, Q2.5, Q97.5) %&gt;% filter(age %in% c(14, 16)) %&gt;% mutate_all(round, digits = 2) ## age Estimate Q2.5 Q97.5 ## 1 14 0.39 0.18 0.61 ## 2 16 0.94 0.67 1.21 Works like a champ. But we haven’t fully covered part of Singer and Willett’s joint hypothesis text. They proposed a joint Null that included the \\(\\gamma_{10} = 0\\). Though it’s clear from the plot that the trajectory increases, we can address the issue more directly with a difference score. For our difference, we’ll subtract the estimate at age = 14 from the one at age = 15. But to that, we’ll have to return to fitted(). So far, we’ve been using the default output which returns summaries of the posterior. To compute a proper difference score, we’ll need to work with all the posterior draws in order to approximate the full distribution. We do that by setting summary = F. And since we’re only interested in the estimates from these two age values, we’ll streamline our nd data. nd &lt;- tibble(coa = 0, peer = mu_peer, age_14 = 0:1) f &lt;- fitted(fit4.6, newdata = nd, re_formula = NA, summary = F) %&gt;% data.frame() str(f) ## &#39;data.frame&#39;: 4000 obs. of 2 variables: ## $ X1: num 0.411 0.511 0.63 0.427 0.426 ... ## $ X2: num 0.679 0.751 0.83 0.66 0.726 ... Now our f object has 4,000 rows and 2 columns. Each of the rows corresponds to one of the 4,000 post-warmup posterior draws. The columns correspond to the two rows in our nd data. To get a slope based on this combination of predictor values, we simply subtract the first column from the second. f &lt;- f %&gt;% transmute(difference = X2 - X1) f %&gt;% ggplot(aes(x = difference)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Based on 4,000 posterior draws, not a single one\\nsuggests the slope is even close to zero. Rather, the\\nposterior mass is concentrated around 0.25.&quot;, x = expression(paste(gamma[0][1], &quot; (i.e., the difference between the two time points)&quot;))) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) Here are the posterior mean and 95% intervals. f %&gt;% summarise(mean = mean(difference), ll = quantile(difference, probs = .025), ul = quantile(difference, probs = .975)) ## mean ll ul ## 1 0.2702342 0.1462111 0.3974354 On page 125, Singer and Willett further mused: When we examined the OLS estimated change trajectories in figure 4.2, we noticed that among children of non-alcoholic parents, those with low values of CPEER tended to have a lower initial status and steeper slopes than those with high values of CPEER. We might therefore ask whether the former group “catches up” to the latter. This is a question about the “vertical” separation between these two groups[’] true change trajectories at some later age, say 16. Within their joint hypothesis testing paradigm, they pose this as testing \\[H_0: 0\\gamma_{00} + 0\\gamma_{01} + 1\\gamma_{02} + 0\\gamma_{10} + 2\\gamma_{12} = 0.\\] From our perspective, this is a differences of differences analysis. That is, first we’ll compute the model implied alcuse estimates for the four combinations of the two levels of age and peer, holding coa constant at 0. Second, we’ll compute the differences between the two peer levels at each age. Third and finally, we’ll compute a difference of those differences. For our first step, recall it was fit4.7 that used the cpeer variable. # first step nd &lt;- crossing(age_14 = c(0, 2), cpeer = c(-.363, .363)) %&gt;% mutate(coa = 0) head(nd) ## # A tibble: 4 x 3 ## age_14 cpeer coa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 -0.363 0 ## 2 0 0.363 0 ## 3 2 -0.363 0 ## 4 2 0.363 0 f &lt;- fitted(fit4.7, newdata = nd, re_formula = NA, summary = F) %&gt;% data.frame() head(f) ## X1 X2 X3 X4 ## 1 0.17502405 0.6605243 0.8520849 1.298802 ## 2 0.20817809 0.6776782 0.9387258 1.235208 ## 3 0.12742797 0.7246396 0.8466041 1.216695 ## 4 0.09864766 0.6662694 0.8765035 1.239332 ## 5 0.13072997 0.7271179 1.0898833 1.436752 ## 6 0.08935016 0.5745774 0.9410148 1.032745 For our initial difference scores, we’ll subtract the estimates for the lower level of cpeer from the higher ones. # step 2 f &lt;- f %&gt;% transmute(`difference at 14` = X2 - X1, `difference at 16` = X4 - X3) head(f) ## difference at 14 difference at 16 ## 1 0.4855003 0.44671666 ## 2 0.4695001 0.29648254 ## 3 0.5972116 0.37009095 ## 4 0.5676217 0.36282849 ## 5 0.5963879 0.34686832 ## 6 0.4852272 0.09173039 For our final difference score, we’ll subtract the first difference score from the second. # step 3 f &lt;- f %&gt;% mutate(`difference in differences` = `difference at 16` - `difference at 14`) head(f) ## difference at 14 difference at 16 difference in differences ## 1 0.4855003 0.44671666 -0.0387836 ## 2 0.4695001 0.29648254 -0.1730176 ## 3 0.5972116 0.37009095 -0.2271206 ## 4 0.5676217 0.36282849 -0.2047932 ## 5 0.5963879 0.34686832 -0.2495196 ## 6 0.4852272 0.09173039 -0.3934969 Here we’ll plot all three. f %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;different differences&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free_y&quot;) Singer and Willett concluded they could “reject the null hypothesis at any conventional level of significance” (p. 126). If we must appeal to the Null, here are the posterior means and 95% intervals for our differences. f %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 3 x 4 ## name mean ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 difference at 14 0.502 0.338 0.658 ## 2 difference at 16 0.284 0.049 0.526 ## 3 difference in differences -0.218 -0.468 0.023 Our results contrast a bit from Singer and Willett’s. Though the bulk of our posterior mass is concentrated around -0.22, zero is a credible value within the difference of differences density. Our best bet is the differences begin to converge over time. However, that rate of that convergence is subtle and somewhat imprecise relative to the effect size. Interpret with caution. 4.8 Evaluating the tenability of a model’s assumptions “Whenever you fit a statistical model, you invoke assumptions” (p. 127). This is the case for multilevel Bayesian models, too. 4.8.1 Checking functional form. We’ve already checked the functional form at level-1 with our version of Figure 4.1. When we made our version of Figure 4.1, we relied on ggplot2::stat_smooth() to compute the id-level OLS trajectories. To make our variants of Figure 4.4, we’ll have to back up and compute them externally with lm(). Here we’ll do so in bulk with a nested data frame. The broom package will help us extract the results. library(broom) o &lt;- alcohol1_pp %&gt;% nest(-id, -coa, -peer) %&gt;% mutate(ols = map(data, ~lm(data = ., alcuse ~ 1 + age_14))) %&gt;% mutate(tidy = map(ols, tidy)) %&gt;% unnest(tidy) %&gt;% # this is unnecessary, but will help with plotting mutate(term = factor(term, levels = c(&quot;(Intercept)&quot;, &quot;age_14&quot;), labels = c(&quot;pi[0]&quot;, &quot;pi[1]&quot;))) head(o) ## # A tibble: 6 x 10 ## id coa peer data ols term estimate std.error statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.26 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[0] 1.78 0.0999 17.8 0.0357 ## 2 1 1 1.26 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[1] 0.134 0.0774 1.73 0.333 ## 3 2 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[0] -0.167 0.373 -0.447 0.732 ## 4 2 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[1] 0.5 0.289 1.73 0.333 ## 5 3 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[0] 0.947 0.118 8.03 0.0789 ## 6 3 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[1] 1.16 0.0914 12.7 0.0501 Now plot. o %&gt;% select(coa:peer, term:estimate) %&gt;% pivot_longer(coa:peer) %&gt;% ggplot(aes(x = value, y = estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(alpha = 2/3) + theme(panel.grid = element_blank(), strip.text = element_text(size = 11)) + facet_grid(term~name, scales = &quot;free&quot;, labeller = label_parsed) With a little more wrangling, we can extract the Pearson’s correlation coefficients for each panel. o %&gt;% select(coa:peer, term:estimate) %&gt;% pivot_longer(coa:peer) %&gt;% group_by(term, name) %&gt;% nest() %&gt;% mutate(r = map_dbl(data, ~cor(.)[2, 1] %&gt;% round(digits = 2))) ## # A tibble: 4 x 4 ## # Groups: term, name [4] ## term name data r ## &lt;fct&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 pi[0] coa &lt;tibble [82 × 2]&gt; 0.39 ## 2 pi[0] peer &lt;tibble [82 × 2]&gt; 0.580 ## 3 pi[1] coa &lt;tibble [82 × 2]&gt; -0.04 ## 4 pi[1] peer &lt;tibble [82 × 2]&gt; -0.19 4.8.2 Checking normality. The basic multilevel model of change yields three variance parameters, \\(\\epsilon_{ij}\\), \\(\\zeta_{0i}\\), and \\(\\zeta_{1i}\\). Each measurement occasion in the model receives a model-implied estimate for each. Singer and Willett referred to those estimates as \\(\\hat{\\epsilon}_{ij}\\), \\(\\hat{\\zeta}_{0i}\\), and \\(\\hat{\\zeta}_{1i}\\). As with frequentist software, our Bayesian software brms will return these estimates. To extract our Bayesian draws for the \\(\\hat{\\epsilon}_{ij}\\)’s, we use the residuals() function. e &lt;- residuals(fit4.6) str(e) ## num [1:246, 1:4] 0.2876 0.2573 -0.0409 -0.393 -0.5973 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; head(e) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.28763647 0.3250755 -0.3914337 0.89494825 ## [2,] 0.25734974 0.3037230 -0.3407331 0.84803822 ## [3,] -0.04088621 0.4411031 -0.9181669 0.82197153 ## [4,] -0.39302478 0.3443342 -1.0392050 0.31328261 ## [5,] -0.59734674 0.3181534 -1.2262349 0.02541651 ## [6,] 0.19833131 0.4624338 -0.6917016 1.09272430 For our fit5, the residuals() function returned a \\(246 \\times 4\\) numeric array. Each row corresponded to one of the rows of the original data set. The four vectors are the familiar summaries Estimate, Est.Error, Q2.5, and Q97.5. If we’d like to work with these in a ggplot2-made plot, we’ll have to convert our e object to a data frame. After we make the conversion, we then make the top left panel of Figure 4.5. e &lt;- e %&gt;% data.frame() e %&gt;% ggplot(aes(sample = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_qq() + ylim(-2, 2) + labs(x = &quot;Normal score&quot;, y = expression(hat(epsilon)[italic(ij)])) + theme(panel.grid = element_blank()) For the right plot on the top, we need to add an id index. That’s as easy as appending the one from the original data. If you followed closely with the text, you may have also noticed this panel is of the standardized residuals. That just means we’ll have to hand-standardize ours before plotting. e %&gt;% bind_cols(alcohol1_pp %&gt;% select(id)) %&gt;% mutate(z = (Estimate - mean(Estimate)) / sd(Estimate)) %&gt;% ggplot(aes(x = id, y = z)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point() + scale_y_continuous(expression(italic(std)~hat(epsilon)[italic(ij)]), limits = c(-2, 2)) + theme(panel.grid = element_blank()) We’ll need to use the ranef() function to return the estimates for the \\(\\zeta\\)’s. z &lt;- ranef(fit4.6) str(z) ## List of 1 ## $ id: num [1:82, 1:4, 1:2] 0.306 -0.488 0.32 -0.36 -0.566 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:82] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; z[[1]][1:6, , &quot;Intercept&quot;] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.3060223 0.3227010 -0.2960562 0.96774592 ## 2 -0.4877992 0.3433833 -1.2087988 0.13958551 ## 3 0.3203825 0.3285308 -0.3448843 0.95481492 ## 4 -0.3596063 0.3580504 -1.0893187 0.29984702 ## 5 -0.5664465 0.3299580 -1.2326926 0.06957911 ## 6 0.8217839 0.3655567 0.1586649 1.58692704 z[[1]][1:6, , &quot;age_14&quot;] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.06515672 0.2416886 -0.436878006 0.5218283 ## 2 -0.08440817 0.2538519 -0.573436784 0.4333823 ## 3 0.45812318 0.2629633 -0.009684557 1.0047397 ## 4 0.15313668 0.2715482 -0.345840207 0.7329594 ## 5 -0.30909870 0.2462948 -0.792318683 0.1900630 ## 6 0.25283161 0.2686093 -0.296467669 0.7683498 For our fit5, the ranef() function returned a list of 1, indexed by id. Therein lay a 3-dimensional array. The first two dimensions are the same as what we got from residuals(), above. The third dimension had two levels: Intercept and age_14. In other words, the third dimension is the one that differentiated between \\(\\hat{\\zeta}_{0i}\\) and \\(\\hat{\\zeta}_{1i}\\). to make this thing a little more useful, let’s convert it to a long-formatted data frame. z &lt;- rbind(z[[1]][ , , &quot;Intercept&quot;], z[[1]][ , , &quot;age_14&quot;]) %&gt;% data.frame() %&gt;% mutate(ranef = rep(c(&quot;hat(zeta)[0][italic(i)]&quot;, &quot;hat(zeta)[1][italic(i)]&quot;), each = n() / 2)) glimpse(z) ## Rows: 164 ## Columns: 5 ## $ Estimate &lt;dbl&gt; 0.30602233, -0.48779924, 0.32038254, -0.35960629, -0.56644649, 0.82178386, 0.21… ## $ Est.Error &lt;dbl&gt; 0.3227010, 0.3433833, 0.3285308, 0.3580504, 0.3299580, 0.3655567, 0.3385925, 0.… ## $ Q2.5 &lt;dbl&gt; -0.29605623, -1.20879884, -0.34488425, -1.08931868, -1.23269262, 0.15866490, -0… ## $ Q97.5 &lt;dbl&gt; 0.96774592, 0.13958551, 0.95481492, 0.29984702, 0.06957911, 1.58692704, 0.92773… ## $ ranef &lt;chr&gt; &quot;hat(zeta)[0][italic(i)]&quot;, &quot;hat(zeta)[0][italic(i)]&quot;, &quot;hat(zeta)[0][italic(i)]&quot;… Now we’re ready to plot the remaining panels on the left of Figure 4.5. z %&gt;% ggplot(aes(sample = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_qq() + ylim(-1, 1) + labs(x = &quot;Normal score&quot;, y = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ranef, labeller = label_parsed, ncol = 1) Here are the ones on the right. z %&gt;% bind_cols( bind_rows( alcohol1_pp %&gt;% distinct(id), alcohol1_pp %&gt;% distinct(id) ) ) %&gt;% mutate(ranef = str_c(&quot;italic(std)~&quot;, ranef)) %&gt;% # note we have to group them before standardizing group_by(ranef) %&gt;% mutate(z = (Estimate - mean(Estimate)) / sd(Estimate)) %&gt;% ggplot(aes(x = id, y = z)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point() + scale_y_continuous(NULL, limits = c(-3, 3)) + theme(panel.grid = element_blank()) + facet_wrap(~ranef, labeller = label_parsed, ncol = 1) If you were paying close attention, you may have noticed that for all three of our id-level deviation estimates, they were summarized not only by a posterior mean but by standard deviations and 95% intervals, too. To give a sense of what that means, here are those last two plots, again, but this time including vertical bars defined by the 95% intervals. z %&gt;% bind_cols( bind_rows( alcohol1_pp %&gt;% distinct(id), alcohol1_pp %&gt;% distinct(id) ) ) %&gt;% ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(shape = 20, size = 1/3) + ylab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ranef, labeller = label_parsed, ncol = 1) When you go Bayesian, even your residuals get full posterior distributions. 4.8.3 Checking homoscedasticity. Here we examine the homoscedasticity assumption by plotting the residual estimates against our predictors. We’ll start with the upper left panel of Figure 4.6. e %&gt;% bind_cols(alcohol1_pp) %&gt;% ggplot(aes(x = age, y = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(alpha = 1/4) + ylab(expression(hat(epsilon)[italic(ij)])) + coord_cartesian(xlim = c(13, 17), ylim = c(-2, 2)) + theme(panel.grid = element_blank()) Here’s a quick way to get the remaining four panels. z %&gt;% bind_cols( bind_rows( alcohol1_pp %&gt;% distinct(id, coa, peer), alcohol1_pp %&gt;% distinct(id, coa, peer) ) ) %&gt;% select(Estimate, ranef, coa, peer) %&gt;% pivot_longer(-c(Estimate, ranef)) %&gt;% ggplot(aes(x = value, y = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(alpha = 1/3) + ylim(-1, 1) + labs(x = &quot;covariate value&quot;, y = NULL) + theme(panel.grid = element_blank(), strip.text = element_text(size = 10)) + facet_grid(ranef~name, labeller = label_parsed, scales = &quot;free&quot;) 4.9 Model-based (Empirical Bayes) estimates of the individual growth parameters In this section, the authors discussed two methods for constructing id-level trajectories: a) use a weighted average of the OLS and multilevel estimates and b) rely solely on the multilevel model by making use of the three sources of residual variation. Our method will be the latter. Here are the data for id == 23. alcohol1_pp %&gt;% select(id:coa, cpeer, alcuse) %&gt;% filter(id == 23) ## # A tibble: 3 x 5 ## id age coa cpeer alcuse ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23 14 1 -1.02 1 ## 2 23 15 1 -1.02 1 ## 3 23 16 1 -1.02 1.73 post_23 &lt;- posterior_samples(fit4.7) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% # make our pis mutate(`pi[0][&quot;,23&quot;]` = b_Intercept + b_coa * 1 + b_cpeer * -1.018, `pi[1][&quot;,23&quot;]` = b_age_14 + `b_age_14:cpeer` * -1.018) head(post_23) ## b_Intercept b_age_14 b_coa b_cpeer b_age_14:cpeer pi[0][&quot;,23&quot;] pi[1][&quot;,23&quot;] ## 1 0.4177742 0.3288345 0.5996322 0.6687331 -0.02671047 0.33663602 0.3560258 ## 2 0.4429282 0.3220195 0.4890968 0.6466944 -0.11915811 0.27369007 0.4433224 ## 3 0.4260338 0.3028079 0.4798662 0.8226055 -0.15641917 0.06848761 0.4620426 ## 4 0.3824585 0.3377296 0.5130731 0.7818481 -0.14104218 0.09961022 0.4813105 ## 5 0.4289239 0.4171968 0.3574086 0.8214710 -0.17184547 -0.04992496 0.5921355 ## 6 0.3319638 0.3274581 0.5437074 0.6683571 -0.27100334 0.19528367 0.6033395 It doesn’t help us much now, but the reason we’ve formatted the names for our two \\(\\pi\\) columns so oddly is because those names will work much nicer in the figure we’ll make, below. Just wait and see. Anyways, more than a couple point estimates, we returned the draws from the full posterior distribution. We might summarize them. post_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 x 4 ## name mean ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;pi[0][\\&quot;,23\\&quot;]&quot; 0.262 -0.065 0.607 ## 2 &quot;pi[1][\\&quot;,23\\&quot;]&quot; 0.423 0.211 0.637 Or we could plot them. post_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;participant-specific parameter estimates&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, labeller = label_parsed, scales = &quot;free_y&quot;) Yet this approach neglects the \\(\\zeta\\)’s. W’ve been extracting the \\(\\zeta\\)’s with ranef(). We also get them when we use posterior_samples(). Here we’ll extract both the \\(\\gamma\\)’s as well as the \\(\\zeta\\)’s for id == 23. post_23 &lt;- posterior_samples(fit4.7) %&gt;% select(starts_with(&quot;b_&quot;), contains(&quot;23&quot;)) glimpse(post_23) ## Rows: 4,000 ## Columns: 7 ## $ b_Intercept &lt;dbl&gt; 0.4177742, 0.4429282, 0.4260338, 0.3824585, 0.4289239, 0.3319638, 0.… ## $ b_age_14 &lt;dbl&gt; 0.3288345, 0.3220195, 0.3028079, 0.3377296, 0.4171968, 0.3274581, 0.… ## $ b_coa &lt;dbl&gt; 0.5996322, 0.4890968, 0.4798662, 0.5130731, 0.3574086, 0.5437074, 0.… ## $ b_cpeer &lt;dbl&gt; 0.6687331, 0.6466944, 0.8226055, 0.7818481, 0.8214710, 0.6683571, 0.… ## $ `b_age_14:cpeer` &lt;dbl&gt; -0.02671047, -0.11915811, -0.15641917, -0.14104218, -0.17184547, -0.… ## $ `r_id[23,Intercept]` &lt;dbl&gt; 0.271850140, 0.383131729, 0.672762382, 0.921352031, 0.671065247, 0.0… ## $ `r_id[23,age_14]` &lt;dbl&gt; -0.1491509044, -0.0431086861, -0.1270336410, -0.1731686206, 0.214594… With the r_id prefix, brms tells you these are residual estimates for the levels in the id grouping variable. Within the brackets, we learn these particular columns are for id == 23, the first with respect to the Intercept and second with respect to the age_14 parameter. Let’s put them to use. post_23 &lt;- post_23 %&gt;% mutate(`pi[0][&quot;,23&quot;]` = b_Intercept + b_coa * 1 + b_cpeer * -1.018 + `r_id[23,Intercept]`, `pi[1][&quot;,23&quot;]` = b_age_14 + `b_age_14:cpeer` * -1.018 + `r_id[23,age_14]`) glimpse(post_23) ## Rows: 4,000 ## Columns: 9 ## $ b_Intercept &lt;dbl&gt; 0.4177742, 0.4429282, 0.4260338, 0.3824585, 0.4289239, 0.3319638, 0.… ## $ b_age_14 &lt;dbl&gt; 0.3288345, 0.3220195, 0.3028079, 0.3377296, 0.4171968, 0.3274581, 0.… ## $ b_coa &lt;dbl&gt; 0.5996322, 0.4890968, 0.4798662, 0.5130731, 0.3574086, 0.5437074, 0.… ## $ b_cpeer &lt;dbl&gt; 0.6687331, 0.6466944, 0.8226055, 0.7818481, 0.8214710, 0.6683571, 0.… ## $ `b_age_14:cpeer` &lt;dbl&gt; -0.02671047, -0.11915811, -0.15641917, -0.14104218, -0.17184547, -0.… ## $ `r_id[23,Intercept]` &lt;dbl&gt; 0.271850140, 0.383131729, 0.672762382, 0.921352031, 0.671065247, 0.0… ## $ `r_id[23,age_14]` &lt;dbl&gt; -0.1491509044, -0.0431086861, -0.1270336410, -0.1731686206, 0.214594… ## $ `pi[0][&quot;,23&quot;]` &lt;dbl&gt; 0.60848616, 0.65682180, 0.74124999, 1.02096225, 0.62114029, 0.275693… ## $ `pi[1][&quot;,23&quot;]` &lt;dbl&gt; 0.20687489, 0.40021375, 0.33500899, 0.30814191, 0.80672987, 0.288120… Here are our updated summaries. post_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 x 4 ## name mean ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;pi[0][\\&quot;,23\\&quot;]&quot; 0.570 -0.105 1.27 ## 2 &quot;pi[1][\\&quot;,23\\&quot;]&quot; 0.509 -0.022 1.03 And here are the updated density plots. post_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;participant-specific parameter estimates&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, labeller = label_parsed, scales = &quot;free_y&quot;) We’ve been focusing on the \\(\\pi\\) parameters. Notice that when we turn our attention to Figure 4.7, we’re now shifting focus slightly to the consequences of those parameters. We’re not attending to trajectories. It’s important to pick up on this distinction because it has consequences for our programming workflow. If you wanted to keep a parameter-centric workflow, we could continue to expand on our posterior_samples() by applying the full composite formula to explicitly add in predictions for various levels of age_14. And we could do that separately or in bulk for the eight participants highlighted in the figure. However pedagogically useful that might be, it’d be very tedious. If we instead take a trajectory-centric perspective, it’ll be more natural and efficient to work with a fitted()-based workflow. Let’s define our nd data. nd &lt;- alcohol1_pp %&gt;% select(id:coa, age_14:alcuse, cpeer) %&gt;% filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% # these next two lines will make plotting easier mutate(id_label = ifelse(id &lt; 10, str_c(&quot;0&quot;, id), id)) %&gt;% mutate(id_label = str_c(&quot;id = &quot;, id_label)) glimpse(nd) ## Rows: 24 ## Columns: 7 ## $ id &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65,… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, … ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2 ## $ alcuse &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, … ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 23&quot;, &quot;id… We’ve isolated the relevant predictor variables for our eight focal participants. Next we’ll pump them through fitted() and wrangle as usual. f &lt;- fitted(fit4.7, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) glimpse(f) ## Rows: 24 ## Columns: 11 ## $ Estimate &lt;dbl&gt; 1.1344894, 1.4480688, 1.7616482, 2.3767968, 2.7029029, 3.0290089, 0.5696337, 1.… ## $ Est.Error &lt;dbl&gt; 0.3562022, 0.2989615, 0.4499632, 0.3646575, 0.3058667, 0.4386758, 0.3439383, 0.… ## $ Q2.5 &lt;dbl&gt; 0.38839366, 0.85915855, 0.88588203, 1.70822381, 2.09593268, 2.17085481, -0.1046… ## $ Q97.5 &lt;dbl&gt; 1.801852, 2.030939, 2.670164, 3.124590, 3.316853, 3.903620, 1.266930, 1.667805,… ## $ id &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15,… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2 ## $ alcuse &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000,… ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 23&quot;, &quot;i… Notice how this time we omitted the re_formula = NA argument. By default, re_formula = NULL, the consequence of which is the output is based on all the parameters in the multilevel model, not just the \\(\\gamma\\)’s. Here are what they look like. f %&gt;% ggplot(aes(x = age, y = Estimate)) + geom_line(size = 1) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~id_label, ncol = 4) Now we’ve warmed up, let’s add in the data and the other lines so make the full version of Figure 4.7. Before we do so, we’ll revisit fitted(). Notice the return of the re_formula = NA argument. The trajectories in our f_gamma_only data frame will only be sensitive to the \\(\\gamma\\)s. f_gamma_only &lt;- fitted(fit4.7, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) glimpse(f_gamma_only) ## Rows: 24 ## Columns: 11 ## $ Estimate &lt;dbl&gt; 1.4986587, 1.6539408, 1.8092228, 1.6446802, 1.7683040, 1.8919278, 0.2615448, 0.… ## $ Est.Error &lt;dbl&gt; 0.1385486, 0.1360113, 0.1881993, 0.1526199, 0.1500623, 0.2123077, 0.1729135, 0.… ## $ Q2.5 &lt;dbl&gt; 1.23327430, 1.39161355, 1.44773073, 1.34457723, 1.47848970, 1.48072747, -0.0651… ## $ Q97.5 &lt;dbl&gt; 1.7802269, 1.9240787, 2.1693423, 1.9508109, 2.0659062, 2.3016916, 0.6065649, 1.… ## $ id &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15,… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2 ## $ alcuse &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000,… ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 23&quot;, &quot;i… Let’s plot! f %&gt;% ggplot(aes(x = age)) + # `id`-specific lines geom_line(aes(y = Estimate), size = 1) + # gamma-centric lines geom_line(data = f_gamma_only, aes(y = Estimate), size = 1/2) + # OLS lines stat_smooth(data = nd, aes(y = alcuse), method = &quot;lm&quot;, se = F, color = &quot;black&quot;, linetype = 2, size = 1/2) + # data points geom_point(data = nd, aes(y = alcuse)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~id_label, ncol = 4) Though our purpose was largely to reproduce Figure 4.7, we might push ourselves a little further. Our Bayesian estimates came with measures of uncertainty, the posterior standard deviations and the 95% intervals. Whenever possible, it’s good form to include some expression of our uncertainty in our plots. Here let’s focus on the id-specific trajectories. f %&gt;% ggplot(aes(x = age, y = Estimate)) + # `id`-specific 95% intervals geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + # `id`-specific lines geom_line(size = 1) + # data points geom_point(data = nd, aes(y = alcuse)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~id_label, ncol = 4) This also clarifies an important visualization point. If you only care about plotting straight lines, you only need two points. However, if you want to express shapes with curves, such as the typically-bowtie-shaped 95% intervals, you need estimates over a larger number of predictor values. Back to fitted()! # we need an expanded version of the `nd` nd_expanded &lt;- alcohol1_pp %&gt;% select(id, coa, cpeer) %&gt;% filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% # this part is important! expand(nesting(id, coa, cpeer), age_14 = seq(from = 0, to = 2, length.out = 30)) %&gt;% mutate(id_label = ifelse(id &lt; 10, str_c(&quot;0&quot;, id), id)) %&gt;% mutate(id_label = str_c(&quot;id = &quot;, id_label), age = age_14 + 14) # pump our `nd_expanded` into `fitted()` f &lt;- fitted(fit4.7, newdata = nd_expanded) %&gt;% data.frame() %&gt;% bind_cols(nd_expanded) glimpse(f) ## Rows: 240 ## Columns: 10 ## $ Estimate &lt;dbl&gt; 1.134489, 1.156116, 1.177742, 1.199368, 1.220994, 1.242620, 1.264246, 1.285873,… ## $ Est.Error &lt;dbl&gt; 0.3562022, 0.3456284, 0.3357889, 0.3267502, 0.3185803, 0.3113477, 0.3051190, 0.… ## $ Q2.5 &lt;dbl&gt; 0.3883937, 0.4351870, 0.4735158, 0.5249442, 0.5708991, 0.6107689, 0.6502656, 0.… ## $ Q97.5 &lt;dbl&gt; 1.801852, 1.806147, 1.819295, 1.827235, 1.835426, 1.843173, 1.854218, 1.865515,… ## $ id &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.… ## $ age_14 &lt;dbl&gt; 0.00000000, 0.06896552, 0.13793103, 0.20689655, 0.27586207, 0.34482759, 0.41379… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;i… ## $ age &lt;dbl&gt; 14.00000, 14.06897, 14.13793, 14.20690, 14.27586, 14.34483, 14.41379, 14.48276,… Notice how we now have many more rows. Let’s plot. f %&gt;% ggplot(aes(x = age, y = Estimate)) + # `id`-specific 95% intervals geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + # `id`-specific lines geom_line(size = 1) + # data points geom_point(data = nd, aes(y = alcuse)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~id_label, ncol = 4) Singer and Willett pointed out that one of the ways in which the multilevel model is more parsimonious than a series of id-specific single-level models is that all id levels share the same \\(\\sigma_\\epsilon\\) parameter. At this point, we should just point out that it’s possible to relax this assumption with modern Bayesian software, such as brms. For ideas on how, check out Donald Williams’ work (e.g., this preprint). Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_0.5.5 loo_2.2.0 brms_2.12.0 Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_3.0.0 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 bridgesampling_1.0-0 ## [17] splines_3.6.3 robustbase_0.93-6 knitr_1.28 shinythemes_1.1.2 ## [21] bayesplot_1.7.1 jsonlite_1.6.1 dbplyr_1.4.2 shiny_1.4.0.2 ## [25] compiler_3.6.3 httr_1.4.1 backports_1.1.6 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.2.4 nlme_3.1-144 crosstalk_1.1.0.1 ## [45] xfun_0.13 ps_1.3.2 rvest_0.3.5 mime_0.9 ## [49] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 DEoptimR_1.0-8 ## [53] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [57] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 ## [61] inline_0.3.15 RColorBrewer_1.1-2 shinystan_2.5.0 yaml_2.2.1 ## [65] gridExtra_2.3 StanHeaders_2.21.0-1 stringi_1.4.6 dygraphs_1.1.1.6 ## [69] checkmate_2.0.0 pkgbuild_1.0.6 rlang_0.4.5 pkgconfig_2.0.3 ## [73] matrixStats_0.56.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [77] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 processx_3.4.2 ## [81] plyr_1.8.6 magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [85] generics_0.0.2 DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [89] withr_2.1.2 mgcv_1.8-31 xts_0.12-0 abind_1.4-5 ## [93] metRology_0.9-28-1 modelr_0.1.6 crayon_1.3.4 utf8_1.1.4 ## [97] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 callr_3.4.3 ## [101] threejs_0.3.3 reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [105] numDeriv_2016.8-1.1 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [109] shinyjs_1.1 "],
["treating-time-more-flexibly.html", "5 Treating Time More Flexibly 5.1 Variably spaced measurement occasions 5.2 Varying numbers of measurement occasions 5.3 Time-varying predictors 5.4 Recentering the effect of TIME Reference Session info", " 5 Treating Time More Flexibly All the illustrative longitudinal data sets in previous chapters share two structural features that simplify analysis. Each is: (1) balanced–everyone is assessed on the identical number of occasions; and (2) time-structured–each set of occasions is identical across individuals. Our analyses have also been limited in that we have used only: (1) time-invariant predictors that describe immutable characteristics of individuals or their environment (except for TIME itself); and (2) a representation of TIME that forces the level-1 individual growth parameters to represent “initial status” and “rate of change.” The multilevel model for change is far more flexible than these examples suggest. With little or no adjustment, you can use the same strategies to analyze more complex data sets. Not only can the waves of data be irregularly spaced, their number and spacing can vary across participants. Each individual can have his or her own data collection schedule and number of waves can vary without limit from person to person. So, too, predictors of change can be time-invariant or time-varying, and the level-1 submodel can be parameterized in a variety of interesting ways. (p. 138, emphasis in the original) 5.1 Variably spaced measurement occasions Many researchers design their studies with the goal of assessing each individual on an identical set of occasions… Yet sometimes, despite a valiant attempt to collect time-structured data, actual measurement occasions will differ. Variation often results from the realities of fieldwork and data collection… So, too, many researchers design their studies knowing full well that the measurement occasions may differ across participants. This is certainly true, for example, of those who use an accelerated cohort design in which an age-heterogeneous cohort of individuals is followed for a constant period of time. Because respondents initial vary in age, and age, not wave, is usually the appropriate metric for analyses (see the discussion of time metrics in section 1.3.2), observed measurement occasions will differ across individuals. (p. 139, emphasis in the original) 5.1.1 The structure of variably spaced data sets. You can find the PIAT data from the CNLSY study in the reading_pp.csv file. library(tidyverse) reading_pp &lt;- read_csv(&quot;data/reading_pp.csv&quot;) head(reading_pp) ## # A tibble: 6 x 5 ## id wave agegrp age piat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 6.5 6 18 ## 2 1 2 8.5 8.33 35 ## 3 1 3 10.5 10.3 59 ## 4 2 1 6.5 6 18 ## 5 2 2 8.5 8.5 25 ## 6 2 3 10.5 10.6 28 On pages 141 and 142, Singer and Willett discussed the phenomena of occasion creep, which is when “the temporal separations of occasions widens as the actual ages exceed design projections”. Here’s what that might look like. reading_pp %&gt;% ggplot(aes(x = age, y = wave)) + geom_vline(xintercept = c(6.5, 8.5, 10.5), color = &quot;white&quot;) + geom_jitter(alpha = .5, height = .33, width = 0) + scale_x_continuous(breaks = c(6.5, 8.5, 10.5)) + scale_y_continuous(breaks = 1:3) + ggtitle(&quot;This is what occasion creep looks like.&quot;, subtitle = &quot;As the waves go by, the variation of the ages widens and their central tendency\\ncreeps away from the ideal point.&quot;) + theme(panel.grid = element_blank()) Here’s how we might make our version of Figure 5.1. set.seed(5) # wrangle reading_pp %&gt;% nest(data = c(wave, agegrp, age, piat)) %&gt;% sample_n(size = 9) %&gt;% unnest(data) %&gt;% # this will help format and order the facets mutate(id = ifelse(id &lt; 10, str_c(&quot;0&quot;, id), id) %&gt;% str_c(&quot;id = &quot;, .)) %&gt;% pivot_longer(contains(&quot;age&quot;)) %&gt;% # plot ggplot(aes(x = value, y = piat, color = name)) + geom_point(alpha = 2/3) + stat_smooth(method = &quot;lm&quot;, se = F, size = 1/2) + scale_color_viridis_d(NULL, option = &quot;B&quot;, end = .5, direction = -1) + xlab(&quot;measure of age&quot;) + coord_cartesian(xlim = c(5, 12), ylim = c(0, 80)) + theme(panel.grid = element_blank()) + facet_wrap(~id) Since it wasn’t clear which id values the authors used in the text, we just randomized. Change the seed to view different samples. 5.1.2 Postulating and fitting multilevel models with variably spaced waves of data. The composite formula for our first model is \\[\\begin{align*} \\text{piat}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} (\\text{agegrp}_{ij} - 6.5) + \\zeta_{0i} + \\zeta_{1i} (\\text{agegrp}_{ij} - 6.5) + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma \\Bigg ), \\text{where} \\\\ \\mathbf \\Sigma &amp; = \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39;, \\text{where} \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\end{align*}\\] It’s the same for the twin model using age rather than agegrp. Notice how we’ve switched from Singer and Willett’s \\(\\sigma^2\\) parameterization to the \\(\\sigma\\) parameterization typical of brms. reading_pp &lt;- reading_pp %&gt;% mutate(agegrp_c = agegrp - 6.5, age_c = age - 6.5) head(reading_pp) ## # A tibble: 6 x 7 ## id wave agegrp age piat agegrp_c age_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 6.5 6 18 0 -0.5 ## 2 1 2 8.5 8.33 35 2 1.83 ## 3 1 3 10.5 10.3 59 4 3.83 ## 4 2 1 6.5 6 18 0 -0.5 ## 5 2 2 8.5 8.5 25 2 2 ## 6 2 3 10.5 10.6 28 4 4.08 In the last chapter, we began familiarizing ourselves with brms::brm() default priors. It’s time to level up. Another approach is to use domain knowledge to set weakly-informative priors. Let’s start with the PIAT. The Peabody Individual Achievement Test is a standardized individual test of scholastic achievement. It yields several subtest scores. The reading subtest is the one we’re focusing on, here. As is typical for such tests, the PIAT scores are normed to yield a population mean of 100 and a standard deviation of 15. With that information alone, even a PIAT novice should have an idea about how to specify the priors. Since our sole predictor variables are versions of age centered at 6.5, we know that the model intercept is interpreted as the expected value on the PIAT when the children are 6.5 years old. If you knew nothing else, you’d guess the mean score would be 100 with a standard deviation around 15. One way to use a weakly-informative prior on the intercept would be to multiply that \\(SD\\) by a number like 2. Next we need a prior for the time variables, age_c and agegrp_c. A one-unit increase in either of these is the expected increase in the PIAT with one year’s passage of age. Bringing in a little domain knowledge, IQ and achievement tests tend to be rather stable over time. However, we also expect children to get better as they age and we also don’t know exactly how these data have been adjusted for the children’s ages. It’s also important to know that it’s typical within the Bayesian world to place Normal priors on \\(\\beta\\) parameters. So one approach would be to center the Normal prior on 0 and put something like twice the PIAT’s standard deviation on the prior’s \\(\\sigma\\). If we were PIAT researchers, we could do much better. But with minimal knowledge of the test, this approach is certainly beats defaults. Next we have the variance parameters. Recall that brms::brm() defaults are Student’s \\(t\\)-distributions with \\(\\nu = 3\\) and \\(\\mu = 0\\). Let’s start there. Now we just need to put values on \\(\\sigma\\). Since the PIAT has a standard deviation of 15 in the population, why not just use 15? If you felt insecure about this, multiply if by a factor of 2 or so. Also recall that when Student’s \\(t\\)-distributions has a \\(\\nu = 3\\), the tails are quite fat. Within the context of Bayesian priors, those fat tails make it easy for the likelihood to dominate the prior even when it’s a good way into the tail. Finally, we have the correlation among the group-level variance parameters, \\(\\sigma_0\\) and \\(\\sigma_1\\). Recall that last chapter we learned the brms::brm() default was lkj(1). To get a sense of what the LKJ does, we’ll simulate from it. McElreath’s rethinking package contains a handy rlkjcorr() function, which will allow us to simulate n draws from a K by K correlation matrix for which \\(\\eta\\) is defined by eta. Let’s take n &lt;- 1e6 draws from two LKJ prior distributions, one with \\(\\eta = 1\\) and the other with \\(\\eta = 4\\). library(rethinking) n &lt;- 1e6 set.seed(5) lkj &lt;- tibble(eta = c(1, 4)) %&gt;% mutate(draws = purrr::map(eta, ~rlkjcorr(n, K = 2, eta = .)[, 2, 1])) %&gt;% unnest(draws) glimpse(lkj) ## Rows: 2,000,000 ## Columns: 2 ## $ eta &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ draws &lt;dbl&gt; 0.59957109, -0.83375155, 0.79069974, -0.05591997, -0.91300025, 0.45343010, 0.363191… Now let’s plot. lkj %&gt;% mutate(eta = factor(eta)) %&gt;% ggplot(aes(x = draws, fill = eta, color = eta)) + geom_density(size = 0, alpha = 2/3) + geom_text(data = tibble( draws = c(.75, .35), y = c(.6, 1.05), label = c(&quot;eta = 1&quot;, &quot;eta = 4&quot;), eta = c(1, 4) %&gt;% as.factor()), aes(y = y, label = label)) + scale_fill_viridis_d(option = &quot;A&quot;, end = .5) + scale_color_viridis_d(option = &quot;A&quot;, end = .5) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(rho)) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) When we use lkj(1), the prior is flat over the parameter space. However, setting lkj(4) is tantamount to a prior with a probability mass concentrated a bit towards zero. It’s a prior that’s skeptical of extremely large or small correlations. Within the context of our multilevel model \\(\\rho\\) parameters, this will be our weakly-regularizing prior. Let’s prepare to fit our models and load brms. detach(package:rethinking, unload = T) library(brms) Fit the models. Following the same form, the differ in that the first uses agegrp_c and the second uses age_c. fit5.1 &lt;- brm(data = reading_pp, family = gaussian, piat ~ 0 + Intercept + agegrp_c + (1 + agegrp_c | id), prior = c(prior(normal(100, 30), class = b, coef = Intercept), prior(normal(0, 30), class = b, coef = agegrp_c), prior(student_t(3, 0, 15), class = sd), prior(student_t(3, 0, 15), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.01&quot;) fit5.2 &lt;- brm(data = reading_pp, family = gaussian, piat ~ 0 + Intercept + age_c + (1 + age_c | id), prior = c(prior(normal(100, 30), class = b, coef = Intercept), prior(normal(0, 30), class = b, coef = age_c), prior(student_t(3, 0, 15), class = sd), prior(student_t(3, 0, 15), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.02&quot;) Focusing first on fit5.1, our analogue to the \\((AGEGRP – 6.5)\\) model displayed in Table 5.2, here is our model summary. print(fit5.1, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: piat ~ 0 + Intercept + agegrp_c + (1 + agegrp_c | id) ## Data: reading_pp (Number of observations: 267) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 89) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 3.378 0.797 1.778 4.919 1.001 1565 1886 ## sd(agegrp_c) 2.173 0.283 1.642 2.771 1.005 1051 2228 ## cor(Intercept,agegrp_c) 0.196 0.224 -0.218 0.645 1.011 577 1324 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 21.197 0.633 19.959 22.437 1.000 6019 3324 ## agegrp_c 5.025 0.304 4.433 5.620 1.000 3776 3072 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.280 0.348 4.602 5.970 1.002 1401 2324 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the age_c model. print(fit5.2, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: piat ~ 0 + Intercept + age_c + (1 + age_c | id) ## Data: reading_pp (Number of observations: 267) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 89) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 2.736 0.849 0.936 4.348 1.005 862 529 ## sd(age_c) 1.990 0.253 1.528 2.518 1.004 1012 1738 ## cor(Intercept,age_c) 0.220 0.234 -0.240 0.672 1.004 521 827 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 21.092 0.581 19.919 22.207 0.999 5611 2862 ## age_c 4.540 0.276 4.004 5.075 1.000 3253 2990 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.179 0.337 4.538 5.854 1.002 1234 2180 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). For a more focused look, we can use fixef() compare our \\(\\gamma\\)s to each other and those in the text. fixef(fit5.1) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 21.197 0.633 19.959 22.437 ## agegrp_c 5.025 0.304 4.433 5.620 fixef(fit5.2) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 21.092 0.581 19.919 22.207 ## age_c 4.540 0.276 4.004 5.075 Here are our \\(\\sigma_\\epsilon\\) summaries. VarCorr(fit5.1)$residual$sd %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## 5.28 0.348 4.602 5.97 VarCorr(fit5.2)$residual$sd %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## 5.179 0.337 4.538 5.854 From a quick glance, you can see they are about the square of the \\(\\sigma_\\epsilon^2\\) estimates in the text. Let’s go ahead and compute the LOO and WAIC. fit5.1 &lt;- add_criterion(fit5.1, c(&quot;loo&quot;, &quot;waic&quot;)) fit5.2 &lt;- add_criterion(fit5.2, c(&quot;loo&quot;, &quot;waic&quot;)) Compare the models with a WAIC difference. loo_compare(fit5.1, fit5.2, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.2 0.0 0.0 -865.5 15.3 77.5 7.3 1730.9 30.7 ## fit5.1 -5.9 3.4 -871.4 13.7 78.6 6.6 1742.8 27.4 The WAIC difference between the two isn’t that large relative to its standard error. The LOO tells a slightly more interesting story. loo_compare(fit5.1, fit5.2, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit5.2 0.0 0.0 -878.4 15.9 90.4 8.1 1756.8 31.8 ## fit5.1 -7.3 3.4 -885.8 14.4 92.9 7.6 1771.5 28.7 With the LOO, the point estimate for the difference is a little larger than twice the size of its standard error, providing a little more certainty in the superiority of the age_c model, fit5.2. The uncertainty in our WAIC and LOO estimates and their differences provides information that was not available for the AIC and the BIC comparisons in the text. We can also compare the WAIC and the LOO with model weights. Given the WAIC, from McElreath (2015) we learn A total weight of 1 is partitioned among the considered models, making it easier to compare their relative predictive accuracy. The weight for a model \\(i\\) in a set of \\(m\\) models is given by: \\[w_i = \\frac{\\text{exp}(-\\frac{1}{2} \\text{dWAIC}_i)}{\\sum_{j = 1}^m \\text{exp}(-\\frac{1}{2} \\text{dWAIC}_i)}\\] where dWAIC is the dWAIC in the compare table output. This example uses WAIC but the formula is the same for any other information criterion, since they are all on the deviance scale. (p. 199) The compare() function McElreath referenced is from his rethinking package, which is meant to accompany his text. We don’t have that function with brms. A rough analogue to the rethinking::compare() function is loo_compare(). We don’t quite have a dWAIC column from loo_compare(). Remember how last chapter we discussed how Aki Vehtari isn’t a fan of converting information criteria to the \\(\\chi^2\\) difference metric with that last \\(-2 \\times ...\\) step? That’s why we have an elpd_diff instead of a dWAIC. But to get the corresponding value, you just multiply those values by -2. And yet if you look closely at the formula for \\(w_i\\), you’ll see that each time the dWAIC term appears, it’s multiplied by \\(-\\frac{1}{2}\\). So we don’t really need that dWAIC value anyway. As it turns out, we’re good to go with our elpd_diff. Thus the above equation simplifies to \\[ w_i = \\frac{\\text{exp}(\\text{elpd_diff}_i)}{\\sum_{j = 1}^m \\text{exp}(\\text{elpd_diff}_i)} \\] But recall you don’t have to do any of this by hand. We have the brms::model_weights() function, which we can use to compute weights with the WAIC or the LOO. model_weights(fit5.1, fit5.2, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.1 fit5.2 ## 0.003 0.997 model_weights(fit5.1, fit5.2, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit5.1 fit5.2 ## 0.001 0.999 Both put the lion’s share of the weight on the age_c model. Back to McElreath (2015): But what do these weights mean? There actually isn’t a consensus about that. But here’s Akaike’s interpretation, which is common. A model’s weight is an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models considered. Here’s the heuristic explanation. First, regard WAIC as the expected deviance of a model on future data. That is to say that WAIC gives us an estimate of \\(\\text{E} (D_\\text{test})\\). Akaike weights convert these deviance values, which are log-likelihoods, to plain likelihoods and then standardize them all. This is just like Bayes’ theorem uses a sum in the denominator to standardize the produce of the likelihood and prior. Therefore the Akaike weights are analogous to posterior probabilities of models, conditional on expected future data. (p. 199, emphasis in the original) 5.2 Varying numbers of measurement occasions As Singer and Willett pointed out, once you allow the spacing of waves to vary across individuals, it is a small leap to allow their number to vary as well. Statisticians say that such data sets are unbalanced. As you would expect, balance facilitates analysis: models can be parameterized more easily, random effects can be estimated more precisely, and computer algorithms will converge more rapidly. Yet a major advantage of the multilevel model for change is that it is easily fit to unbalanced data. (p. 146, emphasis in the original) 5.2.1 Analyzing data sets in which the number of waves per person varies. Here we load the wages_pp.csv data. wages_pp &lt;- read_csv(&quot;data/wages_pp.csv&quot;) glimpse(wages_pp) ## Rows: 6,402 ## Columns: 15 ## $ id &lt;dbl&gt; 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 53,… ## $ lnw &lt;dbl&gt; 1.491, 1.433, 1.469, 1.749, 1.931, 1.709, 2.086, 2.129, 1.982, 1.798, 2.256… ## $ exper &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040… ## $ ged &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ postexp &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040… ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ hispanic &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, … ## $ hgc &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, … ## $ hgc.9 &lt;dbl&gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -2, -2, -… ## $ uerate &lt;dbl&gt; 3.215, 3.215, 3.215, 3.295, 2.895, 2.495, 2.595, 4.795, 4.895, 7.400, 7.400… ## $ ue.7 &lt;dbl&gt; -3.785, -3.785, -3.785, -3.705, -4.105, -4.505, -4.405, -2.205, -2.105, 0.4… ## $ ue.centert1 &lt;dbl&gt; 0.000, 0.000, 0.000, 0.080, -0.320, -0.720, -0.620, 1.580, 0.000, 2.505, 2.… ## $ ue.mean &lt;dbl&gt; 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 5.0965, 5.0… ## $ ue.person.cen &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0800, -0.3200, -0.7200, -0.6200, 1.5800, -0.2015,… ## $ ue1 &lt;dbl&gt; 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 4.895, 4.895, 4.895… Here’s a more focused look along the lines of Table 5.3. wages_pp %&gt;% select(id, exper, lnw, black, hgc, uerate) %&gt;% filter(id %in% c(206, 332, 1028)) ## # A tibble: 20 x 6 ## id exper lnw black hgc uerate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 206 1.87 2.03 0 10 9.2 ## 2 206 2.81 2.30 0 10 11 ## 3 206 4.31 2.48 0 10 6.30 ## 4 332 0.125 1.63 0 8 7.1 ## 5 332 1.62 1.48 0 8 9.6 ## 6 332 2.41 1.80 0 8 7.2 ## 7 332 3.39 1.44 0 8 6.20 ## 8 332 4.47 1.75 0 8 5.60 ## 9 332 5.18 1.53 0 8 4.60 ## 10 332 6.08 2.04 0 8 4.30 ## 11 332 7.04 2.18 0 8 3.40 ## 12 332 8.20 2.19 0 8 4.39 ## 13 332 9.09 4.04 0 8 6.70 ## 14 1028 0.004 0.872 1 8 9.3 ## 15 1028 0.035 0.903 1 8 7.4 ## 16 1028 0.515 1.39 1 8 7.3 ## 17 1028 1.48 2.32 1 8 7.4 ## 18 1028 2.14 1.48 1 8 6.30 ## 19 1028 3.16 1.70 1 8 5.90 ## 20 1028 4.10 2.34 1 8 6.9 To get a sense of the diversity in the number of occasions per id, use group_by() and count(). wages_pp %&gt;% group_by(id) %&gt;% count() %&gt;% ggplot(aes(x = n)) + geom_bar() + scale_x_continuous(&quot;# measurement occasions&quot;, breaks = 1:13) + ylab(&quot;count of cases&quot;) + coord_flip() + theme(panel.grid = element_blank()) The spacing of the measurement occasions also differs a lot across cases. Recall that exper “identifies the specific moment–to the nearest day–in each man’s labor force history associated with each observed value of” lnw (p. 147). Here’s a sense of what that looks like. wages_pp %&gt;% filter(id %in% c(206, 332, 1028)) %&gt;% mutate(id = factor(id)) %&gt;% ggplot(aes(x = exper, y = lnw, color = id)) + geom_point() + geom_line() + scale_color_viridis_d(option = &quot;B&quot;, begin = .35, end = .8) + theme(panel.grid = element_blank()) Uneven for dayz. Here’s the brms version of the composite formula for Model A, the unconditional growth model for lnw. \\[\\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{exper}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma \\Bigg ), \\text{where} \\\\ \\mathbf \\Sigma &amp; = \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39;, \\text{where} \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix}, \\text{and} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\end{align*}\\] To attempt setting priors for this, we need to review what lnw is. From the text: “To adjust for inflation, each hourly wage is expressed in constant 1990 dollars. To address the skewness commonly found in wage data and to linearize the individual wage trajectories, we analyze the natural logarithm of wages, LNW” (p. 147). So it’s the log of participant wages in 1990 dollars. From the official US Social Secutiry website, we learn the average yearly wage in 1990 was $20,172.11. Here’s that natural log for that. log(20172.11) ## [1] 9.912056 However, that’s the yearly wage. In the text, this is conceptualized as rate per hour. If we presume a 40 hour week for 52 weeks, this translates to a little less than $10 per hour. 20172.11 / (40 * 52) ## [1] 9.69813 And here’s what that looks like in a log metric. log(20172.11 / (40 * 52)) ## [1] 2.271933 But keep in mind that “to track wages on a common temporal scale, Murnane and colleagues decided to clock time from each respondent’s first day of work” (p. 147). So the wages at one’s initial point in the study were often entry-level wages. From the official website for the US Department of Labor, we learn the national US minimum wage in 1990 was $3.80 per hour. Here’s what that looks like on the log scale. log(3.80) ## [1] 1.335001 So perhaps this is a better figure to center our prior for the model intercept on. If we stay with a conventional Gaussian prior and put \\(\\mu = 1.335\\), what value should we use for the standard deviation? Well, if that’s the log minimum and 2.27 is the log mean, then there’s less than a log value of 1 between the minimum and the mean. If we’d like to continue our practice of weakly regularizing priors a value of 1 or even 0.5 on the log scale would seem reasonable. For simplicity, we’ll use normal(1.335, 1). Next we need a prior for the expected increase over a single year’s employment. A conservative default might be to center it on zero—no change from year to year. Since as we’ve established a 1 on the log scale is more than the difference between the minimum and average hourly wages in 1990 dollars, we might just use normal(0, 0.5) as a starting point. So then what about our variance parameters? Given these are all entry-level workers and given how little we’d expect them to increase from year to year, a student_t(3, 0, 1) on the log scale would seem pretty permissive. So then here’s how we might formally specify our model priors: \\[\\begin{align*} \\gamma_{00} &amp; \\sim \\operatorname{Normal}(1.335, 1) \\\\ \\gamma_{10} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Student-t} (3, 0, 1) \\\\ \\sigma_0 &amp; \\sim \\operatorname{Student-t} (3, 0, 1) \\\\ \\sigma_1 &amp; \\sim \\operatorname{Student-t} (3, 0, 1) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4) \\end{align*}\\] For a point of comparison, here are the brms::brm() default priors. get_prior(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + (1 + exper | id)) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b exper ## 3 b Intercept ## 4 lkj(1) cor ## 5 cor id ## 6 student_t(3, 0, 10) sd ## 7 sd id ## 8 sd exper id ## 9 sd Intercept id ## 10 student_t(3, 0, 10) sigma Even though our priors are still quite permissive on the scale of the data, they’re much more informative than the defaults. If we had formal backgrounds in the entry-level economy of the US in the early 1900s, we’d be able to specify even better priors. But hopefully this walk-through gives a sense of how to start thinking about model priors. Let’s fit the model. To keep the size of the fits/fit05.03.rds file below the 100MB GitHub limit, we’ll set chains = 3 and compensate by upping iter a little. fit5.3 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b, coef = exper), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.03&quot;) Here are the results. print(fit5.3, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.233 0.011 0.212 0.254 1.003 1556 2750 ## sd(exper) 0.041 0.003 0.036 0.047 1.008 626 1341 ## cor(Intercept,exper) -0.286 0.071 -0.419 -0.137 1.010 546 1556 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.716 0.011 1.695 1.737 1.001 3097 3204 ## exper 0.046 0.002 0.041 0.050 1.001 3171 3333 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.309 0.003 0.303 0.315 1.002 3039 3286 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since the criterion lnw is on the log scale, Singer and Willett pointed out our estimate for \\(\\gamma_{10}\\) indicates a nonlinear growth rate on the natural dollar scale. They further explicated that “if an outcome in a linear relationship, \\(Y\\), is expressed as a natural logarithm and \\(\\hat \\gamma_{01}\\) is the regression coefficient for a predictor \\(X\\), then \\(100(e^{\\hat{\\gamma}_{01}} - 1)\\) is the percentage change in \\(Y\\) per unit difference in \\(X\\)” (p. 148, emphasis in the original). Here’s how to do that conversion with our brms output. post &lt;- posterior_samples(fit5.3) %&gt;% transmute(percent_change = 100 * (exp(b_exper) - 1)) head(post) ## percent_change ## 1 4.737251 ## 2 4.885376 ## 3 4.665477 ## 4 4.637650 ## 5 4.804950 ## 6 4.772361 For our plot, let’s break out Matthew Kay’s handy tidybayes package. With the tidybayes::geom_halfeyeh() function, it’s easy to put horizontal point intervals beneath out parameter densities. Here we’ll use 95% intervals. library(tidybayes) post %&gt;% ggplot(aes(x = percent_change, y = 0)) + geom_halfeyeh(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Percent change&quot;, x = expression(100*(italic(e)^(hat(gamma)[1][0])-1))) + theme(panel.grid = element_blank()) The tidybayes package also has a group of functions that make it easy to summarize posterior parameters with measures of central tendency (i.e., mean, median, mode) and intervals (i.e., percentile based, highest posterior density intervals). Here we’ll use median_qi() to get the posterior median and percentile-based 95% intervals. post %&gt;% median_qi(percent_change) ## percent_change .lower .upper .width .point .interval ## 1 4.673707 4.186993 5.159937 0.95 median qi For our next model, Model B in Table 5.4, we add two time-invariant covariates. In the data, these are listed as black and hgc.9. Before we proceed, let’s rename hgc.9 to be more consistent with tidyverse style. wages_pp &lt;- wages_pp %&gt;% rename(hgc_9 = hgc.9) There we go. Let’s take a look at the distributions of our covariates. wages_pp %&gt;% pivot_longer(c(black, hgc_9)) %&gt;% ggplot(aes(x = value)) + geom_bar() + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) We see black is a dummy variable coded “Black” = 1, “Non-black” = 0. hgc_9 is a somewhat Gaussian ordinal centered around zero. For context, it might also help to check its standard deviation. sd(wages_pp$hgc_9) ## [1] 1.347135 With a mean near 0 and an \\(SD\\) near 1, hgc_9 is almost in a standardized metric. If we wanted to keep with our weakly-regularizing approach, normal(0, 1) or even normal(0, 0.5) would be pretty permissive for both these variables. Recall that we’re predicting wage on the log scale. A \\(\\gamma\\) value of 1 or even 0.5 would be humongous for the social sciences. Since we already have the \\(\\gamma\\) for exper set to normal(0, 0.5), let’s just keep with that. Here’s how we might describe our model in statistical terms: \\[\\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_{i} - 9) + \\gamma_{02} \\text{black}_{i} \\\\ &amp; \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{11} \\text{exper}_{ij} \\times (\\text{hgc}_{i} - 9) + \\gamma_{12} \\text{exper}_{ij} \\times \\text{black}_{i} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ) \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(1.335, 1) \\\\ \\gamma_{01},..., \\gamma_{12} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t} (3, 0, 1) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] The top portion up through the \\(\\mathbf{\\Omega}\\) line is the likelihood. Starting with \\(\\gamma_{00} \\sim \\text{Normal}(1.335, 1)\\) on down, we’ve listed our priors. Here’s how to fit the model with brms. fit5.4 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.04&quot;) Let’s take a look at the results. print(fit5.4, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.227 0.010 0.207 0.247 1.003 1799 3037 ## sd(exper) 0.041 0.003 0.036 0.046 1.003 595 1296 ## cor(Intercept,exper) -0.296 0.069 -0.423 -0.151 1.005 623 1753 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.717 0.013 1.693 1.743 1.000 2180 3321 ## hgc_9 0.035 0.008 0.020 0.050 1.001 2751 3052 ## black 0.016 0.024 -0.033 0.062 1.001 2220 3137 ## exper 0.049 0.003 0.044 0.055 1.001 1896 2619 ## hgc_9:exper 0.001 0.002 -0.002 0.005 1.001 2538 2855 ## black:exper -0.018 0.005 -0.029 -0.007 1.001 1808 3014 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.309 0.003 0.303 0.315 1.001 3015 3306 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The \\(\\gamma\\)s are on par with those in the text. When we convert the \\(\\sigma\\) parameters to the \\(\\sigma^2\\) metric, here’s what they look like. post &lt;- posterior_samples(fit5.4) post %&gt;% transmute(`sigma[0]^2` = sd_id__Intercept^2, `sigma[1]^2` = sd_id__exper^2, `sigma[epsilon]^2` = sigma^2) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_halfeyeh(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + coord_cartesian(ylim = c(1.4, 3.4)) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) We might plot our \\(\\gamma\\)s, too. Here we’ll use tidybayes::stat_pointintervalh() to just focus on the points and intervals. post %&gt;% select(b_Intercept:`b_black:exper`) %&gt;% set_names(str_c(&quot;gamma&quot;, c(&quot;[0][0]&quot;, &quot;[0][1]&quot;, &quot;[0][2]&quot;, &quot;[1][0]&quot;, &quot;[1][1]&quot;, &quot;[1][2]&quot;))) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_pointintervalh(.width = .95, size = 1/2) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) As in the text, our \\(\\gamma_{02}\\) and \\(\\gamma_{11}\\) parameters hovered around zero. For our next model, Model C in Table 5.4, we’ll drop those parameters. fit5.5 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.05&quot;) Let’s take a look at the results. print(fit5.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.227 0.011 0.206 0.248 1.002 1392 2552 ## sd(exper) 0.040 0.003 0.035 0.046 1.008 581 1215 ## cor(Intercept,exper) -0.293 0.070 -0.422 -0.151 1.005 601 1585 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.721 0.011 1.701 1.743 1.000 3031 3766 ## hgc_9 0.038 0.006 0.026 0.051 1.001 2388 3110 ## exper 0.049 0.003 0.044 0.054 1.000 2494 3147 ## exper:black -0.016 0.004 -0.025 -0.007 1.001 2886 3322 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.309 0.003 0.303 0.315 1.003 3050 3055 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Perhaps unsurprisingly, the parameter estimates for fit5.5 ended up quite similar to those from fit5.4. Happily, they’re also similar to those in the text. Let’s compute the WAIC estimates. fit5.3 &lt;- add_criterion(fit5.3, criterion = &quot;waic&quot;) fit5.4 &lt;- add_criterion(fit5.4, criterion = &quot;waic&quot;) fit5.5 &lt;- add_criterion(fit5.5, criterion = &quot;waic&quot;) Compare their WAIC estimates using \\(\\text{elpd}\\) difference scores. loo_compare(fit5.3, fit5.4, fit5.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.5 0.0 0.0 -2052.5 103.8 867.3 27.1 4105.0 207.7 ## fit5.4 -0.2 1.3 -2052.7 103.6 867.2 27.0 4105.3 207.3 ## fit5.3 -2.0 4.2 -2054.5 103.7 878.1 27.2 4109.0 207.4 The differences are subtle. Here are the WAIC weights. model_weights(fit5.3, fit5.4, fit5.5, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.3 fit5.4 fit5.5 ## 0.067 0.424 0.509 When we use weights, almost all goes to fit5.4 and fit5.5. Focusing on the trimmed model, fit5.5, let’s get ready to make our version of Figure 5.2. We’ll start with fitted() work. nd &lt;- crossing(black = 0:1, hgc_9 = c(0, 3)) %&gt;% expand(nesting(black, hgc_9), exper = seq(from = 0, to = 11, length.out = 30)) f &lt;- fitted(fit5.5, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) head(f) ## Estimate Est.Error Q2.5 Q97.5 black hgc_9 exper ## 1 1.721381 0.010767928 1.700710 1.742597 0 0 0.0000000 ## 2 1.739895 0.010327725 1.720119 1.760470 0 0 0.3793103 ## 3 1.758410 0.009961035 1.739433 1.778224 0 0 0.7586207 ## 4 1.776924 0.009676218 1.758353 1.796282 0 0 1.1379310 ## 5 1.795438 0.009480658 1.776846 1.814504 0 0 1.5172414 ## 6 1.813953 0.009379938 1.795780 1.832846 0 0 1.8965517 Here it is, our two-panel version of Figure 5.2. f %&gt;% mutate(black = factor(black, labels = c(&quot;Latinos and Whites&quot;, &quot;Blacks&quot;)), hgc_9 = factor(hgc_9, labels = c(&quot;9th grade dropouts&quot;, &quot;12th grade dropouts&quot;))) %&gt;% ggplot(aes(x = exper, color = black, fill = black)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate)) + scale_fill_viridis_d(NULL, option = &quot;C&quot;, begin = .25, end = .75) + scale_color_viridis_d(NULL, option = &quot;C&quot;, begin = .25, end = .75) + ylab(&quot;lnw&quot;) + coord_cartesian(ylim = c(1.6, 2.4)) + theme(panel.grid = element_blank()) + facet_wrap(~hgc_9) This leads in nicely to a brief discussion of posterior predictive checks (PPC). The basic idea is that good models should be able to retrodict the data used to produce them. Table 5.3 in the text introduced the data set by highlighting three participants and we went ahead and looked at their data in a plot. One way to do a PPC might be to plot their original data atop their model estimates. The fitted() function will help us with the preparatory work. nd &lt;- wages_pp %&gt;% filter(id %in% c(206, 332, 1028)) f &lt;- fitted(fit5.5, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) head(f) ## Estimate Est.Error Q2.5 Q97.5 id lnw exper ged postexp black hispanic hgc hgc_9 uerate ## 1 2.056004 0.1394962 1.785284 2.326442 206 2.028 1.874 0 0 0 0 10 1 9.200 ## 2 2.117000 0.1376096 1.846878 2.386530 206 2.297 2.814 0 0 0 0 10 1 11.000 ## 3 2.214335 0.1524529 1.916464 2.509061 206 2.482 4.314 0 0 0 0 10 1 6.295 ## 4 1.463993 0.1382339 1.188580 1.731079 332 1.630 0.125 0 0 0 1 8 -1 7.100 ## 5 1.647032 0.1113425 1.429756 1.866422 332 1.476 1.625 0 0 0 1 8 -1 9.600 ## 6 1.743188 0.1002504 1.547841 1.941536 332 1.804 2.413 0 0 0 1 8 -1 7.200 ## ue.7 ue.centert1 ue.mean ue.person.cen ue1 ## 1 2.200 0.000 8.831667 0.3683333 9.2 ## 2 4.000 1.800 8.831667 2.1683333 9.2 ## 3 -0.705 -2.905 8.831667 -2.5366667 9.2 ## 4 0.100 0.000 5.906500 1.1935000 7.1 ## 5 2.600 2.500 5.906500 3.6935000 7.1 ## 6 0.200 0.100 5.906500 1.2935000 7.1 Here’s the plot. f %&gt;% mutate(id = str_c(&quot;id = &quot;, id)) %&gt;% ggplot(aes(x = exper)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, color = id)) + geom_point(aes(y = lnw)) + scale_color_viridis_d(option = &quot;B&quot;, begin = .35, end = .8) + labs(subtitle = &quot;The black dots are the original data. The colored points and vertical lines are the participant-specific posterior\\nmeans and 95% intervals.&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~id) Although each participant got their own intercept and slope, the estimates all fall in straight lines. Since we’re only working with time-invariant covariates, that’s about the best we can do. Though our models can express gross trends over time, they’re unable to speak to variation from occasion to occasion. Just a little later on in this chapter and we’ll learn how to do better. 5.2.2 Practical problems that may arise when analyzing unbalanced data sets. With HMC, the issues with non-convergence aren’t quite the same as with maximum likelihood estimation. However, the basic issue still remains: Estimation of variance components requires that enough people have sufficient data to allow quantification of within-person residual variation–variation in the residuals over and above the fixed effects. If too many people have too little data, you will be unable to quantify [have difficulty quantifying] this residual variability. (p. 152) The big difference is that as Bayesians, our priors add additional information that will help us define the posterior distributions of our variance components. Thus our challenge will choosing sensible priors for our \\(\\sigma\\)s. 5.2.2.1 Boundary constraints. Unlike with the frequentist multilevel software discussed in the text, brms will not yield negative values on the \\(\\sigma\\) parameters. This is because the brms default is to set a lower limit of zero on those parameters. For example, see what happens when we execute fit5.3$model. fit5.3$model ## // generated with brms 2.12.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## // data for group-level effects of ID 1 ## int&lt;lower=1&gt; N_1; // number of grouping levels ## int&lt;lower=1&gt; M_1; // number of coefficients per level ## int&lt;lower=1&gt; J_1[N]; // grouping indicator per observation ## // group-level predictor values ## vector[N] Z_1_1; ## vector[N] Z_1_2; ## int&lt;lower=1&gt; NC_1; // number of group-level correlations ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## vector[K] b; // population-level effects ## real&lt;lower=0&gt; sigma; // residual SD ## vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations ## matrix[M_1, N_1] z_1; // standardized group-level effects ## cholesky_factor_corr[M_1] L_1; // cholesky factor of correlation matrix ## } ## transformed parameters { ## matrix[N_1, M_1] r_1; // actual group-level effects ## // using vectors speeds up indexing in loops ## vector[N_1] r_1_1; ## vector[N_1] r_1_2; ## // compute actual group-level effects ## r_1 = (diag_pre_multiply(sd_1, L_1) * z_1)&#39;; ## r_1_1 = r_1[, 1]; ## r_1_2 = r_1[, 2]; ## } ## model { ## // initialize linear predictor term ## vector[N] mu = X * b; ## for (n in 1:N) { ## // add more terms to the linear predictor ## mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n]; ## } ## // priors including all constants ## target += normal_lpdf(b[1] | 1.335, 1); ## target += normal_lpdf(b[2] | 0, 0.5); ## target += student_t_lpdf(sigma | 3, 0, 1) ## - 1 * student_t_lccdf(0 | 3, 0, 1); ## target += student_t_lpdf(sd_1 | 3, 0, 1) ## - 2 * student_t_lccdf(0 | 3, 0, 1); ## target += normal_lpdf(to_vector(z_1) | 0, 1); ## target += lkj_corr_cholesky_lpdf(L_1 | 4); ## // likelihood including all constants ## if (!prior_only) { ## target += normal_lpdf(Y | mu, sigma); ## } ## } ## generated quantities { ## // compute group-level correlations ## corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1); ## vector&lt;lower=-1,upper=1&gt;[NC_1] cor_1; ## // extract upper diagonal of correlation matrix ## for (k in 1:M_1) { ## for (j in 1:(k - 1)) { ## cor_1[choose(k - 1, 2) + j] = Cor_1[j, k]; ## } ## } ## } That returned the Stan code corresponding to our brms::brm() code, above. Notice the second and third lines in the parameters block. Both contained &lt;lower=0&gt;, which indicated the lower bounds for those parameters was zero. See? Stan has you covered. Let’s load the wages_small_pp.csv data. wages_small_pp &lt;- read_csv(&quot;data/wages_small_pp.csv&quot;) %&gt;% rename(hgc_9 = hcg.9) glimpse(wages_small_pp) ## Rows: 257 ## Columns: 5 ## $ id &lt;dbl&gt; 206, 206, 206, 266, 304, 329, 329, 329, 336, 336, 336, 394, 394, 394, 518, 518, 541… ## $ lnw &lt;dbl&gt; 2.028, 2.297, 2.482, 1.808, 1.842, 1.422, 1.308, 1.885, 1.892, 1.279, 2.224, 2.383,… ## $ exper &lt;dbl&gt; 1.874, 2.814, 4.314, 0.322, 0.580, 0.016, 0.716, 1.756, 1.910, 2.514, 3.706, 1.890,… ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,… ## $ hgc_9 &lt;dbl&gt; 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 2, -1, 0… Here’s the distribution of the number of measurement occasions for our small data set. wages_small_pp %&gt;% group_by(id) %&gt;% count() %&gt;% ggplot(aes(x = n)) + geom_bar() + scale_x_continuous(&quot;# measurement occasions&quot;, breaks = 1:13, limits = c(.5, 13)) + ylab(&quot;count of cases&quot;) + coord_flip() + theme(panel.grid = element_blank()) Our brm() code is the same as that for fit5.5, above, with just a slightly different data argument. If we wanted to, we could be hasty and just use update(), instead. But since we’re still practicing setting our priors and such, here we’ll be exhaustive. fit5.6 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.06&quot;) print(fit5.6) ## Warning: There were 114 divergent transitions after warmup. Increasing adapt_delta above 0.8 may ## help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.29 0.05 0.19 0.37 1.00 841 1042 ## sd(exper) 0.04 0.03 0.00 0.12 1.02 178 59 ## cor(Intercept,exper) -0.06 0.32 -0.62 0.60 1.00 662 1326 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.74 0.05 1.64 1.83 1.00 1190 644 ## hgc_9 0.05 0.03 -0.00 0.10 1.00 1607 2810 ## exper 0.05 0.02 0.00 0.10 1.01 662 308 ## exper:black -0.06 0.04 -0.13 0.02 1.00 2812 2056 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.34 0.02 0.30 0.39 1.00 1131 1351 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s walk through this slow. You may have noticed that warning message about divergent transitions. We’ll get to that in a bit. First focus on the parameter estimates for sd(exper). Unlike in the text, our posterior mean is not 0.000. But do remember that our posterior is parameterized in the \\(\\sigma\\) metric. Let’s do a little converting and look at it in a plot. post &lt;- posterior_samples(fit5.6) v &lt;- post %&gt;% transmute(sigma_1 = sd_id__exper) %&gt;% mutate(sigma_2_1 = sigma_1^2) %&gt;% set_names(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;) %&gt;% pivot_longer(everything()) Plot. v %&gt;% ggplot(aes(x = value, y = name)) + geom_halfeyeh(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = parse(text = c(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;))) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) In the \\(\\sigma\\) metric, the posterior is bunched up a little on the boundary, but much of its mass is a gently right-skewed mound concentrated in the 0—0.1 range. When we convert the posterior to the \\(\\sigma^2\\) metric, the parameter appears much more bunched up against the boundary. Because we typically summarize our posteriors with means or medians, the point estimate still moves away from zero. v %&gt;% group_by(name) %&gt;% mean_qi() %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[1] 0.0426 0.002 0.117 0.95 mean qi ## 2 sigma[1]^2 0.0028 0 0.0138 0.95 mean qi v %&gt;% group_by(name) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[1] 0.0362 0.002 0.117 0.95 median qi ## 2 sigma[1]^2 0.0013 0 0.0138 0.95 median qi But it really does start to shoot to zero if we attempt to summarize the central tendency with the mode, as within the maximum likelihood paradigm. v %&gt;% group_by(name) %&gt;% mode_qi() %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[1] 0.0121 0.002 0.117 0.95 mode qi ## 2 sigma[1]^2 0.000300 0 0.0138 0.95 mode qi Backing up to that warning message, we were informed that “Increasing adapt_delta above 0.8 may help.” The adapt_delta parameter ranges from 0 to 1. The brm() default is .8. In my experience, increasing to .9 or .99 is often a good place to start. For this model, .9 wasn’t quite enough, but .99 worked. Here’s how to do it. fit5.7 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .99), file = &quot;fits/fit05.07&quot;) Now look at the summary. print(fit5.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.28 0.05 0.17 0.37 1.01 428 210 ## sd(exper) 0.04 0.03 0.00 0.11 1.01 355 499 ## cor(Intercept,exper) -0.02 0.32 -0.61 0.60 1.00 2737 2523 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.73 0.05 1.64 1.83 1.00 2458 2885 ## hgc_9 0.05 0.02 0.00 0.10 1.00 2485 2990 ## exper 0.05 0.02 0.01 0.10 1.00 2162 2419 ## exper:black -0.05 0.04 -0.12 0.02 1.00 2441 2727 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.35 0.02 0.31 0.40 1.01 607 577 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our estimates were pretty much the same as before. Happily, this time we got our summary without any warning signs. It won’t always be that way, so make sure to take adapt_delta warnings seriously. Now do note that for both fit5.6 and fit5.7, our effective sample sizes for \\(\\sigma_0\\) and \\(\\sigma_1\\) aren’t terribly large relative to the total number of post-warmup draws, 4,000. If it was really important that you had high-quality summary statistics for these parameters, you might need to refit the model with something like iter = 20000, warmup = 2000. In Model B in Table 5.5, Singer and Willett gave the results of a model with the boundary constraints on the \\(\\sigma^2\\) parameters removed. I am not going to attempt something like that with brms. If you’re interested, you’re on your own. But we will fit a version of their Model C where we’ve removed the \\(\\sigma_1\\) parameter. Notice that this results in our removal of the LKJ prior for \\(\\rho_{01}\\), too. Without a \\(\\sigma_1\\), there’s no other parameter for our lonely \\(\\sigma_0\\) to correlate with. fit5.8 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.08&quot;) Here is the basic model summary. print(fit5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 | id) ## Data: wages_small_pp (Number of observations: 257) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.30 0.04 0.22 0.37 1.00 943 1621 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.74 0.05 1.64 1.83 1.00 2986 2890 ## hgc_9 0.05 0.02 -0.00 0.09 1.00 2789 3164 ## exper 0.05 0.02 0.01 0.10 1.00 2956 3167 ## exper:black -0.06 0.03 -0.13 0.01 1.00 2924 3110 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.34 0.02 0.30 0.39 1.00 1380 2268 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). No warning messages and our effective samples for \\(\\sigma_0\\) improved a bit. Compute the WAIC for both models. fit5.7 &lt;- add_criterion(fit5.7, criterion = &quot;waic&quot;) fit5.8 &lt;- add_criterion(fit5.8, criterion = &quot;waic&quot;) Compare. loo_compare(fit5.7, fit5.8, criterion = &quot;waic&quot;) %&gt;% print(simplify = F, digits = 3) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.8 0.000 0.000 -132.460 20.677 69.957 12.202 264.919 41.355 ## fit5.7 -3.651 3.321 -136.111 23.104 72.414 14.745 272.222 46.208 Yep. Those WAIC estimates are quite similar and when you compare them with formal \\(\\text{elpd}\\) difference scores, the standard error is about the same size as the difference itself. Though we’re stepping away from the text a bit, we should explore more alternatives for this boundary issue. The Stan team has put together a Prior Choice Recommendations wiki at https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations. In the Boundary-avoiding priors for modal estimation (posterior mode, MAP, marginal posterior mode, marginal maximum likelihood, MML) section, we read: These are for parameters such as group-level scale parameters, group-level correlations, group-level covariance matrix What all these parameters have in common is that (a) they’re defined on a space with a boundary, and (b) the likelihood, or marginal likelihood, can have a mode on the boundary. Most famous example is the group-level scale parameter tau for the 8-schools hierarchical model. With full Bayes the boundary shouldn’t be a problem (as long as you have any proper prior). But with modal estimation, the estimate can be on the boundary, which can create problems in posterior predictions. For example, consider a varying-intercept varying-slope multilevel model which has an intercept and slope for each group. Suppose you fit marginal maximum likelihood and get a modal estimate of 1 for the group-level correlation. Then in your predictions the intercept and slope will be perfectly correlated, which in general will be unrealistic. For a one-dimensional parameter restricted to be positive (e.g., the scale parameter in a hierarchical model), we recommend Gamma(2,0) prior (that is, p(tau) proportional to tau) which will keep the mode away from 0 but still allows it to be arbitrarily close to the data if that is what the likelihood wants. For details see this paper by Chung et al.: http://www.stat.columbia.edu/~gelman/research/published/chung_etal_Pmetrika2013.pdf Gamma(2,0) biases the estimate upward. When number of groups is small, try Gamma(2,1/A), where A is a scale parameter representing how high tau can be. We should walk those Gamma priors out, a bit. The paper by Chung et al is quite helpful. We’ll first let them give us a little more background in the topic: Zero group-level variance estimates can cause several problems. Zero variance can go against prior knowledge of researchers and results in underestimation of uncertainty in fixed coefficient estimates. Inferences for groups are often of interest to researchers, but when the group-level variance is estimated as zero, the resulting predictions of the group-level errors will all be zero, so one fails to find unexplained differences between groups. In addition, uncertainty in predictions for new and existing groups is also understated. (p. 686) They expounded further on page 687. When a variance parameter is estimated as zero, there is typically a large amount of uncertainty about this variance. One possibility is to declare in such situations that not enough information is available to estimate a multilevel model. However, the available alternatives can be unappealing since, as noted in the introduction, discarding a variance component or setting the variance to zero understates the uncertainty. In particular, standard errors for coefficients of covariates that vary between groups will be too low as we will see in Section 2.2. The other extreme is to fit a regression with indicators for groups (a fixed-effects model), but this will overcorrect for group effects (it is mathematically equivalent to a mixed-effects model with variance set to infinity), and also does not allow predictions for new groups. Degenerate variance estimates lead to complete shrinkage of predictions for new and existing groups and yield estimated prediction standard errors that understate uncertainty. This problem has been pointed out by Li and Lahiri (2010) and Morris and Tang (2011) in small area estimation… If zero variance is not a null hypothesis of interest, a boundary estimate, and the corresponding zero likelihood ratio test statistic, should not necessarily lead us to accept the null hypothesis and to proceed as if the true variance is zero. In their paper, they covered both penalized maximum likelihood and full Bayesian estimation. We’re just going to focus on Bayes, but some of the quotes will contain ML talk. Further, we read: We recommend a class of log-gamma penalties (or gamma priors) that in our default setting (the log-gamma(2, \\(\\lambda\\)) penalty with \\(\\lambda \\rightarrow 0\\)) produce maximum penalized likelihood (MPL) estimates (or Bayes modal estimates) approximately one standard error away from zero when the maximum likelihood estimate is at zero. We consider these priors to be weakly informative in the sense that they supply some direction but still allow inference to be driven by the data. The penalty has little influence when the number of groups is large or when the data are informative about the variance, and the asymptotic mean squared error of the proposed estimator is the same as that of the maximum likelihood estimator. (p. 686) In the upper left panel of Figure 3, Chung and colleagues gave an example of what they mean by \\(\\lambda \\rightarrow 0\\): \\(\\lambda = 0.1\\). Here’s an example of what \\(\\operatorname{Gamma} (2, 0.1)\\) looks like across the parameter space of 0 to 100. tibble(x = c(0, 100)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = dgamma, n = 1e2, args = list(shape = 2, rate = 0.1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Given we’re working with data on the log scale, that’s a massively permissive prior. Let’s zoom in and see what it means for the parameter space of possible values for our data. tibble(x = c(0, 2)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = dgamma, n = 1e2, args = list(shape = 2, rate = .1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Now keep that picture in mind as we read further along in the paper: In addition, with \\(\\lambda \\rightarrow 0\\), the gamma density function has a positive constant derivative at zero, which allows the likelihood to dominate if it is strongly curved near zero. The positive constant derivative implies that the prior is linear at zero so that there is no dead zone near zero. The top-left panel of Figure 3 shows that the gamma(2,0.1) density increases linearly from zero with a gentle slope. The shape will be even flatter with a smaller rate parameter. (p. 691) In case you’re not familiar with the gamma distribution, the rate parameter is what we’ve been calling \\(\\lambda\\). Let’s test this baby out with our model. Here’s how to specify it in brms. fit5.9 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(gamma(2, 0.1), class = sd, group = id, coef = exper), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.09&quot;) Notice that we didn’t bother fooling around with adapt_delta and the model fit just fine. Here are the results. print(fit5.9, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.282 0.050 0.180 0.379 1.003 1147 1660 ## sd(exper) 0.057 0.029 0.010 0.120 1.009 554 1100 ## cor(Intercept,exper) -0.085 0.312 -0.623 0.547 1.001 2132 3021 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.734 0.050 1.638 1.831 1.001 3057 3036 ## hgc_9 0.049 0.025 0.000 0.098 1.001 3301 2706 ## exper 0.050 0.025 0.001 0.099 1.000 3194 2886 ## exper:black -0.051 0.038 -0.126 0.026 1.000 3712 3086 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.344 0.023 0.302 0.392 1.002 1488 2560 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our sd(exper) is still quite close to zero. But notice how not the lower level of the 95% interval is higher than zero. Here’s what it looks like in both \\(\\sigma\\) and \\(\\sigma^2\\) metrics. posterior_samples(fit5.9) %&gt;% transmute(sigma_1 = sd_id__exper) %&gt;% mutate(sigma_2_1 = sigma_1^2) %&gt;% set_names(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_halfeyeh(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = parse(text = c(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;))) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) Let’s zoom in on the leftmost part of the plot. posterior_samples(fit5.9) %&gt;% transmute(sigma_1 = sd_id__exper) %&gt;% mutate(sigma_2_1 = sigma_1^2) %&gt;% set_names(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_halfeyeh(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = parse(text = c(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;))) + coord_cartesian(xlim = c(0, 0.01)) + theme(panel.grid = element_blank()) Although we are still brushing up on the boundary with \\(\\sigma_1^2\\), the mode is no longer at zero. In the discussion, Chung and colleagues pointed out “sometimes weak prior information is available about a variance parameter. When \\(\\alpha = 2\\), the gamma density has its mode at \\(1 / \\lambda\\), and so one can use the \\(\\text{gamma} (\\alpha, \\lambda)\\) prior with \\(1 / \\lambda\\) set to the prior estimate of \\(\\sigma_\\theta\\)” (p. 703). Let’s say we only had our wages_small_pp, but the results of something like the wages_pp data were published by some earlier group of researchers. In this case, we do have good prior data; we have the point estimate from the model of the wages_pp data! Here’s what that was in terms of the median. posterior_samples(fit5.3) %&gt;% median_qi(sd_id__exper) ## sd_id__exper .lower .upper .width .point .interval ## 1 0.04154273 0.036155 0.0467437 0.95 median qi And here’s what that value is when set as the divisor of 1. 1 / 0.04154273 ## [1] 24.0716 What does that distribution look like? tibble(x = c(0, 1)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = dgamma, n = 1e2, args = list(shape = 2, rate = 24.0716)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) So this is much more informative than our gamma(2, 0.1) prior from before. But given the magnitude of the estimate from fit5.3, it’s still fairly liberal. Let’s practice using it. fit5.10 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(gamma(2, 24.0716), class = sd, group = id, coef = exper), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.10&quot;) Check out the results. print(fit5.10, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.283 0.049 0.179 0.375 1.004 929 1653 ## sd(exper) 0.042 0.024 0.007 0.100 1.004 565 972 ## cor(Intercept,exper) -0.038 0.312 -0.612 0.573 1.000 2972 2811 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.734 0.049 1.639 1.832 1.001 2606 2481 ## hgc_9 0.048 0.025 -0.002 0.096 1.001 2622 2757 ## exper 0.051 0.023 0.005 0.096 1.001 2940 2232 ## exper:black -0.055 0.037 -0.129 0.019 1.001 3144 2883 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.345 0.023 0.303 0.392 1.003 1231 1838 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we compare the three ways to specify the \\(\\sigma_1\\) prior with the posterior from the original model fit with the full data set. For simplicity, we’ll just look at the results in the brms-like \\(\\sigma\\) metric. Hopefully by now you’ll know how to do the conversions to get the values into the \\(\\sigma^2\\) metric. tibble(`full data, student_t(3, 0, 1) prior` = VarCorr(fit5.3, summary = F)[[1]][[1]][1:4000, 2], `small data, student_t(3, 0, 1) prior` = VarCorr(fit5.7, summary = F)[[1]][[1]][, 2], `small data, gamma(2, 0.1) prior` = VarCorr(fit5.9, summary = F)[[1]][[1]][, 2], `small data, gamma(2, 24.0716) prior` = VarCorr(fit5.10, summary = F)[[1]][[1]][, 2]) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;full data, student_t(3, 0, 1) prior&quot;, &quot;small data, student_t(3, 0, 1) prior&quot;, &quot;small data, gamma(2, 0.1) prior&quot;, &quot;small data, gamma(2, 24.0716) prior&quot;))) %&gt;% ggplot(aes(x = value, y = 0, fill = name == &quot;full data, student_t(3, 0, 1) prior&quot;)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_halfeyeh(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_manual(values = c(&quot;grey75&quot;, &quot;darkgoldenrod2&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~name, ncol = 1) One thing to notice is that when you’re working with full Bayesian estimation with even a rather vague prior with a boundary on zero, the measure of central tendency in the posterior is away from zero. Things get more compact when you’re working in the \\(\\sigma^2\\) metric. But remember that when we’re fitting our models with brms, we’re in the \\(\\sigma\\) metric, anyway. And with either of these three options, you don’t have a compelling reason to set the \\(\\sigma_\\theta\\) parameter to zero the way you would with ML. Even a rather vague prior will add enough information to the model that we can feel confident about keeping our theoretically-derived \\(\\sigma_1\\) parameter. 5.2.2.2 Nonconvergence [i.e., it’s time to talk chains and such]. As discussed in section 4.3, all multilevel modeling programs implement iterative numeric algorithms for model fitting. (p. 155). This is also true for our Stan-propelled brms software. However, what’s going on under the hood, here, is not what’s happening with the frequentist packages discussed by Singer and Willett. We’re using Hamiltonian Monte Carlo (HMC) to draw from the posterior. To my eye, Paul Bürkner gave in a preprint probably the clearest and most direct introduction to why we need fancy algorithms like HMC to fit Bayesian models. First, Bürkner warmed up by contrasting Bayes with conventional frequentist inference: In frequentist statistics, parameter estimates are usually obtained by finding those parameter values that maximise the likelihood. In contrast, Bayesian statistics estimate the full (joint) posterior distribution of the parameters. This is not only fully consistent with probability theory, but also much more informative than a single point estimate (and an approximate measure of uncertainty commonly known as ‘standard error’). Those iterative algorithms Singer and Willett discussed in this section, that’s what they’re doing. They are maximizing the likelihood. But with Bayes, we have the more challenging goal of describing the entire posterior distribution, which is the product of the likelihood and the prior. As such, Obtaining the posterior distribution analytically is only possible in certain cases of carefully chosen combinations of prior and likelihood, which may considerably limit modelling flexibilty but yield a computational advantage. However, with the increased power of today’s computers, Markov-Chain Monte-Carlo (MCMC) sampling methods constitute a powerful and feasible alternative to obtaining posterior distributions for complex models in which the majority of modeling decisions is made based on theoretical and not computational grounds. Despite all the computing power, these sampling algorithms are computationally very intensive and thus fitting models using full Bayesian inference is usually much slower than in point estimation techniques. However, advantages of Bayesian inference–such as greater modeling flexibility, prior distributions, and more informative results–are often worth the increased computational cost (Gelman, Carlin, Stern, and Rubin 2014). (p. 8) The gritty details are well beyond the scope of this project. If you’d like a more thorough walk-through on why it’s analytically and computationally challenging to get the posterior, I recommend working through the first several chapters in Kruschke’s (2015) Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan. But so anyways, our primary algorithm is HMC as implemented by Stan. You can find all kinds of technical details at https://mc-stan.org/users/documentation/. Because it’s rather difficult to describe our Bayesian multilevel models analytically, we use HMC to draw from the posterior instead. We then summarize the marginal and joint distributions of those parameters with things like measures of central tendency (i.e., means, medians, modes) and spread (i.e., standard deviations, percentile-based intervals). We make lots of plots. And somewhat like with the frequentist iterative algorithms, we need to make sure our sweet Stan-based HMC is working well, too. One way is with trace plots. 5.2.2.2.1 Trace plots. We can get the trace plots for a model by placing a brm() fit object into the plot() function. Here’s an example with the full model, fit5.4. plot(fit5.4) You’ll notice we get two plots for each of the major model parameters, the \\(\\gamma\\)’s, the \\(\\sigma\\)’s and the \\(\\rho\\)’s. The plots on the left are the density plots for each parameter. On the right, we have the actual trace plots. On the \\(x\\)-axis, we have an ordering of the posterior draws; on the \\(y\\), we have the parameter space. Since we the default 4 HMC chains to draw from the posterior, those four chains are depicted by different colored lines. We generally like it when the lines for our chains all overlap with each other in a stable zig-zag sort of way. Trac eplots are sometimes called caterpillar plots because, when things are going well, they often resemble nice multicolored fuzzy caterpillars. If you’d like more control over your trace plot visuals, you might check out the bayesplot package. library(bayesplot) Our main function will be mcmc_trace(). Unlike with the brms::plot() method, bayesplog::mcmc_trace() takes the posterior draws themselves as input. So we’ll have to use posterior_samples() first. By default, posterior_samples() does not extract information about which chain a given draw is from. So we’ll have to add the add_chain = T argument. post &lt;- posterior_samples(fit5.4, add_chain = T) We can use the pars argument to focus on particular parameters. mcmc_trace(post, pars = &quot;sigma&quot;) If we use the pars = vars(...) format, we can use function helpers from dplyr to select subsets of parameters. For example, here’s how we might single out the \\(\\gamma\\)s. mcmc_trace(post, pars = vars(starts_with(&quot;b_&quot;)), facet_args = list(ncol = 2)) Notice how we used the facet_args argument to adjust the number of columns in the output. We can also use familiar ggplot2 functions to customize the plots further. post %&gt;% mutate(`sigma[0]` = sd_id__Intercept, `sigma[1]` = sd_id__exper, `sigma[epsilon]` = sigma) %&gt;% mcmc_trace(pars = vars(starts_with(&quot;sigma[&quot;)), facet_args = list(labeller = label_parsed)) + scale_color_viridis_d(option = &quot;A&quot;) + scale_x_continuous(NULL, breaks = NULL) + ggtitle(&quot;I can&#39;t wait to show these traceplots to my mom.&quot;) + theme_grey() + theme(legend.position = &quot;bottom&quot;,, panel.grid = element_blank(), panel.grid.major.y = element_line(color = &quot;white&quot;, size = 1/4), strip.text = element_text(size = 12)) Trace plots are connected to other important concepts, like autocorrelation and effective sample size. 5.2.2.2.2 Autocorrelation. When using Markov chain Monte Carlo methods, of which HMC is a special case, the notions of autocorrelation and effective sample size are closely connected. Both have to do with the question, How many post-warmup draws from the posterior do I need to take? If you take too few, you won’t have a good sense of the shape of the posterior. If you take more than necessary, you’re just wasting time and computer memory. Here’s how McElreath introduced the topic in his text: So how many samples do we need for accurate inference about the posterior distribution? It depends. First, what really matters is the effective number of samples, not the raw number. The effective number of samples is an estimate of the number of independent samples from the posterior distribution. Markov chains are typically autocorrelated, so that sequential samples are not entirely independent. Stan chains tend to be less autocorrelated than those produced by other engines [e.g., the Gibbs sampler], but there is always some autocorrelation. (p. 255, emphasis in the original) I’m not aware of a way to query the autocorrelations from a brm() fit using brms convenience functions. However, we can get those diagnostics from the bayesplot::mcmc_acf() function. mcmc_acf(post, pars = vars(starts_with(&quot;b_&quot;)), lags = 10) + theme_grey() + theme(panel.grid = element_blank()) The mcmc_acf() function gives a wealth of granular output. The columns among the plots are the specified parameters. The rows are the chains, one for each. In this particular case, the autocorrelations were quite low for all our \\(\\gamma\\) parameters by the second or third lag. That’s really quite good and not uncommon for HMC. Do note, however, that this won’t always be the case. For example, here are the plots for our variance parameters and \\(\\rho_{01}\\). mcmc_acf(post, pars = vars(starts_with(&quot;sd_&quot;), &quot;sigma&quot;, starts_with(&quot;cor&quot;)), lags = 10) + theme_grey() + theme(panel.grid = element_blank(), strip.text = element_text(size = 7)) On the whole, all of them are pretty okay. But notice how the autocorrelations for \\(\\sigma_1\\) and \\(\\rho_{01}\\) remained relatively high up until the 10th lag. The plots from mcmc_act() are quite handy for focused diagnostics. But if you want a more global perspective, they’re too tedious. Fortunately for us, we have other diagnostic tools. 5.2.2.2.3 Effective sample size. Above we quoted McElreath as pointing out “what really matters is the effective number of samples, not the raw number.” With brms, you typically get the effective number of samples in the print() or summary() output. Here it is again for fit4. summary(fit5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.01 0.21 0.25 1.00 1799 3037 ## sd(exper) 0.04 0.00 0.04 0.05 1.00 595 1296 ## cor(Intercept,exper) -0.30 0.07 -0.42 -0.15 1.00 623 1753 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.72 0.01 1.69 1.74 1.00 2180 3321 ## hgc_9 0.03 0.01 0.02 0.05 1.00 2751 3052 ## black 0.02 0.02 -0.03 0.06 1.00 2220 3137 ## exper 0.05 0.00 0.04 0.05 1.00 1896 2619 ## hgc_9:exper 0.00 0.00 -0.00 0.00 1.00 2538 2855 ## black:exper -0.02 0.01 -0.03 -0.01 1.00 1808 3014 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.31 0.00 0.30 0.31 1.00 3015 3306 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See that Eff.Sample column on the right? That’s our effective sample size. Like the autocorrelations, each parameter gets its own estimate. You might compare the numbers to the number of post-warmup iterations, 4,000 in this case and by default. You may wonder, how many effective samples do I need? Back to McElreath: If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. (p. 255) In addition to Eff.Sample from brms summary output, you can also extract the ratio of Eff.Sample to post-warmup draws from the bayesplot::neff_ration() function. Let’s practice with fit5.4. nr &lt;- neff_ratio(fit5.4) str(nr) ## Named num [1:1787] 0.483 0.606 0.492 0.42 0.56 ... ## - attr(*, &quot;names&quot;)= chr [1:1787] &quot;b_Intercept&quot; &quot;b_hgc_9&quot; &quot;b_black&quot; &quot;b_exper&quot; ... Good old str() informed us we got a named numeric vector of 1787 rows. We can display that output by plugging it into mcmc_neff(). mcmc_neff(nr) Because we have so many parameters, it can be hard to understand what’s going on. Let’s try again, this time restricted to the first 50 parameters. mcmc_neff(nr[1:50]) Of those first parameters, mcmc_neff() rank ordered them from lowest \\(\\frac{n_\\text{eff}}{N}\\) to highest. You can see by the legend that the bayesplot team has provided us with a heuristic categorization of those with values above 0.5, those equal to that or fewer, and those equal to 0.1 or fewer. The idea is we typically prefer higher ratios to lower ones. If all you wanted was to plot the effective sample sizes of our primary summary parameters, you can wrangle our nr object and make the plot yourself. nr &lt;- nr %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;parameter&quot;, &quot;ratio&quot;) %&gt;% mutate(n_eff = ratio * 4000) nr %&gt;% slice(1:10) %&gt;% ggplot(aes(x = reorder(parameter, n_eff))) + geom_linerange(aes(ymin = 0, ymax = n_eff)) + geom_point(aes(y = n_eff)) + labs(y = &quot;effective sample size&quot;, x = NULL) + coord_flip(ylim = c(0, 4000)) + theme(panel.grid = element_blank(), axis.text.y = element_text(hjust = 0)) You may have noticed that in the first mcmc_neff() plot, many of the parameters seemed to have \\(\\frac{n_\\text{eff}}{N}\\) ratios above 1. What’s that about? As is turns out, the sampling in Stan is so good that sometimes the HMC chains for a parameter can be negatively autocorrelated. When this is the case, your effective sample size can be larger than your actual sample size. Madness, I know. This was the case for many of our residuals. Here’s a look at the first 10. nr %&gt;% slice(11:20) ## parameter ratio n_eff ## 1 r_id[31,Intercept] 1.402909 5611.635 ## 2 r_id[36,Intercept] 1.125360 4501.439 ## 3 r_id[53,Intercept] 1.342833 5371.333 ## 4 r_id[122,Intercept] 1.299172 5196.689 ## 5 r_id[134,Intercept] 1.099295 4397.179 ## 6 r_id[145,Intercept] 1.120872 4483.487 ## 7 r_id[155,Intercept] 1.121970 4487.881 ## 8 r_id[173,Intercept] 1.462240 5848.962 ## 9 r_id[206,Intercept] 1.532652 6130.609 ## 10 r_id[207,Intercept] 1.093107 4372.428 Look at those sweet ratio and n_eff values! Let’s take the first 6 and check their autocorrelation plots. mcmc_acf(post, pars = vars(`r_id[31,Intercept]`:`r_id[145,Intercept]`), lags = 5) + theme_grey() + theme(panel.grid = element_blank(), strip.text = element_text(size = 8)) See those dips below zero for the first lag in each? That’s what a negative autocorrelation looks like. Beautiful. For more on negative autocorrelations within chains and how it influences the number of effective samples, check out this thread on the Stan forums where many members of the Stan team chimed in. 5.2.2.2.4 \\(\\hat R\\). Return again to the default print() output for a brms::brm() fit. print(fit5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.01 0.21 0.25 1.00 1799 3037 ## sd(exper) 0.04 0.00 0.04 0.05 1.00 595 1296 ## cor(Intercept,exper) -0.30 0.07 -0.42 -0.15 1.00 623 1753 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.72 0.01 1.69 1.74 1.00 2180 3321 ## hgc_9 0.03 0.01 0.02 0.05 1.00 2751 3052 ## black 0.02 0.02 -0.03 0.06 1.00 2220 3137 ## exper 0.05 0.00 0.04 0.05 1.00 1896 2619 ## hgc_9:exper 0.00 0.00 -0.00 0.00 1.00 2538 2855 ## black:exper -0.02 0.01 -0.03 -0.01 1.00 1808 3014 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.31 0.00 0.30 0.31 1.00 3015 3306 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The last column for each parameter is Rhat. “Rhat is a complicated estimate of the convergence of the Markov chains to the target distribution. It should approach 1.00 from above, when all is well” (McElreath, 2015, p. 250). We can extract \\(\\hat R\\) directly with the rhat() function. rhat(fit5.4) %&gt;% str() ## Named num [1:1787] 1 1 1 1 1 ... ## - attr(*, &quot;names&quot;)= chr [1:1787] &quot;b_Intercept&quot; &quot;b_hgc_9&quot; &quot;b_black&quot; &quot;b_exper&quot; ... For our fit5.4, the brms::rhat() function returned a named numeric vector, with one row for each of the 1787 parameters in the model. You can subset the rhat() output to focus on a few parameters. rhat(fit5.4)[1:10] ## b_Intercept b_hgc_9 b_black b_exper ## 0.9995704 1.0014666 1.0000900 0.9998813 ## b_hgc_9:exper b_black:exper sd_id__Intercept sd_id__exper ## 1.0004004 1.0002303 1.0033750 1.0026773 ## cor_id__Intercept__exper sigma ## 1.0042858 1.0006423 Or you might get a more global perspective with a plot. rhat(fit5.4) %&gt;% data.frame() %&gt;% set_names(&quot;rhat&quot;) %&gt;% ggplot(aes(x = rhat)) + geom_vline(xintercept = 1, color = &quot;white&quot;) + geom_histogram(binwidth = .0001) + theme(panel.grid = element_blank()) The bayesplot package offers a convenience function for plotting rhat() output. Here we’ll focus on the first 20 parameters. mcmc_rhat(rhat(fit5.4)[1:20]) + yaxis_text(hjust = 0) By default, mcmc_rhat() does not return text on the y-axis. But you can retrieve that text with the yaxis_text() function. For more on the \\(\\hat R\\), you might check out the Rhat: potential scale reduction statistic subsection of Gabry and Modrák’s vignette, Visual MCMC diagnostics using the bayesplot package. We should also point out that the Stan team has found some deficiencies with the \\(\\hat{R}\\). They’ve made recommendations that will be implemented in the Stan ecosystem sometime soon. In the meantime, you can read all about it in their preprint and in Dan Simpson’s guest blog, Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it. If you learn best by sassy twitter banter, click through this interchange among some of our Stan team all-stars. 5.2.3 Distinguishing among different types of missingness. Missingness, in and of itself, is not necessarily problematic. It all depends upon what statisticians call the type of missingness. In seminal work on this topic, Little (1995), refining earlier work with Rubin (Little &amp; Rubin, 1987), distinguished among three types of missingness: (1) missing completely at random (MCAR); (2) covariate-dependent dropout (CDD); and (3) missing at random (MAR) (see also Schafer, 1997). When we say that data are MCAR, we argue that the observed values are a random sample of all the values that could have been observed (according ot plan), had there been no missing data. …Covariate dependent dropout (CDD) is a less restrictive assumption that permits associations between the probability of missingness and observed predictor values (“covariates”). Data can be CDD even if the probability of missingness is systematically related to either TIME or observed substantive predictors. …When data are MAR, the probability of missingness can depend upon any observed data, for either the predictors or any outcome values. It cannot, however, depend upon an unobserved value of either any predictor or the outcome. (pp. 157–158, emphasis in the original) For some more current introductions to missing data methods, I recommend Enders’ (2010) Applied Missing Data Analysis, for which you can find a free sample chapter here, and Little and Rubin’s (2019) Statistical Analysis with Missing Data, 3rd Edition. You might also check out van Burren’s great online text Flexible Imputation of Missing Data. Second Edition. If you’re a fan of the podcast medium, you might listen to episode 16 from the Quantitude podcast, IF EPISODE=16 THEN EPISODE=-999;, in which Patrick Curran and Greg Hancock do a fine job introducing the basics of missing data. And very happily, brms has several ways to handle missing data, about which you can learn more from Bürkner’s vignette, Handle Missing Values with brms. 5.3 Time-varying predictors A time-varying predictor is a variable whose values may differ over time. Unlike their time-invariant cousins, which record an individual’s static status, time-varying predictors record an individual’s potentially differing status on each associated measurement occasion. Some time-varying predictors have values that change naturally; others have values that change by design. (pp. 159–160, emphasis in the original) 5.3.1 Including the main effect of a time-varying predictor. You can find Ginexi and colleagues’ (2000) unemployment study data in the reading_pp.csv file. unemployment_pp &lt;- read_csv(&quot;data/unemployment_pp.csv&quot;) head(unemployment_pp) ## # A tibble: 6 x 4 ## id months cesd unemp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 103 1.15 25 1 ## 2 103 5.95 16 1 ## 3 103 12.9 33 1 ## 4 641 0.789 27 1 ## 5 641 4.86 7 0 ## 6 641 11.8 25 0 We have 254 unique participants. unemployment_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 254 Here’s one way to compute the number of participants who were never employed during the study. unemployment_pp %&gt;% filter(unemp == 0) %&gt;% distinct(id) %&gt;% count() %&gt;% summarise(never_employed = 254 - n) ## # A tibble: 1 x 1 ## never_employed ## &lt;dbl&gt; ## 1 132 In case it wasn’t clear, participants had up to 3 interviews. By recruiting 254 participants from local unemployment offices, the researchers were able to interview individuals soon after job loss (within the first 2 months). Follow-up interviews were conducted between 3 and 8 months and 10 and 16 months after job loss. (p. 161) Those times were encoded in the months variable. Here’s what that looks like. unemployment_pp %&gt;% ggplot(aes(x = months)) + geom_vline(xintercept = c(3, 8), color = &quot;white&quot;) + geom_histogram(binwidth = .5) + theme(panel.grid = element_blank()) To make some of our data questions easier, we can use those 3- and 8-month thresholds to make an interview variable to indicate the periods during which the interviews were conducted. unemployment_pp &lt;- unemployment_pp %&gt;% mutate(interview = ifelse(months &lt; 3, 1, ifelse(months &gt; 8, 3, 2))) unemployment_pp %&gt;% ggplot(aes(x = interview)) + geom_bar() + theme(panel.grid = element_blank()) With a little wrangling, we can display all possible employment patterns along with counts on how many followed them. unemployment_pp %&gt;% select(-months, -cesd) %&gt;% mutate(interview = str_c(&quot;int_&quot;, interview)) %&gt;% spread(key = interview, value = unemp) %&gt;% group_by(int_1, int_2, int_3) %&gt;% count() %&gt;% arrange(desc(n)) %&gt;% knitr::kable() int_1 int_2 int_3 n 1 1 1 78 1 0 0 55 1 1 0 41 1 NA NA 27 1 1 NA 22 1 0 1 19 1 0 NA 4 1 NA 1 4 1 NA 0 2 NA 1 0 1 NA 1 1 1 It takes a little work to see how Singer and Willett came to the conclusion “62 were always working after the first interview” (p. 161). Based on an analysis of those who had complete data, that corresponds to the pattern in the top row, [1, 0, 0], which we have counted as 55 (i.e., row 2). If you add to that the two rows with missingness on one of the critical values (i.e., [1, 0, NA], [1, NA, 0], and [NA, 1, 1]), that gets you \\(55 + 4 + 2 + 1 = 62\\) We can confirm that “41 were still unemployed at the second interview but working by the third” (p. 161). That’s our pattern [1, 1, 0], shown in row 3. We can also confirm “19 were working by the second interview but unemployed at the third” (p. 161). That’s shown in our pattern [1, 0, 1], shown in row 6. Before we configure our unconditional growth model, we might familiarize ourselves with our criterion variable, cesd. Singer and Willett informed us: Each time participants completed the Center for Epidemiologic Studies’ Depression (CES-D) scale (Radloff, 1977), which asks them to rate, on a four-point scale, the frequency with which they experience each of the 20 depressive symptoms. The CES-D scores can vary from a low or 0 for someone with no symptoms to a high of 80 for someone in serious distress. (p. 161) In addition to Radloff’s original article, you can get a copy of the CES-D here. To help us pick our priors, Brown and Gary (1985) listed the means and standard deviations of the CES-D scores for unemployed African-American adults. They gave the summary statistics broken down by sex: Males: 14.05 (8.86), \\(n = 37\\) Females: 15.35 (9.39), \\(n = 72\\) Based the variables in the data set and the descriptions of it in the text, we don’t have a good sense of the demographic backgrounds of the participants. But with the information we have in hand, a reasonable empirically-based but nonetheless noncommittal prior for baseline CES-D might be something like normal(14.5, 20). A weakly-regularizing prior on change over 1 month might be normal(0, 10). It’d be fair if you wanted to argue about these priors. Try your own! But if you are willing to go along with me, we might write the statistical formula for the unconditional growth model as \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ) \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] Here’s how we might fit that model. fit5.11 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + months + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .99), file = &quot;fits/fit05.11&quot;) Here are the results. print(fit5.11, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + months + (1 + months | id) ## Data: unemployment_pp (Number of observations: 674) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 8.658 0.844 7.063 10.353 1.008 383 1265 ## sd(months) 0.421 0.203 0.027 0.779 1.025 220 548 ## cor(Intercept,months) -0.365 0.231 -0.690 0.241 1.004 807 1122 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 17.695 0.752 16.187 19.155 1.003 1590 2020 ## months -0.421 0.083 -0.587 -0.260 1.000 3101 2711 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.577 0.408 7.785 9.378 1.016 312 848 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are the posteriors for the CES-D at the first day of job loss (i.e., \\(\\gamma_{00}\\)) and the expected rate of change over one month (i.e., \\(\\gamma_{10}\\)). posterior_samples(fit5.11) %&gt;% transmute(`first day of job loss` = b_Intercept, `linear decline by month` = b_months) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;CES-D composite score&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) We might use conditional_effects() to get a quick view on what that might looks like. plot(conditional_effects(fit5.11), plot = FALSE)[[1]] + geom_hline(yintercept = 14.5, color = &quot;grey50&quot;, linetype = 2) + coord_cartesian(ylim = c(0, 20)) + theme(panel.grid = element_blank()) For reference, the dashed grey line is the value we centered our prior for initial status on. 5.3.1.1 Using a composite specification. We might specify Model B, our first model with a time-varying covariate, like this: \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\gamma_{20} \\text{unemp}_{ij} \\big ] + \\big [ \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\big ]\\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ) \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10} \\text{ and } \\gamma_{20} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] Note a few things about the priors. First, we haven’t changed any of the priors from the previous model. All we did was add \\(\\gamma_{20} \\sim \\text{Normal}(0, 10)\\) for our new parameter. Given how weakly-informative our other priors have been for these data, this isn’t an unreasonable approach. However, the meaning for our intercept, \\(\\gamma_{01}\\), has changed. Now it’s the initial status for someone who is employed at baseline. But remember that for Model A, we set that prior with unemployed people in mind. A careful researcher might want to dive back into the literature to see if some lower value than 14.5 would be more reasonable to set for the mean of that prior. However, since the standard deviations for our intercepts priors and the covariate priors are all rather wide and permissive, this just won’t be much of a problem, for us. Buy anyway, second, note that we’ve centered our prior for \\(\\gamma_{20}\\) on zero. This is a weakly-regularizing prior, slightly favoring smaller effects over larger ones. And like before, one could easily argue for different priors. Here’s how to fit the model in brms. fit5.12 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + months + unemp + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .95), file = &quot;fits/fit05.12&quot;) If you compare our results to those in Table 5.7, you’ll see they’re quite similar. print(fit5.12) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + months + unemp + (1 + months | id) ## Data: unemployment_pp (Number of observations: 674) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9.14 0.84 7.45 10.77 1.01 492 732 ## sd(months) 0.55 0.18 0.09 0.84 1.03 212 206 ## cor(Intercept,months) -0.47 0.17 -0.70 -0.02 1.01 913 546 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 12.77 1.25 10.43 15.27 1.00 2025 2810 ## months -0.21 0.09 -0.40 -0.02 1.00 2971 3017 ## unemp 5.01 1.01 2.99 6.96 1.00 2427 2651 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.14 0.42 7.37 9.00 1.01 384 787 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we make our versions of Figure 5.3, let’s first compare \\(\\gamma_{01}\\) posteriors by model. On page 166 of the text, Singer and Willett reported the monthly rate of decline “had been cut in half (to 0.20 from 0.42 in Model A)”. fixef(fit5.11)[&quot;months&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## -0.4207319 0.0829908 -0.5865556 -0.2598241 fixef(fit5.12)[&quot;months&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## -0.20689601 0.09438123 -0.39522573 -0.01768592 You might be wondering why the quote from Singer and Willett used positive numbers while our parameter estimates have negative ones. No, there’s no mistake, there. Negative parameter estimates for monthly trajectories are then same thing as expressing a rate of decline with a positive number. But anyways, you see our estimates are on par with theirs. With our Bayesian paradigm, it’s also easy to get a formal difference distribution. bind_cols( posterior_samples(fit5.11) %&gt;% select(&quot;b_months&quot;), posterior_samples(fit5.12) %&gt;% select(&quot;b_months&quot;) ) %&gt;% set_names(&quot;fit5.11&quot;, &quot;fit5.12&quot;) %&gt;% mutate(dif = fit5.12 - fit5.11) %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(&quot;Difference in &quot;, gamma[1][0]))) + theme(panel.grid = element_blank()) Here’s our posterior for \\(\\gamma_{20}\\), b_unemp. posterior_samples(fit5.12) %&gt;% ggplot(aes(x = b_unemp, y = 0)) + geom_halfeyeh(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Let’s compute our WAIC estimates for fit5.11 and fit5.12. fit5.11 &lt;- add_criterion(fit5.11, criterion = &quot;waic&quot;) fit5.12 &lt;- add_criterion(fit5.12, criterion = &quot;waic&quot;) Now we’ll compare the models by both their WAIC differences and their WAIC weights. loo_compare(fit5.11, fit5.12, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.12 0.0 0.0 -2490.4 21.6 196.0 10.8 4980.8 43.1 ## fit5.11 -23.1 4.6 -2513.5 21.5 179.4 10.0 5027.0 42.9 model_weights(fit5.11, fit5.12, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.11 fit5.12 ## 0 1 By both metrics, fit5.12 came out as the clear favorite. It’s finally time to make our version of the upper left panel of Figure 5.3. We’ll do so using fitted(). nd &lt;- tibble(unemp = 1, months = seq(from = 0, to = 14, by = .5)) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) f %&gt;% ggplot(aes(x = months)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Remain unemployed&quot;) + theme(panel.grid = element_blank()) The upper right panel will take more care. We’ll still use fitted(), but we’ll have to be tricky with how we define the two segments. When we defined the sequence of months values over which we wanted to plot the model trajectory, we just casually set length.out = 30 within the seq() function. But now we need to make sure two of those sequential points are at 5. One way to do so is to use the by = .5 argument within seq(), instead. Since we’ll be defining the end points in our range with integer values, dividing up the sequence by every .5th value will ensure we’ll both be able to stop at 5 and that we’ll have a reasonable amount of values in the sequence to ensure the bowtie-shaped 95% intervals don’t look chunky. But anyway, that also means we’ll need to do a good job determining how many values we’ll need to repeat our desired unemp values over. So here’s a quick way to do the math. Since we’re using every .5 in the sequence, you just subtract the integer at the beginning of the sequence from the integer at the end of the sequence, multiply that value by 2, and then add 1 to the product. Like this: 2 * (5 - 0) + 1 ## [1] 11 2 * (14 - 5) + 1 ## [1] 19 Those are the number of times we need to repeat unemp == 1 and unemp == 0, respectively. You’ll see. Now wrangle and plot. nd &lt;- tibble(unemp = rep(1:0, times = c(11, 19)), months = c(seq(from = 0, to = 5, by = .5), seq(from = 5, to = 14, by = .5))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_segment(x = 5, xend = 5, y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 5, yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 5 + fixef(fit5.12)[3, 1], size = 1/3, linetype = 2) + annotate(geom = &quot;text&quot;, x = 8, y = 14.5, label = &quot;gamma[2][0]&quot;, parse = T) + geom_segment(x = 7, xend = 5.5, y = 14.5, yend = 14.5, arrow = arrow(length = unit(0.05, &quot;inches&quot;))) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Reemployed at 5 months&quot;) + theme(panel.grid = element_blank()) Same deal for the lower left panel of Figure 5.3. 2 * (10 - 0) + 1 ## [1] 21 2 * (14 - 10) + 1 ## [1] 9 Now wrangle and plot. nd &lt;- tibble(unemp = rep(1:0, times = c(21, 9)), months = c(seq(from = 0, to = 10, by = .5), seq(from = 10, to = 14, by = .5))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_segment(x = 10, xend = 10, y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 10, yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 10 + fixef(fit5.12)[3, 1], size = 1/3, linetype = 2) + annotate(geom = &quot;text&quot;, x = 7, y = 13.5, label = &quot;gamma[2][0]&quot;, parse = T) + geom_segment(x = 8, xend = 9.5, y = 13.5, yend = 13.5, arrow = arrow(length = unit(0.05, &quot;inches&quot;))) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Reemployed at 10 months&quot;) + theme(panel.grid = element_blank()) It’s just a little bit trickier to get that lower right panel. Now we need to calculate three values. 2 * (5 - 0) + 1 ## [1] 11 2 * (10 - 5) + 1 ## [1] 11 2 * (14 - 10) + 1 ## [1] 9 Get that plot. nd &lt;- tibble(unemp = rep(c(1, 0, 1), times = c(11, 11, 9)), months = c(seq(from = 0, to = 5, by = .5), seq(from = 5, to = 10, by = .5), seq(from = 10, to = 14, by = .5)), group = rep(letters[1:3], times = c(11, 11, 9))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) lines &lt;- tibble(group = letters[1:2], x = c(5, 10)) %&gt;% mutate(xend = x, y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * x, yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * x + fixef(fit5.12)[3, 1]) arrow &lt;- tibble(x = c(6.75, 8.25), y = 14, xend = c(5.5, 9.5), yend = c(14.5, 13.5)) f %&gt;% ggplot(aes(x = months)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = group), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate, group = group)) + geom_segment(data = lines, aes(x = x, xend = xend, y = y, yend = yend, group = group), size = 1/3, linetype = 2) + annotate(geom = &quot;text&quot;, x = 7.5, y = 14, label = &quot;gamma[2][0]&quot;, parse = T) + geom_segment(data = arrow, aes(x = x, xend = xend, y = y, yend = yend), arrow = arrow(length = unit(0.05, &quot;inches&quot;))) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Reemployed at 5 months\\nunemployed again at 10&quot;) + theme(panel.grid = element_blank()) Now we’ve been on a plotting roll, let’s knock out the leftmost panel of Figure 5.4. It’s just a small extension of what we’ve been doing. 2 * (14 - 0) + 1 ## [1] 29 2 * (3.5 - 0) + 1 ## [1] 8 2 * (14 - 3.5) + 1 ## [1] 22 nd &lt;- tibble(unemp = rep(1:0, times = c(29, 22)), months = c(seq(from = 0, to = 14, by = .5), seq(from = 3.5, to = 14, by = .5))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(label = str_c(&quot;unemp = &quot;, unemp)) f %&gt;% ggplot(aes(x = months, group = unemp)) + # new trick geom_abline(intercept = fixef(fit5.12)[1, 1], slope = fixef(fit5.12)[2, 1], color = &quot;grey80&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + # another new trick geom_text(data = f %&gt;% filter(months == 14), aes(label = label, y = Estimate), hjust = -.05) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Main effects of unemp and time&quot;) + # don&#39;t forget this part coord_cartesian(clip = &quot;off&quot;) + theme(panel.grid = element_blank(), plot.margin = margin(6, 55, 6, 6)) 5.3.1.2 Using a level-1/level-2 specification. If we wanted to reexpress our composite equation for fit5.12 using the level-1/level-2 form, the level-1 model would be \\[ \\text{cesd}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{months}_{ij} + \\pi_{2i} \\text{unemp}_{ij} + \\epsilon_{ij}. \\] Here’s the corresponding level-2 model: \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\zeta_{1i} \\\\ \\pi_{2i} &amp; = \\gamma_{20}. \\end{align*}\\] If we wanted the effects of the time-varying covariate unemp to vary across individuals, then we’d expand the definition of \\(\\pi_{2i}\\) to be \\[ \\pi_{2i} = \\gamma_{20} + \\zeta_{2i}. \\] Although this doesn’t change the way we model \\(\\epsilon_{ij}\\), which remains \\[ \\epsilon_{ij} \\sim \\text{Normal} (0, \\sigma_\\epsilon), \\] it does change the model for the \\(\\zeta\\)s. Within our Stan/brms paradigm, that would now be \\[\\begin{align*} \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{2i} \\end{bmatrix} &amp; \\sim \\text{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ), \\text{where} \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_2 \\end{bmatrix} \\text{and} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} &amp; \\rho_{02} \\\\ \\rho_{01} &amp; 1 &amp; \\rho_{12} \\\\ \\rho_{02} &amp; \\rho_{12} &amp; 1 \\end{bmatrix}. \\end{align*}\\] Reworded slightly from the text (p. 169), by adding one residual parameter, \\(\\zeta_{2i}\\), we got an additional corresponding standard deviation parameter, \\(\\sigma_2\\), and two more correlation parameters, \\(\\rho_{02}\\) and \\(\\rho_{12}\\). Staying with our weakly-regularizing prior approach, the priors for the updated model might look like \\[\\begin{align*} \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10} \\text{ and } \\gamma_{20} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon,..., \\sigma_2 &amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\Omega &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] Singer and Willett then cautioned readers about hastily adding \\(\\zeta\\) parameters to their models, particularly in cases where you’re likely to run into estimation issues, such as boundary constraints. Within our Stan/brms paradigm, we still have to be aware of these difficulties. However, with skillfully-chosen priors, I think you’ll find we can fit more ambitious models than would typically be possible with frequentist estimators. But do beware that as you stretch your data further and further, your choices in likelihoods and priors more heavily influence the results. For more on the topic, check out Michael Frank’s blog post, Mixed effects models: Is it time to go Bayesian by default?, and make sure not to miss the action in the comments section. 5.3.1.3 Time-varying predictors and variance components. When you add a time-varying predictor, it’s not uncommon to see a reduction in \\(\\sigma_\\epsilon^2\\). Here we compare fit5.11 and fit5.12. v &lt;- cbind(VarCorr(fit5.11, summary = F)[[2]][[1]], VarCorr(fit5.12, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% set_names(str_c(&quot;fit5.&quot;, 11:12)) %&gt;% transmute(fit5.11 = fit5.11^2, fit5.12 = fit5.12^2) %&gt;% mutate(`fit5.11 - fit5.12` = fit5.11 - fit5.12) v %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;fit5.11&quot;, &quot;fit5.12&quot;, &quot;fit5.11 - fit5.12&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[epsilon]^2)) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Using the full posterior for both models, here is the percent variance in CES-D explained by unemp. v %&gt;% transmute(percent = (fit5.11 - fit5.12) / fit5.11) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 3) ## percent .lower .upper .width .point .interval ## 1 0.1 -0.188 0.311 0.95 median qi When you go beyond point estimates and factor in full posterior uncertainty, it becomes clear how fragile ad hoc statistics like this can be. Interpret them with caution. 5.3.2 Allowing the effect of a time-varying predictor to vary over time. “Might unemployment status also affect the trajectory’s slope” (p. 171)? Here’s the statistical model: \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{months}_{ij} \\times \\text{unemp}_{ij} \\big ] + \\big [ \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ) \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10}, \\gamma_{20}, \\text{ and } \\gamma_{30} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] Since \\(\\gamma_{30}\\) is an interaction term, it might make sense to give it an ever tighter prior, something like \\(\\text{Normal}(0, 5)\\). Here we’ll just stay wide and loose. fit5.13 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + months + unemp + months:unemp + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.13&quot;) Here are the results. print(fit5.13) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + months + unemp + months:unemp + (1 + months | id) ## Data: unemployment_pp (Number of observations: 674) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9.13 0.85 7.43 10.85 1.01 440 800 ## sd(months) 0.55 0.19 0.10 0.86 1.02 237 291 ## cor(Intercept,months) -0.47 0.18 -0.71 -0.01 1.00 890 613 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.88 1.94 5.95 13.58 1.01 1075 1971 ## months 0.14 0.20 -0.25 0.53 1.00 1130 2054 ## unemp 8.21 1.92 4.53 12.06 1.00 1093 1999 ## months:unemp -0.44 0.22 -0.87 -0.02 1.00 1213 1919 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.13 0.43 7.32 9.01 1.01 335 1087 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s how we might make our version of the middle panel of Figure 5.4. nd &lt;- tibble(unemp = rep(1:0, times = c(29, 22)), months = c(seq(from = 0, to = 14, by = .5), seq(from = 3.5, to = 14, by = .5))) f &lt;- fitted(fit5.13, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(label = str_c(&quot;unemp = &quot;, unemp)) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_abline(intercept = fixef(fit5.13)[1, 1], slope = fixef(fit5.13)[2, 1], color = &quot;grey80&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_text(data = f %&gt;% filter(months == 14), aes(label = label, y = Estimate), hjust = -.05) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Main effects of unemp and time&quot;) + coord_cartesian(clip = &quot;off&quot;) + theme(panel.grid = element_blank(), plot.margin = margin(6, 55, 6, 6)) Here’s the posterior for \\(\\gamma_{10}\\). posterior_samples(fit5.13) %&gt;% ggplot(aes(x = b_months, y = 0)) + geom_halfeyeh(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(gamma[1][0], &quot;, the main effect for time&quot;))) + theme(panel.grid = element_blank()) It’s quite uncertain and almost symmetrically straddles the parameter space between -0.5 and 0.5. The next model follows the form \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{unemp}_{ij} \\times \\text{months}_{ij} \\big ] \\\\ &amp; \\;\\;\\; + \\big [ \\zeta_{0i} + \\zeta_{3i} \\text{unemp}_{ij} \\times \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{3i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ) \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_3 \\end{bmatrix} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{03} \\\\ \\rho_{03} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{20} \\text{ and } \\gamma_{30} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_3 &amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\rho_{03} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] Here’s how to fit it with brms::brm(). fit5.14 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + unemp + months:unemp + (1 + months:unemp | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .99), file = &quot;fits/fit05.14&quot;) It’s easy to miss this if you’re not following along quite carefully with the text, but this model, which corresponds to Equation 5.9 in the text, is NOT Model D. Rather, it’s an intermediary model between Model C and Model D. All this means we can’t compare our results with those in Table 5.7. But here they are, anyway. print(fit5.14) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + unemp + months:unemp + (1 + months:unemp | id) ## Data: unemployment_pp (Number of observations: 674) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 7.98 0.65 6.74 9.29 1.00 1059 2256 ## sd(months:unemp) 0.38 0.21 0.02 0.78 1.02 307 783 ## cor(Intercept,months:unemp) -0.10 0.26 -0.53 0.48 1.00 1343 1910 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 11.21 0.91 9.41 13.00 1.00 2384 2909 ## unemp 6.91 0.93 5.06 8.71 1.00 4438 3178 ## unemp:months -0.30 0.11 -0.50 -0.09 1.00 5707 2918 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.55 0.35 7.87 9.23 1.01 830 2089 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ve already computed the WAIC for fit5.11 and fit5.12. Here we do so for fit5.13 and fit5.14. fit5.13 &lt;- add_criterion(fit5.13, &quot;waic&quot;) fit5.14 &lt;- add_criterion(fit5.14, &quot;waic&quot;) loo_compare(fit5.11, fit5.12, fit5.13, fit5.14, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.13 0.0 0.0 -2490.0 21.4 197.3 10.7 4979.9 42.8 ## fit5.12 -0.4 2.1 -2490.4 21.6 196.0 10.8 4980.8 43.1 ## fit5.14 -14.2 3.3 -2504.2 21.7 169.1 9.7 5008.4 43.4 ## fit5.11 -23.5 4.9 -2513.5 21.5 179.4 10.0 5027.0 42.9 model_weights(fit5.11, fit5.12, fit5.13, fit5.14, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.11 fit5.12 fit5.13 fit5.14 ## 0.000 0.395 0.605 0.000 Yep, it appears fit5.14 is not an improvement on fit5.13 (i.e., our analogue to Model C in the text). Here’s our version of Model D: \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{unemp}_{ij} \\times \\text{months}_{ij} \\big ] \\\\ &amp; \\;\\;\\; + \\big [ \\zeta_{0i} + \\zeta_{3i} \\text{unemp}_{ij} \\times \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{2i} \\\\ \\zeta_{3i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}&#39; \\Bigg ) \\\\ \\mathbf{D} &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 \\end{bmatrix} \\\\ \\mathbf{\\Omega} &amp; = \\begin{bmatrix} 1 &amp; \\rho_{02} &amp; \\rho_{03} \\\\ \\rho_{02} &amp; 1 &amp; \\rho_{23} \\\\ \\rho_{03} &amp; \\rho_{23} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{20} \\text{ and } \\gamma_{30} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon,..., \\sigma_3 &amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\mathbf\\Omega &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] We’ll call it fit5.15. Here’s the brm() code. fit5.15 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + unemp + months:unemp + (1 + unemp + months:unemp | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .9), file = &quot;fits/fit05.15&quot;) print(fit5.15) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + unemp + months:unemp + (1 + unemp + months:unemp | id) ## Data: unemployment_pp (Number of observations: 674) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 6.85 0.87 5.18 8.55 1.00 1475 2217 ## sd(unemp) 4.83 1.72 1.00 7.87 1.01 234 381 ## sd(unemp:months) 0.63 0.22 0.12 1.00 1.03 183 229 ## cor(Intercept,unemp) 0.23 0.22 -0.19 0.67 1.00 1222 2135 ## cor(Intercept,unemp:months) -0.18 0.22 -0.56 0.30 1.00 1120 1659 ## cor(unemp,unemp:months) -0.39 0.28 -0.81 0.25 1.01 491 1184 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 11.16 0.82 9.60 12.72 1.00 3928 3303 ## unemp 6.93 0.93 5.10 8.75 1.00 5312 3013 ## unemp:months -0.29 0.11 -0.50 -0.07 1.00 5674 3343 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.08 0.43 7.28 8.96 1.02 257 828 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we can finally make our version of the right panel of Figure 5.4. f &lt;- fitted(fit5.15, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(label = str_c(&quot;unemp = &quot;, unemp)) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_abline(intercept = fixef(fit5.15)[1, 1], slope = 0, color = &quot;grey80&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_text(data = f %&gt;% filter(months == 14), aes(label = label, y = Estimate), hjust = -.05) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Constraining the effects time\\namong the re-employed&quot;) + coord_cartesian(clip = &quot;off&quot;) + theme(panel.grid = element_blank(), plot.margin = margin(6, 55, 6, 6)) By carefully using filter(), we can extract the posterior for summary the expected CES-D value for “the average unemployed person in the population”, “immediately upon layoff” (p. 173). f %&gt;% filter(unemp == 1 &amp; months == 0) ## Estimate Est.Error Q2.5 Q97.5 unemp months label ## 1 18.0891 0.7704082 16.59355 19.62399 1 0 unemp = 1 To get the decline rate per month, just use fixef() and subset. fixef(fit5.15)[&quot;unemp:months&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## -0.28901058 0.10945969 -0.50183119 -0.07415122 How much lower, on average, are “CES-D scores among those who find a job” right after layoff (p. 173)? Again, just use fixef(). fixef(fit5.15)[&quot;unemp&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 6.928497 0.928391 5.098611 8.745528 But if we’d like to get the posterior for the difference at 12 months later, we’ll need to go back to fitted(). nd &lt;- tibble(unemp = 1:0, months = 12) fitted(fit5.15, newdata = nd, re_formula = NA, summary = F) %&gt;% data.frame() %&gt;% set_names(str_c(c(&quot;un&quot;, &quot;&quot;), &quot;employed_cesd_at_12&quot;)) %&gt;% transmute(difference = unemployed_cesd_at_12 - employed_cesd_at_12) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 3) ## difference .lower .upper .width .point .interval ## 1 3.463 0.784 6.142 0.95 median qi There’s more posterior uncertainty, there, that you might expect from simply using the point estimates. Always beware the posterior uncertainty. We may as well finish off with a little WAIC. fit5.15 &lt;- add_criterion(fit5.15, &quot;waic&quot;) loo_compare(fit5.11, fit5.12, fit5.13, fit5.14, fit5.15, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.15 0.0 0.0 -2489.4 21.8 201.8 11.1 4978.8 43.6 ## fit5.13 -0.5 3.6 -2490.0 21.4 197.3 10.7 4979.9 42.8 ## fit5.12 -1.0 3.5 -2490.4 21.6 196.0 10.8 4980.8 43.1 ## fit5.14 -14.8 3.7 -2504.2 21.7 169.1 9.7 5008.4 43.4 ## fit5.11 -24.1 6.6 -2513.5 21.5 179.4 10.0 5027.0 42.9 model_weights(fit5.11, fit5.12, fit5.13, fit5.14, fit5.15, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.11 fit5.12 fit5.13 fit5.14 fit5.15 ## 0.000 0.194 0.297 0.000 0.509 It’s a toss-up among fit5.15, fit5.13, and fit5.12. 5.3.3 Recentering time-varying predictors. Back to the wages_pp data! Here’s the generic statistical model we’ll be fooling with: \\[\\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{01} (\\text{hgc}_i - 9) + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij}. \\end{align*}\\] We will fit the model with three versions of uerate. If you execute head(wages_pp), you’ll discover they’re already in the data. But it might be worth walking out how to compute those variables. First, centering uerate at 7 is easy enough. Just subtract. wages_pp &lt;- wages_pp %&gt;% mutate(uerate_7 = uerate - 7) Continuing on with the same priors from before, here’s how to fit the new model, our version of Model A on page 175. fit5.16 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.16&quot;) print(fit5.16, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.224 0.011 0.203 0.244 1.000 2042 2733 ## sd(exper) 0.040 0.003 0.035 0.045 1.005 823 1727 ## cor(Intercept,exper) -0.297 0.069 -0.420 -0.152 1.005 798 1565 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.749 0.011 1.726 1.771 1.001 3285 3163 ## exper 0.044 0.003 0.039 0.049 1.000 3057 3453 ## hgc_9 0.040 0.006 0.027 0.053 1.002 2853 2835 ## uerate_7 -0.012 0.002 -0.016 -0.008 1.003 6629 3851 ## exper:black -0.018 0.004 -0.027 -0.010 1.000 3281 3359 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.314 1.001 3682 2851 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). wages_pp &lt;- wages_pp %&gt;% group_by(id) %&gt;% mutate(uerate_id_mu = mean(uerate)) %&gt;% ungroup() %&gt;% mutate(uerate_id_dev = uerate - uerate_id_mu) In the original data set, these were the ue.mean and ue.person.cen variables, respectively. Here’s how to fit the model. fit5.17 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_mu + uerate_id_dev + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.17&quot;) print(fit5.17, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_mu + uerate_id_dev + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.225 0.011 0.204 0.247 1.000 1947 2859 ## sd(exper) 0.040 0.003 0.035 0.046 1.004 729 1578 ## cor(Intercept,exper) -0.315 0.069 -0.443 -0.172 1.003 817 1421 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.873 0.029 1.816 1.930 1.001 2483 3378 ## exper 0.045 0.003 0.040 0.050 1.001 2891 3392 ## hgc_9 0.040 0.006 0.028 0.052 1.001 3302 3550 ## uerate_id_mu -0.018 0.004 -0.025 -0.011 1.001 2476 3070 ## uerate_id_dev -0.010 0.002 -0.014 -0.006 1.000 4999 4091 ## exper:black -0.019 0.005 -0.028 -0.010 1.001 2818 3219 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.315 1.001 4693 2994 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Within the tidyverse, probably the easiest way to center on the first value for each id is to first group_by(id) and then make use of the dplyr::first() function, which you can learn more about here. wages_pp &lt;- wages_pp %&gt;% group_by(id) %&gt;% mutate(uerate_id_1 = first(uerate)) %&gt;% ungroup() %&gt;% mutate(uerate_id_1_dev = uerate - uerate_id_1) In the original data set, these were the ue1 and ue.centert1 variables, respectively. Here’s how to fit the updated model. fit5.18 &lt;- update(fit5.16, newdata = wages_pp, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_1 + uerate_id_1_dev + (1 + exper | id), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.18&quot;) print(fit5.18, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ Intercept + exper + hgc_9 + uerate_id_1 + uerate_id_1_dev + (1 + exper | id) + exper:black - 1 ## Data: wages_pp (Number of observations: 6402) ## Samples: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup samples = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.224 0.011 0.203 0.244 1.000 1957 3415 ## sd(exper) 0.040 0.003 0.036 0.046 1.004 682 1737 ## cor(Intercept,exper) -0.303 0.069 -0.429 -0.160 1.004 762 1913 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.869 0.026 1.816 1.921 1.002 3077 3253 ## exper 0.045 0.003 0.039 0.050 1.001 3285 3349 ## hgc_9 0.040 0.006 0.028 0.052 1.001 3595 3500 ## uerate_id_1 -0.016 0.003 -0.021 -0.011 1.003 3122 3130 ## uerate_id_1_dev -0.010 0.002 -0.014 -0.006 1.000 6916 3669 ## exper:black -0.018 0.005 -0.027 -0.009 1.001 3654 3316 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.314 1.001 4688 3815 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are the WAIC comparisons. fit5.16 &lt;- add_criterion(fit5.16, &quot;waic&quot;) fit5.17 &lt;- add_criterion(fit5.17, &quot;waic&quot;) fit5.18 &lt;- add_criterion(fit5.18, &quot;waic&quot;) loo_compare(fit5.16, fit5.17, fit5.18, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.18 0.0 0.0 -2033.8 104.2 861.7 26.8 4067.7 208.5 ## fit5.16 -3.5 1.9 -2037.3 104.1 862.8 26.9 4074.7 208.3 ## fit5.17 -4.8 1.6 -2038.6 104.3 866.4 27.3 4077.3 208.7 model_weights(fit5.16, fit5.17, fit5.18, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.16 fit5.17 fit5.18 ## 0.029 0.008 0.964 5.3.4 An important caveat: The problem of reciprocal causation. In this section, Singer and Willett gave a typology for time-varying covariates. A variable is defined if, “in advance to data collection, its values are predetermined for everyone under study” (p. 177). Examples are time and the seasons. A variable is ancillary if “its values cannot be influenced by study participants because they are determined by a stochastic process totally external to them” (p. 178). Within the context of a study on people within monogamous relationships, the availability of potential mates within the region would be an example. A variable is contextual if “it describes an ‘external’ stochastic process, but the connection between units is closer–between husbands and wives, parents and children, teachers and students, employers and employees. Because of this proximity, contextual predictors can be influenced by an individual’s contemporaneous outcome values; if so, they are susceptible to issues of reciprocal causation” (p. 179). A variable is internal if it describes an “individual’s potentially changeable status over time” (p. 179). Examples include mood, psychiatric syndromes, and employment status. 5.4 Recentering the effect of TIME Our version of the data from Tomarken, Shelton, Elkins, and Anderson (1997) is saved as medication_pp.csv. medication_pp &lt;- read_csv(&quot;data/medication_pp.csv&quot;) glimpse(medication_pp) ## Rows: 1,242 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 1… ## $ treat &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ wave &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 2, … ## $ day &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 0, 0, 0, 1, 1, 1,… ## $ time.of.day &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000, … ## $ time &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 1.0000000, 1.3333333, 1.6666667, 2.0000000, … ## $ time333 &lt;dbl&gt; -3.3333333, -3.0000000, -2.6666667, -2.3333333, -2.0000000, -1.6666667, -1.33… ## $ time667 &lt;dbl&gt; -6.6666667, -6.3333333, -6.0000000, -5.6666667, -5.3333333, -5.0000000, -4.66… ## $ initial &lt;dbl&gt; 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40,… ## $ final &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60,… ## $ pos &lt;dbl&gt; 106.6667, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.000… The medication_pp data do not come with a reading variable, as in Table 5.9. Here we’ll make one and display the time variables highlighted in Table 5.9. medication_pp &lt;- medication_pp %&gt;% mutate(reading = ifelse(time.of.day == 0, &quot;8 am&quot;, ifelse(time.of.day &lt; .6, &quot;3 pm&quot;, &quot;10 pm&quot;))) medication_pp %&gt;% select(wave, day, reading, time.of.day:time667) %&gt;% head(n = 10) ## # A tibble: 10 x 7 ## wave day reading time.of.day time time333 time667 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 8 am 0 0 -3.33 -6.67 ## 2 2 0 3 pm 0.333 0.333 -3 -6.33 ## 3 3 0 10 pm 0.667 0.667 -2.67 -6 ## 4 4 1 8 am 0 1 -2.33 -5.67 ## 5 5 1 3 pm 0.333 1.33 -2 -5.33 ## 6 6 1 10 pm 0.667 1.67 -1.67 -5 ## 7 7 2 8 am 0 2 -1.33 -4.67 ## 8 8 2 3 pm 0.333 2.33 -1 -4.33 ## 9 9 2 10 pm 0.667 2.67 -0.667 -4 ## 10 10 3 8 am 0 3 -0.333 -3.67 To help get a sense of the balance in the data, here is a bar plot of the distribution of the numbers of measurement occasions within participants. It’s color coded by treatment status. medication_pp %&gt;% mutate(treat = str_c(&quot;treat = &quot;, treat)) %&gt;% group_by(treat, id) %&gt;% count() %&gt;% ggplot(aes(x = n, fill = treat)) + geom_bar() + scale_x_continuous(breaks = c(2,12, 15:21)) + scale_fill_viridis_d(NULL, end = .8) + labs(x = &quot;# measurement occasions&quot;, y = &quot;count of cases&quot;) + coord_flip() + theme(panel.grid = element_blank()) For this section, our basic model will be \\[\\begin{align*} \\text{pos}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} (\\text{time}_{ij} - c) + \\epsilon_{ij} \\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{treat}_i + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{treat}_i + \\zeta_{1i}, \\end{align*}\\] where \\(c\\) is a generic constant. For our there variants of \\(\\text{time}\\), \\(c\\) will be 0 for time, 3.3333333 for time333, and 6.666667 for time667. Our criterion variable is pos, positive mood rating. The text told us these were from “a package of mood diaries (which use a five-point scale to assess positive and negative moods)” (p. 182). However, we don’t know what numerals were assigned to the points on the scale, we don’t know how many items were used, and we don’t even know whether the items were taken from an existing questionnaire. And unfortunately, the citation Singer and Willett gave for the study is from a conference presentation, making it a pain to track down background information on the internet. In such a situation, it’s difficult to figure out how to set our priors. Though suboptimal, we might first get a sense of the pos data with a histogram. medication_pp %&gt;% ggplot(aes(x = pos)) + geom_histogram() + theme(panel.grid = element_blank()) Here’s the range(). range(medication_pp$pos) ## [1] 100.0000 466.6667 Starting with the prior for our intercept, I think there’s a lot of room for argument, here. To keep with our weakly-regularizing approach to priors, it might make sense to use something like normal(200, 100). But then again, we know something about the study design. At the beginning, participants were in treatment for depression, so we’d expect the starting point to be closer to the lower end of the scale. In that case, we might update our approach to something like normal(150, 50). Feel free to play with alternatives. What I hope this illustrates is that our task would be much easier with more domain knowledge. Anyway, given the scale of the data, weakly-regularizing priors for the predictor variables might take the form of something like normal(0, 25). We’ll use student_t(0, 50) on the \\(\\sigma\\)s and stay steady with lkj(4) on \\(\\rho_{01}\\). Here we fit the model with all three versions of time. fit5.19 &lt;- brm(data = medication_pp, family = gaussian, pos ~ 0 + Intercept + time + treat + time:treat + (1 + time | id), prior = c(prior(normal(150, 50), class = b, coef = Intercept), prior(normal(0, 25), class = b), prior(student_t(3, 0, 50), class = sd), prior(student_t(3, 0, 50), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.19&quot;) fit5.20 &lt;- update(fit5.19, newdata = medication_pp, pos ~ 0 + Intercept + time333 + treat + time333:treat + (1 + time333 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.20&quot;) fit5.21 &lt;- update(fit5.19, newdata = medication_pp, pos ~ 0 + Intercept + time667 + treat + time667:treat + (1 + time667 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.21&quot;) Given the size of our posterior standard deviations, our \\(\\gamma\\) posteriors are quite comparable to the point estimates (and their standard errors) in the text. print(fit5.19) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ 0 + Intercept + time + treat + time:treat + (1 + time | id) ## Data: medication_pp (Number of observations: 1242) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 46.94 4.70 38.60 56.88 1.00 1508 2266 ## sd(time) 8.26 0.96 6.54 10.30 1.01 1266 2351 ## cor(Intercept,time) -0.28 0.13 -0.51 -0.02 1.00 1174 1823 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 166.75 8.95 149.63 184.64 1.00 848 1706 ## time -2.40 1.79 -5.89 1.06 1.00 1111 1904 ## treat -2.40 11.25 -24.52 19.83 1.00 937 1645 ## time:treat 5.55 2.37 0.83 10.03 1.00 1105 1827 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.13 0.76 33.69 36.64 1.00 6071 2835 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Given the scales we’re working with, it’s difficult to compare our \\(\\sigma\\) summaries with the \\(\\sigma^2\\) summaries in Table 5.10 of the text. Here we convert and resummarize. v &lt;- posterior_samples(fit5.19) %&gt;% transmute(sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__time^2, sigma_2_epsilon = sigma^2) v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 3 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma_2_0 2172. 1490. 3235. 0.95 median qi ## 2 sigma_2_1 67.3 42.7 106. 0.95 median qi ## 3 sigma_2_epsilon 1233. 1135. 1343. 0.95 median qi Turns out the posterior medians are quite similar to the ML estimates in the text. Here’s what the entire distributions looks like. v %&gt;% set_names(&quot;sigma[0]^2&quot;, &quot;sigma[1]^2&quot;, &quot;sigma[epsilon]^2&quot;) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.text = element_text(size = 11)) + facet_wrap(~name, scales = &quot;free&quot;, labeller = label_parsed) Here’s our version of Figure 5.5. nd &lt;- crossing(treat = 0:1, time = seq(from = 0, to = 7, length.out = 30)) text &lt;- tibble(treat = 0:1, time = 4, y = c(135, 197), label = c(&quot;control&quot;, &quot;treatment&quot;), angle = c(350, 15)) fitted(fit5.19, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = time, fill = treat, color = treat, group = treat)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/3, size = 0) + geom_line(aes(y = Estimate)) + geom_text(data = text, aes(y = y, label = label, angle = angle)) + scale_fill_viridis_c(option = &quot;A&quot;, begin = .3, end = .6) + scale_color_viridis_c(option = &quot;A&quot;, begin = .3, end = .6) + labs(x = &quot;days&quot;, y = &quot;pos&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Although we still have clear evidence of an interaction, adding those 95% intervals makes it look less impressive, doesn’t it? Here are the summaries for the models with the alternative versions of \\((\\text{time}_{ij} - c)\\). print(fit5.20) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ Intercept + time333 + treat + (1 + time333 | id) + time333:treat - 1 ## Data: medication_pp (Number of observations: 1242) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 46.09 4.22 38.57 55.13 1.01 518 855 ## sd(time333) 8.33 0.94 6.68 10.29 1.00 1093 1995 ## cor(Intercept,time333) 0.21 0.13 -0.06 0.45 1.00 1077 1241 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 160.77 8.37 144.20 177.20 1.01 272 496 ## time333 -2.34 1.83 -5.98 1.26 1.00 704 1483 ## treat 13.16 10.88 -8.07 34.39 1.01 274 511 ## time333:treat 5.38 2.46 0.44 10.08 1.01 674 1208 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.13 0.74 33.68 36.62 1.00 3547 2587 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit5.21) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ Intercept + time667 + treat + (1 + time667 | id) + time667:treat - 1 ## Data: medication_pp (Number of observations: 1242) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 57.71 5.46 48.10 69.11 1.00 1145 1928 ## sd(time667) 8.05 0.94 6.41 10.14 1.00 1198 1716 ## cor(Intercept,time667) 0.59 0.09 0.39 0.75 1.00 1473 2164 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 156.81 11.02 135.65 179.42 1.01 647 1113 ## time667 -1.84 1.72 -5.17 1.48 1.00 986 1925 ## treat 24.80 13.14 -1.13 49.54 1.00 764 1174 ## time667:treat 4.62 2.16 0.51 8.89 1.00 1339 2041 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.12 0.75 33.70 36.66 1.00 5143 2690 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It might be easier to compare the \\(\\gamma\\) summaries across models with a well-designed coefficient plot. Here’s an attempt. tibble(name = str_c(&quot;fit5.&quot;, 19:21)) %&gt;% mutate(fixef = map(name, ~get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(fixef) %&gt;% mutate(gamma = rep(c(&quot;gamma[0][0]&quot;, &quot;gamma[1][0]&quot;, &quot;gamma[0][1]&quot;, &quot;gamma[1][1]&quot;), times = 3)) %&gt;% ggplot(aes(x = name, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_pointrange() + xlab(NULL) + coord_flip() + theme(axis.ticks.y = element_blank(), panel.grid = element_blank(), strip.text = element_text(size = 11)) + facet_wrap(~gamma, scales = &quot;free_x&quot;, labeller = label_parsed, ncol = 4) Since we’re juggling less information, we might compare the posteriors for \\(\\sigma_1^2\\) across the three models with good old tidybayes::geom_halfeyeh(). tibble(name = str_c(&quot;fit5.&quot;, 19:21)) %&gt;% mutate(fit = map(name, get)) %&gt;% mutate(sigma_2_1 = map(fit, ~VarCorr(., summary = F)[[1]][[1]][, 1]^2 %&gt;% data.frame() %&gt;% set_names(&quot;sigma_2_1&quot;))) %&gt;% unnest(sigma_2_1) %&gt;% ggplot(aes(x = sigma_2_1, y = name)) + geom_halfeyeh(.width = c(.5, .95)) + labs(x = expression(sigma[1]^2), y = NULL) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) For kicks and giggles, we marked off both 50% and 95% intervals for each. Here’s the same for \\(\\rho_{01}\\). tibble(name = str_c(&quot;fit5.&quot;, 19:21)) %&gt;% mutate(fit = map(name, get)) %&gt;% mutate(rho = map(fit, ~VarCorr(., summary = F)[[1]][[2]][, 2, &quot;Intercept&quot;] %&gt;% data.frame() %&gt;% set_names(&quot;rho&quot;))) %&gt;% unnest(rho) %&gt;% ggplot(aes(x = rho, y = name)) + geom_halfeyeh(.width = c(.5, .95)) + labs(x = expression(rho[0][1]), y = NULL) + coord_cartesian(xlim = c(-1, 1)) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) “As Rogosa and Willett (1985) demonstrate, you can always alter the correlation between the level-1 growth parameters simply by changing the centering constant” (p. 186). Here are the WAIC comparisons. fit5.19 &lt;- add_criterion(fit5.19, &quot;waic&quot;) fit5.20 &lt;- add_criterion(fit5.20, &quot;waic&quot;) fit5.21 &lt;- add_criterion(fit5.21, &quot;waic&quot;) loo_compare(fit5.19, fit5.20, fit5.21, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.19 0.0 0.0 -6242.0 41.5 109.9 7.5 12484.0 82.9 ## fit5.21 -0.4 0.8 -6242.4 41.5 110.4 7.7 12484.8 82.9 ## fit5.20 -0.6 0.6 -6242.6 41.4 110.7 7.6 12485.2 82.8 model_weights(fit5.19, fit5.20, fit5.21, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.19 fit5.20 fit5.21 ## 0.448 0.252 0.300 As one might hope, not much going on. As Singer and Willett alluded to in the text (p. 187), the final model in this chapter is an odd one. Do note the initial and final columns in the data. glimpse(medication_pp) ## Rows: 1,242 ## Columns: 12 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 1… ## $ treat &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ wave &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 2, … ## $ day &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 0, 0, 0, 1, 1, 1,… ## $ time.of.day &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000, … ## $ time &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 1.0000000, 1.3333333, 1.6666667, 2.0000000, … ## $ time333 &lt;dbl&gt; -3.3333333, -3.0000000, -2.6666667, -2.3333333, -2.0000000, -1.6666667, -1.33… ## $ time667 &lt;dbl&gt; -6.6666667, -6.3333333, -6.0000000, -5.6666667, -5.3333333, -5.0000000, -4.66… ## $ initial &lt;dbl&gt; 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40,… ## $ final &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60,… ## $ pos &lt;dbl&gt; 106.6667, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.000… ## $ reading &lt;chr&gt; &quot;8 am&quot;, &quot;3 pm&quot;, &quot;10 pm&quot;, &quot;8 am&quot;, &quot;3 pm&quot;, &quot;10 pm&quot;, &quot;8 am&quot;, &quot;3 pm&quot;, &quot;10 pm&quot;, &quot;8… With this model \\[\\begin{align*} \\text{pos}_{ij} &amp; = \\pi_{0i} \\bigg (\\frac{6.67 - \\text{time}_{ij}}{6.67} \\bigg ) + \\pi_{1i} \\bigg (\\frac {\\text{time}_{ij}}{6.67} \\bigg ) + \\epsilon_{ij} \\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{treat}_i + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{treat}_i + \\zeta_{1i}. \\end{align*}\\] Because of the parameterization, we’ll use both variables simultaneously to indicate time. In the code, below, you’ll notice we no longer have an intercept parameter. Rather, we just have initial and final. As such, both of those parameters get the same prior we’ve been using for the intercept in the previous models. fit5.22 &lt;- brm(data = medication_pp, family = gaussian, pos ~ 0 + initial + final + initial:treat + final:treat + (0 + initial + final | id), prior = c(prior(normal(150, 50), class = b, coef = initial), prior(normal(150, 50), class = b, coef = final), prior(normal(0, 25), class = b), prior(student_t(3, 0, 50), class = sd), prior(student_t(3, 0, 50), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.22&quot;) Our results are quite similar to those in the text. print(fit5.22) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ 0 + initial + final + initial:treat + final:treat + (0 + initial + final | id) ## Data: medication_pp (Number of observations: 1242) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(initial) 46.99 4.73 38.82 57.19 1.00 2005 2792 ## sd(final) 58.86 5.67 48.87 70.90 1.00 2048 2772 ## cor(initial,final) 0.43 0.11 0.20 0.62 1.00 1870 2491 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## initial 167.99 8.58 151.34 184.69 1.00 2417 2233 ## final 156.15 10.28 136.15 176.53 1.00 2848 2835 ## initial:treat -4.29 11.06 -25.81 17.85 1.00 2516 2718 ## final:treat 25.03 13.09 -0.79 50.58 1.00 3264 2614 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.12 0.72 33.73 36.54 1.00 4588 2976 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s finish up with the WAIC. fit5.22 &lt;- add_criterion(fit5.22, &quot;waic&quot;) loo_compare(fit5.19, fit5.20, fit5.21, fit5.22, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.19 0.0 0.0 -6242.0 41.5 109.9 7.5 12484.0 82.9 ## fit5.21 -0.4 0.8 -6242.4 41.5 110.4 7.7 12484.8 82.9 ## fit5.20 -0.6 0.6 -6242.6 41.4 110.7 7.6 12485.2 82.8 ## fit5.22 -1.0 0.6 -6243.0 41.5 111.6 7.7 12485.9 83.0 model_weights(fit5.19, fit5.20, fit5.21, fit5.22, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.19 fit5.20 fit5.21 fit5.22 ## 0.383 0.216 0.257 0.145 All about the same. Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.7.1 tidybayes_2.0.3 brms_2.12.0 Rcpp_1.0.4.6 ## [5] dagitty_0.2-2 rstan_2.19.3 StanHeaders_2.21.0-1 forcats_0.5.0 ## [9] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 ## [13] tidyr_1.0.2 tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 svUnit_0.7-12 DT_0.13 fansi_0.4.1 ## [13] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 bridgesampling_1.0-0 ## [17] splines_3.6.3 knitr_1.28 shinythemes_1.1.2 jsonlite_1.6.1 ## [21] broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [25] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 cli_2.0.2 later_1.0.0 htmltools_0.4.0 ## [33] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 V8_3.0.2 ## [41] cellranger_1.1.0 vctrs_0.2.4 nlme_3.1-144 crosstalk_1.1.0.1 ## [45] xfun_0.13 ps_1.3.2 rvest_0.3.5 miniUI_0.1.1.1 ## [49] mime_0.9 lifecycle_0.2.0 gtools_3.8.2 MASS_7.3-51.5 ## [53] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [57] promises_1.1.0 Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 curl_4.3 gridExtra_2.3 loo_2.2.0 ## [65] stringi_1.4.6 highr_0.8 dygraphs_1.1.1.6 boot_1.3-24 ## [69] pkgbuild_1.0.6 shape_1.4.4 rlang_0.4.5 pkgconfig_2.0.3 ## [73] matrixStats_0.56.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [77] htmlwidgets_1.5.1 labeling_0.3 processx_3.4.2 tidyselect_1.0.0 ## [81] plyr_1.8.6 magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [85] generics_0.0.2 DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [89] withr_2.1.2 mgcv_1.8-31 xts_0.12-0 abind_1.4-5 ## [93] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 ## [97] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 callr_3.4.3 ## [101] threejs_0.3.3 reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [105] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 ## [109] shinyjs_1.1 "],
["modeling-discontinuous-and-nonlinear-change.html", "6 Modeling Discontinuous and Nonlinear Change Reference Session info", " 6 Modeling Discontinuous and Nonlinear Change To be fleshed out later Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_3.6.3 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.5 evaluate_0.14 "],
["examining-the-multilevel-models-error-covariance-structure.html", "7 Examining the Multilevel Model’s Error Covariance Structure Reference Session info", " 7 Examining the Multilevel Model’s Error Covariance Structure To be fleshed out later Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_3.6.3 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.5 evaluate_0.14 "],
["modeling-change-using-covariance-structure-analysis.html", "8 Modeling Change Using Covariance Structure Analysis Reference Session info", " 8 Modeling Change Using Covariance Structure Analysis To be fleshed out later Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_3.6.3 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.5 evaluate_0.14 "],
["a-framework-for-investigating-event-occurrence.html", "9 A Framework for Investigating Event Occurrence 9.1 Should you conduct a survival analysis? The “whether” and “when” test 9.2 Framing a research question about event occurrence 9.3 Censoring: How complete are the data on event occurrence? Reference Session info", " 9 A Framework for Investigating Event Occurrence Researchers who want to study event occurrence must learn how to think about their data in new and unfamiliar ways. Even traditional methods for data description–the use of means and standard deviations–fail to serve researchers well. In this chapter we introduce the essential features of event occurrence data, explaining how and why they create the need for new analytic methods. (pp 305–306) 9.1 Should you conduct a survival analysis? The “whether” and “when” test To determine whether a research question calls for survival analysis, we find it helpful to apply a simple mnemonic we refer to as “the whether and when test.” If your research questions include either word–whether or when–you probably need to use survival methods. (p. 306, emphasis added) 9.1.1 Time to relapse among recently treated alcoholics. Within the addictive-behaviors literature, researchers often study if and when participants relapse (i.e., begin using the substance(s) again). 9.1.2 Length of stay in teaching. Education researchers can use survival analysis to study whether and for how long newly-hired teachers stay in their positions. 9.1.3 Age at first suicide ideation. Suicide is a major health risk and clinical researchers sometimes use survival analysis to whether and when participants have first considered killing themselves. 9.2 Framing a research question about event occurrence Survival analyses share three common characteristics. Each has a clearly defined: Target event, whose occurrence is being studies Beginning of time, an initial starting point when no one under study has yet experienced the target event Metrics for clocking time, a meaningful scale in which event occurrence is recorded (p. 310, emphasis in the original) 9.2.1 Defining event occurrence. “Event occurrence represents an individual’s transition from one ‘state’ to another ‘state’” (p. 310). Though our primary focus will be on binary states (e.g., drinking/abstinent), survival analyses can handle more categories (e.g., whether/when marriages end in divorce or death). 9.2.2 Identifying the “beginning of time.” The “beginning of time” is a moment when everyone in the population occupies one, and only one, of the possible states… Over time, as individuals move from the original state to the next, they experience the target event. The timing of this transition–the distance from the “beginning of time” until the event occurrence–is referred to as the event time. To identify the “beginning of time” in a given study, imagine placing everyone in the population on a time-line, an axis with the “beginning of time” at one end and the last moment when event occurrence could be observed at the other. The goal is to “start the clock” when on one in the population has yet experienced the event but everyone is at least (theoretically) eligible to do so. In the language of survival analysis, you want to start the clock when everyone in the population is at risk of experiencing the event. (pp. 311–312, emphasis in the original) 9.2.3 Specifying a metric for time. We distinguish between data recorded in thin precise units and those recorded in thicker intervals by calling the former continuous time and the latter discrete time. [Though survival methods can handle both discrete and continuous time,] time should be recorded in the smallest possible units relevant to the process under study. No single metric is universally appropriate, and even different studies of the identical event might use different scales. (p. 313, emphasis in the original) 9.3 Censoring: How complete are the data on event occurrence? No matter when data collection begins, and no matter how long it lasts, some sample members are likely to have unknown event times. Statisticians call this problem censoring and they label the people with the unknown event times censored observations. Because censoring is inevitable–and a fundamental conundrum in the study of event occurrence–we now explore it in detail. (p. 316, emphasis in the original) 9.3.1 How and why does censoring arise? Censoring occurs whenever a researcher does not know an individual’s event time. There are two major reasons for censoring: (1) some individuals will never experience the target event; and (2) others will experience the event, but not during the study’s data collection. Some of these latter individuals will experience the event shortly after data collection ends while others will do so at a much later time. As a practical matter, though, these distinctions matter little because you cannot distinguish among them. That, unfortunately, is the nature of censoring: it prevents you from knowing the very quantity of interest–whether and, if so, when the target event occurs for a subset of the sample. (pp. 316–317, emphasis in the original) 9.3.2 Different types of censoring. “Methodologists make two major types of distinctions: first, between non-informative and informative censoring mechanisms, and second, between right- and left-censoring” (p. 318, emphasis in the original). 9.3.2.1 Noninformative versus informative censoring. A noninformative censoring mechanism operates independent of event occurrence and the risk of event occurrence. If censoring is under an investigator’s control, determined in advance by design–as it usually is–then it is noninformative… [Under this mechanism] we can therefore assume that all individuals who remain in the study after the censoring date are representative of everyone who would have remained in the study had censoring not occurred. If censoring occurs because individuals have experienced the event or are likely to do so in the future, the censoring mechanism is informative… Under these circumstances, we can no longer assume that those people who remain in the study after this tie are representative of all individuals who would have remained in the study had censoring not occurred. The noncensored individuals differ systematically from the censored individuals. (pp. 318–319, emphasis in the original) 9.3.2.2 Right- versus left-censoring. Right-censoring arises when an event time is unknown because event occurrence is not observed. Left-censoring arises when an event time is unknown because the beginning of time is not observed…. Because [right-censoring] is the one typically encountered in practice, and because it is the type for which survival methods were developed, references to censoring, unencumbered by a directional modifier, usually refer to right-censoring. How to left-censored observations arise? Often they arise because researchers have not paid sufficient attention to identifying the beginning of time during the design phase. If the beginning of time is defined well–as that moment when all individuals in the population are eligible to experience the event but none have yet done so–left-censoring can be eliminated…. Left-censoring presents challenges not easily addressed even with the most sophisticated of survival methods (Hu &amp; Lawless, 1996). Little progress has been made in this area since Turnbull (1974, 1976) offered some basic descriptive approaches and Flinn and Heckman (1982) and Cox and Oakes (1984) offered some directions for fitting models under a restrictive set of assumptions. The most common advice, followed by Fichman, is to set the left-censored spells aside from analysis…. Redefining the beginning of time to coincide with a precipitating event… is often the best way of resolving the otherwise intractable problems that left-censored data pose. Whenever possible, we suggest that researchers consider such a redefinition or otherwise eliminate left-censored data through design. (pp. 319–320, emphasis in the original) 9.3.3 How does censoring affect statistical analysis? Here we load the teachers.csv data (Singer, 1992). library(tidyverse) teachers &lt;- read_csv(&quot;data/teachers.csv&quot;) glimpse(teachers) ## Rows: 3,941 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,… ## $ t &lt;dbl&gt; 1, 2, 1, 1, 12, 1, 12, 1, 2, 2, 7, 12, 1, 12, 12, 2, 12, 1, 3, 2, 12, 12, 9, 12, 2… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0… Make a version of Figure 9.1. teachers %&gt;% count(censor, t) %&gt;% mutate(censor = if_else(censor == &quot;0&quot;, &quot;not censored&quot;, &quot;censored&quot;)) %&gt;% ggplot(aes(x = t)) + geom_col(aes(y = n)) + geom_text(aes(y = n + 25, label = n)) + scale_x_continuous(&quot;years&quot;, breaks = 1:12) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~censor, nrow = 2) Here’s a descriptive breakdown of those censored or not. teachers %&gt;% group_by(censor) %&gt;% summarise(n = n(), mean = mean(t), sd = sd(t)) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 x 5 ## censor n mean sd percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2207 3.73 2.41 56.0 ## 2 1 1734 9.60 1.78 44.0 Whereas the distribution of the censored occasions is flattish with a bit of a spike at 12, the distribution of the non-censored times has a bit of an exponential look to it. Recall that the exponential distribution is controlled by a single parameter, its rate, and the mean of the exponential distribution is the reciprocal of that rate. If we take the empirical mean and \\(n\\) of the non-censored data and plot those in to the rexp() function, we can simulate exponential data and plot. set.seed(9) tibble(years = rexp(n = 2207, rate = 1 / 3.7)) %&gt;% ggplot(aes(x = years)) + geom_histogram(binwidth = 1, boundary = 0) + scale_x_continuous(breaks = 1:12) + coord_cartesian(xlim = c(0, 12)) + theme(panel.grid = element_blank()) That simulation looks pretty similar to our non-censored data. If we stopped there, we might naïvely presume \\(\\operatorname{Exponential}(1/3.7)\\) is a good model for our data. But this would ignore the censored data. One of the solutions researchers have used is to assign the censored cases the event time they possess at the end of the data collection (e.g., Frank &amp; Keith, 1984). Applying this to our teacher career data (e.g., assigning a career length of 7 years to the 280 teachers censored in the year 7, etc.) yields an estimated mean career duration of 7.5 years. (pp. 322–323) Here’s what that looks like. teachers %&gt;% summarise(mean = mean(t), median = median(t), sd = sd(t)) ## # A tibble: 1 x 3 ## mean median sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6.31 7 3.63 I have no idea where the 7.5 value Singer and Willett presented came from. It’s larger than both the mean and the median in the data. But anyway, this method is patently wrong, so it doesn’t matter: Imputing event times for censored cases simply changes all “nonevents” into “events” and further assumes that all these new “events” occur at the earliest time possible–that is, at the moment of censoring. Surely these decisions are most likely wrong. (p. 323) Stay tuned for methods that are better than patently wrong. Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [7] tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.13 haven_2.2.0 lattice_0.20-38 colorspace_1.4-1 ## [6] vctrs_0.2.4 generics_0.0.2 htmltools_0.4.0 yaml_2.2.1 utf8_1.1.4 ## [11] rlang_0.4.5 pillar_1.4.3 withr_2.1.2 glue_1.4.0 DBI_1.1.0 ## [16] dbplyr_1.4.2 modelr_0.1.6 readxl_1.3.1 lifecycle_0.2.0 munsell_0.5.0 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.5 evaluate_0.14 labeling_0.3 ## [26] knitr_1.28 fansi_0.4.1 broom_0.5.5 Rcpp_1.0.4.6 backports_1.1.6 ## [31] scales_1.1.0 jsonlite_1.6.1 farver_2.0.3 fs_1.4.1 hms_0.5.3 ## [36] digest_0.6.25 stringi_1.4.6 bookdown_0.18 grid_3.6.3 cli_2.0.2 ## [41] tools_3.6.3 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 ellipsis_0.3.0 ## [46] xml2_1.3.1 reprex_0.3.0 lubridate_1.7.8 assertthat_0.2.1 rmarkdown_2.1 ## [51] httr_1.4.1 rstudioapi_0.11 R6_2.4.1 nlme_3.1-144 compiler_3.6.3 "],
["describing-discrete-time-event-occurrence-data.html", "10 Describing Discrete-Time Event Occurrence Data 10.1 The life table 10.2 A framework for characterizing the distribution of discrete-time event occurrence data 10.3 Developing intuition about hazard functions, survivor functions, and median lifetimes 10.4 Quantifying the effects of sampling variation 10.5 A simple and useful strategy for constructing the life table 10.6 Bonus: Fit the discrete-time hazard models with brms Reference Session info", " 10 Describing Discrete-Time Event Occurrence Data In this chapter, [Singer and Willett presented] a framework for describing discrete-time event occurrence data…. As we will [see], the conceptual linchpin for all subsequent survival methods is to approach the analysis on a period-by-period basis. This allows you to examine event occurrence sequentially among those individuals eligible to experience the event at each discrete point in time. (p. 325) 10.1 The life table The fundamental tool for summarizing the sample distribution of event occurrence is the life table. As befits its name, a life table tracks the event histories (the “lives”) of a sample of individuals from the beginning of time (when no one has yet experienced the target event) through the end of data collection. (p. 326, emphasis in the original) To make a life table as presented in Table 10.1, we need to load the teachers.csv data. library(tidyverse) teachers &lt;- read_csv(&quot;data/teachers.csv&quot;) glimpse(teachers) ## Rows: 3,941 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,… ## $ t &lt;dbl&gt; 1, 2, 1, 1, 12, 1, 12, 1, 2, 2, 7, 12, 1, 12, 12, 2, 12, 1, 3, 2, 12, 12, 9, 12, 2… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0… Perhaps the easiest way to make a life table as presented in Table 10.1 is with help from the survival package. # install.packages(&quot;survival&quot;, dependencies = T) library(survival) Here we’ll use the survfit() function to compute survival curves. Within the survfit() function, we’ll use the Surv() function to make a survival object, which will become the criterion within the model formula. It takes two basic arguments, time and event. With the teachers data, t is time in years. In the data, events are encoded in censor. However, it’s important to understand how the event argument expects the data. From the survival reference manual, we read that event is “the status indicator, normally 0=alive, 1=dead. Other choices are TRUE/FALSE (TRUE = death) or 1/2 (2=death).” Note that whereas within our data censor is coded 0 = event 1 = censored, the event argument expects the opposite. A quick way to solve that is to enter 1 - censor. fit10.1 &lt;- survfit(data = teachers, Surv(t, 1 - censor) ~ 1) Use the str() function to survey the results. fit10.1 %&gt;% str() ## List of 16 ## $ n : int 3941 ## $ time : num [1:12] 1 2 3 4 5 6 7 8 9 10 ... ## $ n.risk : num [1:12] 3941 3485 3101 2742 2447 ... ## $ n.event : num [1:12] 456 384 359 295 218 184 123 79 53 35 ... ## $ n.censor : num [1:12] 0 0 0 0 0 0 280 307 255 265 ... ## $ surv : num [1:12] 0.884 0.787 0.696 0.621 0.566 ... ## $ std.err : num [1:12] 0.00576 0.00829 0.01053 0.01245 0.01396 ... ## $ cumhaz : num [1:12] 0.116 0.226 0.342 0.449 0.538 ... ## $ std.chaz : num [1:12] 0.00542 0.00781 0.00992 0.01173 0.01319 ... ## $ type : chr &quot;right&quot; ## $ logse : logi TRUE ## $ conf.int : num 0.95 ## $ conf.type: chr &quot;log&quot; ## $ lower : num [1:12] 0.874 0.774 0.682 0.606 0.55 ... ## $ upper : num [1:12] 0.894 0.8 0.71 0.636 0.581 ... ## $ call : language survfit(formula = Surv(t, 1 - censor) ~ 1, data = teachers) ## - attr(*, &quot;class&quot;)= chr &quot;survfit&quot; We can retrieve the values for the “Year” column from fit10.1$time. The values in the “Time interval” column are a simple transformation from there. fit10.1$time ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 We can find the values in the “Employed at the beginning of the year” column in fit10.1$n.risk and those in the “Who left during the year” column in fit10.1$n.event. fit10.1$n.risk ## [1] 3941 3485 3101 2742 2447 2229 2045 1642 1256 948 648 391 fit10.1$n.event ## [1] 456 384 359 295 218 184 123 79 53 35 16 5 We’ll have to work a little harder to compute the values in the “Censored at the end of the year column.” Here we’ll walk it through in a data frame format. data.frame(n_risk = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(n_risk_1 = lead(n_risk, default = 0)) %&gt;% mutate(n_censored = n_risk - n_event - n_risk_1) ## n_risk n_event n_risk_1 n_censored ## 1 3941 456 3485 0 ## 2 3485 384 3101 0 ## 3 3101 359 2742 0 ## 4 2742 295 2447 0 ## 5 2447 218 2229 0 ## 6 2229 184 2045 0 ## 7 2045 123 1642 280 ## 8 1642 79 1256 307 ## 9 1256 53 948 255 ## 10 948 35 648 265 ## 11 648 16 391 241 ## 12 391 5 0 386 That is, to get the number of those censored at the end of a given year, you take the number employed at the beginning of that year, subtract the number of those who left (i.e., the number who experienced the “event”), and then subtract the number of those employed at the beginning of the next year. Notice our use of the dplyr::lead() function to get the number employed in the next year (learn more about that function here). To get the values in the “Teachers at the beginning of the year who left during the year” column, which is in a proportion metric, we use division. fit10.1$n.event / fit10.1$n.risk ## [1] 0.11570667 0.11018651 0.11576911 0.10758570 0.08908868 0.08254823 0.06014670 0.04811206 ## [9] 0.04219745 0.03691983 0.02469136 0.01278772 Finally, to pull the values in the “All teachers still employed at the end of the year” column, we just execute fit10.1$surv. fit10.1$surv ## [1] 0.8842933 0.7868561 0.6957625 0.6209084 0.5655925 0.5189038 0.4876935 0.4642295 0.4446402 ## [10] 0.4282242 0.4176508 0.4123100 Let’s put that all together in a tibble. most_rows &lt;- tibble(year = fit10.1$time) %&gt;% mutate(time_int = str_c(&quot;[&quot;, year, &quot;, &quot;, year + 1, &quot;)&quot;), n_employed = fit10.1$n.risk, n_left = fit10.1$n.event) %&gt;% mutate(n_censored = n_employed - n_left - lead(n_employed, default = 0), hazard_fun = n_left / n_employed, survivor_fun = fit10.1$surv) most_rows ## # A tibble: 12 x 7 ## year time_int n_employed n_left n_censored hazard_fun survivor_fun ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 [1, 2) 3941 456 0 0.116 0.884 ## 2 2 [2, 3) 3485 384 0 0.110 0.787 ## 3 3 [3, 4) 3101 359 0 0.116 0.696 ## 4 4 [4, 5) 2742 295 0 0.108 0.621 ## 5 5 [5, 6) 2447 218 0 0.0891 0.566 ## 6 6 [6, 7) 2229 184 0 0.0825 0.519 ## 7 7 [7, 8) 2045 123 280 0.0601 0.488 ## 8 8 [8, 9) 1642 79 307 0.0481 0.464 ## 9 9 [9, 10) 1256 53 255 0.0422 0.445 ## 10 10 [10, 11) 948 35 265 0.0369 0.428 ## 11 11 [11, 12) 648 16 241 0.0247 0.418 ## 12 12 [12, 13) 391 5 386 0.0128 0.412 The only thing missing from our version of Table 10.1 is we don’t have a row for Year 0. Here’s a quick and dirty way to manually insert those values. row_1 &lt;- tibble(year = 0, time_int = &quot;[0, 1)&quot;, n_employed = fit10.1$n.risk[1], n_left = NA, n_censored = NA, hazard_fun = NA, survivor_fun = 1) d &lt;- bind_rows(row_1, most_rows) d ## # A tibble: 13 x 7 ## year time_int n_employed n_left n_censored hazard_fun survivor_fun ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 [0, 1) 3941 NA NA NA 1 ## 2 1 [1, 2) 3941 456 0 0.116 0.884 ## 3 2 [2, 3) 3485 384 0 0.110 0.787 ## 4 3 [3, 4) 3101 359 0 0.116 0.696 ## 5 4 [4, 5) 2742 295 0 0.108 0.621 ## 6 5 [5, 6) 2447 218 0 0.0891 0.566 ## 7 6 [6, 7) 2229 184 0 0.0825 0.519 ## 8 7 [7, 8) 2045 123 280 0.0601 0.488 ## 9 8 [8, 9) 1642 79 307 0.0481 0.464 ## 10 9 [9, 10) 1256 53 255 0.0422 0.445 ## 11 10 [10, 11) 948 35 265 0.0369 0.428 ## 12 11 [11, 12) 648 16 241 0.0247 0.418 ## 13 12 [12, 13) 391 5 386 0.0128 0.412 We might walk out the notation in our time_int column a bit. Those intervals reflect a standard partition of time, in which each interval includes the initial time and excludes the concluding time. Adopting common mathematical notation, [brackets] denote inclusions and (parentheses) denote exclusions. Thus, we bracket each interval’s initial time and place a parenthesis around its concluding time. (p. 328, emphasis in the original) The values in the n_employed column the risk set, those who are “eligible to experience the event during that interval” (p. 329, emphasis in the original). 10.2 A framework for characterizing the distribution of discrete-time event occurrence data The fundamental quantity used to assess the risk of event occurrence in each discrete time period is known as hazard. Denoted by \\(h(t_{ij})\\), discrete time hazard is the conditional probability that individual \\(i\\) will experience the event time in period \\(j\\), given that he or she did not experience it in any earlier time period. Because hazard represents the risk of the event occurrence in each discrete time period among those people eligible to experience the event (those in the risk set) hazard tells us precisely what we want to know: whether and when events occurs. (p. 330, emphasis in the original) If we let \\(T_i\\) stand for the discrete value in time person \\(i\\) experiences the event, we can express the conditional probability the event might occur in the \\(j^\\text{th}\\) interval as \\[h(t_{ij}) = \\text{Pr}[T_i = j | T \\geq j].\\] That last part, \\(T \\geq j\\), clarifies the event can only occur once and, therefore, cannot have occurred in any of the prior levels of \\(j\\). More plainly put, imagine the event is death and person \\(i\\) died during the period of \\(T_j = 20\\). In such a case, it’s nonsensical to speak of that \\(i^\\text{th}\\) person’s hazard for the period of \\(T_j = 25\\). They’re already dead. Also, “the discrete-time hazard probabilities expressed as a function of time–labeled \\(h(t_{ij})\\)–is known as the population discrete-time hazard function” (p 330, emphasis in the original). That was expressed in the 6th column in Table 10.1, which we called hazard_fun in our d tibble. d %&gt;% select(year, hazard_fun) ## # A tibble: 13 x 2 ## year hazard_fun ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 NA ## 2 1 0.116 ## 3 2 0.110 ## 4 3 0.116 ## 5 4 0.108 ## 6 5 0.0891 ## 7 6 0.0825 ## 8 7 0.0601 ## 9 8 0.0481 ## 10 9 0.0422 ## 11 10 0.0369 ## 12 11 0.0247 ## 13 12 0.0128 You might notice \\(h(t_{ij})\\) is in a proportion metric and it is not cumulative. If you look above in the code, you’ll see we computed that by hazard_fun = n_left / n_employed. More formally and generally, this is an operationalization of \\[\\hat h(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j},\\] where \\(n \\text{ events}_j\\) is the number of individuals who experienced the event in the \\(j^{th}\\) period and \\(n \\text{ at risk}_j\\) is the number within the period who have not (a) already experienced the event and (b) been censored. Also note that by \\(\\hat h(t_{j})\\), we’re indicating we’re talking about the maximum likelihood estimate for \\(h(t_{j})\\). Because no one is at risk during the initial time point, \\(h(t_0)\\) is undefined (i.e., NA). Here we mimic the top panel of Figure 10.1 and plot our \\(\\hat h(t_{j})\\) over time. d %&gt;% ggplot(aes(x = year, y = hazard_fun)) + geom_line() + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated hazard probability, &quot;*hat(italic(h))(italic(t[j]))), breaks = c(0, .05, .1, .15), limits = c(0, .15)) + theme(panel.grid = element_blank()) 10.2.1 Survivor function. The survivor function provides another way of describing the distribution of event occurrence over time. Unlike the hazard function, which assesses the unique risk associated with each time period, the survivor function cumulates these period-by-period risks of event occurrence (or more properly, nonoccurrence) together to assess the probability that a randomly selected individual will survive will not experience the event. We can formally define the survivor function, \\(S(t_{ij})\\), as \\[S(t_{ij}) = \\text{Pr}[T &gt; j],\\] where \\(S\\) is survival as a function of time, \\(t\\). But since our data are finite, we can only have an estimate of the “true” survivor function, which we call \\(\\hat S(t_{ij})\\). Here it is in a plot, our version of the bottom panel of Figure 10.1. d %&gt;% ggplot(aes(x = year, y = survivor_fun)) + geom_hline(yintercept = .5, color = &quot;white&quot;, linetype = 2) + geom_line() + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated survival probability, &quot;*hat(italic(S))(italic(t[j]))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) 10.2.2 Median lifetime. Having characterized the distribution of event times using the hazard and survivor functions, we often want to identify the distribution’s center. Were there no censoring, all event times would be known, and we could compute a sample mean. But because of censoring, another estimate of central tendency is preferred: the median lifetime. The estimated median lifetime identifies that value for \\(T\\) for which the value of the estimated survivor function is .5. It is the point in time by which we estimate that half of the sample has experienced the target event, half has not. (p. 337, emphasis in the original) If we use filter(), well see our median lifetime rests between years 6 and 7. d %&gt;% filter(year %in% c(6, 7)) %&gt;% # this just simplifies the output select(year, time_int, survivor_fun) ## # A tibble: 2 x 3 ## year time_int survivor_fun ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6 [6, 7) 0.519 ## 2 7 [7, 8) 0.488 Using a simple descriptive approach, we’d just say the median lifetime was between years 6 and 7. We could also follow Miller (1981) and linearly interpolate between the two values of \\(S(t_j)\\) bracketing .5. If we let \\(m\\) be the time interval just before the median lifetime, \\(\\hat S(t_m)\\) be the value of the survivor function in that \\(m^\\text{th}\\) interval, and \\(\\hat S(t_{m + 1})\\) be the survival value in the next interval, the can write \\[\\text{Estimated median lifetime} = m + \\Bigg [\\frac{\\hat S(t_m) - .5}{\\hat S(t_m) - \\hat S(t_{m + 1})} \\Bigg ] \\big ((m + 1) - m \\big).\\] We can compute that by hand like so. m &lt;- 6 m_plus_1 &lt;- 7 stm &lt;- d %&gt;% filter(year == m) %&gt;% pull(survivor_fun) stm_plus_1 &lt;- d %&gt;% filter(year == m_plus_1) %&gt;% pull(survivor_fun) # compute the interpolated median lifetime and save it as `iml` iml &lt;- m + ((stm - .5) / (stm - stm_plus_1)) * ((m + 1) - m) iml ## [1] 6.605691 Now we have the iml value, we can add that information to our version of the lower panel of Figure 10.1. line &lt;- tibble(year = c(0, iml, iml), survivor_fun = c(.5, .5, 0)) d %&gt;% ggplot(aes(x = year, y = survivor_fun)) + geom_path(data = line, color = &quot;white&quot;, linetype = 2) + geom_line() + annotate(geom = &quot;text&quot;, x = iml, y = .55, label = &quot;All teachers (6.6 years)&quot;, hjust = 0) + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated survival probability, &quot;*hat(italic(S))(italic(t[j]))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) We can compute the estimates for the 5- and 10-year survival rates as a direct algebraic transformation of the survival function from those years. d %&gt;% filter(year %in% c(5, 10)) %&gt;% select(year, survivor_fun) %&gt;% mutate(`survival rate (%)` = (100 * survivor_fun) %&gt;% round(digits = 0)) ## # A tibble: 2 x 3 ## year survivor_fun `survival rate (%)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0.566 57 ## 2 10 0.428 43 10.3 Developing intuition about hazard functions, survivor functions, and median lifetimes Developing intuition about these sample statistics requires exposure to estimates computed from a wide range of studies. To jump-start this process, we review results from four studies that differ across three salient dimensions–the type of event investigated, the metric used to record discrete time, and most important, the underlying profile of risk–and discuss how we would examine, and describe, the estimated hazard functions, survivor functions, and median lifetimes. (p. 339) Here we load the four relevant data sets. cocaine &lt;- read_csv(&quot;data/cocaine_relapse.csv&quot;) sex &lt;- read_csv(&quot;data/firstsex.csv&quot;) suicide &lt;- read_csv(&quot;data/suicide.csv&quot;) congress &lt;- read_csv(&quot;data/congress.csv&quot;) # glimpse(cocaine) # glimpse(sex) # glimpse(suicide) # glimpse(congress) We have a lot of leg work in front of use before we can recreate Figure 10.2. First, we’ll feed each of the four data sets into the survfit() function. fit10.2 &lt;- survfit(data = cocaine, Surv(week, 1 - censor) ~ 1) fit10.3 &lt;- survfit(data = sex, Surv(time, 1 - censor) ~ 1) fit10.4 &lt;- survfit(data = suicide, Surv(time, 1 - censor) ~ 1) fit10.5 &lt;- survfit(data = congress, Surv(time, 1 - censor) ~ 1) Given the four fits all follow the same basic form and given our end point is to make the same basic plots for each, we can substantially streamline our code by making a series of custom functions. For our first custom function, make_lt(), we’ll save the general steps for making life tables for each data set. make_lt &lt;- function(fit) { # arrange the lt data for all rows but the first most_rows &lt;- tibble(time = fit$time) %&gt;% mutate(time_int = str_c(&quot;[&quot;, time, &quot;, &quot;, time + 1, &quot;)&quot;), n_risk = fit$n.risk, n_event = fit$n.event) %&gt;% mutate(hazard_fun = n_event / n_risk, survivor_fun = fit$surv) # define the values for t = 2 and t = 1 time_1 &lt;- fit$time[1] time_0 &lt;- time_1 - 1 # define the values for the row for which t = 1 row_1 &lt;- tibble(time = time_0, time_int = str_c(&quot;[&quot;, time_0, &quot;, &quot;, time_1, &quot;)&quot;), n_risk = fit$n.risk[1], n_event = NA, hazard_fun = NA, survivor_fun = 1) # make the full life table lt &lt;- bind_rows(row_1, most_rows) lt } Use make_lt() to make the four life tables. lt_cocaine &lt;- make_lt(fit10.2) lt_sex &lt;- make_lt(fit10.3) lt_suicide &lt;- make_lt(fit10.4) lt_congress &lt;- make_lt(fit10.5) You’ll note that the four survival-curve plots in Figure 10.2 all show the median lifetime using the interpolation method. Here we’ll save the necessary steps to compute that for each model as the make_iml() function. make_iml &lt;- function(lt) { # lt is a generic name for a life table of the # kind we made with our `make_lt()` function # determine the mth row lt_m &lt;- lt %&gt;% filter(survivor_fun &gt; .5) %&gt;% slice(n()) # determine the row for m + 1 lt_m1 &lt;- lt %&gt;% filter(survivor_fun &lt; .5) %&gt;% slice(1) # pull the value for m m &lt;- pull(lt_m, time) # pull the two survival function values stm &lt;- pull(lt_m, survivor_fun) stm1 &lt;- pull(lt_m1, survivor_fun) # plug the values into Equation 10.6 (page 338) iml &lt;- m + ((stm - .5) / (stm - stm1)) * ((m + 1) - m) iml } If you want, you can use make_iml() directly like this. make_iml(lt_cocaine) ## [1] 7.5 However, our approach will be to wrap it in another function, line_tbl(), with which we will save the coordinates necessary for marking off the median lifetimes and them save them in a tibble. line_tbl &lt;- function(lt) { iml &lt;- make_iml(lt) tibble(time = c(lt[1, 1] %&gt;% pull(), iml, iml), survivor_fun = c(.5, .5, 0)) } It works like this. line_tbl(lt_cocaine) ## # A tibble: 3 x 2 ## time survivor_fun ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.5 ## 2 7.50 0.5 ## 3 7.50 0 If you look closely at the hazard function plots in the left column of Figure 10.2, you’ll note they share many common settings (e.g., the basic shape, the label of the \\(y\\)-axis). But there are several parameters we’ll need to set custom settings for. To my eye, those are: the data; the \\(x\\)-axis label, break points, and limits; and the \\(y\\)-axis break points, and limits. With our custom h_plot() function, we’ll leave those parameters free while keeping all the other ggplot2 parameters the same. h_plot &lt;- function(data = data, xlab = xlab, xbreaks = xbreaks, xlimits = xlimits, ybreaks = ybreaks, ylimits = ylimits) { ggplot(data = data, mapping = aes(x = time, y = hazard_fun)) + geom_line() + scale_x_continuous(xlab, breaks = xbreaks, limits = xlimits) + scale_y_continuous(expression(widehat(italic(h(t)))), breaks = ybreaks, limits = ylimits) + theme(panel.grid = element_blank()) } Now we’ll make a similar custom plotting function, s_plot(), for the hazard function plots on the right column of Figure 10.2. s_plot &lt;- function(data = data, xlab = xlab, xbreaks = xbreaks, xlimits = xlimits) { # compute the imterpolated median life value iml &lt;- make_iml(data) # make the imp line values line &lt;- data %&gt;% line_tbl() ggplot(data = data, mapping = aes(x = time, y = survivor_fun)) + geom_path(data = line, color = &quot;white&quot;, linetype = 2) + geom_line() + annotate(geom = &quot;text&quot;, x = iml, y = .6, label = str_c(&quot;widehat(ML)==&quot;, iml %&gt;% round(1)), size = 3, hjust = 0, parse = T) + scale_x_continuous(xlab, breaks = xbreaks, limits = xlimits) + scale_y_continuous(expression(widehat(italic(S(t)))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) } Now we make the eight subplots in bulk, naming them p1, p2, and so on. # cocaine p1 &lt;- lt_cocaine %&gt;% h_plot(xlab = &quot;Weeks after release&quot;, xbreaks = 0:12, xlimits = c(0, 12), ybreaks = c(0, .05, .1, .15), ylimits = c(0, .15)) p2 &lt;- lt_cocaine %&gt;% s_plot(xlab = &quot;Weeks after release&quot;, xbreaks = 0:12, xlimits = c(0, 12)) # sex p3 &lt;- lt_sex %&gt;% h_plot(xlab = &quot;Grade&quot;, xbreaks = 6:12, xlimits = c(6, 12), ybreaks = 0:3 / 10, ylimits = c(0, .325)) p4 &lt;- lt_sex %&gt;% s_plot(xlab = &quot;Grade&quot;, xbreaks = 6:12, xlimits = c(6, 12)) # suicide p5 &lt;- lt_suicide %&gt;% h_plot(xlab = &quot;Age&quot;, xbreaks = 1:9 * 2 + 3, xlimits = c(5, 22), ybreaks = c(0, .05, .1, .15), ylimits = c(0, .16)) p6 &lt;- lt_suicide %&gt;% s_plot(xlab = &quot;Age&quot;, xbreaks = 1:9 * 2 + 3, xlimits = c(5, 22)) # congress p7 &lt;- lt_congress %&gt;% h_plot(xlab = &quot;Terms in office&quot;, xbreaks = 0:8, xlimits = c(0, 8), ybreaks = 0:3 / 10, ylimits = c(0, .3)) p8 &lt;- lt_congress %&gt;% s_plot(xlab = &quot;Terms in office&quot;, xbreaks = 0:8, xlimits = c(0, 8)) Now we’ll use some functions and syntax from the patchwork package to combine the subplots and make Figure 10.2. library(patchwork) p12 &lt;- (p1 + p2) + plot_annotation(title = &quot;A&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) p34 &lt;- (p3 + p4) + plot_annotation(title = &quot;B&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) p56 &lt;- (p5 + p6) + plot_annotation(title = &quot;C&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) p78 &lt;- (p7 + p8) + plot_annotation(title = &quot;D&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) (wrap_elements(p12) / wrap_elements(p34) / wrap_elements(p56) / wrap_elements(p78)) Boom! Looks like a dream. 10.3.1 Identifying periods of high and low risk using hazard functions. It can be useful to evaluate hazard functions based on whether they are monotonic (i.e., have a single distinctive peak and single distinctive trough) and nonmonotonic (i.e., have multiple distinctive peaks or troughs). Globally speaking, the hazard functions for rows A and B are monotonic and the remaining two are nonmonotonic. Singer and Willett remarked “monotonically increasing hazard functions are common when studying events that are ultimately inevitable (or near universal)…. [However,] nonmonotonic hazard functions, like those in Panels C and D, generally arise in studies of long duration” (p. 342). However, when risk is constant over time, hazard functions will not have peaks or troughs. 10.3.2 Survivor functions as a context for evaluating the magnitude of hazard. Unlike with hazard functions, all survivor functions decrease or stay constant over time. They are monotonic (i.e., they never switch direction, they never increase). From the text (p. 344), we learn three ways hazard functions relate to survival functions: When hazard is high, the survivor function drops rapidly. When hazard is low, the survivor function drops slowly. When hazard is zero, the survivor function remains unchanged. 10.3.3 Strengths and limitations of estimated median lifetimes. When examining a median lifetime, we find it helpful to remember three important limitations on its interpretation. First, it identifies only an “average” event time; it tells us little about the distribution of even times and is relatively insensitive to extreme values. Second, the median lifetime is not necessarily a moment when the target event is especially likely to occur…. Third, the median lifetime reveals little about the distribution of risk over time; identical median lifetimes can result from dramatically different survivor and hazard functions. (pp. 345–346, emphasis in the original) Without access to Singer and Willett’s hypothetical data, we’re not in a good position to recreate their Figure 10.3. Even the good folks at IDRE gave up on that one. 10.4 Quantifying the effects of sampling variation We can quantify the uncertainty in the estimates with standard errors. 10.4.1 The standard error of the estimated hazard probabilities. The formula for the frequentist standard errors for the hazard probabilities follows the form \\[se \\big (\\hat h(t_j) \\big) = \\sqrt{\\frac{\\hat h(t_j) \\big (1 - \\hat h(t_j) \\big)}{n \\text{ at risk}_j}}.\\] We can express that equation R code to recreate the first four columns of Table 10.2. We’ll be pulling much of the information from fit10.1. But to show our work within a tibble format, we’ll be adding a column after \\(n_j\\). Our additional n_event column will contain the information pulled from fit10.1$n.event, which we’ll use to compute the \\(\\hat h(t_j)\\). se_h_hat &lt;- tibble(year = fit10.1$time, n_j = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(h_hat = n_event / n_j) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j)) se_h_hat ## # A tibble: 12 x 5 ## year n_j n_event h_hat se_h_hat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3941 456 0.116 0.00510 ## 2 2 3485 384 0.110 0.00530 ## 3 3 3101 359 0.116 0.00575 ## 4 4 2742 295 0.108 0.00592 ## 5 5 2447 218 0.0891 0.00576 ## 6 6 2229 184 0.0825 0.00583 ## 7 7 2045 123 0.0601 0.00526 ## 8 8 1642 79 0.0481 0.00528 ## 9 9 1256 53 0.0422 0.00567 ## 10 10 948 35 0.0369 0.00612 ## 11 11 648 16 0.0247 0.00610 ## 12 12 391 5 0.0128 0.00568 As in the text, our standard errors are pretty small. To get a better sense, here they are in a rug plot. se_h_hat %&gt;% ggplot(aes(x = se_h_hat)) + geom_rug(length = unit(0.25, &quot;in&quot;)) + scale_x_continuous(expression(italic(se)(hat(italic(h))(italic(t[j])))), limits = c(.004, .007)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) Standard errors for discrete hazards probabilities share a property with those for other probabilities: they are less certain (i.e., larger) for probability values near .5 and increasingly certain (i.e., smaller) for probability values approaching 0 and 1. To give a sense of that, here are the corresonding \\(se \\big (\\hat h(t_j) \\big)\\) for a series of \\(\\hat h(t_j)\\) values ranging from 0 to 1, with \\(n_j\\) held constant at 1,000. tibble(n_j = 1000, h_hat = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j)) %&gt;% ggplot(aes(x = h_hat, y = se_h_hat)) + geom_point() + labs(x = expression(hat(italic(h))(italic(t[j]))), y = expression(italic(se)(hat(italic(h))(italic(t[j]))))) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) Also, as the size of the risk set, \\(n_j\\), influences the standard errors in the typical way. All things equal, a larger \\(n\\) will make for a smaller \\(se\\). To give a sense, here’s the same basic plot from above, but this time with \\(n_j = 100, 1,000, \\text{ and } 10,000\\). crossing(n_j = c(100, 1000, 10000), h_hat = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j), n_j = str_c(&quot;italic(n[j])==&quot;, n_j)) %&gt;% ggplot(aes(x = h_hat, y = se_h_hat)) + geom_point() + labs(x = expression(hat(italic(h))(italic(t[j]))), y = expression(italic(se)(hat(italic(h))(italic(t[j]))))) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + facet_wrap(~n_j, nrow = 1, labeller = label_parsed) 10.4.2 Standard error of the estimated survival probabilities. Computing the frequentist standard errors for estimated survival probabilities is more difficult because these are the products of (1 - hazard) for the current and all previous survival probabilities. Computing them is such a pain, Singer and Willett recommend you rely on Greenwood’s (1926) approximation. This follows the form \\[se \\big (\\hat S(t_j) \\big) = \\hat S(t_j) \\sqrt{\\frac{\\hat h(t_1)}{n_1 \\big (1 - \\hat h(t_1) \\big)} + \\frac{\\hat h(t_2)}{n_2 \\big (1 - \\hat h(t_2) \\big)} + \\cdots + \\frac{\\hat h(t_j)}{n_j \\big (1 - \\hat h(t_j) \\big)}}.\\] Here we put the formula to work and finish our version of Table 10.2. For the sake of sanity, we’re simply calling our “Term under the square root sign” column term. Note our use of the cumsum() function. # suspend scientific notation options(scipen = 999) tibble(year = fit10.1$time, n_j = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(h_hat = n_event / n_j) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j), s_hat = fit10.1$surv, term = cumsum(h_hat / (n_j * (1 - h_hat)))) %&gt;% mutate(se_s_hat = s_hat * sqrt(term), std.err = fit10.1$std.err) ## # A tibble: 12 x 9 ## year n_j n_event h_hat se_h_hat s_hat term se_s_hat std.err ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3941 456 0.116 0.00510 0.884 0.0000332 0.00510 0.00576 ## 2 2 3485 384 0.110 0.00530 0.787 0.0000687 0.00652 0.00829 ## 3 3 3101 359 0.116 0.00575 0.696 0.000111 0.00733 0.0105 ## 4 4 2742 295 0.108 0.00592 0.621 0.000155 0.00773 0.0124 ## 5 5 2447 218 0.0891 0.00576 0.566 0.000195 0.00790 0.0140 ## 6 6 2229 184 0.0825 0.00583 0.519 0.000235 0.00796 0.0153 ## 7 7 2045 123 0.0601 0.00526 0.488 0.000267 0.00796 0.0163 ## 8 8 1642 79 0.0481 0.00528 0.464 0.000297 0.00800 0.0172 ## 9 9 1256 53 0.0422 0.00567 0.445 0.000332 0.00811 0.0182 ## 10 10 948 35 0.0369 0.00612 0.428 0.000373 0.00827 0.0193 ## 11 11 648 16 0.0247 0.00610 0.418 0.000412 0.00848 0.0203 ## 12 12 391 5 0.0128 0.00568 0.412 0.000445 0.00870 0.0211 For comparisson, we also added the \\(se \\big (\\hat S(t_j) \\big)\\) values coputed by the survivor package in the final column, std.err. tibble(year = fit10.1$time, n_j = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(h_hat = n_event / n_j) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j), s_hat = fit10.1$surv, term = cumsum(h_hat / (n_j * (1 - h_hat)))) %&gt;% mutate(Greenwood = s_hat * sqrt(term), `survival package` = fit10.1$std.err) %&gt;% pivot_longer(Greenwood:`survival package`) %&gt;% mutate(name = factor(name, levels = c(&quot;survival package&quot;, &quot;Greenwood&quot;))) %&gt;% ggplot(aes(x = year, y = value, color = name)) + geom_point(size = 4) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .55) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(expression(italic(se)), limits = c(0, 0.025)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.125, .9), panel.grid = element_blank()) It’s out of my expertise to comment on which should we should trust more. But Singer and Willett did note that “as an approximation, Greenwood’s formula is accurate only asymptotically” (p. 351). # turn scientific notation back on (R default) options(scipen = 0) 10.5 A simple and useful strategy for constructing the life table How can you construct a life table for your data set? For preliminary analyses, it is easy to use the prepackaged routines available in the major statistical packages. If you choose this approach, be sure to check whether your package allows you to: (1) select the partition of time; and (2) ignore any actuarial corrections invoked due to continuous-time assumptions (that do not hold in discrete time). When event times have been measured using a discrete-time scale, actuarial corrections (discussed in chapter13) are inappropriate. Although most packages clearly document the algorithm being used, we suggest that you double-check by comparing results with one or two estimates computed by hand. (p. 351, emphasis in the original) For more information about the methods we’ve been using via the survival package, browse through the documentation listed on the CRAN page, https://CRAN.R-project.org/package=survival, with a paticular emphasis on the reference manual and A package for survival analysis in R. But back to the text: Despite the simplicity of preprogrammed algorithms, [Singer and Willett] prefer an alternative approach for life table construction. This approach requires construction of a person-period data set, much like the period-period data set used for growth modeling. Once you create the person-period data set, you can compute descriptive statistics sing any standard cross-tabulation routine. (p. 351) 10.5.1 The person-period data set. Here is the person-level data set displayed in Figure 10.4; it’s just a subset of the teachers data. teachers %&gt;% filter(id %in% c(20, 126, 129)) ## # A tibble: 3 x 3 ## id t censor ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20 3 0 ## 2 126 12 0 ## 3 129 12 1 You can transform the person-level survival data set into the person-period variant shown on the right panel of Figure 10.4 with a workflow like this. teachers_pp &lt;- teachers %&gt;% uncount(weights = t) %&gt;% group_by(id) %&gt;% mutate(period = 1:n()) %&gt;% mutate(event = if_else(period == max(period) &amp; censor == 0, 1, 0)) %&gt;% select(-censor) %&gt;% ungroup() teachers_pp %&gt;% filter(id %in% c(20, 126, 129)) ## # A tibble: 27 x 3 ## id period event ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 20 1 0 ## 2 20 2 0 ## 3 20 3 1 ## 4 126 1 0 ## 5 126 2 0 ## 6 126 3 0 ## 7 126 4 0 ## 8 126 5 0 ## 9 126 6 0 ## 10 126 7 0 ## # … with 17 more rows You don’t necessarily need to use ungroup() at the end, but it’s probably a good idea. Anyway, note how the information previously contained in the censor column has been transformed to the event column, which is coded 0 = no event, 1 = event. With this coding, we know a participant has been censored when event == 0 on their max(period) row. We can count the number of teachers in the sample like this. teachers %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 3941 To get a sense of the difference in the data structures, here are the number of rows for the original person-level teachers data and for our person-period transformation. # person-level teachers %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 3941 # person-period teachers_pp %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 24875 Here is the breakdown of the number of rows in the person-period teachers data for which event == 1 or event == 0. teachers_pp %&gt;% count(event) ## # A tibble: 2 x 2 ## event n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 22668 ## 2 1 2207 10.5.2 Using the person-period data set to construct the life table. “All the life table’s essential elements can be computed through cross-tabulation of PERIOD and EVENT in the person-period data set” (p. 354, emphasis in the original). Here’s how we might use the tidyverse do that with teachers_pp. teachers_lt &lt;- teachers_pp %&gt;% # change the coding for `event` in anticipation of the final format mutate(event = str_c(&quot;event = &quot;, event)) %&gt;% group_by(period) %&gt;% count(event) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(total = `event = 0` + `event = 1`) %&gt;% mutate(prop_e_1 = (`event = 1` / total) %&gt;% round(digits = 4)) teachers_lt ## # A tibble: 12 x 5 ## period `event = 0` `event = 1` total prop_e_1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 3485 456 3941 0.116 ## 2 2 3101 384 3485 0.110 ## 3 3 2742 359 3101 0.116 ## 4 4 2447 295 2742 0.108 ## 5 5 2229 218 2447 0.0891 ## 6 6 2045 184 2229 0.0825 ## 7 7 1922 123 2045 0.0601 ## 8 8 1563 79 1642 0.0481 ## 9 9 1203 53 1256 0.0422 ## 10 10 913 35 948 0.0369 ## 11 11 632 16 648 0.0247 ## 12 12 386 5 391 0.0128 Here are the totals Singer and Willett displayed in the bottom row. teachers_lt %&gt;% pivot_longer(`event = 0`:total) %&gt;% group_by(name) %&gt;% summarise(total = sum(value)) %&gt;% pivot_wider(names_from = name, values_from = total) ## # A tibble: 1 x 3 ## `event = 0` `event = 1` total ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 22668 2207 24875 The ability to construct a life table using the person-period data set provides a simple strategy for conducting the descriptive analyses outlined in this chapter. This strategy yields appropriate statistics regardless of the amount, or pattern, of censoring. Perhaps even more important, the person-period data set is the fundamental tool for fitting discrete-time hazard models to data, using methods that we describe in the next chapter. (p. 356) 10.6 Bonus: Fit the discrete-time hazard models with brms The frequentists aren’t the only ones who can discrete-time hazard models. Bayesians can get in on the fun, too. The first step is to decide on an appropriate likelihood function. Why? Because Bayes’ formula requires that we define the likelihood and the priors. As all the parameters in the model are seen through the lens of the likelihood, it’s important we consider it with care. Happily, the exercises in the last section did a great job preparing us for the task. In Table 10.3, Singer and Willett hand computed the discrete hazards (i.e., the values in the “Proportion EVENT = 1” column) by dividing the valued in the “EVENT = 1” column by those in the “Total” column. Discrete hazards are proportions. Proportions have two important characteristics; they are continuous and necessarily range between 0 and 1. You know what else has those characteristics? Probabilities. So far in this text, we have primarily focused on models using the Gaussian likelihood. Though it’s a workhorse, the Gaussian is inappropriate for modeling proportions/probabilities. Good old Gauss is great at modeling unbounded continuous data, but it can fail miserably when working with bounded data and our proportions/probabilities are most certainly bounded. The binomial likelihood, however, is well-suited for handling probabilities. Imagine you have data that can take on values of 0 and 1, such as failures/successes, no’s/yesses, fails/passes, and no-events/events. If you sum up all the 1’s and divide them by the total cases, you get a proportion/probability. The simple binomial model takes just that kind of data—the number of 1’s and the number of total cases. The formula for the binomial likelihood is \\[\\operatorname{Pr} (z | n, p) = \\frac{n!}{z!(n - z)!} p^z (1 - p)^{n - z},\\] where \\(z\\) is the number of cases for which the value is 1, \\(n\\) is the total number of cases, and \\(p\\) is the probability of a 1 across cases. As the data provide us with \\(z\\) and \\(n\\), we end up estimating \\(p\\). If we’re willing to use what’s called a link function, we can estimate \\(p\\) with a linear model with any number of predictors. When fitting binomial regression models, you can take your choice among several link functions, the most popular of which is the logit. This will be our approach. As you may have guesses, using the logit link to fit a binomial model is often termed logistic regression. Welcome to the generalized linear model. Let’s fire up brms. library(brms) Before we fit the model, it will make our lives easier if we redefine period as a factor and rename our event = 1 column as event. We defined period as a factor because we want to fit a model with discrete time. Renaming event = 1 column as event just makes it easier on the brm() function to read the variable. teachers_lt &lt;- teachers_lt %&gt;% mutate(period = factor(period), event = `event = 1`) With this formulation, event is our \\(z\\) term and total is our \\(n\\) term. We’re estimating \\(p\\). Behold the mode syntax. fit10.6 &lt;- brm(data = teachers_lt, family = binomial, event | trials(total) ~ 0 + period, prior(normal(0, 4), class = b), chains = 4, cores = 1, iter = 2000, warmup = 1000, seed = 10, file = &quot;fits/fit10.06&quot;) Check the summary. print(fit10.6) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + period ## Data: teachers_lt (Number of observations: 12) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## period1 -2.04 0.05 -2.13 -1.94 1.00 9137 2606 ## period2 -2.09 0.05 -2.20 -1.98 1.00 8729 2940 ## period3 -2.03 0.06 -2.15 -1.92 1.00 7900 2844 ## period4 -2.12 0.06 -2.24 -2.00 1.00 8786 3296 ## period5 -2.33 0.07 -2.47 -2.19 1.00 10265 3263 ## period6 -2.41 0.07 -2.55 -2.26 1.00 8986 2771 ## period7 -2.75 0.09 -2.94 -2.57 1.00 8836 2904 ## period8 -2.99 0.11 -3.22 -2.77 1.00 7321 3070 ## period9 -3.13 0.14 -3.41 -2.86 1.00 8581 2945 ## period10 -3.27 0.18 -3.64 -2.94 1.00 9343 3194 ## period11 -3.68 0.26 -4.20 -3.23 1.00 8858 2801 ## period12 -4.38 0.44 -5.34 -3.61 1.00 7155 2713 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because we used the 0 + Intercept syntax in the presence of a factor predictor, period, we suppressed the default intercept. Instead, we have separate “intercepts” for each of the 12 levels period. Because we used the conventional logit link, the parameters are all on the log-odds scale. Happily, we can use the brms::inv_logit_scaled() function to convert them back into the probability metric. Here’s a quick and dirty conversion using the output from fixef(). fixef(fit10.6) %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## period1 0.11556681 0.5127071 0.105893972 0.12577169 ## period2 0.11016816 0.5137191 0.099802929 0.12091456 ## period3 0.11565147 0.5141849 0.104605310 0.12776431 ## period4 0.10751864 0.5149124 0.096579461 0.11886693 ## period5 0.08900044 0.5175769 0.078091779 0.10030044 ## period6 0.08249433 0.5185773 0.072114919 0.09410460 ## period7 0.06007781 0.5235268 0.050259573 0.07110459 ## period8 0.04789439 0.5286891 0.038575357 0.05903044 ## period9 0.04201526 0.5356367 0.031835698 0.05406505 ## period10 0.03650190 0.5439742 0.025697642 0.05032226 ## period11 0.02448447 0.5638504 0.014804677 0.03823187 ## period12 0.01232412 0.6089401 0.004763989 0.02642533 Compare these Estimate values with the values in the “Estimated hazard probability” column from Table 10.2 in the text (p. 349). They are very close. We can go further and look at these hazard estimates in a plot. We’ll use the tidybayes::stat_lineribbon() function to plot their posterior means atop their 50% and 95% intervals. library(tidybayes) posterior_samples(fit10.6) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(1:12) %&gt;% pivot_longer(everything(), names_to = &quot;period&quot;, values_to = &quot;hazard&quot;) %&gt;% mutate(period = period %&gt;% as.double()) %&gt;% ggplot(aes(x = period, y = hazard)) + stat_lineribbon(.width = c(.5, .95), size = 1/3) + # add the hazard estimates from `survival::survfit()` geom_point(data = tibble(period = fit10.1$time, hazard = fit10.1$n.event / fit10.1$n.risk), aes(y = hazard), size = 2, color = &quot;violetred1&quot;) + scale_fill_manual(&quot;CI&quot;, values = c(&quot;grey75&quot;, &quot;grey60&quot;)) + scale_x_continuous(breaks = 1:12) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.925, .825), panel.grid = element_blank()) For comparison sake, those violet dots in the foreground are the hazard estimates from the frequentist survival::survfit() function. Turns out our Bayesian results are very similar to the frequentist ones. Hopefully this isn’t a surprise. There was a lot of data and we used fairly weak priors. For simple models under those conditions, frequentist and Bayesian results should be pretty close. Remember that \\(se \\big (\\hat h(t_j) \\big)\\) formula from back in section 10.4.1? That doesn’t quite apply to the posterior standard deviations from our Bayesian model. Even so, our posterior \\(SD\\)s will be very similar to the ML standard errors. Let’s compare those in a plot, too. posterior_samples(fit10.6) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(1:12) %&gt;% pivot_longer(everything(), names_to = &quot;period&quot;, values_to = &quot;hazard&quot;) %&gt;% mutate(period = period %&gt;% as.double()) %&gt;% group_by(period) %&gt;% summarise(sd = sd(hazard)) %&gt;% bind_cols(se_h_hat %&gt;% select(se_h_hat)) %&gt;% pivot_longer(-period) %&gt;% mutate(name = factor(name, levels = c(&quot;sd&quot;, &quot;se_h_hat&quot;), labels = c(&quot;Bayesian&quot;, &quot;ML&quot;))) %&gt;% ggplot(aes(x = period, y = value, color = name)) + geom_point(size = 3, position = position_dodge(width = .25)) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .55) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(expression(italic(se)), limits = c(0, 0.01)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.09, .9), panel.grid = element_blank()) Man those are close. We can also use our brms output to depict the survivor function. On page 337 in the text (Equation 10.5), Singer and Willett demonstrated how to define the survivor function in terms of the hazard function. It follows the form \\[\\hat S (t_j) = [1 - \\hat h (t_j)][1 - \\hat h (t_{j - 1})][1 - \\hat h (t_{j - 2})]...[1 - \\hat h (t_1)].\\] In words, “each year’s estimated survival probability is the successive product of the complement of the estimated hazard function probabilities across this and all previous years” (p. 337, emphasis in the original). Here’s how you might do that with the output from posterior_samples(). post &lt;- posterior_samples(fit10.6) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% # transform the hazards from the log-odds metric to probabilities mutate_all(inv_logit_scaled) %&gt;% set_names(str_c(&quot;h&quot;, 1:12)) %&gt;% # take the &quot;complement&quot; of each hazard mutate_all(~1 - .) %&gt;% # apply Equation 10.5 transmute(s0 = 1, s1 = h1, s2 = h1 * h2, s3 = h1 * h2 * h3, s4 = h1 * h2 * h3 * h4, s5 = h1 * h2 * h3 * h4 * h5, s6 = h1 * h2 * h3 * h4 * h5 * h6, s7 = h1 * h2 * h3 * h4 * h5 * h6 * h7, s8 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8, s9 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9, s10 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10, s11 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10 * h11, s12 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10 * h11 * h12) glimpse(post) ## Rows: 4,000 ## Columns: 13 ## $ s0 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ s1 &lt;dbl&gt; 0.8759833, 0.8928610, 0.8770696, 0.8916896, 0.8940710, 0.8768920, 0.8790859, 0.888935… ## $ s2 &lt;dbl&gt; 0.7784079, 0.7997735, 0.7751955, 0.7981630, 0.7986837, 0.7753155, 0.7782731, 0.794978… ## $ s3 &lt;dbl&gt; 0.6892693, 0.7111279, 0.6814154, 0.7090891, 0.7105205, 0.6878702, 0.6898066, 0.702378… ## $ s4 &lt;dbl&gt; 0.6192489, 0.6372226, 0.6052336, 0.6377087, 0.6423957, 0.6073570, 0.6227674, 0.615946… ## $ s5 &lt;dbl&gt; 0.5624273, 0.5825095, 0.5481830, 0.5836618, 0.5857847, 0.5510695, 0.5659978, 0.562662… ## $ s6 &lt;dbl&gt; 0.5159581, 0.5327154, 0.5010668, 0.5375498, 0.5363627, 0.5098096, 0.5224954, 0.512750… ## $ s7 &lt;dbl&gt; 0.4844501, 0.5033074, 0.4703992, 0.5058764, 0.5010981, 0.4804348, 0.4910555, 0.481777… ## $ s8 &lt;dbl&gt; 0.4609974, 0.4809437, 0.4504937, 0.4836799, 0.4799726, 0.4569260, 0.4711993, 0.453072… ## $ s9 &lt;dbl&gt; 0.4444808, 0.4633637, 0.4294656, 0.4651418, 0.4634998, 0.4336775, 0.4475742, 0.437006… ## $ s10 &lt;dbl&gt; 0.4292414, 0.4430966, 0.4179325, 0.4418124, 0.4437674, 0.4195906, 0.4227990, 0.426975… ## $ s11 &lt;dbl&gt; 0.4141679, 0.4317032, 0.4061066, 0.4321481, 0.4326038, 0.4119427, 0.4132678, 0.415482… ## $ s12 &lt;dbl&gt; 0.4113961, 0.4185036, 0.4043163, 0.4190495, 0.4252176, 0.4072654, 0.4037310, 0.412529… We’ll learn how to simplify that syntax with help from the cumprod() function in the next chapter. Now we have our survival estimates, we can make our Bayesian version of the lower panel of Figure 10.1. # this will help us depict the median lifetime line &lt;- tibble(period = c(0, iml, iml), survival = c(.5, .5, 0)) # wrangle post %&gt;% pivot_longer(everything(), values_to = &quot;survival&quot;) %&gt;% mutate(period = str_remove(name, &quot;s&quot;) %&gt;% as.double()) %&gt;% # plot! ggplot(aes(x = period, y = survival)) + geom_path(data = line, color = &quot;white&quot;, linetype = 2) + stat_lineribbon(.width = .95, size = 1/3, fill = &quot;grey75&quot;) + # add the survival estimates from `survival::survfit()` geom_point(data = tibble(period = fit10.1$time, survival = fit10.1$surv), size = 2, color = &quot;violetred1&quot;) + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated survival probability, &quot;*hat(italic(S))(italic(t[j]))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) Like we did with our hazard plot above, we superimposed the frequentist estimates as violet dots atop the Bayesian posterior means and intervals. Because of how narrow the posteriors were, we only showed the 95% intervals, here. As with the hazard estimates, our Bayesian survival estimates are very close to the ones we computed with ML. Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3 brms_2.12.0 Rcpp_1.0.4.6 patchwork_1.0.0 survival_3.1-12 forcats_0.5.0 ## [7] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_3.0.0 ## [13] ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 ## [9] farver_2.0.3 rstan_2.19.3 svUnit_0.7-12 DT_0.13 ## [13] fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [17] bridgesampling_1.0-0 splines_3.6.3 knitr_1.28 shinythemes_1.1.2 ## [21] bayesplot_1.7.1 jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 ## [25] shiny_1.4.0.2 compiler_3.6.3 httr_1.4.1 backports_1.1.6 ## [29] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 ## [33] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 ## [37] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 glue_1.4.0 ## [41] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.2.4 nlme_3.1-144 ## [45] crosstalk_1.1.0.1 xfun_0.13 ps_1.3.2 rvest_0.3.5 ## [49] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [53] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [57] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 ## [65] loo_2.2.0 stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [69] rlang_0.4.5 pkgconfig_2.0.3 matrixStats_0.56.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [77] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 magrittr_1.5 ## [81] bookdown_0.18 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.3 haven_2.2.0 withr_2.1.2 xts_0.12-0 ## [89] abind_1.4-5 modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 ## [93] utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 digest_0.6.25 ## [101] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [105] viridisLite_0.3.0 shinyjs_1.1 "],
["fitting-basic-discrete-time-hazard-models.html", "11 Fitting Basic Discrete-Time Hazard Models 11.1 Toward a statistical model for discrete-time hazard 11.2 formal representation of the population discrete-time hazard model 11.3 Fitting a discrete-time hazard model to data 11.4 Interpreting parameter estimates 11.5 Displaying fitted hazard and survivor functions 11.6 Comparing models using deviance statistics and information criteria 11.7 Statistical inference using [uncertainty in the Bayesian posterior] Reference Session info Footnote", " 11 Fitting Basic Discrete-Time Hazard Models In this chapter and the next, we present statistical models of hazard for data collected in discrete time. The relative simplicity of these models makes them an ideal entrée into the world of survival analysis. In subsequent chapters, we extend these basic ideas to situations in which event occurrence is recorded in continuous time. Good data analysis involves more than using a computer package to fit a statistical model to data. To conduce a credible discrete-time survival analysis, you must: (1) specify a suitable model for hazard and understand its assumptions; (2) use sample data to estimate the model parameters; (3) interpret results in terms of your research questions; (4) evaluate model fit and [express the uncertainty in the] model parameters; and (5) communicate your findings. (pp. 357–358) 11.1 Toward a statistical model for discrete-time hazard Time to load Capaldi, Crosby, and Stoolmiller’s (1996) firstsex.csv data. library(tidyverse) sex &lt;- read_csv(&quot;data/firstsex.csv&quot;) glimpse(sex) ## Rows: 180 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28,… ## $ time &lt;dbl&gt; 9, 12, 12, 12, 11, 9, 12, 11, 12, 11, 12, 11, 9, 12, 10, 12, 7, 12, 10, 12, 11, 12, 11, 12, … ## $ censor &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,… ## $ pt &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,… ## $ pas &lt;dbl&gt; 1.9788670, -0.5454916, -1.4049800, 0.9741806, -0.6356313, -0.2428857, -0.8685001, 0.4535947,… Here are the cases broken down by time and censor status. sex %&gt;% count(time, censor) ## # A tibble: 7 x 3 ## time censor n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7 0 15 ## 2 8 0 7 ## 3 9 0 24 ## 4 10 0 29 ## 5 11 0 25 ## 6 12 0 26 ## 7 12 1 54 Since these data show no censoring before the final time point, it is straightforward to follow along with the text (p. 358) and compute the percent who had already had sex by the 12th grade. sex %&gt;% count(censor) %&gt;% mutate(percent = 100 * (n / sum(n))) ## # A tibble: 2 x 3 ## censor n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 126 70 ## 2 1 54 30 Here we break the data down by our central predictor, pt, which is coded “0 for boys who lived with both biological parents” and “1 for boys who experienced one or more parenting transitions” before the 7th grade. sex %&gt;% count(pt) %&gt;% mutate(percent = 100 * (n / sum(n))) ## # A tibble: 2 x 3 ## pt n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 72 40 ## 2 1 108 60 11.1.1 Plots of within-group hazard functions and survivor functions. Plots of sample hazard functions and survivor functions estimates separately for groups distinguished by their predictor values are invaluable exploratory tools. If a predictor is categorical, like PT, construction of these displays is straightforward. If a predictor is continuous, you should just temporarily categorize its values for plotting purposes. (pp. 358–359, emphasis in the original) To make our version of the descriptive plots in Figure 11.1, we’ll need to first load the survival package. library(survival) fit11.1 will be of the cases for which pt == 0 and fit11.2 will be of the cases for which pt == 1. With fit11.3, we use all cases regardless of pt status. fit11.1 &lt;- survfit(data = sex %&gt;% filter(pt == 0), Surv(time, 1 - censor) ~ 1) fit11.2 &lt;- survfit(data = sex %&gt;% filter(pt == 1), Surv(time, 1 - censor) ~ 1) fit11.3 &lt;- survfit(data = sex, Surv(time, 1 - censor) ~ 1) Before we plot the results, it might be handy to arrange the fit results in life tables. We can streamline that code with the custom make_lt() function from last chapter. make_lt &lt;- function(fit) { # arrange the lt data for all rows but the first most_rows &lt;- tibble(time = fit$time) %&gt;% mutate(time_int = str_c(&quot;[&quot;, time, &quot;, &quot;, time + 1, &quot;)&quot;), n_risk = fit$n.risk, n_event = fit$n.event) %&gt;% mutate(n_censored = n_risk - n_event - lead(n_risk, default = 0), hazard_fun = n_event / n_risk, survivor_fun = fit$surv) # define the values for t = 2 and t = 1 time_1 &lt;- fit$time[1] time_0 &lt;- time_1 - 1 # define the values for the row for which t = 1 row_1 &lt;- tibble(time = time_0, time_int = str_c(&quot;[&quot;, time_0, &quot;, &quot;, time_1, &quot;)&quot;), n_risk = fit$n.risk[1], n_event = NA, n_censored = NA, hazard_fun = NA, survivor_fun = 1) # make the full life table lt &lt;- bind_rows(row_1, most_rows) lt } We’ll use make_lt() separately for each fit, stack the results from the first on top of those from the second, and add a pt column to index the rows. This will be our version of Table 11.1 (p. 360). lt &lt;- bind_rows(make_lt(fit11.1), make_lt(fit11.2), make_lt(fit11.3)) %&gt;% mutate(pt = factor(rep(c(&quot;pt = 0&quot;, &quot;pt = 1&quot;, &quot;overall&quot;), each = n() / 3))) %&gt;% select(pt, everything()) lt ## # A tibble: 21 x 8 ## pt time time_int n_risk n_event n_censored hazard_fun survivor_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 6 [6, 7) 72 NA NA NA 1 ## 2 pt = 0 7 [7, 8) 72 2 0 0.0278 0.972 ## 3 pt = 0 8 [8, 9) 70 2 0 0.0286 0.944 ## 4 pt = 0 9 [9, 10) 68 8 0 0.118 0.833 ## 5 pt = 0 10 [10, 11) 60 8 0 0.133 0.722 ## 6 pt = 0 11 [11, 12) 52 10 0 0.192 0.583 ## 7 pt = 0 12 [12, 13) 42 8 34 0.190 0.472 ## 8 pt = 1 6 [6, 7) 108 NA NA NA 1 ## 9 pt = 1 7 [7, 8) 108 13 0 0.120 0.880 ## 10 pt = 1 8 [8, 9) 95 5 0 0.0526 0.833 ## # … with 11 more rows Here is the code for the top panel of Figure 11.1. p1 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = hazard_fun, color = pt, group = pt)) + geom_line() + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .5) + scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) + scale_y_continuous(&quot;estimated hazard probability&quot;, limits = c(0, .5)) + theme(panel.grid = element_blank()) Now make the plot for the bottom panel. p2 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = survivor_fun, color = pt, group = pt)) + geom_hline(yintercept = .5, color = &quot;white&quot;, linetype = 2) + geom_line() + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .5) + scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) + scale_y_continuous(&quot;estimated survival probability&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) Combine the two ggplot2 objects with patchwork syntax to make our version of Figure 11.1. library(patchwork) (p1 / p2) + plot_layout(guides = &quot;collect&quot;) On page 361, Singer and Willett compared the hazard probabilities at grades 8 and 11 for boys in the two pt groups. We can make that comparison with filter(). lt %&gt;% filter(time %in% c(8, 11) &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, hazard_fun) ## # A tibble: 4 x 3 ## pt time hazard_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 8 0.0286 ## 2 pt = 0 11 0.192 ## 3 pt = 1 8 0.0526 ## 4 pt = 1 11 0.283 Compare the two groups on the hazard probabilities at grade 9. lt %&gt;% filter(time == 9 &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, hazard_fun) ## # A tibble: 2 x 3 ## pt time hazard_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 9 0.118 ## 2 pt = 1 9 0.178 Now compare them on their hazard probabilities in grade 12. lt %&gt;% filter(time == 12 &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, hazard_fun) ## # A tibble: 2 x 3 ## pt time hazard_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 12 0.190 ## 2 pt = 1 12 0.474 At the top of page 362, Singer and Willett compared the percentages of boys who were virgins at grades 9 and 12, by pt status. Those percentages are straight algebraic transformations of the corresponding survival function values. lt %&gt;% filter(time %in% c(9, 12) &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, survivor_fun) %&gt;% mutate(percent_virgins = (100 * survivor_fun) %&gt;% round(digits = 1)) ## # A tibble: 4 x 4 ## pt time survivor_fun percent_virgins ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 9 0.833 83.3 ## 2 pt = 0 12 0.472 47.2 ## 3 pt = 1 9 0.685 68.5 ## 4 pt = 1 12 0.185 18.5 Now let’s finish off by computing the interpolated median lifetime values for each with our custom make_iml() function. make_iml &lt;- function(lt) { # lt is a generic name for a life table of the # kind we made with our `make_lt()` function # determine the mth row lt_m &lt;- lt %&gt;% filter(survivor_fun &gt; .5) %&gt;% slice(n()) # determine the row for m + 1 lt_m1 &lt;- lt %&gt;% filter(survivor_fun &lt; .5) %&gt;% slice(1) # pull the value for m m &lt;- pull(lt_m, time) # pull the two survival function values stm &lt;- pull(lt_m, survivor_fun) stm1 &lt;- pull(lt_m1, survivor_fun) # plug the values into Equation 10.6 (page 338) iml &lt;- m + ((stm - .5) / (stm - stm1)) * ((m + 1) - m) iml } Put make_iml() to work. make_iml(lt %&gt;% filter(pt == &quot;pt = 0&quot;)) ## [1] 11.75 make_iml(lt %&gt;% filter(pt == &quot;pt = 1&quot;)) ## [1] 9.952381 11.1.2 What kind of statistical model do these graphs suggest? To postulate a statistical model to represent the relationship between the population discrete-time hazard function and predictors, we must deal with two complications apparent in these displays. One is that any hypothesized model must describe the shape of the entire discrete-time hazard function over time, not just its value in any one period, in much the same way that a multilevel model for change characterizes the shape of entire individual growth trajectories over time. A second complication is that, as a conditional probability, the value of discrete-time hazard must lie between 0 and 1. Any reasonable statistical model for hazard must recognize this constraint, precluding the occurrence of theoretically impossible values. (p. 362, emphasis in the original) 11.1.2.1 The bounded nature of hazard. A conventional way to handle the bounded nature of probabilities is transform the scale of the data. Cox (1972) recommended either the odds and log-odds (i.e., logit) transformations. Given a probability, \\(p\\), we compute the odds as \\[\\text{odds} = \\frac{p}{1 - p}.\\] Log-odds is a minor extension; you simply take the log of the odds, which we can formally express as \\[\\text{log-odds} = \\log \\Bigg (\\frac{p}{1 - p} \\Bigg ).\\] To make the conversions easy, we’ll define1 a couple convenience functions: odds() and log_odds(). odds &lt;- function(p) { p / (1 - p) } log_odds &lt;- function(p) { log(p / (1 - p)) } Here’s how they work. tibble(p = seq(from = 0, to = 1, by = .1)) %&gt;% mutate(odds = odds(p), log_odds = log_odds(p)) ## # A tibble: 11 x 3 ## p odds log_odds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 -Inf ## 2 0.1 0.111 -2.20 ## 3 0.2 0.25 -1.39 ## 4 0.3 0.429 -0.847 ## 5 0.4 0.667 -0.405 ## 6 0.5 1 0 ## 7 0.6 1.5 0.405 ## 8 0.7 2.33 0.847 ## 9 0.8 4. 1.39 ## 10 0.9 9. 2.20 ## 11 1 Inf Inf Before we make our version of Figure 11.2, it might be instructive to see how odds and log-odds compare to probabilities in a plot. Here we’ll compare them to probabilities ranging from .01 to .99. tibble(p = seq(from = .01, to = .99, by = .01)) %&gt;% mutate(odds = odds(p), `log(odds)` = log_odds(p)) %&gt;% pivot_longer(-p) %&gt;% mutate(name = factor(name, levels = c(&quot;odds&quot;, &quot;log(odds)&quot;))) %&gt;% ggplot(aes(x = p, y = value)) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;transformed scale&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Odds are bounded to values of zero and above and have an inflection at 1. Log-odds are unbounded and have an inflection point at 0. Here we’ll save the odds and log-odds for our hazard functions within the lt life table. lt &lt;- lt %&gt;% mutate(odds = odds(hazard_fun), log_odds = log_odds(hazard_fun)) lt ## # A tibble: 21 x 10 ## pt time time_int n_risk n_event n_censored hazard_fun survivor_fun odds log_odds ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 6 [6, 7) 72 NA NA NA 1 NA NA ## 2 pt = 0 7 [7, 8) 72 2 0 0.0278 0.972 0.0286 -3.56 ## 3 pt = 0 8 [8, 9) 70 2 0 0.0286 0.944 0.0294 -3.53 ## 4 pt = 0 9 [9, 10) 68 8 0 0.118 0.833 0.133 -2.01 ## 5 pt = 0 10 [10, 11) 60 8 0 0.133 0.722 0.154 -1.87 ## 6 pt = 0 11 [11, 12) 52 10 0 0.192 0.583 0.238 -1.44 ## 7 pt = 0 12 [12, 13) 42 8 34 0.190 0.472 0.235 -1.45 ## 8 pt = 1 6 [6, 7) 108 NA NA NA 1 NA NA ## 9 pt = 1 7 [7, 8) 108 13 0 0.120 0.880 0.137 -1.99 ## 10 pt = 1 8 [8, 9) 95 5 0 0.0526 0.833 0.0556 -2.89 ## # … with 11 more rows We’re ready to make and combine the subplots for our version of Figure 11.2. # hazard p1 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = hazard_fun, color = pt, group = pt)) + geom_line() + scale_y_continuous(NULL, breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;Estimated hazard&quot;) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # odds p2 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = odds, color = pt, group = pt)) + geom_line() + scale_y_continuous(NULL, breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;Estimated odds&quot;) + theme(legend.position = &quot;none&quot;) # log-odds p3 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = log_odds, color = pt, group = pt)) + geom_line() + scale_y_continuous(NULL, limits = c(-4, 0)) + labs(subtitle = &quot;Estimated logit(hazard)&quot;) + theme(legend.position = &quot;none&quot;) (p1 / p2 / p3 ) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .5) &amp; scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) 11.1.2.2 What statistical model could have generated these sample data? With the survival models from the prior sections, we were lazy and just used the survival package. But recall from the end of the last chapter that we can fit the analogous models brms using the binomial likelihood. This subsection is a great place to practice those some more. The fitted lines Singer and Willett displayed in Figure 11.3 can all be reproduced with binomial regression. However, the sex data are not in a convenient form to fit those models. Just like we did in last chapter, we’ll want to take a two-step process whereby we first convert the data to the long (i.e., person-period) format and then summarize. Happily, we can accomplish that first step by uploading the data in the firstsex_pp.csv file, which are already in the long format. sex_pp &lt;- read_csv(&quot;data/firstsex_pp.csv&quot;) glimpse(sex_pp) ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9,… ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, … ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,… ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,… ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916,… Now we’ll compute the desired summary values and wrangle a bit. sex_aggregated &lt;- sex_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(period) %&gt;% count(event, pt) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(total = event + no_event, period_center = period - mean(period), peroid_factor = factor(period), pt = factor(pt)) sex_aggregated ## # A tibble: 12 x 7 ## period pt event no_event total period_center peroid_factor ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 0 2 70 72 -2.5 7 ## 2 7 1 13 95 108 -2.5 7 ## 3 8 0 2 68 70 -1.5 8 ## 4 8 1 5 90 95 -1.5 8 ## 5 9 0 8 60 68 -0.5 9 ## 6 9 1 16 74 90 -0.5 9 ## 7 10 0 8 52 60 0.5 10 ## 8 10 1 21 53 74 0.5 10 ## 9 11 0 10 42 52 1.5 11 ## 10 11 1 15 38 53 1.5 11 ## 11 12 0 8 34 42 2.5 12 ## 12 12 1 18 20 38 2.5 12 Note how we saved the grade values in three columns: period has them as continuous values, which will be hand for plotting; period_center has them as mean-centered continuous values, which will make fitting the linear model in the middle panel easier; and period_factor has them saved as a factor, which will help us fit the model in the bottom panel. Fire up brms. library(brms) Before we fit the models, it might be good to acknowledge we’re jumping ahead of the authors, a bit. Singer and Willett didn’t discuss fitting discrete time hazard models until section 11.3.2. Sure, their focus was on the frequentist approach using maximum likelihood. But the point still stands. If these model fitting details feel a bit rushed, they are. Any anxious feelings aside, now fit the three binomial models. We continue to use weakly-regularizing priors for each. # top panel fit11.4 &lt;- brm(data = sex_aggregated, family = binomial, event | trials(total) ~ 0 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.04&quot;) # middle panel fit11.5 &lt;- brm(data = sex_aggregated, family = binomial, event | trials(total) ~ 0 + pt + period_center, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.05&quot;) # bottom panel fit11.6 &lt;- brm(data = sex_aggregated, family = binomial, event | trials(total) ~ 0 + pt + peroid_factor, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.06&quot;) Check the model summaries. print(fit11.4) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + pt ## Data: sex_aggregated (Number of observations: 12) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## pt0 -2.16 0.17 -2.50 -1.83 1.00 3006 2359 ## pt1 -1.44 0.12 -1.67 -1.20 1.00 3569 2236 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.5) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + pt + period_center ## Data: sex_aggregated (Number of observations: 12) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## pt0 -2.23 0.18 -2.60 -1.88 1.00 3375 3009 ## pt1 -1.35 0.12 -1.60 -1.11 1.00 3695 3007 ## period_center 0.43 0.07 0.31 0.56 1.00 3267 2960 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.6) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + pt + peroid_factor ## Data: sex_aggregated (Number of observations: 12) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## pt0 -2.98 0.30 -3.61 -2.40 1.01 1115 1570 ## pt1 -2.10 0.26 -2.64 -1.62 1.01 967 1416 ## peroid_factor8 -0.80 0.47 -1.78 0.13 1.01 1579 2551 ## peroid_factor9 0.68 0.34 0.01 1.36 1.01 1245 2005 ## peroid_factor10 1.13 0.33 0.46 1.80 1.01 1199 1433 ## peroid_factor11 1.30 0.35 0.61 1.99 1.01 1239 1640 ## peroid_factor12 1.78 0.36 1.10 2.48 1.01 1177 1631 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can extract the fitted values and their summaries for each row in the data with fitted(). To get them in the log-odds metric, we need to set scale = &quot;linear&quot;. Here’s a quick example with fit11.4. fitted(fit11.4, scale = &quot;linear&quot;) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] -2.155601 0.1690484 -2.498164 -1.833945 ## [2,] -1.436847 0.1176147 -1.673421 -1.204534 ## [3,] -2.155601 0.1690484 -2.498164 -1.833945 ## [4,] -1.436847 0.1176147 -1.673421 -1.204534 ## [5,] -2.155601 0.1690484 -2.498164 -1.833945 ## [6,] -1.436847 0.1176147 -1.673421 -1.204534 ## [7,] -2.155601 0.1690484 -2.498164 -1.833945 ## [8,] -1.436847 0.1176147 -1.673421 -1.204534 ## [9,] -2.155601 0.1690484 -2.498164 -1.833945 ## [10,] -1.436847 0.1176147 -1.673421 -1.204534 ## [11,] -2.155601 0.1690484 -2.498164 -1.833945 ## [12,] -1.436847 0.1176147 -1.673421 -1.204534 If we convert that output to a data frame, tack on the original data values, and wrangle a bit, we’ll be in good shape to make the top panel of Figure 11.3. Below we’ll do that for each of the three panels, saving them as p1, p2, and p3. # logit(hazard) is horizontal with time p1 &lt;- fitted(fit11.4, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate), alpha = 1/2) + geom_point(aes(y = log_odds(event / total))) + scale_y_continuous(NULL, limits = c(-4, 0)) + labs(subtitle = &quot;logit(hazard) is horizontal with time&quot;) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # logit(hazard) is linear with time p2 &lt;- fitted(fit11.5, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate), alpha = 1/2) + geom_point(aes(y = log_odds(event / total))) + labs(subtitle = &quot;logit(hazard) is linear with time&quot;, y = &quot;logit(hazard)&quot;) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.position = &quot;none&quot;) # logit(hazard) is completely general with time p3 &lt;- fitted(fit11.6, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate), alpha = 1/2) + geom_point(aes(y = log_odds(event / total))) + labs(subtitle = &quot;logit(hazard) is completely general with time&quot;, y = NULL) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.position = &quot;none&quot;) Now combine the plots with patchwork syntax. (p1 / p2 / p3) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) In addition to the posterior means (i.e., our analogues to the fitted values in the text), we added the 95% Bayesian intervals to give a better sense of the uncertainty in each model. Singer and Willet mused the unconstrained model (fit6) was a better fit to the data than the other two. We can quantify that with a LOO comparison. fit11.4 &lt;- add_criterion(fit11.4, &quot;loo&quot;) fit11.5 &lt;- add_criterion(fit11.5, &quot;loo&quot;) fit11.6 &lt;- add_criterion(fit11.6, &quot;loo&quot;) loo_compare(fit11.4, fit11.5, fit11.6) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit11.6 0.0 0.0 -31.6 1.8 5.1 0.7 63.2 3.5 ## fit11.5 -0.6 3.3 -32.3 3.2 3.5 1.2 64.5 6.4 ## fit11.4 -27.5 10.3 -59.2 10.3 9.1 2.5 118.3 20.7 Here are the LOO weights. model_weights(fit11.4, fit11.5, fit11.6, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit11.4 fit11.5 fit11.6 ## 0.000 0.344 0.656 11.2 formal representation of the population discrete-time hazard model Earlier equations for the hazard function omitted substantive predictors. Now consider the case where \\(X_{1ij}, X_{2ij}, . . ., X_{Pij}\\) stand for the \\(P\\) predictors which may or may not vary across individuals \\(i\\) and time periods \\(j\\). Thus \\(x_{pij}\\) is the value for the \\(i^\\text{th}\\) individual on the \\(p^\\text{th}\\) variable during the \\(j^\\text{th}\\) period. We can use this to define the conditional hazard function as \\[h(t_{ij}) = \\text{Pr} [T_i = j | T \\geq j \\text{ and } X_{1ij} = x_{1ij}, X_{2ij} = x_{2ij}, ..., X_{Pij} = x_{pij}].\\] Building further and keeping the baseline shape of the discrete hazard function flexible, we want a method that allows each of the \\(j\\) time periods to have its own value. Imagine a set of \\(J\\) dummy variables, \\(D_1, D_2, ..., D_J\\), marking off each of the time periods. For example, say \\(J = 6\\), we could depict this in a tibble like so. tibble(period = 1:6) %&gt;% mutate(d1 = if_else(period == 1, 1, 0), d2 = if_else(period == 2, 1, 0), d3 = if_else(period == 3, 1, 0), d4 = if_else(period == 4, 1, 0), d5 = if_else(period == 5, 1, 0), d6 = if_else(period == 6, 1, 0)) ## # A tibble: 6 x 7 ## period d1 d2 d3 d4 d5 d6 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 0 0 ## 2 2 0 1 0 0 0 0 ## 3 3 0 0 1 0 0 0 ## 4 4 0 0 0 1 0 0 ## 5 5 0 0 0 0 1 0 ## 6 6 0 0 0 0 0 1 If we were to use a set of dummies of this kind in a model, we would omit the conventional regression intercept, replacing it with the \\(J\\) dummies. Now presume we’re fitting a hazard model using the logit link, \\(\\operatorname{logit} h(t_{ij})\\). We can express the discrete conditional hazard model with a general functional form with respect to time as \\[\\operatorname{logit} h(t_{ij}) = [\\alpha_1 D_{1ij} + \\alpha_2 D_{2ij} + \\cdots + \\alpha_J D_{Jij}] + [\\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\cdots + \\beta_P X_{Pij}],\\] where the \\(\\alpha\\) parameters are the \\(J\\) time-period dummies and the \\(\\beta\\) parameters are for other time-varying or time-invariant predictors. This is just the type of model we used to fit fit116. For that model, the basic equation was \\[\\operatorname{logit} h(t_{ij}) = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ],\\] where the only substantive predictor was the time-invariant pt. However, that formula could be a little misleading. Recall the formula: fit11.6$formula ## event | trials(total) ~ 0 + pt + peroid_factor We suppressed the default regression intercept with the ~ 0 + syntax and the only two predictors were pt and peroid_factor. Both were saved as factor variables. Functionally, that’s why period_factor worked as \\(\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}\\), a series of 5 dummy variables with no reference category. The same basic thing goes for pt. Because pt was a factor used in a model formula with no conventional intercept, it acted as if it was a series of 2 dummy variables with no reference category. Thus, we might rewrite the model equation for fit6 as \\[\\operatorname{logit} h(t_{ij}) = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_0 \\text{PT}_{0i} + \\beta_1 \\text{PT}_{1i} ].\\] And since we’re practicing fitting these models as Bayesians, the fit6 equation with a fuller expression of the likelihood and the priors looks like \\[\\begin{align*} \\text{event}_{ij} &amp; = \\operatorname{Binomial}(n = \\text{trials}_{ij}, p_{ij}) \\\\ \\operatorname{logit} (p_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_0 \\text{PT}_{0i} + \\beta_1 \\text{PT}_{1i} ] \\\\ \\alpha_7, \\alpha_8, ..., \\alpha_{12} &amp; \\sim \\operatorname{Normal}(0, 4) \\\\ \\beta_0 \\text{ and } \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 4), \\end{align*}\\] where we’re describing the model in terms of the criterion, event, rather than in terms of \\(h(t_{ij})\\). And what is the criterion, event? It’s a vector of counts. The binomial likelihood allows us to model vectors of counts in terms of the number of trials, as indexed by our trials vector, and the (conditional) probability of a “1” in a given trial. In this context, \\(h(t_{ij}) = p_{ij}\\). 11.2.1 What do the parameters represent? Given our factor coding of pt, our two submodels for the equations in the last section are \\[\\begin{align*} \\text{when PT = 0: } \\operatorname{logit} h(t_j) &amp; = [\\alpha_7 D_7 + \\alpha_8 D_8 + \\cdots + \\alpha_{12} D_{12}] + \\beta_0 \\\\ \\text{when PT = 1: } \\operatorname{logit} h(t_j) &amp; = [\\alpha_7 D_7 + \\alpha_8 D_8 + \\cdots + \\alpha_{12} D_{12}] + \\beta_1, \\end{align*}\\] where we used Singer and Willett’s simplified notation and dropped all the \\(i\\) subscripts and most of the \\(j\\) subscripts. 11.2.2 An alternative representation of the model. In the previous sections, we expressed the model in terms of the logit of the criterion or the \\(p\\) parameter of the likelihood. Another strategy is the express the criterion (or \\(p\\)) in its natural metric and put the nonlinear portion on the right side of the equation. If we consider the generic discrete conditional hazard function, that would follow the form \\[h(t_{ij}) = \\frac{1}{1 + e^{-([\\alpha_1 D_{1ij} + \\alpha_2 D_{2ij} + \\cdots + \\alpha_J D_{Jij}] + [\\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\cdots + \\beta_P X_{Pij}])}}.\\] This is just a particular kind of logistic regression model. It also clarifies that “by specifying a linear relationship between predictors and logit hazard we imply a nonlinear relationship between predictors and raw hazard” (p. 377, emphasis in the original). We can explore what that might look like with our version of Figure 11.4. Here we continue to use fit6, but this time we’ll save the output from fitted() before plotting. f &lt;- fitted(fit11.6, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) f ## Estimate Est.Error Q2.5 Q97.5 period pt event no_event total period_center peroid_factor ## 1 -2.9823801 0.3027754 -3.6052434 -2.4029398 7 0 2 70 72 -2.5 7 ## 2 -2.1040306 0.2627053 -2.6400659 -1.6186357 7 1 13 95 108 -2.5 7 ## 3 -3.7780523 0.4274048 -4.6832736 -2.9802509 8 0 2 68 70 -1.5 8 ## 4 -2.8997028 0.3986271 -3.7552060 -2.1762555 8 1 5 90 95 -1.5 8 ## 5 -2.3039891 0.2752605 -2.8577347 -1.7842792 9 0 8 60 68 -0.5 9 ## 6 -1.4256396 0.2336338 -1.8961418 -0.9931028 9 1 16 74 90 -0.5 9 ## 7 -1.8484071 0.2612627 -2.3727600 -1.3615568 10 0 8 52 60 0.5 10 ## 8 -0.9700576 0.2258581 -1.4365800 -0.5480139 10 1 21 53 74 0.5 10 ## 9 -1.6845420 0.2727390 -2.2317108 -1.1589977 11 0 10 42 52 1.5 11 ## 10 -0.8061924 0.2488261 -1.3015364 -0.3373509 11 1 15 38 53 1.5 11 ## 11 -1.1987436 0.2818114 -1.7638193 -0.6609502 12 0 8 34 42 2.5 12 ## 12 -0.3203941 0.2668366 -0.8597507 0.1816277 12 1 18 20 38 2.5 12 Make the subplots. # logit(hazard) p1 &lt;- f %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;logit(hazard)&quot;, y = NULL) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # odds p2 &lt;- f %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = exp) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;odds&quot;, y = NULL) + coord_cartesian(ylim = c(0, .8)) + theme(legend.position = &quot;none&quot;) # hazard p3 &lt;- f %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = inv_logit_scaled) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;hazard (i.e., probability)&quot;, y = NULL) + coord_cartesian(ylim = c(0, .5)) + theme(legend.position = &quot;none&quot;) Combine the subplots with patchwork. (p1 / p2 / p3) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) The mutate_at() conversions we made for p2 and p3 were based on the guidelines in Table 11.2. Those were: tibble(`original scale` = c(&quot;logit&quot;, &quot;odds&quot;, &quot;logit&quot;), `desired scale` = c(&quot;odds&quot;, &quot;probability&quot;, &quot;probability&quot;), transformation = c(&quot;exp(logit)&quot;, &quot;odds / (1 + odds)&quot;, &quot;1 / (1 + exp(-1 * logit))&quot;)) %&gt;% knitr::kable() original scale desired scale transformation logit odds exp(logit) odds probability odds / (1 + odds) logit probability 1 / (1 + exp(-1 * logit)) We accomplished the transformation in the bottom row with the brms::inv_logit_scaled() function. 11.3 Fitting a discrete-time hazard model to data As Singer and Willett wrote, “with data collected on a random sample of individuals from a target population, you can easily fit a discrete-time hazard model, estimate its parameters using maximum likelihood methods, and evaluate goodness-of-fit” (pp. 378–379. As we’ve already demonstrated, you can fit them with Bayesian software, too. Though we’ll be focusing on brms, you might also want to check out the rstanarm package, about which you can learn more from Brilleman, Elci, Novik, and Wolfe’s preprint, Bayesian Survival Analysis Using the rstanarm R Package, Brilleman’s Estimating Survival (Time-to-Event) Models with rstanarm vignette, and the Survival models in rstanarm thread in the Stan forums. 11.3.1 Adding predictors to the person-period data set. At the beginning of section 11.1.2.2, we already loaded a version of the person-period data including the discrete-time dummies. It has our substantive predictors pt and pas, too. We saved it as sex_pp. Here’s a glimpse(). sex_pp %&gt;% glimpse() ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9,… ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, … ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,… ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,… ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916,… 11.3.2 Maximum likelihood estimates [and Bayesian posteriors] for the discrete-time hazard model. We’re not going to walk through all the foundational equations for the likelihood and log-likelihood functions (Equations 11.11 through 11.13). For our purposes, just note that “it turns out that the standard logistic regression routines widely available in all major statistical packages, when applied appropriately in the person-period data set, actually provide estimates of the parameters of the discrete-time hazard model” (p. 383, emphasis in the original). Happily, this is what we’ve been doing. Bayesian logistic regression via the binomial likelihood has been our approach. And since we’re Bayesians, the same caveat applies to survival models as applied to the other longitudinal models we fit in earlier chapters. We’re not just maximizing likelihoods, here. Bayes’s formula requires us to multiply the likelihood by the prior. \\[\\underbrace{p(\\theta | d)}_\\text{posterior} \\propto \\underbrace{p(d | \\theta)}_\\text{likelihood} \\; \\underbrace{p(\\theta)}_\\text{prior}\\] 11.3.3 Fitting the discrete-time hazard model to data. In one sense, fitting discrete-hazard models with Bayesian logistic regression is old hat, for us. We’ve been doing that since the end of last chapter. But one thing I haven’t clarified is, up to this point, we have been using the aggregated binomial format. To show what I mean, we might look at the data we used for our last model, fit11.6. sex_aggregated ## # A tibble: 12 x 7 ## period pt event no_event total period_center peroid_factor ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 0 2 70 72 -2.5 7 ## 2 7 1 13 95 108 -2.5 7 ## 3 8 0 2 68 70 -1.5 8 ## 4 8 1 5 90 95 -1.5 8 ## 5 9 0 8 60 68 -0.5 9 ## 6 9 1 16 74 90 -0.5 9 ## 7 10 0 8 52 60 0.5 10 ## 8 10 1 21 53 74 0.5 10 ## 9 11 0 10 42 52 1.5 11 ## 10 11 1 15 38 53 1.5 11 ## 11 12 0 8 34 42 2.5 12 ## 12 12 1 18 20 38 2.5 12 Now recall the formula for the binomial likelihood from the end of last chapter: \\[\\text{Pr} (z | n, p) = \\frac{n!}{z!(n - z)!} p^z (1 - p)^{n - z},\\] where \\(z\\) is the number of cases for which the value is 1, \\(n\\) is the total number of cases, and \\(p\\) is the constant chance of a 1 across cases. We refer to binomial data as aggregated with \\(n &gt; 1\\). Our \\(n\\) vector in the sex_aggregated, total, ranged from 38 to 108. Accordingly, our \\(z\\) vector, event, was always some value equal or lower to that in the same row for total. The person-period data, sex_pp, contain the same information but in a different format. Instead, each event cell only takes on a value of 0 or 1 (i.e., \\(n = 1\\)). If you were to sum up all the values in the total column of the sex_aggregated data, you’d return 822. sex_aggregated %&gt;% summarise(sum = sum(total)) ## # A tibble: 1 x 1 ## sum ## &lt;int&gt; ## 1 822 This is also the total number of rows in the sex_pp data. sex_pp %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 822 It’s also the case that when \\(n = 1\\), the right side of the equation for the binomial function reduces to \\[p^z (1 - p)^{1 - z}.\\] Whether you are working with aggregated or un-aggregated data, both are suited to fit logistic regression models with the binomial likelihood. Just specify the necessary information in the model syntax. For brms, the primary difference is how you use the trials() function. When we fit our logistic regression models using the aggregated data, we specified trials(total), which informed the brm() function what the \\(n\\) values were. In the case of unaggregated binomial data, we can just state trials(1). Each sell is the outcome \\(z\\) for a single trial. Here’s how to use that with Models A through D on page 385. # model a fit11.7 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.07&quot;) # model b fit11.8 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.08&quot;) # model c fit11.9 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pas, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.09&quot;) # model d fit11.10 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.10&quot;) 11.4 Interpreting parameter estimates Here are the model summaries in bulk. print(fit11.7) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 ## Data: sex_pp (Number of observations: 822) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.42 0.27 -2.98 -1.91 1.00 5256 2256 ## d8 -3.16 0.41 -4.05 -2.45 1.00 3795 1956 ## d9 -1.73 0.22 -2.19 -1.32 1.00 4750 2432 ## d10 -1.30 0.21 -1.71 -0.88 1.00 5076 2613 ## d11 -1.18 0.23 -1.64 -0.74 1.00 5062 2689 ## d12 -0.74 0.24 -1.23 -0.27 1.00 5070 2662 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.8) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt ## Data: sex_pp (Number of observations: 822) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -3.00 0.31 -3.62 -2.43 1.00 3358 2965 ## d8 -3.73 0.42 -4.60 -2.96 1.00 3911 2413 ## d9 -2.28 0.27 -2.82 -1.77 1.00 2981 3153 ## d10 -1.82 0.25 -2.33 -1.34 1.00 2803 3189 ## d11 -1.65 0.27 -2.18 -1.13 1.00 3017 2707 ## d12 -1.18 0.27 -1.71 -0.64 1.00 3388 2817 ## pt 0.86 0.21 0.44 1.27 1.00 1752 2304 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.9) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pas ## Data: sex_pp (Number of observations: 822) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.49 0.28 -3.06 -1.97 1.00 5613 3027 ## d8 -3.20 0.41 -4.08 -2.47 1.00 5041 2792 ## d9 -1.74 0.22 -2.18 -1.33 1.00 6378 2773 ## d10 -1.30 0.22 -1.75 -0.88 1.00 5745 2686 ## d11 -1.14 0.24 -1.61 -0.70 1.00 5150 3215 ## d12 -0.65 0.24 -1.15 -0.19 1.00 5412 3082 ## pas 0.44 0.11 0.22 0.67 1.00 6587 3449 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.10) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas ## Data: sex_pp (Number of observations: 822) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.90 0.31 -3.56 -2.30 1.00 2644 2387 ## d8 -3.61 0.43 -4.51 -2.83 1.00 3094 2761 ## d9 -2.15 0.28 -2.72 -1.63 1.00 2306 2394 ## d10 -1.69 0.26 -2.24 -1.18 1.00 2136 2649 ## d11 -1.51 0.27 -2.05 -0.99 1.00 2542 2813 ## d12 -1.00 0.28 -1.57 -0.46 1.00 2621 2732 ## pt 0.64 0.24 0.19 1.11 1.00 1433 1850 ## pas 0.30 0.12 0.06 0.54 1.00 3444 3031 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Although the text distinguishes between \\(\\alpha\\) and \\(\\beta\\) parameters (i.e., intercept and slope parameters, respectively), our brms output makes no such distinction. These are all of class = b, population-level \\(\\beta\\) parameters. When viewed in bulk, all those print() calls yield a lot of output. We can arrange the parameter summaries similar to those in Table 11.3 with help from broom::tidy(). library(broom) bind_rows(tidy(fit11.7) %&gt;% mutate(model = &quot;a&quot;), tidy(fit11.8) %&gt;% mutate(model = &quot;b&quot;), tidy(fit11.9) %&gt;% mutate(model = &quot;c&quot;), tidy(fit11.10) %&gt;% mutate(model = &quot;d&quot;)) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate(term = str_remove(term, &quot;b_&quot;), model = str_c(&quot;model &quot;, model), e_sd = str_c(round(estimate, digits = 2), &quot; (&quot;, round(std.error, digits = 2), &quot;)&quot;)) %&gt;% select(model, term, e_sd) %&gt;% pivot_wider(names_from = model, values_from = e_sd) %&gt;% knitr::kable() term model a model b model c model d d7 -2.42 (0.27) -3 (0.31) -2.49 (0.28) -2.9 (0.31) d8 -3.16 (0.41) -3.73 (0.42) -3.2 (0.41) -3.61 (0.43) d9 -1.73 (0.22) -2.28 (0.27) -1.74 (0.22) -2.15 (0.28) d10 -1.3 (0.21) -1.82 (0.25) -1.3 (0.22) -1.69 (0.26) d11 -1.18 (0.23) -1.65 (0.27) -1.14 (0.24) -1.51 (0.27) d12 -0.74 (0.24) -1.18 (0.27) -0.65 (0.24) -1 (0.28) pt NA 0.86 (0.21) NA 0.64 (0.24) pas NA NA 0.44 (0.11) 0.3 (0.12) 11.4.1 The time indicators. As a group, the \\(\\hat \\alpha\\)s are [Bayesian] estimates for the baseline logit hazard function. The amount and direction of variation in their values describe the shape of this function and tell us whether risk increases, decreases, or remains steady over time. (p. 387) A coefficient plot might help us get a sense of that across the four models. bind_rows(tidy(fit11.7) %&gt;% mutate(model = &quot;a&quot;), tidy(fit11.8) %&gt;% mutate(model = &quot;b&quot;), tidy(fit11.9) %&gt;% mutate(model = &quot;c&quot;), tidy(fit11.10) %&gt;% mutate(model = &quot;d&quot;)) %&gt;% filter(str_detect(term, &quot;d&quot;)) %&gt;% mutate(model = str_c(&quot;model &quot;, model), term = factor(str_remove(term, &quot;b_&quot;), levels = str_c(&quot;d&quot;, 12:7))) %&gt;% ggplot(aes(x = term, y = estimate, ymin = lower, ymax = upper)) + geom_pointrange(fatten = 2.5) + labs(x = NULL, y = &quot;posterior (log-odds scale)&quot;) + coord_flip() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~model, nrow = 1) “The fairly steady increase over time in the magnitude of the \\(\\hat \\alpha\\)s in each model [in the coefficient plot] shows that, in this sample of boys, the risk of first intercourse increases over time” (p. 387). When comparing the \\(\\hat \\alpha\\)s across models, it’s important to recall that the presence/absence of substantive covariates means each model has a different baseline group. Because they were in the log-odds scale, the model output and our coefficient plot can be difficult to interpret. With the brms::inv_logit_scaled(), we can convert the \\(\\hat \\alpha\\)s to the hazard (i.e., probability) metric. bind_rows(tidy(fit11.7) %&gt;% mutate(model = &quot;a&quot;), tidy(fit11.8) %&gt;% mutate(model = &quot;b&quot;), tidy(fit11.9) %&gt;% mutate(model = &quot;c&quot;), tidy(fit11.10) %&gt;% mutate(model = &quot;d&quot;)) %&gt;% filter(str_detect(term, &quot;d&quot;)) %&gt;% mutate(model = str_c(&quot;model &quot;, model), term = factor(str_remove(term, &quot;b_&quot;), levels = str_c(&quot;d&quot;, 12:7))) %&gt;% mutate_at(vars(estimate:upper), .funs = inv_logit_scaled) %&gt;% ggplot(aes(x = term, y = estimate, ymin = lower, ymax = upper)) + geom_pointrange(fatten = 2.5) + labs(x = NULL, y = &quot;posterior (hazard scale)&quot;) + coord_flip() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~model, nrow = 1) Building further, here’s our version of Table 11.4. tidy(fit11.7) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate(`time period` = str_remove(term, &quot;b_d&quot;) %&gt;% as.double(), predictor = str_remove(term, &quot;b_&quot;)) %&gt;% select(`time period`, predictor, estimate) %&gt;% mutate(`fitted odds` = exp(estimate), `fitted hazard` = inv_logit_scaled(estimate)) %&gt;% mutate_if(is.double, round, digits = 4) %&gt;% knitr::kable() time period predictor estimate fitted odds fitted hazard 7 d7 -2.4168 0.0892 0.0819 8 d8 -3.1564 0.0426 0.0408 9 d9 -1.7306 0.1772 0.1505 10 d10 -1.2959 0.2736 0.2148 11 d11 -1.1753 0.3087 0.2359 12 d12 -0.7416 0.4764 0.3227 11.4.2 Dichotomous substantive predictors. Here’s the summary for pt from fit11.8 (i.e., Model B). fixef(fit11.8)[&quot;pt&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.8572376 0.2126537 0.4366487 1.2708265 If we take the anti-log (i.e., exponentiate) of that coefficient, we’ll return an odds ratio. Here’s the conversion with just the posterior mean. fixef(fit11.8)[&quot;pt&quot;, 1] %&gt;% exp() ## [1] 2.356642 To get a better sense of the conversion, here it is in a plot. library(tidybayes) posterior_samples(fit11.8) %&gt;% transmute(`log-odds` = b_pt, `hazard ratio` = exp(b_pt)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;log-odds&quot;, &quot;hazard ratio&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeyeh(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior for pt&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) This tells us that, in every grade, the estimated odds of first intercourse are nearly two and one half times higher for boys who experienced a parenting transition in comparison to boys raised with both biological parents. In substantive terms, an odds ratio of this magnitude represents a substantial, and potentially important, effect. (p. 398) To reframe the odds ratio in terms of the other group (i.e., pt == 0), take the reciprocal. 1 / exp(fixef(fit11.8)[7, 1]) ## [1] 0.4243326 “This tells us that the estimated odds of first intercourse for boys who did not experience a parenting transition are approximately 40% of the odds for boys who did. These complimentary ways of reporting effect sizes are equivalent” (p. 389) 11.4.3 Continuous substantive predictors. Here’s the conditional effect of pas from fit11.9 (i.e., Model C). fixef(fit11.9)[&quot;pas&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.4445845 0.1135034 0.2176975 0.6663583 To understand pas, our measure of parental antisocial behavior, it will help to look at its range. range(sex_pp$pas) ## [1] -1.716180 2.781413 Exponentiating (i.e., taking the anti-log) the posterior of a continuous predictor is a legitimate way to convert it to a hazard ratio. posterior_samples(fit11.9) %&gt;% transmute(`log-odds` = b_pas, `hazard ratio` = exp(b_pas)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;log-odds&quot;, &quot;hazard ratio&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeyeh(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior for pas&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Here’s how to compute the hazard ratio for a 2-unit difference in pas. exp(fixef(fit11.9)[7, 1] * 2) ## [1] 2.433107 Here’s what that looks like in a plot. posterior_samples(fit11.9) %&gt;% transmute(`log-odds` = b_pas, `hazard ratio` = exp(b_pas), `hr for a 2-unit difference` = exp(b_pas * 2)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;log-odds&quot;, &quot;hazard ratio&quot;, &quot;hr for a 2-unit difference&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeyeh(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior for pas&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) 11.4.4 Polytomous substantive predictors. Unfortunately, neither the sex nor the sex_pp data sets contain the polytomous version of pt as Singer described in this section. However, we can simulate a similar set of dummy variables which will allow us to trace the basic steps in Singer and Willett’s workflow. Since we’ve been working with the sex_pp data for the past few models, we’ll continue using it here. However, this creates a minor challenge. What we want to do is use the sample() function to randomly assign cases with values of 1, 2, or 3 conditional on whether pt == 0. The catch is, we need to make sure that random value is constant for each case. Our solution will be to first nest the data such that each case only has one row. Here’s what that looks like. set.seed(11) sex_pp &lt;- sex_pp %&gt;% nest(data = c(period, event, d7, d8, d9, d10, d11, d12, pas)) %&gt;% mutate(random = sample(1:3, size = n(), replace = T)) %&gt;% mutate(pt_cat = ifelse(pt == 0, pt, random)) %&gt;% mutate(pt1 = ifelse(pt_cat == 1, 1, 0), pt2 = ifelse(pt_cat == 2, 1, 0), pt3 = ifelse(pt_cat == 3, 1, 0)) %&gt;% select(id, pt, random, pt_cat:pt3, data) sex_pp %&gt;% head() ## # A tibble: 6 x 8 ## id pt random pt_cat pt1 pt2 pt3 data ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 1 0 2 0 0 0 0 &lt;tibble [3 × 9]&gt; ## 2 2 1 2 2 0 1 0 &lt;tibble [6 × 9]&gt; ## 3 3 0 1 0 0 0 0 &lt;tibble [6 × 9]&gt; ## 4 5 1 1 1 1 0 0 &lt;tibble [6 × 9]&gt; ## 5 6 0 1 0 0 0 0 &lt;tibble [5 × 9]&gt; ## 6 7 1 2 2 0 1 0 &lt;tibble [3 × 9]&gt; Here are the number of cases for each of the four levels of our pseudovariable pt_cat. sex_pp %&gt;% count(pt_cat) ## # A tibble: 4 x 2 ## pt_cat n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 72 ## 2 1 41 ## 3 2 30 ## 4 3 37 Our breakdown isn’t exactly the same as the one in the text (p. 391), but we’re in the ballpark. Before we’re ready to fit our next model, we’ll need to unnest() the data, which will transform sex_pp back into the familiar long format. sex_pp &lt;- sex_pp %&gt;% unnest(data) Fit the model with the dummies pt1, pt2, and pt3. fit11.11 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt1 + pt2 + pt3, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.11&quot;) Here is the model summary. print(fit11.11) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt1 + pt2 + pt3 ## Data: sex_pp (Number of observations: 822) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -3.03 0.32 -3.67 -2.44 1.00 3268 3011 ## d8 -3.75 0.41 -4.61 -3.02 1.00 3833 2963 ## d9 -2.29 0.27 -2.84 -1.78 1.00 3248 3154 ## d10 -1.81 0.26 -2.32 -1.31 1.00 3480 2880 ## d11 -1.64 0.27 -2.18 -1.13 1.00 3545 2633 ## d12 -1.15 0.27 -1.69 -0.64 1.00 4116 3278 ## pt1 0.65 0.27 0.12 1.17 1.00 2828 2876 ## pt2 0.70 0.29 0.14 1.28 1.00 2873 2861 ## pt3 1.22 0.28 0.68 1.75 1.00 2533 3055 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can use the tidy() function and a few lines of wrangling code to make a version of the table in the middle of page 391. tidy(fit11.11) %&gt;% filter(str_detect(term, &quot;b_pt&quot;)) %&gt;% mutate(predictor = str_remove(term, &quot;b_&quot;), `odds ratio` = exp(estimate)) %&gt;% select(predictor, estimate, `odds ratio`) %&gt;% mutate_if(is.double, round, digits = 3) ## predictor estimate odds ratio ## 1 pt1 0.650 1.916 ## 2 pt2 0.703 2.020 ## 3 pt3 1.222 3.394 Because our data did not include the original values for pt1 through pt3, the results in our table will not match those in the text. We did get pretty close, though, eh? Hopefully this gives a sense of the workflow. 11.5 Displaying fitted hazard and survivor functions This will be an extension of what we’ve already been doing. 11.5.1 A strategy for a single categorical substantive predictor. We can make our version of Table 11.5 like so. To reduce clutter, we will use abbreviated column names. tibble(time = 7:12, alpha = fixef(fit11.8)[1:6, 1], beta = fixef(fit11.8)[7, 1]) %&gt;% mutate(lh0 = alpha, lh1 = alpha + beta) %&gt;% mutate(h0 = inv_logit_scaled(lh0), h1 = inv_logit_scaled(lh1)) %&gt;% mutate(s0 = cumprod(1 - h0), s1 = cumprod(1 - h1)) %&gt;% # this just simplifies the output mutate_if(is.double, round, digits = 4) ## # A tibble: 6 x 9 ## time alpha beta lh0 lh1 h0 h1 s0 s1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 -3.00 0.857 -3.00 -2.15 0.0473 0.105 0.953 0.895 ## 2 8 -3.73 0.857 -3.73 -2.87 0.0235 0.0537 0.930 0.847 ## 3 9 -2.28 0.857 -2.28 -1.42 0.0927 0.194 0.844 0.683 ## 4 10 -1.82 0.857 -1.82 -0.965 0.139 0.276 0.727 0.494 ## 5 11 -1.65 0.857 -1.65 -0.794 0.161 0.311 0.610 0.340 ## 6 12 -1.18 0.857 -1.18 -0.320 0.236 0.421 0.466 0.197 For the alpha and beta columns, we just subset the values from fixef(). The two logit-hazard columns, lh0 and lh1, were simple algebraic transformations of alpha and beta, respectively. To make the two hazard columns, h0 and h1, we applied the inv_logit_scaled() function to lh0 and lh1, respectively. To make the two survival columns, s0 and s1, we applied the cumprod() function to one minus the two hazard columns. Note how all this is based off of the posterior means. There’s enough going on with Table 11.5 that it makes sense to ignore uncertainty But when we’re ready to go beyond table glancing and actually make a plot, we will go beyond posterior means and reintroduce the uncertainty in the model. Two of these plots are quite similar to two of the subplots from Figure 11.4, back in Section 11.2.1. But recall that though those plots were based on fit11.6, which was based on the aggregated data, the plots we are about to make will be based on fit11.8, the analogous model based on the disaggregated person-period data. Regardless of whether the logistic regression model is based on aggregated data, the post-processing approach will involve the fitted() function. However, the specifics of how we use fitted() will differ. For the disaggregated data used to fit fit11.8, here is how we might define the newdata, pump it through the model via fitted(), and wrangle. nd &lt;- crossing(pt = 0:1, period = 7:12) %&gt;% mutate(d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- fitted(fit11.8, newdata = nd, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) f ## Estimate Est.Error Q2.5 Q97.5 pt period d7 d8 d9 d10 d11 d12 ## 1 -3.0028021 0.3071880 -3.619907 -2.4291047 pt = 0 7 1 0 0 0 0 0 ## 2 -3.7263185 0.4234433 -4.604059 -2.9568205 pt = 0 8 0 1 0 0 0 0 ## 3 -2.2806265 0.2698021 -2.821580 -1.7680331 pt = 0 9 0 0 1 0 0 0 ## 4 -1.8225996 0.2538985 -2.331516 -1.3371696 pt = 0 10 0 0 0 1 0 0 ## 5 -1.6512001 0.2679558 -2.184255 -1.1296853 pt = 0 11 0 0 0 0 1 0 ## 6 -1.1776296 0.2692006 -1.706033 -0.6423001 pt = 0 12 0 0 0 0 0 1 ## 7 -2.1455645 0.2786570 -2.733983 -1.6350970 pt = 1 7 1 0 0 0 0 0 ## 8 -2.8690809 0.4020457 -3.736634 -2.1654320 pt = 1 8 0 1 0 0 0 0 ## 9 -1.4233889 0.2360307 -1.907156 -0.9714672 pt = 1 9 0 0 1 0 0 0 ## 10 -0.9653619 0.2252534 -1.401366 -0.5339492 pt = 1 10 0 0 0 1 0 0 ## 11 -0.7939625 0.2459580 -1.273436 -0.3284309 pt = 1 11 0 0 0 0 1 0 ## 12 -0.3203919 0.2590562 -0.820268 0.1769375 pt = 1 12 0 0 0 0 0 1 Here we make and save the upper two panels of Figure 11.6. # logit(hazard) p1 &lt;- f %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;fitted logit(hazard)&quot;, y = NULL) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # hazard p2 &lt;- f %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = inv_logit_scaled) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;fitted hazard&quot;, y = NULL) + coord_cartesian(ylim = c(0, .5)) + theme(legend.position = &quot;none&quot;) Before we’re ready to make the last panel, we’ll redo our fitted() work, this time including predicted values for grade 6. nd &lt;- crossing(pt = 0:1, period = 6:12) %&gt;% mutate(d6 = if_else(period == 6, 1, 0), d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- fitted(fit11.8, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) f ## Estimate Est.Error Q2.5 Q97.5 pt period d6 d7 d8 d9 d10 d11 d12 ## 1 0.50000000 0.00000000 0.50000000 0.50000000 pt = 0 6 1 0 0 0 0 0 0 ## 2 0.04922053 0.01429394 0.02608644 0.08098007 pt = 0 7 0 1 0 0 0 0 0 ## 3 0.02546482 0.01024019 0.00991189 0.04941514 pt = 0 8 0 0 1 0 0 0 0 ## 4 0.09521774 0.02309532 0.05616910 0.14578711 pt = 0 9 0 0 0 1 0 0 0 ## 5 0.14187768 0.03057243 0.08854622 0.20797593 pt = 0 10 0 0 0 0 1 0 0 ## 6 0.16419164 0.03653532 0.10117336 0.24421919 pt = 0 11 0 0 0 0 0 1 0 ## 7 0.23885945 0.04842691 0.15367900 0.34472680 pt = 0 12 0 0 0 0 0 0 1 ## 8 0.70020505 0.04432433 0.60746019 0.78088420 pt = 1 6 1 0 0 0 0 0 0 ## 9 0.10757917 0.02618851 0.06099762 0.16313332 pt = 1 7 0 1 0 0 0 0 0 ## 10 0.05728498 0.02082329 0.02327936 0.10289794 pt = 1 8 0 0 1 0 0 0 0 ## 11 0.19676002 0.03676760 0.12930068 0.27458817 pt = 1 9 0 0 0 1 0 0 0 ## 12 0.27804687 0.04460934 0.19759942 0.36959628 pt = 1 10 0 0 0 0 1 0 0 ## 13 0.31371815 0.05225591 0.21866970 0.41862246 pt = 1 11 0 0 0 0 0 1 0 ## 14 0.42185661 0.06217035 0.30570677 0.54411934 pt = 1 12 0 0 0 0 0 0 1 The values for grade 6 (i.e., those for when d6 == 1) are nonsensical. The main reason we included d6 in the fitted results and in the nd data is so we’d have the slots in our f object. In the code block below, we’ll fill those slots with the appropriate values (0) and then convert the hazard summaries to the survival (i.e., cumulative probability) metric. f &lt;- f %&gt;% mutate(Estimate = if_else(period == 6, 0, Estimate), Q2.5 = if_else(period == 6, 0, Q2.5), Q97.5 = if_else(period == 6, 0, Q97.5)) %&gt;% group_by(pt) %&gt;% mutate(s = cumprod(1 - Estimate), s_lower = cumprod(1 - Q2.5), s_upper = cumprod(1 - Q97.5)) %&gt;% select(pt:d12, s:s_upper) f %&gt;% glimpse() ## Rows: 14 ## Columns: 12 ## Groups: pt [2] ## $ pt &lt;chr&gt; &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 1&quot;, &quot;pt = 1&quot;, &quot;… ## $ period &lt;int&gt; 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 10, 11, 12 ## $ d6 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 ## $ d7 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 ## $ d8 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ## $ d9 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0 ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0 ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1 ## $ s &lt;dbl&gt; 1.0000000, 0.9507795, 0.9265680, 0.8383423, 0.7194003, 0.6012808, 0.4576592, 1.0000000, 0.8… ## $ s_lower &lt;dbl&gt; 1.0000000, 0.9739136, 0.9642602, 0.9100986, 0.8295128, 0.7455882, 0.6310070, 1.0000000, 0.9… ## $ s_upper &lt;dbl&gt; 1.00000000, 0.91901993, 0.87360643, 0.74624588, 0.59104470, 0.44670024, 0.29271070, 1.00000… Make and save the final panel. # save the interpolated median lifetime values imls &lt;- c(make_iml(lt %&gt;% filter(pt == &quot;pt = 0&quot;)), make_iml(lt %&gt;% filter(pt == &quot;pt = 1&quot;))) # hazard p3 &lt;- f %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_segment(x = imls[1], xend = imls[1], y = -Inf, yend = .5, color = &quot;white&quot;, linetype = 2) + geom_segment(x = imls[2], xend = imls[2], y = -Inf, yend = .5, color = &quot;white&quot;, linetype = 2) + geom_ribbon(aes(ymin = s_lower, ymax = s_upper), size = 0, alpha = 1/6) + geom_line(aes(y = s)) + scale_y_continuous(NULL, breaks = c(0, .5, 1)) + labs(subtitle = &quot;fitted survival probability&quot;) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;none&quot;) Combine the subplots to finish off our version of Figure 11.6. (p1 / p2 / p3) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) Here is the breakdown of what percentage of boys will still be virgins at grades 9 and 12, based on pt status, as indicated by fit11.8. f %&gt;% filter(period %in% c(9, 12)) %&gt;% mutate_if(is.double, ~ (. * 100) %&gt;% round(digits = 0)) %&gt;% mutate(`percent virgins` = str_c(s, &quot; [&quot;, s_lower, &quot;, &quot;, s_upper, &quot;]&quot;)) %&gt;% select(period, pt, `percent virgins`) %&gt;% arrange(period) ## # A tibble: 4 x 3 ## # Groups: pt [2] ## period pt `percent virgins` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 9 pt = 0 84 [91, 75] ## 2 9 pt = 1 68 [80, 54] ## 3 12 pt = 0 46 [63, 29] ## 4 12 pt = 1 19 [35, 9] 11.5.2 Extending this strategy to multiple predictors (some of which are continuous). It is easy to display fitted hazard and survivor functions for model involving multiple predictor by extending these ideas in a straightforward manner. Instead of plotting one fitted function for each predictor value, select several prototypical predictor values (using strategies presented in section 4.5.3 and plot fitted functions for combinations of these values. (p. 394, emphasis in the original) We’ll be focusing on fit11.10, which includes both pt and sas as substantive predictors. pt only takes two values, 0 and 1. For pas, we’ll use the conventional -1, 0, and 1. Here’s the fitted()-related code. nd &lt;- crossing(pt = 0:1, pas = -1:1) %&gt;% expand(nesting(pt, pas), period = 6:12) %&gt;% mutate(d6 = if_else(period == 6, 1, 0), d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- fitted(fit11.10, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) head(f) ## Estimate Est.Error Q2.5 Q97.5 pt pas period d6 d7 d8 d9 d10 d11 d12 ## 1 0.42485980 0.029968357 0.368464812 0.48403874 0 -1 6 1 0 0 0 0 0 0 ## 2 0.04085235 0.012738516 0.019953216 0.07001607 0 -1 7 0 1 0 0 0 0 0 ## 3 0.02134740 0.008830432 0.008064858 0.04289951 0 -1 8 0 0 1 0 0 0 0 ## 4 0.08160469 0.021075901 0.046419952 0.12849732 0 -1 9 0 0 0 1 0 0 0 ## 5 0.12223623 0.028381639 0.073395909 0.18434817 0 -1 10 0 0 0 0 1 0 0 ## 6 0.14284857 0.033352645 0.086573345 0.21547653 0 -1 11 0 0 0 0 0 1 0 Make the two subplots. # logit(hazard) p1 &lt;- f %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt), pas = str_c(&quot;pas = &quot;, pas)) %&gt;% mutate(pas = factor(pas, levels = str_c(&quot;pas = &quot;, 1:-1))) %&gt;% filter(period &gt; 6) %&gt;% ggplot(aes(x = period, group = pas, fill = pas, color = pas)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;fitted logit(hazard)&quot;, y = NULL) + coord_cartesian(ylim = c(0, .5)) + facet_wrap(~pt) # hazard p2 &lt;- f %&gt;% mutate(Estimate = if_else(period == 6, 0, Estimate), Q2.5 = if_else(period == 6, 0, Q2.5), Q97.5 = if_else(period == 6, 0, Q97.5)) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt), pas = str_c(&quot;pas = &quot;, pas)) %&gt;% mutate(pas = factor(pas, levels = str_c(&quot;pas = &quot;, 1:-1))) %&gt;% group_by(pt, pas) %&gt;% mutate(s = cumprod(1 - Estimate), s_lower = cumprod(1 - Q2.5), s_upper = cumprod(1 - Q97.5)) %&gt;% ggplot(aes(x = period, group = pas, fill = pas, color = pas)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_ribbon(aes(ymin = s_lower, ymax = s_upper), size = 0, alpha = 1/6) + geom_line(aes(y = s)) + scale_y_continuous(NULL, breaks = c(0, .5, 1)) + labs(subtitle = &quot;fitted survival probability&quot;) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~pt) Combine the subplots to make our version of Figure 11.7. ((p1 / p2) &amp; scale_fill_viridis_d(NULL, option = &quot;D&quot;, end = .8, direction = -1) &amp; scale_color_viridis_d(NULL, option = &quot;D&quot;, end = .8, direction = -1) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank())) + plot_layout(guides = &quot;collect&quot;) Here we departed from the text a bit by separating the subplots by pt status. They’re already cluttered enough as is. 11.5.3 Two cautions when interpreting fitted hazard and survivor functions. Beware of inferring statistical interaction of a substantive predictor and time when examining plots if fitted hazard and survivor functions. The root of this difficulty is in our use of a link function. Because the model expresses the linear effect of the predictor on logit hazard, you cannot draw a conclusion about the stability of an effect using graphs plotted on a raw hazard scale. In fact, the logic works in the opposite direction. If the size of the gap between fitted hazard functions is constant over time, [the] effect of the predictor must vary over time! (pp. 396-397, emphasis in the original) Also, please don’t confuse plots of fitted values with descriptive sample-based plots. Hopefully our inclusion of 95% intervals helps prevent this. 11.6 Comparing models using deviance statistics and information criteria We now introduce two important questions that we usually address before interpreting parameters and displaying results: Which of the alternative models fits better: Might a predictor’s observed effect be the result of nothing more than sampling variation? (p. 397) Much of the material in this section will be a refresher from the material we covered in Section 4.6. 11.6.1 The deviance statistic. The log-likelihood, LL, is a summary statistic routinely output (in some form) by any program that provides ML estimates. As discussed in section 4.6, its relative magnitude across a series of models fit to the same set of data can be informative (although its absolute magnitude is not). The larger the LL statistic, the better the fit. (pp. 397–398) Note that in some form part. Frequentist software typically returns the LL for a given model as a sigle value. As we learned way back in Section 4.6, we can use the log_lik() function to get the LL information from our brms fits. However, form the brms reference manual we discover log_lik() returns an “S x N matrix containing the pointwise log-likelihood samples, where S is the number of samples and N is the number of observations in the data” (p. 112). Using fit11.7 as a test case, here’s what that looks like. log_lik(fit11.7) %&gt;% str() ## num [1:4000, 1:822] -0.1018 -0.0737 -0.0921 -0.0772 -0.066 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL To compute the LL for each HMC iteration, you sum across the rows. Deviance is that sum multiplied by -2. Here’s that in a tibble. ll &lt;- fit11.7 %&gt;% log_lik() %&gt;% as_tibble(.name_repair = ~ str_c(&quot;c&quot;, 1:822)) %&gt;% mutate(ll = rowSums(.)) %&gt;% mutate(deviance = -2 * ll) %&gt;% select(ll, deviance, everything()) ## New names: ## * `` -&gt; c1 ## * `` -&gt; c2 ## * `` -&gt; c3 ## * `` -&gt; c4 ## * `` -&gt; c5 ## * ... ll ## # A tibble: 4,000 x 824 ## ll deviance c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -330. 660. -0.102 -0.0219 -2.10 -0.102 -0.0219 -0.130 -0.191 -0.313 -0.311 -0.102 -0.0219 -0.130 ## 2 -330. 660. -0.0737 -0.0740 -1.76 -0.0737 -0.0740 -0.190 -0.304 -0.192 -0.471 -0.0737 -0.0740 -0.190 ## 3 -331. 663. -0.0921 -0.0257 -1.97 -0.0921 -0.0257 -0.150 -0.153 -0.389 -0.396 -0.0921 -0.0257 -0.150 ## 4 -331. 662. -0.0772 -0.0677 -1.78 -0.0772 -0.0677 -0.184 -0.344 -0.173 -0.391 -0.0772 -0.0677 -0.184 ## 5 -329. 658. -0.0660 -0.0456 -1.88 -0.0660 -0.0456 -0.166 -0.189 -0.193 -0.328 -0.0660 -0.0456 -0.166 ## 6 -327. 654. -0.0826 -0.0529 -1.76 -0.0826 -0.0529 -0.190 -0.200 -0.285 -0.399 -0.0826 -0.0529 -0.190 ## 7 -329. 659. -0.115 -0.0390 -1.78 -0.115 -0.0390 -0.186 -0.169 -0.209 -0.381 -0.115 -0.0390 -0.186 ## 8 -330. 660. -0.0946 -0.0243 -2.20 -0.0946 -0.0243 -0.117 -0.303 -0.324 -0.321 -0.0946 -0.0243 -0.117 ## 9 -330. 660. -0.0930 -0.0378 -2.12 -0.0930 -0.0378 -0.128 -0.242 -0.294 -0.231 -0.0930 -0.0378 -0.128 ## 10 -328. 656. -0.105 -0.0540 -1.88 -0.105 -0.0540 -0.166 -0.258 -0.263 -0.544 -0.105 -0.0540 -0.166 ## # … with 3,990 more rows, and 810 more variables: c13 &lt;dbl&gt;, c14 &lt;dbl&gt;, c15 &lt;dbl&gt;, c16 &lt;dbl&gt;, c17 &lt;dbl&gt;, ## # c18 &lt;dbl&gt;, c19 &lt;dbl&gt;, c20 &lt;dbl&gt;, c21 &lt;dbl&gt;, c22 &lt;dbl&gt;, c23 &lt;dbl&gt;, c24 &lt;dbl&gt;, c25 &lt;dbl&gt;, c26 &lt;dbl&gt;, ## # c27 &lt;dbl&gt;, c28 &lt;dbl&gt;, c29 &lt;dbl&gt;, c30 &lt;dbl&gt;, c31 &lt;dbl&gt;, c32 &lt;dbl&gt;, c33 &lt;dbl&gt;, c34 &lt;dbl&gt;, c35 &lt;dbl&gt;, ## # c36 &lt;dbl&gt;, c37 &lt;dbl&gt;, c38 &lt;dbl&gt;, c39 &lt;dbl&gt;, c40 &lt;dbl&gt;, c41 &lt;dbl&gt;, c42 &lt;dbl&gt;, c43 &lt;dbl&gt;, c44 &lt;dbl&gt;, ## # c45 &lt;dbl&gt;, c46 &lt;dbl&gt;, c47 &lt;dbl&gt;, c48 &lt;dbl&gt;, c49 &lt;dbl&gt;, c50 &lt;dbl&gt;, c51 &lt;dbl&gt;, c52 &lt;dbl&gt;, c53 &lt;dbl&gt;, ## # c54 &lt;dbl&gt;, c55 &lt;dbl&gt;, c56 &lt;dbl&gt;, c57 &lt;dbl&gt;, c58 &lt;dbl&gt;, c59 &lt;dbl&gt;, c60 &lt;dbl&gt;, c61 &lt;dbl&gt;, c62 &lt;dbl&gt;, ## # c63 &lt;dbl&gt;, c64 &lt;dbl&gt;, c65 &lt;dbl&gt;, c66 &lt;dbl&gt;, c67 &lt;dbl&gt;, c68 &lt;dbl&gt;, c69 &lt;dbl&gt;, c70 &lt;dbl&gt;, c71 &lt;dbl&gt;, ## # c72 &lt;dbl&gt;, c73 &lt;dbl&gt;, c74 &lt;dbl&gt;, c75 &lt;dbl&gt;, c76 &lt;dbl&gt;, c77 &lt;dbl&gt;, c78 &lt;dbl&gt;, c79 &lt;dbl&gt;, c80 &lt;dbl&gt;, ## # c81 &lt;dbl&gt;, c82 &lt;dbl&gt;, c83 &lt;dbl&gt;, c84 &lt;dbl&gt;, c85 &lt;dbl&gt;, c86 &lt;dbl&gt;, c87 &lt;dbl&gt;, c88 &lt;dbl&gt;, c89 &lt;dbl&gt;, ## # c90 &lt;dbl&gt;, c91 &lt;dbl&gt;, c92 &lt;dbl&gt;, c93 &lt;dbl&gt;, c94 &lt;dbl&gt;, c95 &lt;dbl&gt;, c96 &lt;dbl&gt;, c97 &lt;dbl&gt;, c98 &lt;dbl&gt;, ## # c99 &lt;dbl&gt;, c100 &lt;dbl&gt;, c101 &lt;dbl&gt;, c102 &lt;dbl&gt;, c103 &lt;dbl&gt;, c104 &lt;dbl&gt;, c105 &lt;dbl&gt;, c106 &lt;dbl&gt;, ## # c107 &lt;dbl&gt;, c108 &lt;dbl&gt;, c109 &lt;dbl&gt;, c110 &lt;dbl&gt;, c111 &lt;dbl&gt;, c112 &lt;dbl&gt;, … Since we have distributions for the LL and deviance, we may as well visualize them in a plot. ll %&gt;% pivot_longer(ll:deviance) %&gt;% mutate(name = factor(name, levels = c(&quot;ll&quot;, &quot;deviance&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Here’s how to compute the LL and deviance distributions for each of our four models, fit11.7 through fit11.10, in bulk. ll &lt;- tibble(model = str_c(&quot;model &quot;, letters[1:4]), name = str_c(&quot;fit11.&quot;, 7:10)) %&gt;% mutate(fit = map(name, get)) %&gt;% mutate(ll = map(fit, ~log_lik(.) %&gt;% data.frame() %&gt;% transmute(ll = rowSums(.)))) %&gt;% select(-fit) %&gt;% unnest(ll) %&gt;% mutate(deviance = -2 * ll) ll %&gt;% glimpse() ## Rows: 16,000 ## Columns: 4 ## $ model &lt;chr&gt; &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;m… ## $ name &lt;chr&gt; &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;f… ## $ ll &lt;dbl&gt; -329.7999, -330.1944, -331.3949, -331.1617, -329.0912, -326.9543, -329.3654, -329.8801, -3… ## $ deviance &lt;dbl&gt; 659.5997, 660.3887, 662.7898, 662.3234, 658.1824, 653.9087, 658.7309, 659.7603, 659.8338, … Now plot the LL and deviance distributions for each. ll %&gt;% pivot_longer(ll:deviance, names_to = &quot;statistic&quot;) %&gt;% mutate(statistic = factor(statistic, levels = c(&quot;ll&quot;, &quot;deviance&quot;))) %&gt;% ggplot(aes(x = value, y = model)) + geom_halfeyeh(point_interval = median_qi, .width = .95, normalize = &quot;panels&quot;) + labs(x = NULL, y = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~statistic, scales = &quot;free_x&quot;) 11.6.2 Deviance-based hypothesis tests for individual predictors. Singer and Willett wrote: “Comparing deviance statistics for pairs of nested models that differ only by a single substantive predictor permits evaluation of the ‘statistical significance’ of that predictor” (p. 399). I’m just not going to appeal to null-hypothesis significance testing in this project and, as an extension, I am not going to appeal to tests using the \\(\\chi^2\\) distribution. But sure, you could take our deviance distributions and compare them with difference distributions. Singer and Willett made four deviance comparisons in this section. Here’s what that might look like using our deviance distributions. ll %&gt;% select(model, deviance) %&gt;% mutate(iter = rep(1:4000, times = 4)) %&gt;% pivot_wider(names_from = model, values_from = deviance) %&gt;% mutate(`a - b` = `model a` - `model b`, `a - c` = `model a` - `model c`, `c - d` = `model c` - `model d`, `b - d` = `model b` - `model d`) %&gt;% pivot_longer(contains(&quot;-&quot;)) %&gt;% ggplot(aes(x = value, y = name)) + geom_halfeyeh(point_interval = median_qi, .width = .95, normalize = &quot;panels&quot;) + labs(x = &quot;deviance difference distribution&quot;, y = NULL) + theme(panel.grid = element_blank()) But really, like no one does this with Bayesian models. If you think you have a good theoretical reason to use this approach, do not cite this project as a justification. I do not endorse it. 11.6.3 Deviance-based hypothesis tests for groups of predictors. We won’t be doing this. 11.6.4 Comparing nonnested models using [WAIC and LOO]. Now we return to our preferred methods for model comparison. Use the add_criterion() function to compute the WAIC and LOO and add their output to the model fits. fit11.7 &lt;- add_criterion(fit11.7, c(&quot;loo&quot;, &quot;waic&quot;)) fit11.8 &lt;- add_criterion(fit11.8, c(&quot;loo&quot;, &quot;waic&quot;)) fit11.9 &lt;- add_criterion(fit11.9, c(&quot;loo&quot;, &quot;waic&quot;)) fit11.10 &lt;- add_criterion(fit11.10, c(&quot;loo&quot;, &quot;waic&quot;)) First compare Models B and C (i.e., fit11.8 and fit11.9, respectively). loo_compare(fit11.8, fit11.9, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit11.8 0.0 0.0 -324.4 17.6 7.1 0.6 648.8 35.2 ## fit11.9 -1.4 4.6 -325.8 17.6 7.2 0.6 651.6 35.1 loo_compare(fit11.8, fit11.9, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit11.8 0.0 0.0 -324.4 17.6 7.0 0.6 648.8 35.2 ## fit11.9 -1.4 4.6 -325.8 17.6 7.2 0.6 651.6 35.1 In a head-to-head comparison, Model B is a little better than Model C. However, the standard error for their difference score is about three times as large as the difference itself. This is not a difference I would write home about. Now compare Models A through D. loo_compare(fit11.7, fit11.8, fit11.9, fit11.10, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit11.10 0.0 0.0 -322.6 17.6 8.0 0.6 645.1 35.1 ## fit11.8 -1.8 2.6 -324.4 17.6 7.1 0.6 648.8 35.2 ## fit11.9 -3.3 2.8 -325.8 17.6 7.2 0.6 651.6 35.1 ## fit11.7 -9.6 4.8 -332.1 17.6 6.2 0.5 664.3 35.3 loo_compare(fit11.7, fit11.8, fit11.9, fit11.10, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit11.10 0.0 0.0 -322.5 17.5 8.0 0.6 645.1 35.1 ## fit11.8 -1.9 2.6 -324.4 17.6 7.0 0.6 648.8 35.2 ## fit11.9 -3.3 2.8 -325.8 17.6 7.2 0.6 651.6 35.1 ## fit11.7 -9.6 4.8 -332.1 17.6 6.1 0.5 664.2 35.3 Model D (i.e., fit11.10, the full model) has the best (i.e., lowest) WAIC and LOO estimates. However, the standard errors for its difference scores with the other models is on the large side, particularly for Models B and C. So sure, adding either pt or sas to the model helps a bit and adding them both helps a little more, but neither predictor is a huge winner when you take that model complexity penalty into account. As discussed earlier, we can also compare the models using weights. Here we’ll use the WAIC, LOO, and stacking weights to compare all four models. model_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit11.7 fit11.8 fit11.9 fit11.10 ## 0.000 0.132 0.032 0.836 model_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit11.7 fit11.8 fit11.9 fit11.10 ## 0.000 0.131 0.032 0.836 model_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = &quot;stacking&quot;) %&gt;% round(digits = 3) ## fit11.7 fit11.8 fit11.9 fit11.10 ## 0.000 0.313 0.209 0.478 Model D has the best showing across the three weighting schemes. 11.7 Statistical inference using [uncertainty in the Bayesian posterior] I generally take a model-based approach to Bayesian statistics and I prefer to scrutinize marginal posteriors, consider effect sizes, and use graphical depictions of my models (e.g., posterior predictive checks) over hypothesis testing. Further extending that approach, here, puts us at further odds with the content in the test. In addition, the authors spent some time discussing the asymptotic properties of ML standard errors. Our Bayesian approach is not based on asymptotic theory and we just don’t need to concern ourselves with whether our marginal posteriors are Gaussian. They often are, which is nice. But we summarize our posteriors with percentile-based 95% intervals, we are not presuming they are symmetric or Gaussian. 11.7.1 The Wald chi-square statistic. This will not be our approach. On page 404, Singer and Willett wrote: “The logistic regression analysis routines in all major statistical packages routinely output asymptotic standard errors.” This comment presumes we’re focusing on frequentist packages. Our rough analogue to frequentist standard errors is our Bayesian posterior standard deviations. The authors focused on the two substantive predictors from Model D (i.e., fit11.10). Here’s another look at the brms summary. print(fit11.10) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas ## Data: sex_pp (Number of observations: 822) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.90 0.31 -3.56 -2.30 1.00 2644 2387 ## d8 -3.61 0.43 -4.51 -2.83 1.00 3094 2761 ## d9 -2.15 0.28 -2.72 -1.63 1.00 2306 2394 ## d10 -1.69 0.26 -2.24 -1.18 1.00 2136 2649 ## d11 -1.51 0.27 -2.05 -0.99 1.00 2542 2813 ## d12 -1.00 0.28 -1.57 -0.46 1.00 2621 2732 ## pt 0.64 0.24 0.19 1.11 1.00 1433 1850 ## pas 0.30 0.12 0.06 0.54 1.00 3444 3031 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Recall that the second column for our ‘Population-Level Effects’, ‘Est.Error’, contains the standard deviation for each dimension of the posteriors listed (i.e., for each parameter ranging from d7 to pas). This is similar, but distinct from, the frequentist standard error. Instead of focusing on \\(p\\)-values connected to standard errors, why not look at the marginal posteriors directly? post &lt;- posterior_samples(fit11.10) post %&gt;% pivot_longer(b_pt:b_pas) %&gt;% ggplot(aes(x = value, y = name, fill = stat(x &gt; 0))) + stat_slabh() + scale_fill_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + labs(x = &quot;marginal posterior&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 2)) + theme(panel.grid = element_blank()) If we’d like to keep with the NHST perspective, zero is not a particularly credible value for either parameter. But neither are negative values in generals. In terms of uncertainty, look how much wider the posterior for pt is when compared with pas. And don’t forget that these are on the log-odds scale. Looking at those densities might lead one to ask, Exactly what proportion of the posterior draws for each is zero or below? You can compute that like this. post %&gt;% pivot_longer(b_pt:b_pas) %&gt;% group_by(name) %&gt;% summarise(`percent zero or below` = 100 * mean(value &lt;= 0)) ## # A tibble: 2 x 2 ## name `percent zero or below` ## &lt;chr&gt; &lt;dbl&gt; ## 1 b_pas 0.875 ## 2 b_pt 0.175 Less that 1% of the draws were zero or below for each. 11.7.2 [Asymmetric credible intervals] for parameters and odds ratios. Whether we use percentile-based credible intervals, as we typically do, or use highest posterior density intervals, neither depends on asymptotic theory nor do they depend on the posterior standard deviation. That is, our Bayesian intervals do not presume the marginal posteriors are Gaussian. Let’s look back at the summary output for fit11.10, this time using the fixef() function. fixef(fit11.10) ## Estimate Est.Error Q2.5 Q97.5 ## d7 -2.9005763 0.3137696 -3.55588990 -2.3026882 ## d8 -3.6075406 0.4307296 -4.50648338 -2.8348577 ## d9 -2.1499960 0.2776953 -2.72499466 -1.6279351 ## d10 -1.6942049 0.2629563 -2.23593367 -1.1815440 ## d11 -1.5143291 0.2680454 -2.05015155 -0.9908001 ## d12 -1.0035752 0.2808239 -1.56552178 -0.4577481 ## pt 0.6393620 0.2377291 0.18562424 1.1109096 ## pas 0.3039824 0.1230509 0.06386673 0.5388084 We find the lower- and upper-limits for our percentile-based Bayesian credible intervals in the last two columns. If you’d like HDIs instead, use the convenience functions from tidybayes. post %&gt;% pivot_longer(b_pt:b_pas) %&gt;% group_by(name) %&gt;% mean_hdi(value) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_pas 0.304 0.0628 0.536 0.95 mean hdi ## 2 b_pt 0.639 0.186 1.11 0.95 mean hdi We can exponentiate our measures of central tendency (e.g., posterior means) and posterior intervals to transform them out of the log-odds metric an into the odds-ratio metric. Here are the results working directly with fixef(). fixef(fit11.10)[c(&quot;pt&quot;, &quot;pas&quot;), c(1, 3:4)] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## pt 1.895271 1.20397 3.037120 ## pas 1.355245 1.06595 1.713963 Keep in mind that fixating on just the 95% intervals is a little NHST-centric. Since we have entire posterior distributions to summarize, we might consider other intervals. Here we use another graphical approach by using tidybayes_statintervalh() to mark off the 10, 30, 50, 70, and 90% intervals for both substantive predictors. Both are in the odds-ratio metric. post %&gt;% pivot_longer(b_pt:b_pas) %&gt;% mutate(`odds ratio` = exp(value)) %&gt;% ggplot(aes(x = `odds ratio`, y = name)) + stat_intervalh(size = 5, .width = seq(from = .1, to = .9, by = .2)) + scale_color_grey(&quot;CI level:&quot;, start = .8, end = .2) + scale_x_continuous(breaks = 1:3) + ylab(NULL) + coord_cartesian(xlim = c(1, 3)) + theme(legend.position = &quot;top&quot;, panel.grid = element_blank()) The frequentist 95% confidence intervals are asymmetric when expressed in the odds-ratio metric and so are our various Bayesian intervals. However, the asymmetry in our Bayesian intervals is less noteworthy because there was no explicit assumption of symmetry when they were in the log-odds metric. Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3 broom_0.5.5 brms_2.12.0 Rcpp_1.0.4.6 patchwork_1.0.0 survival_3.1-12 ## [7] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [6] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [11] svUnit_0.7-12 DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 splines_3.6.3 knitr_1.28 shinythemes_1.1.2 ## [21] bayesplot_1.7.1 jsonlite_1.6.1 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [26] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.3 ## [36] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.2.4 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.2 rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [51] gtools_3.8.2 zoo_1.8-7 scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [56] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 stringi_1.4.6 ## [66] highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 pkgconfig_2.0.3 ## [71] matrixStats_0.56.0 HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [76] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 ## [81] magrittr_1.5 bookdown_0.18 R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [86] pillar_1.4.3 haven_2.2.0 withr_2.1.2 xts_0.12-0 abind_1.4-5 ## [91] modelr_0.1.6 crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 ## [96] grid_3.6.3 readxl_1.3.1 callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [101] digest_0.6.25 xtable_1.8-4 httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 ## [106] viridisLite_0.3.0 shinyjs_1.1 Footnote R already has a built-in function to convert probabilities to the log-odds scale. Somewhat confusingly, it’s called qlogis(). You can learn more by executing ?qlogis or by browsing through this great blog post by Roman Cheplyaka. It’s generally a good idea to stick to the functions in base R rather than make your own, like we did earlier in this chapter (see this twitter thread). Since the name of qlogis() isn’t the easiest to remember, you can always execute something like log_odds &lt;- qlogis or logit &lt;- qlogis at the beginning of your scripts and then use that as a thin wrapper for qlogis().↩ "],
["extending-the-discrete-time-hazard-model.html", "12 Extending the Discrete-Time Hazard Model 12.1 Alternative specification for the “main effect” of TIME 12.2 Using the complementary log-log link to specify a discrete-time hazard model 12.3 Time-varying predictors 12.4 The linear additivity assumption: Uncovering violations and simple solutions 12.5 The proportionality assumption: Uncovering violations and simple solutions 12.6 The no unobserved heterogeneity assumption: No simple solution 12.7 Residual analysis Reference Session info Footnote", " 12 Extending the Discrete-Time Hazard Model Like all statistical models, the basic discrete-time hazard model invokes assumptions about the population that may, or may not, hold in practice. Because no model should be adopted without scrutiny, we devote this chapter to examining its assumptions, demonstrating how to evaluate their tenability and relax their constraints when appropriate. In doing so, we illustrate practical principles of data analysis and offer theoretical insights into the model’s behavior and interpretation. (p. 407) 12.1 Alternative specification for the “main effect” of TIME Use of a completely general specification for TIME [as explored in the last chapter] is an analytic decision, not an integral feature of the model. Nothing about the model or its estimation requires adoption of this, or any other, particular specification for TIME (p. 409, emphasis in the original) In the next page, Singer and Willett listed three circumstances under which we might consider alternatives to the completely general approach to time. They were in studies with many discrete time periods, when hazard is expected to be near zero in some time periods, and when some time periods have small risk sets. In the subsections to follow, we will explore each in turn. 12.1.1 An ordered series of polynomial specifications for TIME. Load the Gamse and Conger’s (1997) tenure_pp.csv data. library(tidyverse) tenure_pp &lt;- read_csv(&quot;data/tenure_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(tenure_pp) ## Rows: 1,474 ## Columns: 12 ## $ id &lt;dbl&gt; 111, 111, 111, 111, 111, 211, 211, 211, 211, 211, 211, 311, 311, 311, 311, 311, 311, 311, 31… ## $ period &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3,… ## $ event &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ d1 &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,… ## $ d2 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ d3 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,… ## $ d4 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ d5 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ d6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ d7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ d8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ d9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,… Let’s confirm these data are composed of the records of \\(n = 260\\) early-career academics. tenure_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 260 Here’s a way to count how many cases were censored. tenure_pp %&gt;% group_by(id) %&gt;% arrange(desc(period)) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(event) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 x 3 ## event n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 94 36.2 ## 2 1 166 63.8 Let’s fire up brms. library(brms) As discussed in the prose and displayed in Tables 12.1 and 12.2, we will fit seven models, ranging from a constant (i.e., intercept only) model to a general (i.e., discrete factor) model. In the last chapter, we discussed how one can fit a general model with a series of \\(J\\) dummies or equivalently with the time variable, period in these data, set as a factor. Here we’ll do both. In preparation, we’ll make a period_f version of period. tenure_pp &lt;- tenure_pp %&gt;% mutate(period_f = factor(period)) Now fit the models. # constant fit12.1 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 1, prior(normal(0, 4), class = Intercept), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.01&quot;) # linear fit12.2 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.02&quot;) # quadratic fit12.3 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.03&quot;) # cubic fit12.4 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.04&quot;) # fourth order fit12.5 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3) + I(period^4), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, control = list(max_treedepth = 12), seed = 12, file = &quot;fits/fit12.05&quot;) # fifth order fit12.6 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3) + I(period^4) + I(period^5), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 3000, warmup = 2000, control = list(max_treedepth = 13), seed = 12, file = &quot;fits/fit12.06&quot;) # general fit12.7 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.07&quot;) # general with `factor(period)` fit12.8 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + period_f, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.08&quot;) Before we compare the models with information criteria, it might be handy to look at the hazard functions for each. A relatively quick way is with the conditional_effects() function. p2 &lt;- plot(conditional_effects(fit12.2), plot = F)[[1]] + ggtitle(&quot;linear&quot;) p3 &lt;- plot(conditional_effects(fit12.3), plot = F)[[1]] + ggtitle(&quot;quadratic&quot;) p4 &lt;- plot(conditional_effects(fit12.4), plot = F)[[1]] + ggtitle(&quot;cubic&quot;) p5 &lt;- plot(conditional_effects(fit12.5), plot = F)[[1]] + ggtitle(&quot;fourth order&quot;) p6 &lt;- plot(conditional_effects(fit12.6), plot = F)[[1]] + ggtitle(&quot;fifth order&quot;) p7 &lt;- plot(conditional_effects(fit12.8), cat_args = list(size = 3/2), plot = F)[[1]] + ggtitle(&quot;general&quot;) Because it contains no predictors, we cannot use conditional_effects() to make a plot for the constant model (i.e., fit12.1). We’ll have to do that by hand. p1 &lt;- tibble(period = 1:9) %&gt;% ggplot(aes(x = period)) + geom_ribbon(aes(ymin = fixef(fit12.1)[, 3] %&gt;% inv_logit_scaled(), ymax = fixef(fit12.1)[, 4] %&gt;% inv_logit_scaled()), alpha = 1/5) + geom_line(aes(y = fixef(fit12.1)[, 1] %&gt;% inv_logit_scaled()), size = 1, color = &quot;blue1&quot;) + ggtitle(&quot;constant&quot;) + ylab(&quot;event | trials(1)&quot;) Now combine and format the subplots with patchwork. library(patchwork) (((p1 + p2 + p3 + p4 + p5 + p6) &amp; scale_x_continuous(breaks = 1:9)) + p7) &amp; coord_cartesian(ylim = c(0, .5)) &amp; theme(panel.grid = element_blank()) We are going to depart from Singer and Willett and no longer entertain using deviance for Bayesian model comparison. But we will compare then using the WAIC and the LOO. fit12.1 &lt;- add_criterion(fit12.1, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.2 &lt;- add_criterion(fit12.2, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.3 &lt;- add_criterion(fit12.3, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.4 &lt;- add_criterion(fit12.4, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.5 &lt;- add_criterion(fit12.5, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.6 &lt;- add_criterion(fit12.6, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.7 &lt;- add_criterion(fit12.7, c(&quot;loo&quot;, &quot;waic&quot;)) fit12.8 &lt;- add_criterion(fit12.8, c(&quot;loo&quot;, &quot;waic&quot;)) Before comparing the models in bulk, as in Table 12.2, let’s confirm that whether we use the dummy variable method (fit12.7) or the factor variable method (fit12.8), the results for the general model are the same. loo_compare(fit12.7, fit12.8, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.7 0.0 0.0 -425.0 22.4 9.3 1.0 849.9 44.9 ## fit12.8 0.0 0.0 -425.0 22.4 9.3 1.0 849.9 44.9 loo_compare(fit12.7, fit12.8, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.7 0.0 0.0 -424.9 22.4 9.3 1.0 849.8 44.9 ## fit12.8 0.0 0.0 -424.9 22.4 9.3 1.0 849.8 44.9 Yep, both WAIC and LOO confirm both methods are equivalent. One of the main reasons we used the factor method, here, was because it made it easier to plot the results with conditional_effects(). But with the following model comparisons, we’ll focus on fit12.1 through fit12.7. loo_compare(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.4 0.0 0.0 -420.5 22.1 3.9 0.5 841.1 44.1 ## fit12.5 -0.6 0.9 -421.2 22.2 4.7 0.6 842.4 44.3 ## fit12.3 -0.8 2.1 -421.3 22.1 3.2 0.4 842.6 44.1 ## fit12.6 -1.3 0.8 -421.8 22.2 5.4 0.7 843.6 44.5 ## fit12.7 -4.4 1.5 -425.0 22.4 9.3 1.0 849.9 44.9 ## fit12.2 -15.2 5.9 -435.7 22.1 1.9 0.1 871.4 44.1 ## fit12.1 -99.2 13.1 -519.8 25.1 1.0 0.1 1039.5 50.2 loo_compare(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.4 0.0 0.0 -420.5 22.1 3.9 0.5 841.0 44.1 ## fit12.5 -0.6 0.9 -421.2 22.2 4.7 0.6 842.3 44.3 ## fit12.3 -0.8 2.1 -421.3 22.1 3.2 0.4 842.6 44.1 ## fit12.6 -1.3 0.8 -421.8 22.2 5.4 0.7 843.6 44.5 ## fit12.7 -4.4 1.5 -424.9 22.4 9.3 1.0 849.8 44.9 ## fit12.2 -15.2 5.9 -435.7 22.1 1.9 0.1 871.4 44.1 ## fit12.1 -99.2 13.1 -519.8 25.1 1.0 0.1 1039.5 50.2 Our results are very similar to those in the AIC column in Table 12.2. Just for kicks and giggles, here are the model weights based on the LOO, WAIC, and stacking method. model_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 ## 0.000 0.000 0.206 0.439 0.230 0.120 0.005 model_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 ## 0.000 0.000 0.205 0.438 0.230 0.121 0.006 model_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = &quot;stacking&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 ## 0.017 0.000 0.405 0.578 0.000 0.000 0.000 Across the comparison methods, the overall pattern is the cubic model (fit12.4) is marginally better than the rest, but that both the quadratic and fourth-order models were quite close. Unlike when you use model deviance, the parsimony corrections used in the information-criteria-based methods all suggest the general model is overfit. Before considering whether these differences in [information criteria] are sufficient to warrant use of an alternative specification for TIME, let us examine the corresponding fitted logit hazard functions. Doing so not only highlights the behavior of logit hazard, it also offers a graphical means of comparing the fit of competing specifications. (p. 413, emphasis in the original) In preparation for our Figure 12.1, we’ll make a custom function called make_fitted(), which will streamline some of the data wrangling code. make_fitted &lt;- function(fit, scale, ...) { fitted(fit, newdata = nd, scale = scale, ...) %&gt;% data.frame() %&gt;% bind_cols(nd) } In addition to taking different brms fit objects as input, make_fitted() will allow us to adjust the scale of the output. As you’ll see, we will need to work with to settings in the coming plots. For our first batch of code, we’ll use scale = &quot;linear&quot;. Because the newdata for our version of the general model (i.e., fit12.8) requires the predictor to be called period_f and the rest of the models require the predictor to be named period, we’ll make and save the results for the former first, redefine our newdata, apply make_fitted() to the rest of the models, and then combine them all. Here it is in one fell swoop. nd &lt;- tibble(period_f = 1:9) f &lt;- make_fitted(fit12.8, scale = &quot;linear&quot;) %&gt;% rename(period = period_f) # this will simplify the `mutate()` code below models &lt;- c(&quot;constant&quot;, &quot;linear&quot;, &quot;quadratic&quot;, &quot;cubic&quot;, &quot;general&quot;) nd &lt;- tibble(period = 1:9) f &lt;- bind_rows(make_fitted(fit12.1, scale = &quot;linear&quot;), # constant make_fitted(fit12.2, scale = &quot;linear&quot;), # linear make_fitted(fit12.3, scale = &quot;linear&quot;), # quadratic make_fitted(fit12.4, scale = &quot;linear&quot;), # cubic f) %&gt;% # general mutate(model = factor(rep(models, each = 9), levels = models)) # what have we done? glimpse(f) ## Rows: 45 ## Columns: 6 ## $ Estimate &lt;dbl&gt; -2.0628440, -2.0628440, -2.0628440, -2.0628440, -2.0628440, -2.0628440, -2.0628440, -2.06… ## $ Est.Error &lt;dbl&gt; 0.08140815, 0.08140815, 0.08140815, 0.08140815, 0.08140815, 0.08140815, 0.08140815, 0.081… ## $ Q2.5 &lt;dbl&gt; -2.22322400, -2.22322400, -2.22322400, -2.22322400, -2.22322400, -2.22322400, -2.22322400… ## $ Q97.5 &lt;dbl&gt; -1.90425837, -1.90425837, -1.90425837, -1.90425837, -1.90425837, -1.90425837, -1.90425837… ## $ period &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3,… ## $ model &lt;fct&gt; constant, constant, constant, constant, constant, constant, constant, constant, constant,… Now we’re in good shape to make and save our version of the top panel of Figure 12.1. p1 &lt;- f %&gt;% ggplot(aes(x = period, y = Estimate, color = model)) + geom_line() + scale_color_viridis_d(option = &quot;A&quot;, direction = -1) + ylab(&quot;Fitted logit(hazard)&quot;) + coord_cartesian(ylim = -c(6, 0)) + theme(panel.grid = element_blank()) The bottom two panels require we redo our make_fitted() code from above, this time setting scale = &quot;response&quot;. nd &lt;- tibble(period_f = 1:9) f &lt;- make_fitted(fit12.8, scale = &quot;response&quot;) %&gt;% rename(period = period_f) nd &lt;- tibble(period = 1:9) f &lt;- bind_rows(make_fitted(fit12.1, scale = &quot;response&quot;), # constant make_fitted(fit12.2, scale = &quot;response&quot;), # linear make_fitted(fit12.3, scale = &quot;response&quot;), # quadratic make_fitted(fit12.4, scale = &quot;response&quot;), # cubic f) %&gt;% # general mutate(model = factor(rep(models, each = 9), levels = models)) Now make and save the bottom left panel for Figure 12.1. p2 &lt;- f %&gt;% filter(model %in% c(&quot;quadratic&quot;, &quot;general&quot;)) %&gt;% ggplot(aes(x = period, y = Estimate, color = model)) + geom_line() + scale_color_viridis_d(option = &quot;A&quot;, end = .5, direction = -1) + ylab(&quot;Fitted hazard&quot;) + coord_cartesian(ylim = c(0, .4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Our f data will also work to make the final bottom right panel for the figure, but we’ll need to convert the Estimate values from the hazard metric to the survival-probability metric. In addition, we will need to add in values for when period = 0. Here we wrangle, plot, and save in one block. new_rows &lt;- tibble(Estimate = 0, period = 0, model = factor(c(&quot;quadratic&quot;, &quot;general&quot;), levels = models)) p3 &lt;- f %&gt;% filter(model %in% c(&quot;quadratic&quot;, &quot;general&quot;)) %&gt;% select(Estimate, period, model) %&gt;% # add the `new_rows` data bind_rows(new_rows) %&gt;% arrange(model, period) %&gt;% group_by(model) %&gt;% # convert hazards to survival probabilities mutate(Estimate = cumprod(1 - Estimate)) %&gt;% # plot! ggplot(aes(x = period, y = Estimate, color = model)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line() + scale_color_viridis_d(option = &quot;A&quot;, end = .5, direction = -1) + scale_y_continuous(&quot;Fitted survival probability&quot;, breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Now combine the subplots and format a little with patchwork to make our version of Figure 12.1. p1 + p2 + p3 + plot_layout(guides = &quot;collect&quot;) &amp; scale_x_continuous(&quot;Years after hire&quot;, breaks = 0:9, limits = c(0, 9)) Instead of the two-row layout in the text, it seemed simpler to arrange the panels all in one row. This way we can let the full version of the line-color legend serve for all three panels. 12.1.2 Criteria for comparing alternative specification. The decline in the deviance statistic across models indicates that fit improves with increasing complexity of the temporal specification. To evaluate the magnitude of this decline, we must also account for the increased number of parameters in the model. You should not adopt a more complex specification if it fits no better than a simpler one. But if an alternative specification is (nearly) as good as the most general one, it may be “good enough.” At the same time, we would not want an alternative that performs measurably worse than we know we can do. (p. 415) We won’t be comparing deviances with \\(\\chi^2\\) tests, here. As to information criteria, we got ahead of the authors a bit and presented those comparisons in the last section. Although our use of the WAIC and the LOO is similar to Singer and Willett’s use of the AIC and BIC in that they yield no formal hypothesis test in the form of a \\(p\\)-value, their estimates and difference scores do come with standard errors. In the middle of page 416, the authors focused on comparing the constant and linear models, the linear and quadratic models, and the quadratic and general models. The LOO and WAIC estimates for each were near identical. For the sake of simplicity, here are those three focused comparisons using the LOO. # the constant and linear models l1 &lt;- loo_compare(fit12.1, fit12.2, criterion = &quot;loo&quot;) # the linear and quadratic models l2 &lt;- loo_compare(fit12.2, fit12.3, criterion = &quot;loo&quot;) # the quadratic and general models l3 &lt;- loo_compare(fit12.3, fit12.7, criterion = &quot;loo&quot;) l1 %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.2 0.0 0.0 -435.7 22.1 1.9 0.1 871.4 44.1 ## fit12.1 -84.0 11.6 -519.8 25.1 1.0 0.1 1039.5 50.2 l2 %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.3 0.0 0.0 -421.3 22.1 3.2 0.4 842.6 44.1 ## fit12.2 -14.4 5.5 -435.7 22.1 1.9 0.1 871.4 44.1 l3 %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.3 0.0 0.0 -421.3 22.1 3.2 0.4 842.6 44.1 ## fit12.7 -3.7 2.4 -425.0 22.4 9.3 1.0 849.9 44.9 If we presume the LOO differences follow a normal distribution, we can use their point estimates and standard errors to plot those distributions using simulated data from good old rnorm(). library(tidybayes) n &lt;- 1e6 models &lt;- c(&quot;linear - constant&quot;, &quot;quadratic - linear&quot;, &quot;quadratic - general&quot;) set.seed(12) # wrangle tibble(loo_difference = c(rnorm(n, mean = l1[2, 1] * -2, sd = l1[2, 2] * 2), rnorm(n, mean = l2[2, 1] * -2, sd = l2[2, 2] * 2), rnorm(n, mean = l3[2, 1] * -2, sd = l3[2, 2] * 2))) %&gt;% mutate(models = factor(rep(models, each = n), levels = models)) %&gt;% # plot! ggplot(aes(x = loo_difference, y = 0)) + stat_halfeyeh(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;LOO-difference simulations based on 1,000,000 draws&quot;, x = &quot;difference distribution&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~models, scales = &quot;free&quot;) The LOO difference for the linear and constant models is decisive. The difference for the quadratic and linear models is fairly large on the information-criteria scale, but the uncertainty in that distribution is fairly large relative to its location (i.e., its mean), which might temper overly-strong conclusions about how much better the quadratic was compared to the linear. The comparison between the quadratic and the general produced a simulation with a modest location and rather large uncertainty relative to the magnitude of that location. All in all, “all signs point to the superiority of the quadratic specification, which fits nearly as well as the general mode, but with fewer parameters” (p. 416). In the next page, Singer and Willett briefly focused on comparing the cubic and quadratic models. Here are their LOO and WAIC caparisons. loo_compare(fit12.3, fit12.4, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.4 0.0 0.0 -420.5 22.1 3.9 0.5 841.1 44.1 ## fit12.3 -0.8 2.1 -421.3 22.1 3.2 0.4 842.6 44.1 loo_compare(fit12.3, fit12.4, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.4 0.0 0.0 -420.5 22.1 3.9 0.5 841.0 44.1 ## fit12.3 -0.8 2.1 -421.3 22.1 3.2 0.4 842.6 44.1 In the text, the results for the AIC and BIC differed. Our LOO and WAIC results both agree with the AIC: the cubic model has a slightly lower LOO and WAIC estimate compared to the quadratic. However, the standard errors for the formal difference score are about twice the size of that difference and the absolute magnitude of the difference is rather small to begin with. Here’s what it looks like if we compare them using LOO weights, WAIC weights, and stacking weights. model_weights(fit12.3, fit12.4, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.3 fit12.4 ## 0.319 0.681 model_weights(fit12.3, fit12.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.3 fit12.4 ## 0.318 0.682 model_weights(fit12.3, fit12.4, weights = &quot;stacking&quot;) %&gt;% round(digits = 3) ## fit12.3 fit12.4 ## 0.331 0.669 Across all three weight comparisons, there was a modest edge for the quadratic model. But back to Singer and Willett: Although decision rules cannot substitute for judgment, intuition, and common sense, we nevertheless conclude by offering two guidelines for selecting among alternative specifications: If a smooth specification works nearly as well as the completely general one, appreciably better than all simpler ones, and no worse than all more complex ones, consider adopting it. If no smooth specifications meet these criteria, retain the completely general specification. If this decision process leads you to a polynomial specification, then you can interpret the model’s parameters easily, as we [will discuss in a bit]. (p. 417, emphasis in the original) Before moving on, we might point out that our Bayesian brms-based framework offers a different option: model averaging. We plot the hazard and survival curves based on weighted averages of multiple models. The weights can be based on various criteria. One approach would be to use the model weights from the LOO or the WAIC. As an example, here we use the LOO weights for the quadratic and cubic models. nd &lt;- tibble(period = 1:9) pp &lt;- pp_average(fit12.3, fit12.4, weights = &quot;loo&quot;, newdata = nd, method = &quot;pp_expect&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) The pp_average() function works much like fitted() or predict(). If you input models and perhaps newdata, it will return estimates that are the weighted averages of the specified models. With the weights = &quot;loo&quot; argument, we indicated our desired weights were those from the LOO, just as we computed earlier with the model_weights() function. With the method = &quot;pp_expect&quot; argument, we indicated we wanted fitted values like we would get from fitted(). Here we plot the results in terms of hazard and survival. # hazard p1 &lt;- pp %&gt;% ggplot(aes(x = period)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5) + geom_line(aes(y = Estimate)) + scale_x_continuous(&quot;Years after hire&quot;, breaks = 0:9, limits = c(0, 9)) + ylab(&quot;hazard&quot;) + theme(panel.grid = element_blank()) # survival p2 &lt;- pp %&gt;% select(-Est.Error) %&gt;% bind_rows(tibble(Estimate = 0, Q2.5 = 0, Q97.5 = 0, period = 0)) %&gt;% arrange(period) %&gt;% mutate_at(vars(Estimate:Q97.5), .funs = ~ cumprod(1 - .)) %&gt;% ggplot(aes(x = period)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5) + geom_line(aes(y = Estimate)) + scale_x_continuous(&quot;Years after hire&quot;, breaks = 0:9) + scale_y_continuous(&quot;survival&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) # combine (p1 | p2) + plot_annotation(title = &quot;Behold the fitted hazard and survival curves based on a weighted\\naverage of the quadratic and linear models!&quot;) 12.1.3 Interpreting parameters from linear, quadratic, and cubic specifications. “One advantage of a polynomial specification is that you can often interpret its parameters directly” (p. 417). For the polynomial models in this section, Singer and Willett used the \\(TIME - c\\) specification for period where \\(c\\) is a centering constant. They used \\(c = 5\\). Before we can refit our polynomial models with this parameterization, we’ll want to make a new period variable with this centering. We’ll call it period_5. tenure_pp &lt;- tenure_pp %&gt;% mutate(period_5 = period - 5) # how do the two `period` variables compare? tenure_pp %&gt;% distinct(period, period_5) ## # A tibble: 9 x 2 ## period period_5 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -4 ## 2 2 -3 ## 3 3 -2 ## 4 4 -1 ## 5 5 0 ## 6 6 1 ## 7 7 2 ## 8 8 3 ## 9 9 4 Now refit the quadratic model using period_5. fit12.9 &lt;- update(fit12.3, newdata = tenure_pp, event | trials(1) ~ 0 + Intercept + period_5 + I(period_5^2), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.09&quot;) Check the model summary. print(fit12.9) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ Intercept + period_5 + I(period_5^2) - 1 ## Data: tenure_pp (Number of observations: 1474) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.41 0.11 -1.63 -1.21 1.00 2477 2167 ## period_5 0.61 0.06 0.50 0.73 1.00 2656 2283 ## Iperiod_5E2 -0.13 0.03 -0.18 -0.08 1.00 2221 2022 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Focusing just on the posterior means, this yields the following formula for the quadratic discrete-time hazard model: \\(\\operatorname{logit} \\hat h (t_j) =\\;\\) -1.4140 \\(\\text{one } +\\) 0.6147\\((\\text{time}_j - 5)\\) -0.1281\\((\\text{time}_j - 5)^2\\). Singer and Willett used the term “flipover point” for the point at which the quadratic function reaches its peak or trough. If we let \\(c\\) be the centering constant for time variable, \\(\\alpha_1\\) be the linear coefficient for time, and \\(\\alpha_2\\) be the quadratic coefficient for time, we define the flipover point as \\[\\text{flipover point} = [c - 1/2 (\\alpha_1 / \\alpha_2)].\\] Here’s what that looks like using the posterior samples. posterior_samples(fit12.9) %&gt;% transmute(c = 5, a1 = b_period_5, a2 = b_Iperiod_5E2) %&gt;% mutate(`flipover point` = c - 0.5 * (a1 / a2)) %&gt;% ggplot(aes(x = `flipover point`, y = 0)) + stat_halfeyeh(.width = c(.5, .95)) + scale_x_continuous(breaks = 7:12) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Just as each parameter has a posterior distribution, the flipover point, which is a function of two of the parameters, also has a posterior distribution. To understand what this flipover distribution means, it might be helpful to look at it in another way. For that, we’ll employ fitted(). nd &lt;- tibble(period = seq(from = 0, to = 12, by = .1)) %&gt;% mutate(period_5 = period - 5) f &lt;- fitted(fit12.9, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(nd, iter = 1:4000, nesting(period, period_5))) f %&gt;% glimpse() ## Rows: 484,000 ## Columns: 5 ## $ name &lt;chr&gt; &quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;, &quot;X5&quot;, &quot;X6&quot;, &quot;X7&quot;, &quot;X8&quot;, &quot;X9&quot;, &quot;X10&quot;, &quot;X11&quot;, &quot;X12&quot;, &quot;X13&quot;, &quot;X14&quot;, &quot;… ## $ value &lt;dbl&gt; -7.475609, -7.301111, -7.128899, -6.958974, -6.791335, -6.625982, -6.462916, -6.302136, -6… ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ period &lt;dbl&gt; 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, … ## $ period_5 &lt;dbl&gt; -5.0, -4.9, -4.8, -4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4.0, -3.9, -3.8, -3.7, -3.6, … Now make a logit hazard spaghetti plot. f %&gt;% # how many lines would you like? filter(iter &lt;= 30) %&gt;% ggplot(aes(x = period, y = value, group = iter)) + geom_line(alpha = 1/2) + ylab(&quot;logit hazard&quot;) + coord_cartesian(xlim = c(0, 11), ylim = c(-5, 0)) + theme(panel.grid = element_blank()) To keep the plot manageable, we filtered to just the first 30 posterior draws. Each was depicted with its own hazard line. Note how each of those hazard lines peaks at a different point along the \\(x\\)-axis. Most peak somewhere around 7.4. Some take on notably higher values. Now fit a cubic model using period_5, \\((TIME_j - 5)\\), as the measure of time. fit12.10 &lt;- update(fit12.4, newdata = tenure_pp, event | trials(1) ~ 0 + Intercept + period_5 + I(period_5^2) + I(period_5^3), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.10&quot;) Check the model summary. print(fit12.10) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ Intercept + period_5 + I(period_5^2) + I(period_5^3) - 1 ## Data: tenure_pp (Number of observations: 1474) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.45 0.11 -1.68 -1.23 1.00 2583 2293 ## period_5 0.75 0.10 0.56 0.94 1.00 1647 1839 ## Iperiod_5E2 -0.11 0.02 -0.17 -0.07 1.00 2362 2373 ## Iperiod_5E3 -0.02 0.01 -0.04 0.00 1.00 1894 2195 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Focusing again on just the posterior means, this yields the following formula for the cubic discrete-time hazard model: \\(\\operatorname{logit} \\hat h (t_j) =\\;\\) -1.4510 \\(\\text{one } +\\) 0.7475\\((\\text{time}_j - 5)\\) -0.1149\\((\\text{time}_j - 5)^2\\) -0.0180\\((\\text{time}_j - 5)^3\\). Now if we let \\(c\\), \\(\\alpha_1\\) and \\(\\alpha_2\\) retain their meanings from before and further let \\(\\alpha_3\\) stand for the cubic term for time, we can define the two flipover points in the cubic logit hazard model as \\[\\text{flipover points} = c + \\frac{-\\alpha_2 \\pm \\sqrt{\\alpha_2^2 - 3 \\alpha_1 \\alpha_3}}{3 \\alpha_3}.\\] Do you see that \\(\\pm\\) sign in the numerator? That’s what gives us the two points. Now apply the formula to fit12.10 and plot. # extract the posterior draws post &lt;- posterior_samples(fit12.10) %&gt;% transmute(c = 5, a1 = b_period_5, a2 = b_Iperiod_5E2, a3 = b_Iperiod_5E3) # flipover point with &quot;+&quot; in the numerator p1 &lt;- post %&gt;% mutate(`flipover point 1` = c + (- a2 + sqrt(a2^2 - 3 * a1 * a3)) / (3 * a3)) %&gt;% filter(!is.na(`flipover point 1`)) %&gt;% filter(`flipover point 1` &gt; -50 &amp; `flipover point 1` &lt; 50) %&gt;% ggplot(aes(x = `flipover point 1`, y = 0)) + stat_halfeyeh(.width = c(.5, .95),) + annotate(geom = &quot;text&quot;, x = -30, y = .85, label = &quot;italic(c)+frac(-alpha[2]+sqrt(alpha[2]^2-3*alpha[1]*alpha[3]), 3*alpha[3])&quot;, hjust = 0, family = &quot;Times&quot;, parse = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-30, 20)) # flipover point with &quot;-&quot; in the numerator p2 &lt;- post %&gt;% mutate(`flipover point 2` = c + (- a2 - sqrt(a2^2 - 3 * a1 * a3)) / (3 * a3)) %&gt;% filter(!is.na(`flipover point 2`)) %&gt;% ggplot(aes(x = `flipover point 2`, y = 0)) + stat_halfeyeh(.width = c(.5, .95)) + annotate(geom = &quot;text&quot;, x = 8.2, y = .85, label = &quot;italic(c)+frac(-alpha[2]-sqrt(alpha[2]^2-3*alpha[1]*alpha[3]), 3*alpha[3])&quot;, hjust = 0, family = &quot;Times&quot;, parse = T) + scale_y_continuous(NULL, breaks = NULL) # combine! (p1 | p2) &amp; theme(panel.grid = element_blank()) The plot on the right looks similar to the flipover plot from fit12.9. But look at the massive uncertainty in the flipover point in the plot on the left. If you play around with the code, you’ll see the \\(x\\)-axis extends far beyond the boundaries in the plot. Another spaghetti plot might help show what’s going on. # redifine the `newdata` nd &lt;- tibble(period = seq(from = -8, to = 11, by = .1)) %&gt;% mutate(period_5 = period - 5) # employ `fitted()` and wrangle f &lt;- fitted(fit12.10, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(nd, iter = 1:4000, nesting(period, period_5))) # plot! f %&gt;% filter(iter &lt;= 30) %&gt;% ggplot(aes(x = period, y = value, group = iter)) + geom_line(alpha = 1/2) + ylab(&quot;logit hazard&quot;) + coord_cartesian(xlim = c(-7, 10), ylim = c(-13, 0)) + theme(panel.grid = element_blank()) On the region to the right of 0 on the \\(x\\)-axis, the plot looks a lot like the one for the quadratic model. But look at how wildly the lines fan out on the left side of 0. Since that’s the region where we find the second flipover point, all that uncertainty got baked into its marginal posterior. Just because I think it looks cool, here’s a version of that plot with lines corresponding to all 4,000 posterior draws. f %&gt;% ggplot(aes(x = period, y = value, group = iter)) + geom_line(alpha = 1/10, size = 1/10) + ylab(&quot;logit hazard&quot;) + coord_cartesian(xlim = c(-7, 10), ylim = c(-13, 0)) + theme(panel.grid = element_blank()) 12.2 Using the complementary log-log link to specify a discrete-time hazard model So far we’ve been using the logit transformation [which] represented a natural choice because it allowed us to: (1) specify the model using familiar terminology; (2) use widely available software for estimation; and (3) exploit interpretative strategies with which many empirical researchers are comfortable. Just like the choice of a completely general specification for the main effect of TIME, use of a logit link is an analytic decision. Nothing about the way in which the model is postulated or fit requires the adoption of this, or any other, particular link function. (p. 419–420, emphasis in the original) The complimentary log-log transformation–clog-log for short–is a widely-used alternative. It follows the form \\[\\operatorname{clog-log} = \\log \\big (- \\log (1 - p) \\big),\\] where \\(p\\) is a probability value. In words, “while the logit transformation yields the logarithm of the odds of event occurrence, the clog-log transformation yields the logarithm of the negated logarithm of the probability of event nonoccurrence” (p. 420, emphasis in the original). 12.2.1 The clog-log transformation: When and why it is useful. Here’s our version of Figure 12.2, where we compare the logit and clog-log transformations. # simulate the data tibble(p = seq(from = .00001, to = .99999, length.out = 1e4)) %&gt;% mutate(logit = log(p / (1 - p)), cloglog = log(-log(1 - p))) %&gt;% pivot_longer(-p) %&gt;% mutate(name = factor(name, levels = c(&quot;logit&quot;, &quot;cloglog&quot;), labels = c(&quot;Logit&quot;, &quot;Complementary log-log&quot;))) %&gt;% # plot ggplot(aes(x = p, y = value, color = name)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_line(size = 1) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) + scale_y_continuous(&quot;transformed hazard probability&quot;, breaks = -3:3 * 5, limits = c(-15, 15)) + xlab(&quot;hazard probability&quot;) + theme(legend.background = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(fill = &quot;grey92&quot;, color = &quot;grey92&quot;), legend.position = c(.25, .85), panel.grid = element_blank()) Both transformations extend to the full \\(-\\infty\\) to \\(\\infty\\) parameter space. But whereas the logit is symmetric around zero and has a memorable point corresponding to \\(p = .5\\) (i.e., 0), the clog-log is asymmetric and has a less-intuitive point corresponding to \\(p = .5\\) (i.e., -0.3665129). Though somewhat odd, the advantage of the clog-log is it provides a discrete-time statistical model for hazard that has a built-in proportional hazards assumption, and not a proportional odds assumption (as in the case of the logit link). This would be completely unremarkable except for one thing: it provides a conceptual parallelism between the clog-log discrete-time hazard model and the models that we will ultimately describe for continuous-time survival analysis. (p. 421, emphasis in the original) 12.2.2 A discrete-time hazard model using the complementary log-log link. Singer and Willett (p. 422): Any discrete-time hazard model postulated using a logit link can be rewritten using a clog-log link, simply by substituting transformations of the outcome. For example, we can write a general discrete-time hazard model for \\(J\\) time periods and \\(P\\) substantive predictors as: \\[\\begin{align*} \\operatorname{clog-log} h(t_j) &amp; = [\\alpha_1 D_1 + \\alpha_2 D_2 + \\cdots + \\alpha_J D_J] \\\\ &amp; \\;\\; + [\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_P X_P]. \\end{align*}\\] Now reload the firstsex_pp.csv data to test this baby out. sex_pp &lt;- read_csv(&quot;data/firstsex_pp.csv&quot;) glimpse(sex_pp) ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9,… ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, … ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,… ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,… ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916,… Our next task is to recreate Figure 12.3, which shows the “sample hazard functions for the grade of first intercourse data displayed on different scales,” namely the logit and clog-log (p. 423). The key word here is “sample,” meaning we’re not fitting full models. For our version of the corresponding figure from the last chapter (i.e., Figure 11.2 from page 363), we computed the sample logit hazard functions with life tables based on the output from the survival::survfit() function. I am not aware that you can get hazards in the clog-log scale from the survfit() function. The folks at IDRE solved the problem by fitting four maximum likelihood models using the glm() function (see here). We’ll follow a similar approach, but with weakly-regularizing priors using the brms::brm() function. ## logit # pt == 0 fit12.11 &lt;- brm(data = sex_pp %&gt;% filter(pt == 0), family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.11&quot;) # pt == 1 fit12.12 &lt;- brm(data = sex_pp %&gt;% filter(pt == 1), family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.12&quot;) ## clog-log # pt == 0 fit12.13 &lt;- brm(data = sex_pp %&gt;% filter(pt == 0), family = binomial(link = cloglog), event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.13&quot;) # pt == 1 fit12.14 &lt;- brm(data = sex_pp %&gt;% filter(pt == 1), family = binomial(link = cloglog), event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.14&quot;) The primary line of code that distinguishes fit12.11 and fit12.12 from fit12.13 and fit12.14 is family = binomial(link = cloglog). With the family argument, we switched out the default logit link for the clog-log link. Before we can make our version of Figure 12.5, we will redefine our nd data for fitted(), pump our four fit objects into our custom make_fitted() function from earlier, and wrangle a little. nd &lt;- tibble(period = 7:12) %&gt;% mutate(d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- bind_rows(make_fitted(fit12.11, scale = &quot;linear&quot;), make_fitted(fit12.12, scale = &quot;linear&quot;), make_fitted(fit12.13, scale = &quot;linear&quot;), make_fitted(fit12.14, scale = &quot;linear&quot;)) %&gt;% mutate(pt = rep(str_c(&quot;pt = &quot;, c(0:1, 0:1)), each = n() / 4), link = rep(c(&quot;logit&quot;, &quot;clog-log&quot;), each = n() / 2)) f %&gt;% glimpse() ## Rows: 24 ## Columns: 13 ## $ Estimate &lt;dbl&gt; -3.6759906, -3.6291011, -2.0585557, -1.9032077, -1.4559047, -1.4836458, -2.0077659, -2.94… ## $ Est.Error &lt;dbl&gt; 0.7452492, 0.7126282, 0.3684063, 0.3817483, 0.3533212, 0.4079584, 0.2966442, 0.4566804, 0… ## $ Q2.5 &lt;dbl&gt; -5.3798295, -5.2487341, -2.8453966, -2.7152387, -2.1943648, -2.3116730, -2.6237891, -3.92… ## $ Q97.5 &lt;dbl&gt; -2.43884354, -2.38763382, -1.39491123, -1.21280831, -0.79546524, -0.72122095, -1.45588706… ## $ period &lt;int&gt; 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12 ## $ d7 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0 ## $ d10 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0 ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0 ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1 ## $ pt &lt;chr&gt; &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 1&quot;, &quot;pt = 1&quot;, &quot;pt = 1&quot;,… ## $ link &lt;chr&gt; &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;,… Make our version of Figure 12.3. f %&gt;% ggplot(aes(x = period, group = interaction(pt, link), color = pt)) + geom_line(aes(y = Estimate, linetype = link)) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) + scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) + ylab(&quot;transformed hazard probability&quot;) + coord_cartesian(ylim = c(-4, 0)) + theme(panel.grid = element_blank()) Our next step is to fit proper models to the data. First, we will refit fit11.8 from last chapter. Then we’ll fit the clog-log analogue to the same model. The two models, respectively, follow the form \\[\\begin{align*} \\operatorname{logit} h(t_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ] \\; \\text{and} \\\\ \\operatorname{clog-log} h(t_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ]. \\end{align*}\\] Fit the models. # logit fit11.8 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.08&quot;) # clog-log fit12.15 &lt;- brm(data = sex_pp, family = binomial(link = cloglog), event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.15&quot;) Both models used the binomial likelihood. They only differed in which link function we used to transform the data. To further explicate, we can more fully write out our Bayesian clog-log model as \\[\\begin{align*} \\text{event}_{ij} &amp; = \\operatorname{Binomial}(n = 1, p_{ij}) \\\\ \\operatorname{clog-log} (p_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_{1i} ] \\\\ \\alpha_7, \\alpha_8, ..., \\alpha_{12} &amp; \\sim \\operatorname{Normal}(0, 4) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 4). \\end{align*}\\] Here are the posterior means for fit11.8 and fit12.15, as depicted in the firs three columns of Table 12.3. pars &lt;- bind_rows(fixef(fit11.8) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;par&quot;), fixef(fit12.15) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;par&quot;)) %&gt;% mutate(link = rep(c(&quot;logit&quot;, &quot;clog-log&quot;), each = n() / 2), par = factor(par, levels = c(str_c(&quot;d&quot;, 7:12), &quot;pt&quot;))) pars %&gt;% select(par, link, Estimate) %&gt;% pivot_wider(names_from = link, values_from = Estimate) %&gt;% select(par, `clog-log`, logit) %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 7 x 3 ## par `clog-log` logit ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 d7 -2.97 -3.00 ## 2 d8 -3.68 -3.73 ## 3 d9 -2.32 -2.28 ## 4 d10 -1.90 -1.82 ## 5 d11 -1.77 -1.65 ## 6 d12 -1.35 -1.18 ## 7 pt 0.760 0.857 They might be easier to compare in a coefficient plot. pars %&gt;% ggplot(aes(x = link, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_pointrange() + labs(x = NULL, y = &quot;transformed hazard&quot;) + coord_flip() + theme(axis.text.y = element_text(hjust = 0), panel.grid = element_blank()) + facet_wrap(~par, ncol = 1) Instead of comparing them with deviances, we will compare the two models using Bayesian information criteria. For simplicity, we’ll focus on the LOO. fit12.15 &lt;- add_criterion(fit12.15, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(fit12.15, fit11.8, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.15 0.0 0.0 -324.3 17.6 7.0 0.5 648.6 35.2 ## fit11.8 -0.1 0.2 -324.4 17.6 7.1 0.6 648.8 35.2 model_weights(fit12.15, fit11.8, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.15 fit11.8 ## 0.53 0.47 From a LOO perspective, they’re basically the same. The parameter summaries are also quite similar between the two models. A coefficient plot might make it easy to see. “Numerical similarity is common when fitting identical models with alternate link functions (and net risks of event occurrence are low) and suggests that choice of a link function should depend on other considerations” (p. 423). We already know from the last chapter that we can convert logits to probabilities with the function \\[p = \\frac{1}{1 + e^{- \\text{logit}}}.\\] The relevant inverse transformation for the clog-log link is \\[p = 1 - e^{\\Big (- e^{( \\text{clog-log})} \\Big)}.\\] Now use those formulas to convert our Estimate values into the hazard metric, as shown in the last two columns of Table 12.3. pars %&gt;% filter(par != &quot;pt&quot;) %&gt;% mutate(Estimate = if_else(str_detect(link, &quot;logit&quot;), 1 / (1 + exp(-1 * Estimate)), 1 - exp(-exp(Estimate)))) %&gt;% select(par, link, Estimate) %&gt;% pivot_wider(names_from = link, values_from = Estimate) %&gt;% select(par, `clog-log`, logit) %&gt;% mutate(`clog-log - logit` = `clog-log` - logit) %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 6 x 4 ## par `clog-log` logit `clog-log - logit` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 d7 0.05 0.0473 0.0027 ## 2 d8 0.0249 0.0235 0.0014 ## 3 d9 0.0938 0.0927 0.001 ## 4 d10 0.139 0.139 -0.0001 ## 5 d11 0.157 0.161 -0.0037 ## 6 d12 0.229 0.236 -0.0064 For kicks, we threw in a column of their differences. The clog-log - logit column highlights how in general, fitted hazard functions from models estimated with both link functions will be indistinguishable unless hazard is high, once again suggesting that the quality of the estimates does not provide a rationale for selecting one of these link functions over the other. (p. 424) Now focus on the pt parameter for both models. In both cases, we antilog [i.e., exponentiate] parameter estimates, but whereas an antilogged [i.e., exponentiated] coefficient from a model with a logit link is an odds ratio, an antilogged coefficient from a model with a clog-log link is a hazard ratio. (p. 424) Here’s that in a plot. p1 &lt;- posterior_samples(fit11.8) %&gt;% ggplot(aes(x = b_pt %&gt;% exp(), y = 0)) + geom_halfeyeh(.width = c(.5, .95)) + labs(title = &quot;fit11.8 (logit link)&quot;, subtitle = &quot;Exponentiating this parameter yields an odds ratio.&quot;) p2 &lt;- posterior_samples(fit12.15) %&gt;% ggplot(aes(x = b_pt %&gt;% exp(), y = 0)) + geom_halfeyeh(.width = c(.5, .95)) + labs(title = &quot;fit12.15 (clog-log link)&quot;, subtitle = &quot;Exponentiating this parameter yields a hazard ratio.&quot;) (p1 | p2) &amp; scale_x_continuous(&quot;b_pt (exponentiated)&quot;, limits = c(1, 5)) &amp; scale_y_continuous(NULL, breaks = NULL) &amp; theme(panel.grid = element_blank()) For the clog-log model, then, in every grade from 7th to 12th, we estimate the hazard of first intercourse for boys who experienced a parenting transition to be [about] 2.2 times the hazard for their peers raised with both biological parents. This interpretation contrasts with that from the model with a logit link, which suggests that the odds of first intercourse are [about] 2.4 times the odds for boys who experienced a parenting transition. (pp. 424–425, emphasis in the original) 12.2.3 Choosing between logit and clog-log links for discrete-time hazard models. The primary advantage of the clog-log link is that in invoking a proportional hazards assumption it yields a direct analog to the continuous time hazard model…. If you believe that the underlying metric for time is truly continuous and that the only reason you observe discretized values is due to measurement difficulties, a model specified with a clog-log link has much to recommend it…. [Yet,] if data are collected in truly discrete time, the clog-log specification has no particular advantage. As Beck (1999); Beck, Katz, and Tucker (1998); and Sueyoshi (1995) argue, the proportional hazards assumption is no more sacred than the proportional odds assumption, and while consistency across models is noble, so, too, is simplicity (which decreases the chances of mistake). (p. 426) 12.3 Time-varying predictors “Discrete-time survival analysis adopts naturally to the inclusion of time-varying predictors. Because models are fit using a person-period data set, a time-varying predictor simply takes on its appropriate value for each person in each period” (p. 427). 12.3.1 Assumptions underlying a model with time-varying predictors. Load the depression data from Wheaton, Rozell, and Hall (1997). depression_pp &lt;- read_csv(&quot;data/depression_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(depression_pp) ## Rows: 36,997 ## Columns: 22 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ onset &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ age &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 3… ## $ censor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ censage &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 3… ## $ aged &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ nsibs &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,… ## $ sibs12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ sibs34 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ sibs56 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ sibs78 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ sibs9plus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ bigfamily &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ period &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,… ## $ event &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pd &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pdnow &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ one &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ age_18 &lt;dbl&gt; -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9… ## $ age_18sq &lt;dbl&gt; 196, 169, 144, 121, 100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64,… ## $ age_18cub &lt;dbl&gt; -2744, -2197, -1728, -1331, -1000, -729, -512, -343, -216, -125, -64, -27, -8, -1, 0, 1, … Here is the participant age range. range(depression_pp$age) ## [1] 17 57 We might count how many participants experienced a parantal divorce, pd, like this. depression_pp %&gt;% group_by(id) %&gt;% summarise(pd = if_else(sum(pd) &gt; 0, 1, 0)) %&gt;% ungroup() %&gt;% count(pd) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 x 3 ## pd n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1248 89.6 ## 2 1 145 10.4 Here we fit the discrete hazard model based on a quadratic treatment of \\(\\text{age} - 18\\) with and the time-varying predictor pd and without/with the time-invariant predictor female. fit12.16 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.16&quot;) fit12.17 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.17&quot;) Check the summaries. print(fit12.16) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd ## Data: depression_pp (Number of observations: 36997) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.25 0.08 -4.40 -4.10 1.00 1752 1516 ## age_18 0.06 0.01 0.04 0.08 1.00 1467 1608 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 2573 2358 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 2195 2469 ## pd 0.42 0.16 0.09 0.73 1.00 1402 1354 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit12.17) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female ## Data: depression_pp (Number of observations: 36997) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.58 0.11 -4.79 -4.38 1.00 1762 2512 ## age_18 0.06 0.01 0.04 0.08 1.00 2162 2085 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 3435 2851 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 2695 2648 ## pd 0.40 0.16 0.07 0.71 1.00 1319 1178 ## female 0.54 0.11 0.33 0.76 1.00 1982 2294 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we make our version of Figure 12.4, we’ll need to make an aggregated version of the data, which will allow us to replicate the dots and plus signs. We’ll use the same basic wrangling steps from when we made sex_aggregated from back in Chapter 11. depression_aggregated &lt;- depression_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(age_18) %&gt;% count(event, pd) %&gt;% ungroup() %&gt;% complete(age_18, event, pd) %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0)), age = age_18 + 18, total = event + no_event) %&gt;% mutate(proportion = event / total) %&gt;% mutate(logit = log(proportion / (1 - proportion))) depression_aggregated ## # A tibble: 72 x 8 ## age_18 pd event no_event age total proportion logit ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -14 pd = 0 1 1353 4 1354 0.000739 -7.21 ## 2 -14 pd = 1 NA 39 4 NA NA NA ## 3 -13 pd = 0 NA 1344 5 NA NA NA ## 4 -13 pd = 1 NA 48 5 NA NA NA ## 5 -12 pd = 0 4 1333 6 1337 0.00299 -5.81 ## 6 -12 pd = 1 1 54 6 55 0.0182 -3.99 ## 7 -11 pd = 0 5 1322 7 1327 0.00377 -5.58 ## 8 -11 pd = 1 NA 60 7 NA NA NA ## 9 -10 pd = 0 2 1313 8 1315 0.00152 -6.49 ## 10 -10 pd = 1 1 66 8 67 0.0149 -4.19 ## # … with 62 more rows Now make Figure 12.4. nd &lt;- crossing(age_18 = -14:21, pd = 0:1) %&gt;% mutate(age = age_18 + 18) # hazard (top panel) p1 &lt;- fitted(fit12.16, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0))) %&gt;% ggplot(aes(x = age, group = pd, color = pd)) + geom_line(aes(y = Estimate)) + geom_point(data = depression_aggregated, aes(y = proportion, shape = pd), show.legend = F) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 40)) + scale_y_continuous(&quot;proportion experiencing event&quot;, limits = c(0, 0.06)) # logit(hazard) (top panel) p2 &lt;- fitted(fit12.16, newdata = nd, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0))) %&gt;% ggplot(aes(x = age, group = pd, color = pd)) + geom_line(aes(y = Estimate)) + geom_point(data = depression_aggregated, aes(y = logit, shape = pd), show.legend = F) + scale_x_continuous(breaks = 0:8 * 5, limits = c(0, 40)) + scale_y_continuous(&quot;logit(proportion experiencing event)&quot;, limits = c(-8, -2)) # combine ( (p1 / p2) &amp; scale_shape_manual(values = c(3, 16)) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; theme(panel.grid = element_blank()) ) + plot_layout(guides = &quot;collect&quot;) 12.3.2 Interpreting and displaying time-varying predictors’ effects. We jumped the gun a little, but to repeat: you can fit a discrete-time hazard model with time-varying predictors using exactly the same strategies presented in chapter 11. In the person-period data set, use a logistic regression routine to regress the event indicator on variables representing the main effect of TIME and the desired predictors. (pp. 434–435, emphasis in the original) For example, here is the statistical formula we used when we fit fit12.17: \\[\\begin{align*} \\text{event}_{ij} &amp; = \\operatorname{Binomial}(n = 1, p_{ij}) \\\\ \\operatorname{logit} (p_{ij}) &amp; = [\\alpha_0 + \\alpha_1 (\\text{age}_{ij} - 18) + \\alpha_2 (\\text{age}_{ij} - 18)^2 + \\alpha_3 (\\text{age}_{ij} - 18)^3] \\\\ &amp; \\; \\; + [\\beta_1 \\text{pd}_{ij} + \\beta_2 \\text{female}_{i}] \\\\ \\alpha_0, ..., \\alpha_3 &amp; \\sim \\operatorname{Normal}(0, 4) \\\\ \\beta_1 \\text{ and } \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 4), \\end{align*}\\] where \\(\\operatorname{logit} (p_{ij}) = \\operatorname{logit} \\hat h(t_{ij})\\). If you’d like to compare our results for those displayed by Singer and Willett in Equation 12.8, here are our posterior means. fixef(fit12.17)[, 1] %&gt;% round(digits = 4) ## Intercept age_18 Iage_18E2 Iage_18E3 pd female ## -4.5830 0.0599 -0.0075 0.0002 0.3981 0.5435 The values are pretty close. Though we won’t compare fit12.16 and fit12.17 using deviances, we will use information criteria. Before you execute this code on your end, heads up! In my experience, large data sets like this often result in long computation times for Bayesian information criteria. Between the two, the LOO often takes longer. So here we’ll go with the WAIC. Even so, it took just under four hours to execute both add_criterion() lines. Proceed with causion. fit12.16 &lt;- add_criterion(fit12.16, &quot;waic&quot;) # 1.833328 hours fit12.17 &lt;- add_criterion(fit12.17, &quot;waic&quot;) # 1.901613 hours loo_compare(fit12.16, fit12.17, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.17 0.0 0.0 -2075.6 86.0 6.0 0.4 4151.3 171.9 ## fit12.16 -12.1 5.0 -2087.7 86.4 5.0 0.4 4175.5 172.8 model_weights(fit12.16, fit12.17, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.16 fit12.17 ## 0 1 Based on both the WAIC difference and the WAIC weights, fit12.17 is the clear favorite. From that model, Here’s a plot of the antilogged (i.e., exponentiated) posteriors for pd and female, which yields their odds ratios. posterior_samples(fit12.17) %&gt;% pivot_longer(b_pd:b_female) %&gt;% mutate(`odds ratio` = exp(value)) %&gt;% ggplot(aes(x = `odds ratio`, y = name)) + geom_halfeyeh(.width = c(.5, .95), normalize = &quot;xy&quot;) + ylab(NULL) + coord_cartesian(ylim = c(1.4, 2.4)) + theme(panel.grid = element_blank()) Here is our version of Figure 12.5. nd &lt;- crossing(female = 0:1, pd = 0:1) %&gt;% expand(nesting(female, pd), age_18 = -14:21) %&gt;% mutate(age = age_18 + 18) f &lt;- fitted(fit12.17, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(sex = if_else(female == 0, &quot;male&quot;, &quot;female&quot;), pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0))) # hazard (top panel) p1 &lt;- f %&gt;% ggplot(aes(x = age, group = pd, color = pd, fill = pd)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5, size = 0) + geom_line(aes(y = Estimate)) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 40)) + scale_y_continuous(&quot;fitted hazard&quot;, limits = c(0, 0.04)) # survival (bottom panel) p2 &lt;- f %&gt;% group_by(sex, pd) %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = ~cumprod(1 - .)) %&gt;% ggplot(aes(x = age, group = pd, color = pd, fill = pd)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5, size = 0) + geom_line(aes(y = Estimate)) + scale_x_continuous(breaks = 0:8 * 5, limits = c(0, 40)) + scale_y_continuous(&quot;fitted survival probability&quot;, limits = c(0, 1)) + theme(strip.background.x = element_blank(), strip.text.x = element_blank()) # combine ( (p1 / p2) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; theme(panel.grid = element_blank()) &amp; facet_wrap(~sex) ) + plot_layout(guides = &quot;collect&quot;) Note how with this many age levels, our discrete-time models are beginning to look like continuous-time models. 12.3.3 Two caveats: The problems of state and rate dependence. A time-varying predictor is state-dependent if its values at time \\(t_j\\) are affected by an individual’s state (event occurrence status) at time \\(t_j\\): \\(EVENT_{ij}\\). A time-varying predictor is rate-dependent if its values at time \\(t_j\\) are affected by the individuals value of hazard (the “rate”) at time \\(t_j\\): \\(h (t_{ij})\\). (p. 440, emphasis in the original) 12.4 The linear additivity assumption: Uncovering violations and simple solutions Because the focus on hazard causes you to analyze group level summaries, model violations can be more difficult to discern [than in other kinds of regression models]. We therefore devote this section to introducing practical strategies for diagnosing and correcting violations of the linear additivity assumption. (p. 443) 12.4.1 Interactions between substantive predictors. We do not advocate fishing expeditions. Open searches for interactions can be counterproductive, leading to the discovery of many “effects” that are little more than sampling variation. But there are at least two circumstances when a guided search for interactions is crucial: When theory (or common sense!) suggests that two (or more) predictors will interact in the prediction of the outcome. If you hypothesize the existence of interactions a priori, your search will be targeted and efficient. When examining the effects of “question” predictor(s), variables whose effects you intend to emphasize in your report. You need to be certain that these predictors’ effects do not differ according ot levels of other important predictors, lest you misinterpret your major findings. With this in mind, we now demonstrate how to (1) explore your data for the possibility of statistical interactions; and (2) include the additional appropriate terms when necessary. (p. 444, emphasis in the original) Load the data from Keiley and Martin (2002)2. firstarrest_pp &lt;- read_csv(&quot;data/firstarrest_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(firstarrest_pp) ## Rows: 15,834 ## Columns: 19 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,… ## $ time &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 17, 17, 17, 17, … ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,… ## $ abused &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,… ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ ablack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ d8 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ d9 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,… ## $ d10 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ d11 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ d13 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ d14 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ d15 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ d16 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,… ## $ d17 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,… ## $ d18 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ period &lt;dbl&gt; 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13… ## $ event &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,… The total \\(N\\) is 1553. firstarrest_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1553 \\(n = 887\\) were abused. firstarrest_pp %&gt;% filter(abused == 1) %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 887 \\(n = 342\\) were arrested between the ages of 8 and 18. firstarrest_pp %&gt;% filter(censor == 0) %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 342 Since the two focal predictors in this section with be black and abused, here are the counts broken down by both. firstarrest_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(black, abused) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 4 x 4 ## black abused n percent ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0 434 27.9 ## 2 0 1 605 39.0 ## 3 1 0 232 14.9 ## 4 1 1 282 18.2 Here’s how to hand compute the values for the sample logit(hazard) functions depicted in the top panels of Figure 12.6. # wrangle firstarrest_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(period) %&gt;% count(event, black, abused) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% drop_na(event) %&gt;% mutate(total = event + no_event, logit = log(event / total / (1 - event / total)), race = factor(ifelse(black == 1, &quot;Black&quot;, &quot;White&quot;), levels = c(&quot;White&quot;, &quot;Black&quot;)), abused = ifelse(abused == 1, &quot;abused&quot;, &quot;not abused&quot;) %&gt;% factor()) %&gt;% # plot! ggplot(aes(x = period, y = logit)) + geom_line(aes(color = abused, group = abused)) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_x_continuous(&quot;age&quot;, breaks = 7:19, limits = c(7, 19)) + scale_y_continuous(&quot;sample logit(hazard)&quot;, limits = c(-7, -2)) + theme(panel.grid = element_blank()) + facet_wrap(~race) For kicks and giggles, it might informative to fit a series of subset Bayesian models to make similar versions of those sample hazard functions. Before we fit the models, let’s make our lives easier and make a factor version of our time variable, period. firstarrest_pp &lt;- firstarrest_pp %&gt;% mutate(period_f = factor(period)) Fit the four subset models. # white, not abused fit12.18 &lt;- brm(data = firstarrest_pp %&gt;% filter(black == 0 &amp; abused == 0), family = binomial, event | trials(1) ~ 0 + period_f, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.18&quot;) # white, abused fit12.19 &lt;- update(fit12.18, newdata = firstarrest_pp %&gt;% filter(black == 0 &amp; abused == 1), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.19&quot;) # black, not abused fit12.20 &lt;- update(fit12.18, newdata = firstarrest_pp %&gt;% filter(black == 1 &amp; abused == 0), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.20&quot;) # black, abused fit12.21 &lt;- update(fit12.18, newdata = firstarrest_pp %&gt;% filter(black == 1 &amp; abused == 1), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.21&quot;) We might use fitted() via our custom make_fitted() function to perform some of the pre-plotting computation and wrangling. nd &lt;- tibble(period_f = 8:18) # this will simplify the `mutate()` code below models &lt;- c(&quot;constant&quot;, &quot;linear&quot;, &quot;quadratic&quot;, &quot;cubic&quot;, &quot;general&quot;) f &lt;- bind_rows(make_fitted(fit12.18, scale = &quot;linear&quot;), # white, not abused make_fitted(fit12.19, scale = &quot;linear&quot;), # white, abused make_fitted(fit12.20, scale = &quot;linear&quot;), # black, not abused make_fitted(fit12.21, scale = &quot;linear&quot;)) %&gt;% # black, abused mutate(race = factor(rep(c(&quot;White&quot;, &quot;Black&quot;), each = n() / 2), levels = c(&quot;White&quot;, &quot;Black&quot;)), abuse = rep(c(&quot;not abused&quot;, &quot;abused&quot;, &quot;not abused&quot;, &quot;abused&quot;), each = n() / 4)) # what have we done? glimpse(f) ## Rows: 44 ## Columns: 7 ## $ Estimate &lt;dbl&gt; -6.128145, -7.755830, -5.423982, -4.486341, -7.810388, -4.115688, -3.844492, -3.036432, -… ## $ Est.Error &lt;dbl&gt; 0.9680716, 1.7239523, 0.7002682, 0.4659827, 1.8007659, 0.3791418, 0.3375237, 0.2284309, 0… ## $ Q2.5 &lt;dbl&gt; -8.365665, -11.947351, -6.983962, -5.506780, -12.087179, -4.930636, -4.552779, -3.508916,… ## $ Q97.5 &lt;dbl&gt; -4.586103, -5.231566, -4.234613, -3.673362, -5.144784, -3.418779, -3.236886, -2.615172, -… ## $ period_f &lt;int&gt; 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9,… ## $ race &lt;fct&gt; White, White, White, White, White, White, White, White, White, White, White, White, White… ## $ abuse &lt;chr&gt; &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not … Now plot. f %&gt;% ggplot(aes(x = period_f, y = Estimate, group = abuse, color = abuse)) + geom_line() + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_x_continuous(&quot;age&quot;, breaks = 7:19, limits = c(7, 19)) + ylab(&quot;sample logit(hazard)&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~race) Notice how this looks different from the version of the plot, above, in that there are five points with very low values on the \\(y\\)-axis. Did you notice the drop_na(event) line in the code when we computed the sample logit(hazard) values by hand? Those points with missing values in the data are what caused those very low log(hazard) estimates in the models. Next we’ll fit the primary statistical models with both black and abused, without and with their interaction term, based on the full data set. You’ll see that when we use all cases, those odd low logit(hazard) values go away. fit12.22 &lt;- brm(data = firstarrest_pp, family = binomial, event | trials(1) ~ 0 + d8 + d9 + d10 + d11 + d12 + d13 + d14 + d15 + d16 + d17 + d18 + abused + black, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.22&quot;) fit12.23 &lt;- update(fit12.22, newdata = firstarrest_pp, event | trials(1) ~ 0 + d8 + d9 + d10 + d11 + d12 + d13 + d14 + d15 + d16 + d17 + d18 + abused + black + abused:black, chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.23&quot;) Let’s compare the models with information criteria. Not unlike with fit12.16 and fit12.17, these are big models for a big data set and it took about an hour on my 2012-era laptop to return the WAIC. The LOO would likely have taken longer. Proceed with caution. fit12.22 &lt;- add_criterion(fit12.22, &quot;waic&quot;) # 48.72123 mins fit12.23 &lt;- add_criterion(fit12.23, &quot;waic&quot;) # 13.57897 mins loo_compare(fit12.22, fit12.23, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.23 0.0 0.0 -1538.9 65.4 14.1 1.1 3077.8 130.7 ## fit12.22 -0.8 2.2 -1539.7 65.3 12.9 1.0 3079.4 130.6 model_weights(fit12.22, fit12.23, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.22 fit12.23 ## 0.314 0.686 Our results diverge a bit from those in the text. For the deviance test on page 446, Singer and Willett reported a difference of \\(4.05 \\; (p &lt; .05, df = 1)\\) in favor of the full model (i.e., fit12.23). Though both our WAIC difference score and the WAIC weights favor fit12.23, it’s by a hair. “As in linear (or logistic) regression, we interpret interaction effects by simultaneously considering all the constituent parameters, for the cross-product term and its main-effect components” (p. 446, emphasis in the original). Rather than the point-estimate table Singer and Willett displayed at the bottom of the page, we’ll present the full posterior distributions odds ratios in a faceted plot. posterior_samples(fit12.23) %&gt;% mutate(iter = 1:n()) %&gt;% expand(nesting(iter, b_abused, b_black, `b_abused:black`), abused = 0:1, black = 0:1) %&gt;% mutate(`odds ratio` = exp(b_abused * abused + b_black * black + `b_abused:black` * abused * black), abused = str_c(&quot;abused = &quot;, abused), black = str_c(&quot;black = &quot;, black)) %&gt;% ggplot(aes(x = `odds ratio`, y = 0)) + geom_halfeyeh(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_grid(abused~black) Here’s our version of the lower panel of Figure 12.6. # define the `newdata` nd &lt;- firstarrest_pp %&gt;% distinct(abused, black, d8, d9, d10, d11, d12, d13, d14, d15, d16, d17, d18, period) # use `fitted()` and wrangle make_fitted(fit12.23, scale = &quot;linear&quot;) %&gt;% bind_cols(nd) %&gt;% mutate(abused = ifelse(abused == 1, &quot;abused&quot;, &quot;not abused&quot;) %&gt;% factor(), sex = factor(ifelse(black == 1, &quot;Black&quot;, &quot;White&quot;), levels = c(&quot;White&quot;, &quot;Black&quot;))) %&gt;% # plot! ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = abused, color = abused)) + geom_ribbon(alpha = 1/5, size = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_x_continuous(&quot;age&quot;, breaks = 7:19, limits = c(7, 19)) + ylab(&quot;fitted logit(hazard)&quot;) + coord_cartesian(ylim = c(-8, -2)) + theme(panel.grid = element_blank()) + facet_wrap(~sex) 12.4.2 Nonlinear effects. There are two general strategies for exploring the linearity assumption. The simplest approach–although somewhat limited–is to fit additional models, replacing the raw predictor with a re-expressed version. Although the additional models also invoke a linearity constraint, use of re-expressed predictors guarantees that the effects represent nonlinear relationships for the raw predictors. The ladder of power (section 6.2.1) provides a dizzying array of options. The second approach is to categorize each continuous variable into a small number of groups, create a series of dummy variables representing group membership, and visually examine the pattern of parameter estimates for consecutive dummies to deduce the appropriate functional form. If the pattern is linear, retain the predictor in its raw state; if not, explore an alternative specification. As the first approach is straightforward, we illustrate the second, using the depression onset data presented in section 12.3. (pp. 447–448, emphasis in the original) Here’s another look at those depression_pp data. depression_pp %&gt;% glimpse() ## Rows: 36,997 ## Columns: 22 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ onset &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ age &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 3… ## $ censor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ censage &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 3… ## $ aged &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ nsibs &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,… ## $ sibs12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ sibs34 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ sibs56 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ sibs78 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ sibs9plus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ bigfamily &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ period &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,… ## $ event &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pd &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pdnow &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ one &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ age_18 &lt;dbl&gt; -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9… ## $ age_18sq &lt;dbl&gt; 196, 169, 144, 121, 100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64,… ## $ age_18cub &lt;dbl&gt; -2744, -2197, -1728, -1331, -1000, -729, -512, -343, -216, -125, -64, -27, -8, -1, 0, 1, … Our three models will be cubic with respect to our time variable, age_18. The focal predictor whose form (non)linear form we’re interested in is nsibs. Singer and Willett’s first model, Model A (see Table 12.4, p. 449), treated nsibs as linear. Fit the model with brms. # model a (linear) # 3.409222 hours fit12.24 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + nsibs, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.24&quot;) Check the model summary. print(fit12.24) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + nsibs ## Data: depression_pp (Number of observations: 36997) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.35 0.12 -4.59 -4.12 1.00 2270 2430 ## age_18 0.06 0.01 0.04 0.08 1.00 2415 2116 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 4695 3220 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 3366 3561 ## pd 0.36 0.17 0.03 0.67 1.00 2050 1765 ## female 0.56 0.11 0.35 0.77 1.00 2711 2497 ## nsibs -0.08 0.02 -0.12 -0.04 1.00 2811 2330 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we invert the antilog of nsibs and summarize the posterior with median_qi(). posterior_samples(fit12.24) %&gt;% median_qi(1 / exp(b_nsibs)) %&gt;% mutate_if(is.double, round, digits = 3) ## 1/exp(b_nsibs) .lower .upper .width .point .interval ## 1 1.087 1.044 1.133 0.95 median qi Singer and Willett described nsibs as “highly skewed” (p. 448). Let’s take a look. depression_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(x = nsibs)) + geom_bar() + theme(panel.grid = element_blank()) Yep, sure is. Here’s a way to save Singer and Willette’s discretized version of nsibs and then break the cases down in terms of \\(n\\) and percentage, which matches nicely with the numbers at the bottom of page 449. depression_pp &lt;- depression_pp %&gt;% mutate(nsibs_cat = case_when( nsibs == 0 ~ &quot;0&quot;, nsibs %in% c(1, 2) ~ &quot;1 or 2&quot;, nsibs %in% c(3, 4) ~ &quot;3 or 4&quot;, nsibs %in% c(5, 6) ~ &quot;5 or 6&quot;, nsibs %in% c(7, 8) ~ &quot;7 or 8&quot;, nsibs &gt;= 9 ~ &quot;9 or more&quot; ) ) depression_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(nsibs_cat) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(digits = 1)) ## # A tibble: 6 x 3 ## nsibs_cat n percent ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 98 7 ## 2 1 or 2 672 48.2 ## 3 3 or 4 330 23.7 ## 4 5 or 6 159 11.4 ## 5 7 or 8 72 5.2 ## 6 9 or more 62 4.5 Now fit our version of Model B, in which we replace the original linear predictor nsibs with a series of dummies: sibs12, sibs34,…, sibs9plus. # model b (nonlinear) # about 5 hours fit12.25 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + sibs12 + sibs34 + sibs56 + sibs78 + sibs9plus, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.25&quot;) Here’s the model summary. print(fit12.25) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + sibs12 + sibs34 + sibs56 + sibs78 + sibs9plus ## Data: depression_pp (Number of observations: 36997) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.52 0.21 -4.93 -4.13 1.00 1181 1587 ## age_18 0.06 0.01 0.04 0.08 1.00 2925 2270 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 5508 2807 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 3698 3619 ## pd 0.36 0.16 0.03 0.66 1.00 2996 2466 ## female 0.56 0.11 0.35 0.77 1.00 3005 2586 ## sibs12 0.04 0.20 -0.35 0.42 1.00 1171 1745 ## sibs34 0.03 0.21 -0.39 0.44 1.00 1208 1727 ## sibs56 -0.49 0.26 -1.02 0.01 1.00 1334 1720 ## sibs78 -0.78 0.34 -1.46 -0.13 1.00 1251 1877 ## sibs9plus -0.68 0.35 -1.39 -0.03 1.00 1642 2049 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It might be easier to compare the various sibs* coefficients with a coefficient plot. posterior_samples(fit12.25) %&gt;% pivot_longer(contains(&quot;sibs&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;)) %&gt;% ggplot(aes(x = value, y = name)) + stat_intervalh(size = 5, .width = c(.1, .5, .9)) + scale_color_grey(&quot;CI level:&quot;, start = .8, end = .2) + labs(x = &quot;sibs coeficients&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Here is the breakdown by bigfamily. depression_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(bigfamily) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(digits = 2)) ## # A tibble: 2 x 3 ## bigfamily n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1100 79.0 ## 2 1 293 21.0 Now fit our version of Model C, the dichotomized bigfamily model. # model c (dichotomized) # 3.736503 hours fit12.26 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + bigfamily, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.26&quot;) Check the model summary. print(fit12.26) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + bigfamily ## Data: depression_pp (Number of observations: 36997) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.48 0.11 -4.69 -4.27 1.00 2079 2323 ## age_18 0.06 0.01 0.04 0.08 1.00 2889 2518 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 4086 3129 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 3344 3179 ## pd 0.35 0.16 0.04 0.66 1.00 2055 2071 ## female 0.56 0.11 0.35 0.77 1.00 2147 2338 ## bigfamily -0.62 0.14 -0.91 -0.34 1.00 2823 2254 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summaries look very similar to the values in the rightmost column of Table 12.4 in the text. Here is a look at the antilog of our bigfamily coefficient. # just to make the x-axis pretty breaks &lt;- c(exp(fixef(fit12.26)[&quot;bigfamily&quot;, c(1, 3:4)]), 1) %&gt;% as.vector() labels &lt;- c(exp(fixef(fit12.26)[&quot;bigfamily&quot;, c(1, 3:4)]), 1) %&gt;% round(digits = 3) %&gt;% as.vector() # plot! posterior_samples(fit12.26) %&gt;% ggplot(aes(x = exp(b_bigfamily), y = 0)) + geom_vline(xintercept = 1, color = &quot;white&quot;) + geom_halfeyeh(.width = c(.5, .95)) + scale_x_continuous(&quot;bigfamily odds ratio&quot;, breaks = breaks, labels = labels) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Now compute the WAIC estimates for fit12.24 through fit12.26 and compare them by their WAIC differences and WAIC weights. fit12.24 &lt;- add_criterion(fit12.24, criterion = &quot;waic&quot;) # 1.524841 hours fit12.25 &lt;- add_criterion(fit12.25, criterion = &quot;waic&quot;) # 1.285394 hours fit12.26 &lt;- add_criterion(fit12.26, criterion = &quot;waic&quot;) # 1.741655 hours loo_compare(fit12.24, fit12.25, fit12.26, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.26 0.0 0.0 -2066.3 85.7 6.8 0.5 4132.5 171.3 ## fit12.24 -2.9 2.9 -2069.2 85.8 7.1 0.5 4138.4 171.5 ## fit12.25 -3.6 1.1 -2069.9 85.8 10.8 0.7 4139.8 171.7 model_weights(fit12.24, fit12.25, fit12.26, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.24 fit12.25 fit12.26 ## 0.049 0.025 0.926 Our WAIC estimates are very similar to the AIC estimates presented in Table 12.4. By both the differences and the weights, fit12.16 (Model C) is the best among the three. Though its rank is decisive with the weights, it’s less impressive if you compare the se_diff values with the elpd_diff values. Those suggest there’s a lot of uncertainty in our WAIC differences. 12.5 The proportionality assumption: Uncovering violations and simple solutions All the discrete hazard models postulated so far invoke another common, but restrictive assumption: that each predictor has an identical effect in every time period under study. This constraint, known as the proportionality assumption, stipulates that a predictor’s effect does not depend on the respondent’s duration in the initial state…. Yet is it not possible, even likely, that the effects of some predictors will vary over time? (p. 451, emphasis in the original) 12.5.1 Discrete-time hazard models that do not invoke a proportionality assumption. “There are dozens of ways of violating the proportionality assumption” (p. 452). We see three such examples in panels B through D in Figure 12.7. Here’s our version of the plot. p1 &lt;- crossing(z = 0:1, x = 1:8) %&gt;% mutate(y = -2.1 + -0.2 * (x - 1) + 1.1 * z, z = factor(z)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z)) p2 &lt;- crossing(z = 0:1, x = 1:8) %&gt;% mutate(y = -4.8 + 0.28 * (x - 1) + 0.01 * z + 0.25 * x * z, z = factor(z)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z)) p3 &lt;- crossing(z = 0:1, x = 1:8) %&gt;% mutate(y = -4.8 + 0.25 * (x - 1) + 4.3 * z + -0.5 * x * z, z = factor(z)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z)) p4 &lt;- crossing(z1 = 0:1, x = 1:8) %&gt;% mutate(z2 = rep(0:1, times = n() / 2), y = -2.8 + -0.2 * (x - 1) + 1.8 * z1 + -1.4 * z1 * z2, z1 = factor(z1)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z1)) # combine ( (p1 + p2 + p3 + p4) + plot_annotation(tag_levels = &quot;A&quot;) ) &amp; scale_size_manual(values = c(1, 1/2)) &amp; scale_x_continuous(&quot;time period&quot;, breaks = 0:8, limits = c(0, 8)) &amp; scale_y_continuous(&quot;logit hazard&quot;, limits = c(-5, -0.5)) &amp; theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) In case is wasn’t clear, I just winged it on the equations for the y values in each subplot. If you’d like to annotate the subplots with the arrows and \\(\\beta\\) labels as depicted in the original Figure 12.7, consider it a homework exercise. 12.5.2 Investigating the proportionality assumption in practice. Load the data from Graham’s (1997) dissertation. mathdropout_pp &lt;- read_csv(&quot;data/mathdropout_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(mathdropout_pp) ## Rows: 9,558 ## Columns: 19 ## $ id &lt;dbl&gt; 201303, 201303, 201304, 201304, 201304, 201305, 201305, 201311, 201311, 201311, 201311, 2013… ## $ lastpd &lt;dbl&gt; 2, 2, 3, 3, 3, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 1, 5, 5, 5, 5, 5, 2, 2, 3, 3, 3,… ## $ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,… ## $ one &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ hs11 &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,… ## $ hs12 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,… ## $ coll1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,… ## $ coll2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ coll3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ fhs11 &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,… ## $ fhs12 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,… ## $ fcoll1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,… ## $ fcoll2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ fcoll3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ period &lt;dbl&gt; 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3,… ## $ event &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,… ## $ ltime &lt;dbl&gt; 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 0, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2,… ## $ fltime &lt;dbl&gt; 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2,… The data are composed of \\(n = 3790\\) high school students. mathdropout_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 3790 Here is the division by female. mathdropout_pp %&gt;% distinct(id, female) %&gt;% count(female) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 x 3 ## female n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1875 49.5 ## 2 1 1915 50.5 Singer and Willett also wrote: “only 93 men and 39 women took a mathematics class in each of the next five terms” (p. 456). I think this is a typo. Here’s the break down by censor and female at the fifth time period (period == 5). mathdropout_pp %&gt;% filter(period == 5) %&gt;% count(censor, female) ## # A tibble: 4 x 3 ## censor female n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 0 39 ## 2 0 1 39 ## 3 1 0 93 ## 4 1 1 51 Fitting the three models displayed in Table 12.5 is a mild extension from our previous models. From a brm() perspective, there’s nothing new, here. fit12.27 &lt;- brm(data = mathdropout_pp, family = binomial, event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.27&quot;) fit12.28 &lt;- brm(data = mathdropout_pp, family = binomial, event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.28&quot;) fit12.29 &lt;- brm(data = mathdropout_pp, family = binomial, event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female + fltime, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.29&quot;) Heads up about the formula for fit12.28 (Model B). Many of the examples in the text and the corresponding data sets we’ve been working with included pre-computed interaction terms. We have been quietly ignoring those and making our interactions by hand in our formula arguments. Here we went ahead with the text and just used event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3. If you wanted a more verbose version of that code, we could have specified either event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female:hs11 + female:hs12 + female:coll1 + female:coll2 + female:coll3 or event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female:(hs11 + hs12 + coll1 + coll2 + coll3). All three return the same results. Anyway, let’s check the model summaries. print(fit12.27) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female ## Data: mathdropout_pp (Number of observations: 9558) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## hs11 -2.13 0.06 -2.25 -2.02 1.00 3632 3041 ## hs12 -0.94 0.05 -1.03 -0.85 1.00 3847 3351 ## coll1 -1.45 0.06 -1.57 -1.32 1.00 4234 2948 ## coll2 -0.62 0.08 -0.77 -0.47 1.00 4774 2821 ## coll3 -0.77 0.15 -1.06 -0.49 1.00 5414 3086 ## female 0.38 0.05 0.28 0.47 1.00 2812 3040 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit12.28) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3 ## Data: mathdropout_pp (Number of observations: 9558) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## hs11 -2.01 0.07 -2.15 -1.88 1.00 4113 2849 ## hs12 -0.96 0.06 -1.08 -0.85 1.00 4260 3156 ## coll1 -1.48 0.09 -1.66 -1.32 1.00 3919 2829 ## coll2 -0.71 0.10 -0.91 -0.52 1.00 4192 2962 ## coll3 -0.87 0.19 -1.25 -0.52 1.00 4536 3184 ## fhs11 0.16 0.10 -0.04 0.34 1.00 4154 2783 ## fhs12 0.42 0.08 0.26 0.57 1.00 4443 3030 ## fcoll1 0.44 0.12 0.21 0.67 1.00 3738 2962 ## fcoll2 0.58 0.14 0.30 0.86 1.00 3917 3029 ## fcoll3 0.60 0.29 0.05 1.18 1.00 4688 3271 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit12.29) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female + fltime ## Data: mathdropout_pp (Number of observations: 9558) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## hs11 -2.05 0.07 -2.18 -1.92 1.00 2788 3029 ## hs12 -0.93 0.05 -1.02 -0.83 1.00 3355 2872 ## coll1 -1.50 0.06 -1.63 -1.37 1.00 3676 3135 ## coll2 -0.72 0.09 -0.89 -0.55 1.00 3556 2766 ## coll3 -0.91 0.16 -1.22 -0.61 1.00 3500 2957 ## female 0.23 0.08 0.08 0.38 1.00 2275 2578 ## fltime 0.12 0.05 0.03 0.21 1.00 2328 2703 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The results are similar to those in Table 12.5. Based on fit12.27 (Model A), here is the posterior for the odds ratio for female. posterior_samples(fit12.27) %&gt;% ggplot(aes(x = exp(b_female), y = 0)) + geom_halfeyeh(.width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;odds ratio for female&quot;) + theme(panel.grid = element_blank()) With regard to the interaction terms for fit12.28 (Model B), Singer and Willett remarked: “Notice how the estimates rise over time” (p. 457). It might be easiest to observe that in a plot. posterior_samples(fit12.28) %&gt;% pivot_longer(starts_with(&quot;b_f&quot;)) %&gt;% mutate(name = factor(str_remove(name, &quot;b_&quot;), levels = c(str_c(&quot;fhs&quot;, 11:12), str_c(&quot;fcoll&quot;, 1:3)))) %&gt;% ggplot(aes(x = value, y = name)) + stat_intervalh(size = 5, .width = c(.1, .5, .9)) + scale_color_grey(&quot;CI level:&quot;, start = .8, end = .2) + labs(x = &quot;interaction terms&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Make and save the subplots for Figure 12.8. # Within-group sample hazard functions p1 &lt;- mathdropout_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(period) %&gt;% count(event, female) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(total = event + no_event, logit = log(event / total / (1 - event / total))) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = logit)) + geom_line(aes(color = female), show.legend = F) + scale_y_continuous(&quot;sample logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Within-group sample hazard functions&quot;) # Model A: Main effect of female nd &lt;- mathdropout_pp %&gt;% distinct(female, period, hs11, hs12, coll1, coll2, coll3) p2 &lt;- make_fitted(fit12.27, scale = &quot;linear&quot;) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = female, color = female)) + geom_ribbon(alpha = 1/5, size = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_y_continuous(&quot;fitted logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Model A: Main effect of female&quot;) # Model B: Completely general\\ninteraction between female and time nd &lt;- mathdropout_pp %&gt;% distinct(female, period, hs11, hs12, coll1, coll2, coll3, fhs11, fhs12, fcoll1, fcoll2, fcoll3) p3 &lt;- make_fitted(fit12.28, scale = &quot;linear&quot;) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = female, color = female)) + geom_ribbon(alpha = 1/5, size = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_y_continuous(&quot;fitted logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Model B: Completely general\\ninteraction between female and time&quot;) # Model C: Interaction\\nbetween female and time nd &lt;- mathdropout_pp %&gt;% distinct(female, period, hs11, hs12, coll1, coll2, coll3, fltime) p4 &lt;- make_fitted(fit12.29, scale = &quot;linear&quot;) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = female, color = female)) + geom_ribbon(alpha = 1/5, size = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_y_continuous(&quot;fitted logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Model C: Interaction\\nbetween female and time&quot;) Now combine the subplots, augment them in bulk, and return our version of Figure 12.8. (p1 + p2 + p3 + p4 + plot_layout(guides = &quot;collect&quot;)) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) &amp; scale_x_continuous(&quot;term&quot;, breaks = 1:5, labels = c(&quot;HS 11&quot;, &quot;HS 12&quot;, &quot;C 1&quot;, &quot;C 2&quot;, &quot;C 3&quot;)) &amp; coord_cartesian(ylim = c(-2.3, 0)) &amp; theme(panel.grid = element_blank()) Compute the information criteria for the three models. Since these were much faster to compute than for some of the earlier models in this chapter, we’ll go ahead and compute both the LOO and the WAIC. fit12.27 &lt;- add_criterion(fit12.27, c(&quot;loo&quot;, &quot;waic&quot;)) # 2.016566 mins fit12.28 &lt;- add_criterion(fit12.28, c(&quot;loo&quot;, &quot;waic&quot;)) # 1.755284 mins fit12.29 &lt;- add_criterion(fit12.29, c(&quot;loo&quot;, &quot;waic&quot;)) # 2.884658 mins Now compare the models with information criteria differences and weights. loo_compare(fit12.27, fit12.28, fit12.29, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.29 0.0 0.0 -4905.9 51.3 7.0 0.1 9811.8 102.6 ## fit12.28 -2.2 1.2 -4908.1 51.3 10.0 0.2 9816.2 102.6 ## fit12.27 -2.3 2.5 -4908.2 51.2 6.0 0.1 9816.4 102.5 loo_compare(fit12.27, fit12.28, fit12.29, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.29 0.0 0.0 -4905.9 51.3 7.0 0.1 9811.8 102.6 ## fit12.28 -2.2 1.2 -4908.1 51.3 10.0 0.2 9816.2 102.6 ## fit12.27 -2.3 2.5 -4908.2 51.2 6.0 0.1 9816.3 102.5 model_weights(fit12.27, fit12.28, fit12.29, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.27 fit12.28 fit12.29 ## 0.086 0.092 0.822 model_weights(fit12.27, fit12.28, fit12.29, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.27 fit12.28 fit12.29 ## 0.086 0.092 0.821 You’ll note both the LOO and WAIC estimates are very close to those displayed in Table 12.5. When we take their standard errors into account, fit12.29 (Model C) is marginally better than the other two models. fit12.29 also took most of the LOO and WAIC weights. How do we interpret the gender differential implied by Model C? Because we have centered TIME at 1, the coefficient for FEMALE (0.2275) estimates the differential in time period 1, which here is 11th grade. Antilogging yields 1.26, which leads us to estimate that in 11th grade, the odds of ending one’s mathematics course-taking career are 26% higher for females. (p. 460, emphasis in the original) Let’s examine those results with our Bayesian fit. posterior_samples(fit12.29) %&gt;% mutate(`log odds` = b_female, `odds ratio` = exp(b_female)) %&gt;% pivot_longer(contains(&quot;odds&quot;)) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;the effects of female in two metrics&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) Now examine the odds ratio for three different educational periods. posterior_samples(fit12.29) %&gt;% mutate(`12th grade` = exp(b_female + b_fltime), `1st semester of college` = exp(b_female + 2 * b_fltime), `3rd semester of college` = exp(b_female + 4 * b_fltime)) %&gt;% pivot_longer(contains(&quot; &quot;)) %&gt;% ggplot(aes(x = value, y = 0)) + geom_halfeyeh(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;The interaction effect in different periods&quot;, x = &quot;odds ratio&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~name, scales = &quot;free&quot;) 12.6 The no unobserved heterogeneity assumption: No simple solution All the hazard models discussed in this book–both the discrete-time models we are discussing now and the continuous-time models we will soon introduce–impose an additional assumption to which we have alluded: the assumption of no unobserved heterogeneity. Every model assumes that the population hazard function for individual \\(i\\) depends only on his or her predictor values. Any pair of individuals who share identical predictor profiles will have identical population functions…. Many data sets will not conform to this assumption. As in the multilevel model for change (and regular regression for that matter), pairs of individuals who share predictor profiles are very likely to have different outcomes…. Unobserved heterogeneity can have serious consequences. In their classic 1985 paper, Vaupel and Yaskin elegantly demonstrate what they call “heterogeneity’s ruses”–that ability of unobserved heterogeneity to create the misimpression that a hazard function follows a particular form, when in fact it may not…. Is it possible to fit a hazard model that accounts for unobserved heterogeneity? As you might expect, doing so requires that we have either additional data (for example, data on repeated events within individuals) or that we invoke other–perhaps less tenable–assumptions about the distribution of event time errors (Aalen, 1988; Heckman and Singer, 1984; Vaupel, Manton, &amp; Stallard, 1979; Scheike &amp; Jensen, 1997; Mare, 1994). As a result, most empirical researchers–and we–proceed ahead, if not ignoring the problem, at least not addressing it. In the remainder of this book, we assume that all heterogeneity is observed and attributable to the predictors included in our models. 12.7 Residual analysis Before concluding that your model is sound, you should ascertain how well it performs for individual cases. As in regular regression, we [can] address this question by examining residuals. Residuals compare–usually through subtraction–an outcome’s “observed” value to its model-based “expected” value. For a discrete-time hazard model, a simple difference will not suffice because each person has not a single outcome but a set of outcomes–one for each time period when he or she was at risk. This suggests the need for a residual defined at the person-period level. A further complication is that the observed outcome in every time period has a value of either 0 or 1 while its expected value—the predicted hazard probability—lies between these extremes. (p. 463, emphasis in the original) Starting on page 464, Singer and Willett illustrated a residual analysis using Model D from Table 11.3. In the last chapter, we called that fit11.10. Here it is, again. fit11.10 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.10&quot;) With brms, users can extract the residuals of a brm() fit with the residuals() function. residuals(fit11.10) %&gt;% str() ## num [1:822, 1:4] -0.0968 -0.0526 0.8174 -0.0839 -0.0448 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; The output is similar to what we get from fitted(). We have a numeric array of 822 rows and 4 columns. There are 822 rows because that is the number of rows in the original data set with which we fit the model, sex_pp. sex_pp %&gt;% glimpse() ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9,… ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, … ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,… ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,… ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,… ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916,… Just as we often express the uncertainty in our Bayesian models with parameter summaries from the posterior, we also express the uncertainty of our residuals. Thus, the four columns returned by the residuals() function are the familiar summary columns of Estimate, Est.Error, Q2.5, and Q97.5. On page 465, Singer and Willett showcased the deviance residuals for eight participants. We’re going to diverge from them a little. In the plot, below, we’ll look at the residuals for the first 10 cases in the data, by period. residuals(fit11.10) %&gt;% data.frame() %&gt;% bind_cols(sex_pp) %&gt;% mutate(event = factor(event), period = factor(str_c(&quot;period &quot;, period), levels = str_c(&quot;period &quot;, 7:12))) %&gt;% # reduce the number of cases filter(id &lt; 11) %&gt;% ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = event)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(fatten = 2/3) + scale_color_viridis_d(option = &quot;A&quot;, end = .6) + scale_x_continuous(breaks = 1:10, labels = rep(c(1, &quot;&quot;, 10), times = c(1, 8, 1))) + ylab(&quot;residual&quot;) + theme(legend.position = &quot;top&quot;, panel.grid = element_blank()) + facet_wrap(~period, nrow = 1) As is often the case in coefficient plots, the dots are the posterior means and the intersecting lines are the percentile-based 95% intervals. In the sex_pp data, the event variable encodes when participants experience the event within a given time range. Hopefully the color coding highlights how with hazard models, the residuals are always positive when the criterion variable is a 1 and always negative when the criterion is 0. Now we have a bit of a handle on the output from residuals(), let’s plot in bulk. residuals(fit11.10) %&gt;% data.frame() %&gt;% bind_cols(sex_pp) %&gt;% mutate(event = factor(event)) %&gt;% ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = event)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(fatten = 3/4, alpha = 1/2) + scale_color_viridis_d(option = &quot;A&quot;, end = .6) + ylab(&quot;residual&quot;) + theme(legend.position = &quot;top&quot;, panel.grid = element_blank()) This plot is our analogue to the top portion of Singer and Willette’s Figure 12.9. But whereas we plotted our residual summaries, they plotted the expected values of their deviance residuals. In contrast with the material on page 464, I am not going to discuss deviance residuals or the sums of the squared deviance residuals. Our brms workflow offers an alternative. Before we offer our alternative, we might focus on deviance residuals for just a moment: Deviance residuals are so named because, when squared, they represent an individual’s contribution to the deviance statistic for that time period. The sum of the squared deviance residuals across all the records in a person-period data set yields the deviance statistic for the specified model…. The absolute value of a deviance residual indicates how well the model fits that person’s data for that period. Large absolute values identify person-period records whose outcomes are poorly predicted. (p. 464) Back in Chapter 11, we computed the LOO for fit11.10. Let’s take a look at that. loo(fit11.10) ## ## Computed from 4000 by 822 log-likelihood matrix ## ## Estimate SE ## elpd_loo -322.6 17.6 ## p_loo 8.0 0.6 ## looic 645.1 35.1 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Notice the part of the output that read “All Pareto k estimates are good (k &lt; 0.5).” We can pull those Pareto k estimates like so. loo(fit11.10)$diagnostics %&gt;% data.frame() %&gt;% glimpse() ## Rows: 822 ## Columns: 2 ## $ pareto_k &lt;dbl&gt; -0.032388134, -0.008484953, 0.086619632, -0.065923005, 0.016843525, -0.110736705, 0.005088… ## $ n_eff &lt;dbl&gt; 2357.788, 2687.321, 1818.382, 4990.905, 5142.514, 5254.677, 4909.712, 4457.979, 4186.437, … We formatted the output for convenience. Notice there are 822 rows—one for each case in the data. Almost like a computational byproduct, brms returned pareto_k and n_eff values when we computed the LOO estimates for the model. Our focus will be on the pareto_k column. Here are those pareto_k values in a plot. loo(fit11.10)$diagnostics %&gt;% data.frame() %&gt;% # attach the `id` values bind_cols(sex_pp) %&gt;% ggplot(aes(x = id, y = pareto_k)) + geom_point(alpha = 3/4) + geom_text(data = . %&gt;% filter(pareto_k &gt; .2), aes(x = id + 2, label = id), size = 3, hjust = 0) + theme(panel.grid = element_blank()) To learn the technical details about pareto_k, check out Vehtari, Gelman, and Gabry’s Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC or Vehtari, Simpson, Gelman, Yao, and Gabry’s Pareto smoothed importance sampling. In short, we can use pareto_k values to flag cases that were overly-influential on the model in a way that’s a little like Singer and Willett’s deviance residuals. As pointed out in the reference manual for the loo package, which brms used to compute all this, the makers of the loo package warn against pareto_k values when they get much larger than \\(0.5\\). We should be a little worried by values that exceed the \\(0.7\\) threshold and it’s very likely a problem when they get larger than \\(1\\). In this case, they’re all below \\(0.4\\) and all is good. To learn more about pareto_k values and what the loo package can do for you, check out Vehtari and Gabry’s vignette, Using the loo package. Reference Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY, US: Oxford University Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.3 patchwork_1.0.0 brms_2.12.0 Rcpp_1.0.4.6 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_3.0.0 ggplot2_3.3.0 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [6] base64enc_0.1-3 fs_1.4.1 rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [11] svUnit_0.7-12 DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 lubridate_1.7.8 ## [16] xml2_1.3.1 bridgesampling_1.0-0 knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 broom_0.5.5 dbplyr_1.4.2 shiny_1.4.0.2 compiler_3.6.3 ## [26] httr_1.4.1 backports_1.1.6 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] cli_2.0.2 later_1.0.0 htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.3 ## [36] igraph_1.2.5 coda_0.19-3 gtable_0.3.0 glue_1.4.0 reshape2_1.4.4 ## [41] cellranger_1.1.0 vctrs_0.2.4 nlme_3.1-144 crosstalk_1.1.0.1 xfun_0.13 ## [46] ps_1.3.2 rvest_0.3.5 mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [51] gtools_3.8.2 zoo_1.8-7 scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [56] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 StanHeaders_2.21.0-1 stringi_1.4.6 ## [66] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 pkgconfig_2.0.3 matrixStats_0.56.0 ## [71] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [76] tidyselect_1.0.0 processx_3.4.2 plyr_1.8.6 magrittr_1.5 bookdown_0.18 ## [81] R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [86] withr_2.1.2 xts_0.12-0 abind_1.4-5 modelr_0.1.6 crayon_1.3.4 ## [91] arrayhelpers_1.1-0 utf8_1.1.4 rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [96] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 digest_0.6.25 xtable_1.8-4 ## [101] httpuv_1.5.2 stats4_3.6.3 munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 Footnote In their reference section, Singer and Willett indicated this was a manuscript submitted for publication. To my knowledge, it was never published.↩ "]
]
