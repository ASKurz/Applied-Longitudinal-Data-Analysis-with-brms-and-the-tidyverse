[["index.html", "Applied longitudinal data analysis in brms and the tidyverse version 0.0.3 What and why Caution: Work in progress R setup License and citation", " Applied longitudinal data analysis in brms and the tidyverse version 0.0.3 A Solomon Kurz 2023-06-12 What and why This project is based on Singer and Willett’s classic (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence. You can download the data used in the text at http://www.bristol.ac.uk/cmm/learning/support/singer-willett.html and find a wealth of ideas on how to fit the models in the text at https://stats.idre.ucla.edu/other/examples/alda/. My contributions show how to fit these models and others like them within a Bayesian framework. I make extensive use of Paul Bürkner’s brms package (Bürkner, 2017, 2018, 2022b), which makes it easy to fit Bayesian regression models in R (R Core Team, 2022) using Hamiltonian Monte Carlo (HMC) via the Stan probabilistic programming language (Carpenter et al., 2017). Much of the data wrangling and plotting code is done with packages connected to the tidyverse (Wickham et al., 2019; Wickham, 2022). Caution: Work in progress This release contains drafts of Chapters 1 through 7 and 9 through 13. Chapters 1 through 7 provide the motivation and foundational principles for fitting longitudinal multilevel models. Chapters 9 through 13 motivation and foundational principles for fitting discrete-time survival analyses. In addition to fleshing out more of the chapters, I plan to add more goodies like introductions to multivariate longitudinal models and mixed-effect location and scale models. But there is no time-table for this project. To keep up with the latest changes, check in at the GitHub repository, https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse, or follow my announcements on twitter at https://twitter.com/SolomonKurz. R setup To get the full benefit from this ebook, you’ll need some software. Happily, everything will be free (provided you have access to a decent personal computer and an good internet connection). First, you’ll need to install R, which you can learn about at https://cran.r-project.org/. Though not necessary, your R experience might be more enjoyable if done through the free RStudio interface, which you can learn about at https://rstudio.com/products/rstudio/. Once you have installed R, execute the following to install the bulk of the add-on packages. This will probably take a few minutes to finish. Go make yourself a coffee. packages &lt;- c(&quot;bayesplot&quot;, &quot;brms&quot;, &quot;broom&quot;, &quot;devtools&quot;, &quot;flextable&quot;, &quot;GGally&quot;, &quot;ggmcmc&quot;, &quot;ggrepel&quot;, &quot;gtools&quot;, &quot;loo&quot;, &quot;patchwork&quot;, &quot;psych&quot;, &quot;Rcpp&quot;, &quot;remotes&quot;, &quot;rstan&quot;, &quot;StanHeaders&quot;, &quot;survival&quot;, &quot;tidybayes&quot;, &quot;tidyverse&quot;) install.packages(packages, dependencies = T) A couple of the other packages are not officially available via the Comprehensive R Archive Network (CRAN; https://cran.r-project.org/). You can download them directly from GitHub by executing the following. devtools::install_github(&quot;stan-dev/cmdstanr&quot;) devtools::install_github(&quot;rmcelreath/rethinking&quot;) It’s possible you’ll have problems installing some of these packages. Here are some likely suspects and where you can find help: for difficulties installing brms, go to https://github.com/paul-buerkner/brms#how-do-i-install-brms or search around in the brms section of the Stan forums; for difficulties installing cmdstanr, go to https://mc-stan.org/cmdstanr/articles/cmdstanr.html; for difficulties installing rethinking, go to https://github.com/rmcelreath/rethinking#quick-installation; and for difficulties installing rstan, go to https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started. License and citation This book is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details, here. In short, you can use my work. Just please give me the appropriate credit the same way you would for any other scholarly resource. Here’s the citation information: @book{kurzAppliedLongitudinalDataAnalysis2023, title = {Applied longitudinal data analysis in brms and the tidyverse}, author = {Kurz, A. Solomon}, year = {2023}, month = {6}, edition = {version 0.0.3}, url = {https://bookdown.org/content/4253/} } References Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017 Bürkner, P.-C. (2022b). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., &amp; Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01 R Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Wickham, H. (2022). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 "],["a-framework-for-investigating-change-over-time.html", "1 A Framework for Investigating Change over Time 1.1 When might you study change over time? 1.2 Distinguishing between two types of questions about change 1.3 Three important features of a study of change Session info", " 1 A Framework for Investigating Change over Time It is possible to measure change, and to do it well, if you have longitudinal data (D. Rogosa et al., 1982; Willett, 1989). Cross-sectional data–so easy to collect and so widely available–will not suffice. In this chapter, we describe why longitudinal data are necessary for studying change. (Singer &amp; Willett, 2003, p. 3, emphasis in the original) 1.1 When might you study change over time? Perhaps a better question is: When wouldn’t you? 1.2 Distinguishing between two types of questions about change On page 8, Singer and Willett proposed there are two fundamental questions for longitudinal data analysis: “How does the outcome change over time?” and “Can we predict differences in these changes?” Within the hierarchical framework, we often speak about two levels of change. We address within-individual change at level-1. The goal of a level-1 analysis is to describe the shape of each person’s individual growth trajectory. In the second stage of an analysis of change, known as level-2, we ask about interindividual differences in change… The goal of a level-2 analysis is to detect heterogeneity in change across individuals and to determine the relationship between predictors and the shape of each person’s individual growth trajectory. (p. 8, emphasis in the original) 1.3 Three important features of a study of change Three or more waves of data An outcome whose values change systematically over time A sensible metric for clocking time 1.3.1 Multiple waves of data. Singer and Willett criticized two-waves data on two grounds. First, it cannot tell us about the shape of each person’s individual growth trajectory, the focus of our level-1 question. Did all the change occur immediately after the first assessment? Was progress steady or delayed? Second, it cannot distinguish true change from measurement error. If measurement error renders pretest scores too low and posttest scores too high, you might conclude erroneously that scores increase over time when a longer temporal view would suggest the opposite. In statistical terms, two-waves studies cannot describe individual trajectories of change and they confound true change with measurement error (see D. Rogosa et al., 1982). (p. 10, emphasis in the original) I am not a fan of this ‘true change/measurement error’ way of speaking and would rather speak in terms of systemic and [seemingly] un-systemic changes among means and variances. Otherwise put, I’d rather speak in terms of trait and state. Two waves of data do not allow us to disentangle systemic mean differences from stable means and substantial variances for those means. Two waves of data do not allow us to disentangle changes in traits from stable traits but important differences in states. For an introduction to this way of thinking, check out Nezlek’s (2007) A multilevel framework for understanding relationships among traits, states, situations and behaviors. 1.3.2 A sensible metric for time. Choice of a time metric affects several interrelated decisions about the number and spacing of data collection waves…. Our overarching point is that there is no single answer to the seemingly simple question about the most sensible metric for time. You should adopt whatever scale makes most sense for your outcomes and your research question…. Our point is simple: choose a metric for the that reflect the cadence you expect to be most useful for your outcome. (p. 11) Data collection waves can be evenly spaced or not. E.g., if you anticipate a time period of rapid nonlinear change, it might be helpful to increase the density of assessments during that period. Everyone does not have to have the same assessment schedule. If all are assessed on the same schedule, we describe the data as time-structured. When assessment schedules vary across participants, the data are termed time-unstructured. The data are balanced if all participants have the same number of waves. Issues like attrition and so on lead to unbalanced data. Though they may have some pedagogical use, I have not found these terms useful in practice. 1.3.3 A continuous outcome that changes systematically over time. To my eye, the most interesting part of this section is the discussion of measurement validity over time. E.g., when we say the metric in which the outcome is measured must be preserved across time, we mean that the outcome scores must be equatable over time–a given value of the outcome on any occasion must represent the same “amount” of the outcome on every occasion. Outcome equatability is easiest to ensure when you use the identical instrument for measurement repeatedly over time. (p. 13) This isn’t as simple as is sounds. Though it’s beyond the scope of this project, you might learn more about this from a study of the longitudinal measurement invariance literature. To dive in, see the first couple chapters in Newsom’s (2015) text, Longitudinal structural equation modeling: A comprehensive introduction. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] digest_0.6.31 R6_2.5.1 bookdown_0.34 fastmap_1.1.1 ## [5] xfun_0.39 cachem_1.0.8 knitr_1.42 htmltools_0.5.5 ## [9] rmarkdown_2.21 cli_3.6.1 sass_0.4.6 jquerylib_0.1.4 ## [13] compiler_4.3.0 rstudioapi_0.14 tools_4.3.0 evaluate_0.21 ## [17] bslib_0.4.2 rlang_1.1.1 jsonlite_1.8.4 References Newsom, J. T. (2015). Longitudinal structural equation modeling: A comprehensive introduction. Routledge. http://www.longitudinalsem.com/ Nezlek, J. B. (2007). A multilevel framework for understanding relationships among traits, states, situations and behaviours. European Journal of Personality, 21(6), 789–810. https://doi.org/10.1002/per.640 Rogosa, D., Brandt, D., &amp; Zimowski, M. (1982). A growth curve approach to the measurement of change. Psychological Bulletin, 92(3), 726–748. https://doi.org/10.1037/0033-2909.92.3.726 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Willett, J. B. (1989). Some results on reliability for the longitudinal measurement of change: Implications for the design of studies of individual growth. Educational and Psychological Measurement, 49(3), 587–602. https://doi.org/10.1177/001316448904900309 "],["exploring-longitudinal-data-on-change.html", "2 Exploring Longitudinal Data on Change 2.1 Creating a longitudinal data set 2.2 Descriptive analysis of individual change over time 2.3 Exploring differences in change across people 2.4 Improving the precision and reliability of OLS single-level-Bayesian-estimated rates of change: Lessons for research design Session info", " 2 Exploring Longitudinal Data on Change Wise researchers conduct descriptive exploratory analyses of their data before fitting statistical models. As when working with cross-sectional data, exploratory analyses of longitudinal data con reveal general patterns, provide insight into functional form, and identify individuals whose data do not conform to the general pattern. The exploratory analyses presented in this chapter are based on numerical and graphical strategies already familiar from cross-sectional work. Owing to the nature of longitudinal data, however, they are inevitably more complex in this new setting. (Singer &amp; Willett, 2003, p. 16) 2.1 Creating a longitudinal data set In longitudinal work, data-set organization is less straightforward because you can use two very different arrangements: A person-level data set, in which each person has one record and multiple variables contain the data from each measurement occasion A person-period data set, in which each person has multiple records—one for each measurement occasion (p. 17, emphasis in the original) These are also sometimes referred to as the wide and long data formats, respectively. As you will see, we will use two primary functions from the tidyverse to convert data from one format to another. 2.1.1 The person-level data set. Here we load the person-level data from this UCLA web site. These are the NLY data (see Raudenbush &amp; Chan, 2016) shown in the top of Figure 2.1. library(tidyverse) tolerance &lt;- read_csv(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/tolerance1.txt&quot;, col_names = T) head(tolerance, n = 16) ## # A tibble: 16 × 8 ## id tol11 tol12 tol13 tol14 tol15 male exposure ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 2.23 1.79 1.9 2.12 2.66 0 1.54 ## 2 45 1.12 1.45 1.45 1.45 1.99 1 1.16 ## 3 268 1.45 1.34 1.99 1.79 1.34 1 0.9 ## 4 314 1.22 1.22 1.55 1.12 1.12 0 0.81 ## 5 442 1.45 1.99 1.45 1.67 1.9 0 1.13 ## 6 514 1.34 1.67 2.23 2.12 2.44 1 0.9 ## 7 569 1.79 1.9 1.9 1.99 1.99 0 1.99 ## 8 624 1.12 1.12 1.22 1.12 1.22 1 0.98 ## 9 723 1.22 1.34 1.12 1 1.12 0 0.81 ## 10 918 1 1 1.22 1.99 1.22 0 1.21 ## 11 949 1.99 1.55 1.12 1.45 1.55 1 0.93 ## 12 978 1.22 1.34 2.12 3.46 3.32 1 1.59 ## 13 1105 1.34 1.9 1.99 1.9 2.12 1 1.38 ## 14 1542 1.22 1.22 1.99 1.79 2.12 0 1.44 ## 15 1552 1 1.12 2.23 1.55 1.55 0 1.04 ## 16 1653 1.11 1.11 1.34 1.55 2.12 0 1.25 With person-level data, each participant has a single row. In these data, participants are indexed by their id number. To see how many participants are in these data, just count() the rows. tolerance %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 16 The nrow() function will work, too. tolerance %&gt;% nrow() ## [1] 16 With the base R cor() function, you can get the Pearson’s correlation matrix shown in Table 2.1. cor(tolerance[ , 2:6]) %&gt;% round(digits = 2) ## tol11 tol12 tol13 tol14 tol15 ## tol11 1.00 0.66 0.06 0.14 0.26 ## tol12 0.66 1.00 0.25 0.21 0.39 ## tol13 0.06 0.25 1.00 0.59 0.57 ## tol14 0.14 0.21 0.59 1.00 0.83 ## tol15 0.26 0.39 0.57 0.83 1.00 We used the round() function to limit the number of decimal places in the output. Leave it off and you’ll see cor() returns up to seven decimal places instead. It can be hard to see the patters within a matrix of numerals. It might be easier in a plot. cor(tolerance[ , 2:6]) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;row&quot;) %&gt;% pivot_longer(-row, names_to = &quot;column&quot;, values_to = &quot;correlation&quot;) %&gt;% mutate(row = factor(row) %&gt;% fct_rev(.)) %&gt;% ggplot(aes(x = column, y = row)) + geom_raster(aes(fill = correlation)) + geom_text(aes(label = round(correlation, digits = 2)), size = 3.5) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red4&quot;, limits = c(0, 1)) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme(axis.ticks = element_blank()) If all you wanted was the lower diagonal, you could use the lowerCor() function from the psych package (Revelle, 2022). psych::lowerCor(tolerance[ , 2:6]) ## tol11 tol12 tol13 tol14 tol15 ## tol11 1.00 ## tol12 0.66 1.00 ## tol13 0.06 0.25 1.00 ## tol14 0.14 0.21 0.59 1.00 ## tol15 0.26 0.39 0.57 0.83 1.00 For more ways to compute, organize, and visualize correlations within the tidyverse paradigm, check out the corrr package (Kuhn et al., 2020). 2.1.2 The person-period data set. Here are the person-period data (i.e., those shown in the bottom of Figure 2.1). tolerance_pp &lt;- read_csv(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/tolerance1_pp.txt&quot;, col_names = T) tolerance_pp %&gt;% slice(c(1:9, 76:80)) ## # A tibble: 14 × 6 ## id age tolerance male exposure time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 11 2.23 0 1.54 0 ## 2 9 12 1.79 0 1.54 1 ## 3 9 13 1.9 0 1.54 2 ## 4 9 14 2.12 0 1.54 3 ## 5 9 15 2.66 0 1.54 4 ## 6 45 11 1.12 1 1.16 0 ## 7 45 12 1.45 1 1.16 1 ## 8 45 13 1.45 1 1.16 2 ## 9 45 14 1.45 1 1.16 3 ## 10 1653 11 1.11 0 1.25 0 ## 11 1653 12 1.11 0 1.25 1 ## 12 1653 13 1.34 0 1.25 2 ## 13 1653 14 1.55 0 1.25 3 ## 14 1653 15 2.12 0 1.25 4 With data like these, the simple use of count() or nrow() won’t help us discover how many participants there are in the tolerance_pp data. One quick way is to count() the number of distinct() id values. tolerance_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 16 A fundamental skill is knowing how to convert longitudinal data in one format to the other. If you’re using packages within the tidyverse, the pivot_longer() function will get you from the person-level format to the person-period format. tolerance %&gt;% # this is the main event pivot_longer(-c(id, male, exposure), names_to = &quot;age&quot;, values_to = &quot;tolerance&quot;) %&gt;% # here we remove the `tol` prefix from the `age` values and then save the numbers as integers mutate(age = str_remove(age, &quot;tol&quot;) %&gt;% as.integer()) %&gt;% # these last two lines just make the results look more like those in the last code chunk arrange(id, age) %&gt;% slice(c(1:9, 76:80)) ## # A tibble: 14 × 5 ## id male exposure age tolerance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 0 1.54 11 2.23 ## 2 9 0 1.54 12 1.79 ## 3 9 0 1.54 13 1.9 ## 4 9 0 1.54 14 2.12 ## 5 9 0 1.54 15 2.66 ## 6 45 1 1.16 11 1.12 ## 7 45 1 1.16 12 1.45 ## 8 45 1 1.16 13 1.45 ## 9 45 1 1.16 14 1.45 ## 10 1653 0 1.25 11 1.11 ## 11 1653 0 1.25 12 1.11 ## 12 1653 0 1.25 13 1.34 ## 13 1653 0 1.25 14 1.55 ## 14 1653 0 1.25 15 2.12 You can learn more about the pivot_longer() function here and here. As hinted at in the above hyperlinks, the opposite of the pivot_longer() function is pivot_wider(). We can use pivot_wider() to convert the person-period tolerance_pp data to the same format as the person-level tolerance data. tolerance_pp %&gt;% # we&#39;ll want to add that `tol` prefix back to the `age` values mutate(age = str_c(&quot;tol&quot;, age)) %&gt;% # this variable is just in the way. we&#39;ll drop it select(-time) %&gt;% # here&#39;s the main action pivot_wider(names_from = age, values_from = tolerance) ## # A tibble: 16 × 8 ## id male exposure tol11 tol12 tol13 tol14 tol15 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0 1.54 2.23 1.79 1.9 2.12 2.66 ## 2 45 1 1.16 1.12 1.45 1.45 1.45 1.99 ## 3 268 1 0.9 1.45 1.34 1.99 1.79 1.34 ## 4 314 0 0.81 1.22 1.22 1.55 1.12 1.12 ## 5 442 0 1.13 1.45 1.99 1.45 1.67 1.9 ## 6 514 1 0.9 1.34 1.67 2.23 2.12 2.44 ## 7 569 0 1.99 1.79 1.9 1.9 1.99 1.99 ## 8 624 1 0.98 1.12 1.12 1.22 1.12 1.22 ## 9 723 0 0.81 1.22 1.34 1.12 1 1.12 ## 10 918 0 1.21 1 1 1.22 1.99 1.22 ## 11 949 1 0.93 1.99 1.55 1.12 1.45 1.55 ## 12 978 1 1.59 1.22 1.34 2.12 3.46 3.32 ## 13 1105 1 1.38 1.34 1.9 1.99 1.9 2.12 ## 14 1542 0 1.44 1.22 1.22 1.99 1.79 2.12 ## 15 1552 0 1.04 1 1.12 2.23 1.55 1.55 ## 16 1653 0 1.25 1.11 1.11 1.34 1.55 2.12 2.2 Descriptive analysis of individual change over time The following “descriptive analyses [are intended to] reveal the nature and idiosyncrasies of each person’s temporal pattern of growth, addressing the question: How does each person change over time” (p. 23)? 2.2.1 Empirical growth plots. Empirical growth plots show individual-level sequence in a variable of interest over time. We’ll put age on the \\(x\\)-axis, tolerance on the y-axis, and make our variant of Figure 2.2 with geom_point(). It’s the facet_wrap() part of the code that splits the plot up by id. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + geom_point() + coord_cartesian(ylim = c(1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ id) By default, ggplot2 sets the scales of the \\(x\\)- and \\(y\\)-axes to the same values across subpanels. If you’d like to free that constraint, play around with the scales argument within facet_wrap(). 2.2.2 Using a trajectory to summarize each person’s empirical growth record. If we wanted to connect the dots, we might just add a geom_line() line. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + geom_point() + geom_line() + coord_cartesian(ylim = c(1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ id) However, Singer and Willett recommend two other approaches: nonparametric smoothing parametric functions 2.2.2.1 Smoothing the empirical growth trajectory nonparametrically. For our version of Figure 2.3, we’ll use a loess smoother. When using the stat_smooth() function in ggplot2, you can control how smooth or wiggly the line is with the span argument. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + geom_point() + stat_smooth(method = &quot;loess&quot;, se = F, span = .9) + coord_cartesian(ylim = c(1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ id) 2.2.2.2 Smoothing the empirical growth trajectory using OLS single-level Bayesian regression. Although “fitting person-specific regression models, one individual at a time, is hardly the most efficient use of longitudinal data” (p. 28), we may as well play along with the text. It’ll have pedagogical utility. You’ll see. For this section, we’ll take a cue from Hadley Wickham and use group_by() and nest() to make a tibble composed of tibbles (i.e., a nested tibble). by_id &lt;- tolerance_pp %&gt;% group_by(id) %&gt;% nest() You can get a sense of what we did with head(). by_id %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: id [6] ## id data ## &lt;dbl&gt; &lt;list&gt; ## 1 9 &lt;tibble [5 × 5]&gt; ## 2 45 &lt;tibble [5 × 5]&gt; ## 3 268 &lt;tibble [5 × 5]&gt; ## 4 314 &lt;tibble [5 × 5]&gt; ## 5 442 &lt;tibble [5 × 5]&gt; ## 6 514 &lt;tibble [5 × 5]&gt; As indexed by id, each participant now has their own data set stored in the data column. To get a better sense, we’ll use our double-bracket subsetting skills to open up the first data set, the one for id == 9. If you’re not familiar with this skill, you can learn more from Chapter 9 of Roger Peng’s great (2019) online book, R programming for data science, or Jenny Bryan’s fun and useful talk, Behind every great plot there’s a great deal of wrangling. by_id$data[[1]] ## # A tibble: 5 × 5 ## age tolerance male exposure time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 2.23 0 1.54 0 ## 2 12 1.79 0 1.54 1 ## 3 13 1.9 0 1.54 2 ## 4 14 2.12 0 1.54 3 ## 5 15 2.66 0 1.54 4 Our by_id data object has many data sets stored in a higher-level data set. The code we used is verbose, but that’s what made it human-readable. Now we have our nested tibble, we can make a function that will fit the simple linear model tolerance ~ 1 + time to each id-level data set. Why use time as the predictor? you ask. On page 29 in the text, Singer and Willett clarified they fit their individual models with \\((\\text{age} - 11)\\) in order to have the model intercepts centered at 11 years old rather than 0. If we wanted to, we could make an \\((\\text{age} - 11)\\) variable like so. by_id$data[[1]] %&gt;% mutate(age_minus_11 = age - 11) ## # A tibble: 5 × 6 ## age tolerance male exposure time age_minus_11 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 2.23 0 1.54 0 0 ## 2 12 1.79 0 1.54 1 1 ## 3 13 1.9 0 1.54 2 2 ## 4 14 2.12 0 1.54 3 3 ## 5 15 2.66 0 1.54 4 4 Did you notice how our age_minus_11 variable is the same as the time variable already in the data set? Yep, that’s why we’ll be using time in the model. In our data, \\((\\text{age} - 11)\\) is encoded as time. Singer and Willett used OLS to fit their exploratory models. We could do that to with the lm() function and we will do a little of that in this project. But let’s get frisky and fit the models as Bayesians, instead. Our primary statistical package for fitting Bayesian models will be Paul Bürkner’s brms. Let’s open it up. library(brms) Since this is our first Bayesian model, we should start slow. The primary model-fitting function in brms is brm(). The function is astonishingly general and includes numerous arguments, most of which have sensible defaults. The primary two arguments are data and formula. I’m guessing they’re self-explanatory. I’m not going to go into detail on the three arguments at the bottom of the code. We’ll go over them later. For simple models like these, I would have omitted them entirely, but given the sparsity of the data (i.e., 5 data points per model), I wanted to make sure we gave the algorithm a good chance to arrive at reasonable estimates. fit2.1 &lt;- brm(data = by_id$data[[1]], formula = tolerance ~ 1 + time, prior = prior(normal(0, 2), class = b), iter = 4000, chains = 4, cores = 4, seed = 2, file = &quot;fits/fit02.01&quot;) We just fit a single-level Bayesian regression model for our first participant. We saved the results as an object named fit2.1. We can return a useful summary of fit2.1 with either print() or summary(). Since it’s less typing, we’ll use print(). print(fit2.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ 1 + time ## Data: by_id$data[[1]] (Number of observations: 5) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.91 0.53 0.82 3.01 1.00 3732 2555 ## time 0.12 0.22 -0.36 0.57 1.00 3877 2596 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.58 0.41 0.21 1.66 1.00 1675 2367 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The ‘Intercept’ and ‘time’ coefficients are the primary regression parameters. Also notice ‘sigma’, which is our variant of the residual standard error you might get from an OLS output (e.g., from base R lm()). Since we’re Bayesians, the output summaries do not contain \\(p\\)-values. But we do get posterior standard deviations (i.e., the ‘Est.Error’ column) and the upper- and lower-levels of the percentile-based 95% intervals. You probably heard somewhere that Bayesian statistics require priors. We can see what those were by pulling them out of our fit2.1 object. fit2.1$prior ## prior class coef group resp dpar nlpar lb ub source ## normal(0, 2) b user ## normal(0, 2) b time (vectorized) ## student_t(3, 2.1, 2.5) Intercept default ## student_t(3, 0, 2.5) sigma 0 default The prior in the top line, normal(0, 2), is for all parameters of class = b. We actually specified this in our brm() code, above, with the code snip: prior = prior(normal(0, 2), class = b). At this stage in the project, my initial impulse was to leave this line blank and save the discussion of how to set priors by hand for later. However, the difficulty is that the first several models we’re fitting are all of \\(n = 5\\). Bayesian statistics handle small-\\(n\\) models just fine. However, when your \\(n\\) gets small, the algorithms we use to implement our Bayesian models benefit from priors that are at least modestly informative. As it turns out, the brms default priors are flat for parameters of class = b. They offer no information beyond that contained in the likelihood. To stave off algorithm problems with our extremely-small-\\(n\\) data subsets, we used normal(0, 2) instead. In our model, the only parameter of class = b is the regression slope for time. On the scale of the data, normal(0, 2) is a vary-permissive prior for our time slope. In addition to our time slope parameter, our model contained an intercept and a residual variance. From the fit2.1$prior output, we can see those were student_t(3, 2.1, 2.5) and student_t(3, 0, 2.5), respectively. brms default priors are designed to be weakly informative. Given the data and the model, these priors have a minimal influence on the results. We’ll focus more on priors later in the project. For now just recognize that even if you don’t specify your priors, you can’t escape using some priors when using brm(). This is a good thing. Okay, so that was the model for just one participant. We want to do that for all 16. Instead of repeating that code 15 times, we can work in bulk. With brms, you can reuse a model with the update() function. Here’s how to do that with the data from our second participant. fit2.2 &lt;- update(fit2.1, newdata = by_id$data[[2]], control = list(adapt_delta = .9), file = &quot;fits/fit02.02&quot;) Peek at the results. print(fit2.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ 1 + time ## Data: by_id$data[[2]] (Number of observations: 5) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.14 0.34 0.41 1.82 1.00 3271 2166 ## time 0.18 0.14 -0.10 0.47 1.00 3025 2275 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.35 0.31 0.11 1.19 1.00 1298 1934 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Different participants yield different model results. Looking ahead a bit, we’ll need to know how to get the \\(R^2\\) for a single-level Gaussian model. With brms, you do that with the bayes_R2() function. bayes_R2(fit2.2) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.6224969 0.2508595 0.01038104 0.8148581 Though the default spits out summary statistics, you can get the full posterior distribution for the \\(R^2\\) by specifying summary = F. bayes_R2(fit2.2, summary = F) %&gt;% str() ## num [1:8000, 1] 0.7786 0.7812 0.7978 0.7382 0.0153 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;R2&quot; Our code returned a numeric vector. If you’d like to plot the results with ggplot2, you’ll need to convert the vector to a data frame. bayes_R2(fit2.2, summary = F) %&gt;% data.frame() %&gt;% ggplot(aes(x = R2)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(expression(italic(R)[Bayesian]^2), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) You’ll note how non-Gaussian the Bayesian \\(R^2\\) can be. Also, with the combination of default minimally-informative priors and only 5 data points, there’ massive uncertainty in the shape. As such, the value of central tendency will vary widely based on which statistic you use. bayes_R2(fit2.2, summary = F) %&gt;% data.frame() %&gt;% summarise(mean = mean(R2), median = median(R2), mode = tidybayes::Mode(R2)) ## mean median mode ## 1 0.6224969 0.7477771 0.8149813 By default, bayes_R2() returns the mean. You can get the median with the robust = TRUE argument. To pull the mode, you’ll need to use summary = F and feed the results into a mode function, like tidybayes::Mode(). I should also point out the brms package did not get these \\(R^2\\) values by traditional method used in, say, OLS estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out Gelman, Goodrich, Gabry, and Vehtari’s (2019) paper, [R-squared for Bayesian regression models]https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1549100). With a little tricky programming, we can use the purrr::map() function to serially fit this model to each of our participant-level data sets. We’ll save the results as fits. fits &lt;- by_id %&gt;% mutate(model = map(data, ~update(fit2.1, newdata = ., seed = 2))) Let’s walk through what we did. The map() function takes two primary arguments, .x and .f, respectively. We set .x = data, which meant we wanted to iterate over the contents in our data vector. Recall that each row of data itself contained an entire data set–one for each of the 16 participants. It’s with the second argument .f that we indicated what we wanted to do with our rows of data. We set that to .f = ~update(fit2.1, newdata = ., seed = 2). With the ~ syntax, we entered in a formula, which was update(fit2.1, newdata = ., seed = 2). Just like we did with fit2.2, above, we reused the model formula and other technical specs from fit2.1. Now notice the middle part of the formula, newdata = .. That little . refers to the element we specified in the .x argument. What this combination means is that for each of the 16 rows of our nested by_id tibble, we plugged in the id-specific data set into update(fit, newdata[[i]]) where i is simply meant as a row index. The new column, model, contains the output of each of the 16 iterations. print(fits) ## # A tibble: 16 × 3 ## # Groups: id [16] ## id data model ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 9 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 2 45 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 3 268 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 4 314 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 5 442 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 6 514 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 7 569 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 8 624 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 9 723 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 10 918 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 11 949 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 12 978 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 13 1105 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 14 1542 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 15 1552 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; ## 16 1653 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt; Next, we’ll want to extract the necessary summary information from our fits to remake our version of Table 2.2. There’s a lot of info in that table, so let’s take it step by step. First, we’ll extract the posterior means (i.e., “Estimate”) and standard deviations (i.e., “se”) for the initial status and rate of change of each model. We’ll also do the same for sigma (i.e., the square of the “Residual variance”). mean_structure &lt;- fits %&gt;% mutate(coefs = map(model, ~ posterior_summary(.)[1:2, 1:2] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;coefficients&quot;))) %&gt;% unnest(coefs) %&gt;% select(-data, -model) %&gt;% unite(temp, Estimate, Est.Error) %&gt;% pivot_wider(names_from = coefficients, values_from = temp) %&gt;% separate(b_Intercept, into = c(&quot;init_stat_est&quot;, &quot;init_stat_sd&quot;), sep = &quot;_&quot;) %&gt;% separate(b_time, into = c(&quot;rate_change_est&quot;, &quot;rate_change_sd&quot;), sep = &quot;_&quot;) %&gt;% mutate_if(is.character, ~ as.double(.) %&gt;% round(digits = 2)) %&gt;% ungroup() head(mean_structure) ## # A tibble: 6 × 5 ## id init_stat_est init_stat_sd rate_change_est rate_change_sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 1.91 0.53 0.12 0.22 ## 2 45 1.16 0.38 0.17 0.14 ## 3 268 1.54 0.53 0.03 0.22 ## 4 314 1.32 0.41 -0.03 0.16 ## 5 442 1.57 0.46 0.06 0.19 ## 6 514 1.43 0.33 0.26 0.15 It’s simpler to extract the residual variance. Recall that because brms gives that in the standard deviation metric (i.e., \\(\\sigma\\)), you need to square it to return it in a variance metric (i.e., \\(\\sigma^2\\)). residual_variance &lt;- fits %&gt;% mutate(residual_variance = map_dbl(model, ~ posterior_summary(.)[3, 1])^2) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% select(id, residual_variance) head(residual_variance) ## # A tibble: 6 × 2 ## # Groups: id [6] ## id residual_variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0.33 ## 2 45 0.12 ## 3 268 0.35 ## 4 314 0.14 ## 5 442 0.23 ## 6 514 0.12 We’ll extract our Bayesian \\(R^2\\) summaries, next. Given how nonnormal these are, we’ll use the posterior median rather than the mean. We get that by using the robust = T argument within the bayes_R2() function. r2 &lt;- fits %&gt;% mutate(r2 = map_dbl(model, ~ bayes_R2(., robust = T)[1])) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% select(id, r2) head(r2) ## # A tibble: 6 × 2 ## # Groups: id [6] ## id r2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0.33 ## 2 45 0.75 ## 3 268 0.19 ## 4 314 0.22 ## 5 442 0.24 ## 6 514 0.86 Here we combine all the components with a series of left_join() statements and present it in a flextable-type table. table &lt;- fits %&gt;% unnest(data) %&gt;% group_by(id) %&gt;% slice(1) %&gt;% select(id, male, exposure) %&gt;% left_join(mean_structure, by = &quot;id&quot;) %&gt;% left_join(residual_variance, by = &quot;id&quot;) %&gt;% left_join(r2, by = &quot;id&quot;) %&gt;% rename(residual_var = residual_variance) %&gt;% select(id, init_stat_est:r2, everything()) %&gt;% ungroup() table %&gt;% flextable::flextable() .cl-2b77fd3c{}.cl-2b5f3fd6{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2b722254{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2b723f78{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2b723f82{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2b723f8c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}idinit_stat_estinit_stat_sdrate_change_estrate_change_sdresidual_varr2maleexposure91.910.530.120.220.330.3301.54451.160.380.170.140.120.7511.162681.540.530.030.220.350.1910.903141.320.41-0.030.160.140.2200.814421.570.460.060.190.230.2401.135141.430.330.260.150.120.8610.905691.820.080.050.040.010.8501.996241.120.110.020.040.010.3610.987231.270.20-0.050.080.050.4400.819181.020.620.140.260.460.3401.219491.730.55-0.090.220.320.3010.939781.040.650.620.270.490.8711.591,1051.540.350.160.150.130.6711.381,5421.210.440.240.180.200.7501.441,5521.200.690.150.290.640.3001.041,6530.960.390.250.160.120.8401.25 We can make the four stem-and-leaf plots of Figure 2.4 with serial combinations of pull() and stem(). # fitted initial status table %&gt;% pull(init_stat_est) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 9 | 6 ## 10 | 24 ## 11 | 26 ## 12 | 017 ## 13 | 2 ## 14 | 3 ## 15 | 447 ## 16 | ## 17 | 3 ## 18 | 2 ## 19 | 1 # fitted rate of change table %&gt;% pull(rate_change_est) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## -0 | 953 ## 0 | 2356 ## 1 | 24567 ## 2 | 456 ## 3 | ## 4 | ## 5 | ## 6 | 2 # residual variance table %&gt;% pull(residual_var) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 0 | 115 ## 1 | 22234 ## 2 | 03 ## 3 | 235 ## 4 | 69 ## 5 | ## 6 | 4 # r2 statistic table %&gt;% pull(r2) %&gt;% stem(scale = 2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 1 | 9 ## 2 | 24 ## 3 | 00346 ## 4 | 4 ## 5 | ## 6 | 7 ## 7 | 55 ## 8 | 4567 To make Figure 2.5, we’ll combine information from the original data and the ‘Estimates’ (i.e., posterior means) from our Bayesian models we’ve encoded in mean_structure. by_id %&gt;% unnest(data) %&gt;% ggplot(aes(x = time, y = tolerance, group = id)) + geom_point() + geom_abline(data = mean_structure, aes(intercept = init_stat_est, slope = rate_change_est, group = id), color = &quot;blue&quot;) + scale_x_continuous(breaks = 0:4, labels = 0:4 + 11) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ id) 2.3 Exploring differences in change across people “Having summarized how each individual changes over time, we now examine similarities and differences in these changes across people” (p. 33). 2.3.1 Examining the entire set of smooth trajectories. The key to making our version of the left-hand side of Figure 2.6 is two stat_smooth() lines. The first one will produce the overall smooth. The second one, the one including the aes(group = id) argument, will give the id-specific smooths. tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + stat_smooth(method = &quot;loess&quot;, se = F, span = .9, size = 2) + stat_smooth(aes(group = id), method = &quot;loess&quot;, se = F, span = .9, size = 1/4) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) To get the linear OLS trajectories, just switch method = \"loess\" to method = \"lm\". tolerance_pp %&gt;% ggplot(aes(x = age, y = tolerance)) + stat_smooth(method = &quot;lm&quot;, se = F, span = .9, size = 2) + stat_smooth(aes(group = id), method = &quot;lm&quot;, se = F, span = .9, size = 1/4) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) But we wanted to be Bayesians. We already have the id-specific trajectories. All we need now is one based on all the data. fit2.3 &lt;- update(fit2.1, newdata = tolerance_pp, file = &quot;fits/fit02.03&quot;) Here’s the model summary. summary(fit2.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ 1 + time ## Data: tolerance_pp (Number of observations: 80) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.36 0.09 1.18 1.53 1.00 8428 6179 ## time 0.13 0.04 0.06 0.20 1.00 8241 5833 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.47 0.04 0.40 0.55 1.00 7172 5912 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before, we used posterior_summary() to isolate the posterior means and \\(SD\\)s. We can also use the fixef() function for that. fixef(fit2.3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 1.3586256 0.09030747 1.1822918 1.5349912 ## time 0.1309137 0.03704237 0.0573547 0.2031036 With a little subsetting, we can extract just the means from each. fixef(fit2.3)[1, 1] ## [1] 1.358626 fixef(fit2.3)[2, 1] ## [1] 0.1309137 For this plot, we’ll work more directly with the model formulas to plot the trajectories. We can use init_stat_est and rate_change_est from the mean_structure object as stand-ins for \\(\\beta_{0i}\\) and \\(\\beta_{1i}\\) from our model equation, \\[\\text{tolerance}_{ij} = \\beta_{0i} + \\beta_{1i} \\cdot \\text{time}_{ij} + \\epsilon_{ij},\\] where \\(i\\) indexes children and \\(j\\) indexes time points. All we need to do is plug in the appropriate values for time and we’ll have the fitted tolerance values for each level of id. After a little wrangling, the data will be in good shape for plotting. tol_fitted &lt;- mean_structure %&gt;% mutate(`11` = init_stat_est + rate_change_est * 0, `15` = init_stat_est + rate_change_est * 4) %&gt;% select(id, `11`, `15`) %&gt;% pivot_longer(-id, names_to = &quot;age&quot;, values_to = &quot;tolerance&quot;) %&gt;% mutate(age = as.integer(age)) head(tol_fitted) ## # A tibble: 6 × 3 ## id age tolerance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 11 1.91 ## 2 9 15 2.39 ## 3 45 11 1.16 ## 4 45 15 1.84 ## 5 268 11 1.54 ## 6 268 15 1.66 We’ll plot the id-level trajectories with those values and geom_line(). To get the overall trajectory, we’ll get tricky with fixef(fit2.3) and geom_abline(). tol_fitted %&gt;% ggplot(aes(x = age, y = tolerance, group = id)) + geom_line(color = &quot;blue&quot;, linewidth = 1/4) + geom_abline(intercept = fixef(fit2.3)[1, 1] + fixef(fit2.3)[2, 1] * -11, slope = fixef(fit2.3)[2, 1], color = &quot;blue&quot;, linewidth = 2) + coord_cartesian(ylim = c(0, 4)) + theme(panel.grid = element_blank()) 2.3.2 Using the results of model fitting to frame questions about change. If you’re new to the multilevel model, the ideas in this section are foundational. To learn about the observed average pattern of change, we examine the sample averages of the fitted intercepts and slopes; these tell us about the average initial status and the average annual rate of change in the sample as a whole. To learn about the observed individual differences in change, we examine the sample variances and standard deviations of the intercepts and slopes; these tell us about the observed variability in initial status. And to learn about the observed relationship between initial status and the rate of change, we can examine the sample covariance or correlation between intercepts and slopes. Formal answers to these questions require the multilevel model for change of chapter 3. But we can presage this work by conducting simple descriptive analyses of the estimated intercepts and slopes. (p. 36, emphasis in the original) Here are the means and standard deviations presented in Table 2.3. mean_structure %&gt;% pivot_longer(ends_with(&quot;est&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 × 3 ## name mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 init_stat_est 1.36 0.29 ## 2 rate_change_est 0.13 0.17 Here’s how to get the Pearson’s correlation coefficient. mean_structure %&gt;% select(init_stat_est, rate_change_est) %&gt;% cor() %&gt;% round(digits = 2) ## init_stat_est rate_change_est ## init_stat_est 1.00 -0.44 ## rate_change_est -0.44 1.00 2.3.3 Exploring the relationship between change and time-invariant predictors. “Evaluating the impact of predictors helps you uncover systematic patterns in the individual change trajectories corresponding to interindividual variation in personal characteristics” (p. 37). 2.3.3.1 Graphically examining groups of smoothed individual growth trajectories. If we’d like Bayesian estimates differing by male, we’ll need to fit an interaction model. fit2.4 &lt;- update(fit2.1, newdata = tolerance_pp, tolerance ~ 1 + time + male + time:male, file = &quot;fits/fit02.04&quot;) Check the model summary. print(fit2.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ time + male + time:male ## Data: tolerance_pp (Number of observations: 80) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.36 0.12 1.12 1.59 1.00 4768 5832 ## time 0.10 0.05 0.01 0.20 1.00 4735 5007 ## male 0.00 0.18 -0.35 0.35 1.00 3732 4615 ## time:male 0.07 0.07 -0.08 0.21 1.00 3615 4506 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.47 0.04 0.40 0.55 1.00 5824 4889 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s how to use fixef() and the model equation to get fitted values for tolerance based on specific values for time and male. tol_fitted_male &lt;- tibble(male = rep(0:1, each = 2), age = rep(c(11, 15), times = 2)) %&gt;% mutate(time = age - 11) %&gt;% mutate(tolerance = fixef(fit2.4)[1, 1] + fixef(fit2.4)[2, 1] * time + fixef(fit2.4)[3, 1] * male + fixef(fit2.4)[4, 1] * time * male) tol_fitted_male ## # A tibble: 4 × 4 ## male age time tolerance ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 11 0 1.36 ## 2 0 15 4 1.77 ## 3 1 11 0 1.36 ## 4 1 15 4 2.03 Now we’re ready to make our Bayesian version of the top panels of Figure 2.7. tol_fitted %&gt;% # we need to add `male` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, male), by = &quot;id&quot;) %&gt;% ggplot(aes(x = age, y = tolerance, color = factor(male))) + geom_line(aes(group = id), linewidth = 1/4) + geom_line(data = tol_fitted_male, linewidth = 2) + scale_color_viridis_d(end = .75) + coord_cartesian(ylim = c(0, 4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ male) Before we can do the same thing with exposure, we’ll need to dichotomize it by its median. A simple way is with a conditional statement within the if_else() function. tolerance_pp &lt;- tolerance_pp %&gt;% mutate(exposure_01 = if_else(exposure &gt; median(exposure), 1, 0)) Now fit the second interaction model. fit2.5 &lt;- update(fit2.4, newdata = tolerance_pp, tolerance ~ 1 + time + exposure_01 + time:exposure_01, file = &quot;fits/fit02.05&quot;) Here’s the summary. print(fit2.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: tolerance ~ time + exposure_01 + time:exposure_01 ## Data: tolerance_pp (Number of observations: 80) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.39 0.12 1.16 1.62 1.00 4553 5444 ## time 0.04 0.05 -0.05 0.14 1.00 4133 4860 ## exposure_01 -0.07 0.16 -0.39 0.26 1.00 3956 4430 ## time:exposure_01 0.18 0.07 0.04 0.31 1.00 3443 4382 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.43 0.03 0.37 0.51 1.00 5970 5400 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now use fixef() and the model equation to get fitted values for tolerance based on specific values for time and exposure_01. tol_fitted_exposure &lt;- crossing(exposure_01 = 0:1, age = c(11, 15)) %&gt;% mutate(time = age - 11) %&gt;% mutate(tolerance = fixef(fit2.5)[1, 1] + fixef(fit2.5)[2, 1] * time + fixef(fit2.5)[3, 1] * exposure_01 + fixef(fit2.5)[4, 1] * time * exposure_01, exposure = if_else(exposure_01 == 1, &quot;high exposure&quot;, &quot;low exposure&quot;) %&gt;% factor(., levels = c(&quot;low exposure&quot;, &quot;high exposure&quot;))) tol_fitted_exposure ## # A tibble: 4 × 5 ## exposure_01 age time tolerance exposure ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 11 0 1.39 low exposure ## 2 0 15 4 1.56 low exposure ## 3 1 11 0 1.32 high exposure ## 4 1 15 4 2.20 high exposure Did you notice in the last lines in the second mutate() how we made a version of exposure that is a factor? That will come in handy for labeling and ordering the subplots. Now make our Bayesian version of the bottom panels of Figure 2.7. tol_fitted %&gt;% # we need to add `exposure_01` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, exposure_01), by = &quot;id&quot;) %&gt;% mutate(exposure = if_else(exposure_01 == 1, &quot;high exposure&quot;, &quot;low exposure&quot;) %&gt;% factor(., levels = c(&quot;low exposure&quot;, &quot;high exposure&quot;))) %&gt;% ggplot(aes(x = age, y = tolerance, color = exposure)) + geom_line(aes(group = id), linewidth = 1/4) + geom_line(data = tol_fitted_exposure, linewidth = 2) + scale_color_viridis_d(option = &quot;A&quot;, end = .75) + coord_cartesian(ylim = c(0, 4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ exposure) 2.3.3.2 The relationship between OLS-Estimated single-level Bayesian trajectories and substantive predictors “To investigate whether fitted trajectories vary systematically with predictors, we can treat the estimated intercepts and slopes as outcomes and explore the relationship between them and predictors” (p. 39). Here are the left panels of Figure 2.8. p1 &lt;- mean_structure %&gt;% pivot_longer(ends_with(&quot;est&quot;)) %&gt;% mutate(name = factor(name, labels = c(&quot;Fitted inital status&quot;, &quot;Fitted rate of change&quot;))) %&gt;% # we need to add `male` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, male), by = &quot;id&quot;) %&gt;% ggplot(aes(x = factor(male), y = value, color = name)) + geom_point(alpha = 1/2) + scale_color_viridis_d(option = &quot;B&quot;, begin = .2, end = .7) + labs(x = &quot;male&quot;, y = NULL) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ name, scale = &quot;free_y&quot;, ncol = 1) p1 Here are the right panels. p2 &lt;- mean_structure %&gt;% pivot_longer(ends_with(&quot;est&quot;)) %&gt;% mutate(name = factor(name, labels = c(&quot;Fitted inital status&quot;, &quot;Fitted rate of change&quot;))) %&gt;% # we need to add `male` values to `tol_fitted` left_join(tolerance_pp %&gt;% select(id, exposure), by = &quot;id&quot;) %&gt;% ggplot(aes(x = exposure, y = value, color = name)) + geom_point(alpha = 1/2) + scale_color_viridis_d(option = &quot;B&quot;, begin = .2, end = .7) + scale_x_continuous(breaks = 0:2, limits = c(0, 2.4)) + labs(y = NULL) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ name, scale = &quot;free_y&quot;, ncol = 1) p2 Did you notice how we saved those last two plots as p1 and p2? We can use syntax from the patchwork package (Pedersen, 2022) to combine them into one compound plot. library(patchwork) p1 + p2 + scale_y_continuous(breaks = NULL) As interesting as these plots are, do remember that “the need for ad hoc correlations has been effectively replaced by the widespread availability of computer software for fitting the multilevel model for change directly” (pp. 41–42). As you’ll see, Bürkner’s brms package is one of the foremost in that regard. 2.4 Improving the precision and reliability of OLS single-level-Bayesian-estimated rates of change: Lessons for research design Statisticians assess the precision of a parameter estimate in terms of its sampling variation, a measure of the variability that would be found across infinite resamplings from the same population. The most common measure of sampling variability is an estimate’s standard error, the square root of its estimated sampling variance. Precision and standard error have an inverse relationship; the smaller the standard error, the more precise the estimate. (p. 41, emphasis in the original) So here’s the deal: When Singer and Willett wrote “Statisticians assess…” a more complete expression would have been ‘Frequentist statisticians assess…’ Bayesian statistics are not based on asymptotic theory. They do not presume an idealized infinite distribution of replications. Rather, Bayesian statistics use Bayes theorem to estimate the probability of the parameters given the data. That probability has a distribution. Analogous to frequentist statistics, we often summarize that distribution (i.e., the posterior distribution) in terms of central tendency (e.g., posterior mean, posterior median, posterior mode) and spread. Spread? you say. We typically express spread in one or both of two ways. One typical expression of spread is the 95% intervals. In the Bayesian world, these are often called credible or probability intervals. The other typical expression of spread is the posterior standard deviation. In brms, this of typically summarized in the ‘Est.error’ column of the output of functions like print() and posterior_summary() and so on. The posterior standard deviation is analogous to the frequentist standard error. Philosophically and mechanically, they are not the same. But in practice, they are often quite similar. Later we read: Unlike precision which describes how well an individual slope estimate measures that person’s true rate of change, reliability describes how much the rate of change varies across people. Precision has meaning for the individual; reliability has meaning for the group. (p. 42) I have to protest. True, if we were working within a Classical Test Theory paradigm, this would be correct. But this places reliability with the context of group-based cross-sectional design. Though this is a popular design, it is not the whole story (i.e., see this book!). For introductions to more expansive and person-specific notions of reliability, check out Lee Cronbach’s Generalizability Theory (Brennan, 2001; Cronbach et al., 1972; also Cranford et al., 2006; LoPilato et al., 2015; Shrout &amp; Lane, 2012). Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.2 brms_2.19.0 Rcpp_1.0.10 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 ## [8] purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] svUnit_1.0.6 shinythemes_1.2.0 splines_4.3.0 later_1.3.1 ## [5] gamm4_0.2-6 xts_0.13.1 lifecycle_1.0.3 StanHeaders_2.26.25 ## [9] processx_3.8.1 lattice_0.21-8 vroom_1.6.3 MASS_7.3-58.4 ## [13] crosstalk_1.2.0 ggdist_3.3.0 backports_1.4.1 magrittr_2.0.3 ## [17] sass_0.4.6 rmarkdown_2.21 jquerylib_0.1.4 httpuv_1.6.11 ## [21] zip_2.3.0 askpass_1.1 pkgbuild_1.4.0 minqa_1.2.5 ## [25] multcomp_1.4-23 abind_1.4-5 tidybayes_3.0.4 TH.data_1.1-2 ## [29] tensorA_0.36.2 sandwich_3.0-2 gdtools_0.3.3 inline_0.3.19 ## [33] crul_1.4.0 xslt_1.4.4 bridgesampling_1.1-2 codetools_0.2-19 ## [37] DT_0.27 xml2_1.3.4 tidyselect_1.2.0 bayesplot_1.10.0 ## [41] httpcode_0.3.0 farver_2.1.1 lme4_1.1-33 matrixStats_0.63.0 ## [45] stats4_4.3.0 base64enc_0.1-3 jsonlite_1.8.4 ellipsis_0.3.2 ## [49] survival_3.5-5 emmeans_1.8.6 systemfonts_1.0.4 projpred_2.5.0 ## [53] tools_4.3.0 ragg_1.2.5 glue_1.6.2 mnormt_2.1.1 ## [57] gridExtra_2.3 xfun_0.39 mgcv_1.8-42 distributional_0.3.2 ## [61] loo_2.6.0 withr_2.5.0 fastmap_1.1.1 boot_1.3-28.1 ## [65] fansi_1.0.4 shinyjs_2.1.0 openssl_2.0.6 callr_3.7.3 ## [69] digest_0.6.31 timechange_0.2.0 R6_2.5.1 mime_0.12 ## [73] estimability_1.4.1 textshaping_0.3.6 colorspace_2.1-0 gtools_3.9.4 ## [77] markdown_1.7 threejs_0.3.3 utf8_1.2.3 generics_0.1.3 ## [81] fontLiberation_0.1.0 data.table_1.14.8 prettyunits_1.1.1 htmlwidgets_1.6.2 ## [85] pkgconfig_2.0.3 dygraphs_1.1.1.6 gtable_0.3.3 htmltools_0.5.5 ## [89] fontBitstreamVera_0.1.1 bookdown_0.34 scales_1.2.1 posterior_1.4.1 ## [93] knitr_1.42 rstudioapi_0.14 tzdb_0.4.0 reshape2_1.4.4 ## [97] uuid_1.1-0 coda_0.19-4 checkmate_2.2.0 nlme_3.1-162 ## [101] curl_5.0.0 nloptr_2.0.3 cachem_1.0.8 zoo_1.8-12 ## [105] flextable_0.9.1 parallel_4.3.0 miniUI_0.1.1.1 pillar_1.9.0 ## [109] grid_4.3.0 vctrs_0.6.2 shinystan_2.6.0 promises_1.2.0.1 ## [113] arrayhelpers_1.1-0 xtable_1.8-4 evaluate_0.21 mvtnorm_1.1-3 ## [117] cli_3.6.1 compiler_4.3.0 rlang_1.1.1 crayon_1.5.2 ## [121] rstantools_2.3.1 labeling_0.4.2 ps_1.7.5 plyr_1.8.8 ## [125] stringi_1.7.12 rstan_2.21.8 psych_2.3.3 viridisLite_0.4.2 ## [129] munsell_0.5.0 colourpicker_1.2.0 V8_4.3.0 Brobdingnag_1.2-9 ## [133] fontquiver_0.2.1 Matrix_1.5-4 hms_1.1.3 bit64_4.0.5 ## [137] gfonts_0.2.0 shiny_1.7.4 highr_0.10 igraph_1.4.2 ## [141] RcppParallel_5.1.7 bslib_0.4.2 bit_4.0.5 officer_0.6.2 ## [145] katex_1.4.1 equatags_0.2.0 References Brennan, R. L. (2001). Generalizability Theory. Springer-Verlag. https://doi.org/10.1007/978-1-4757-3456-0 Cranford, J. A., Shrout, P. E., Iida, M., Rafaeli, E., Yip, T., &amp; Bolger, N. (2006). A procedure for evaluating sensitivity to within-person change: Can mood measures in diary studies detect change reliably? Personality and Social Psychology Bulletin, 32(7), 917–929. https://doi.org/10.1177/0146167206287721 Cronbach, L. J., Gleser, G. C., Nanda, H., &amp; Rajaratnam, N. (1972). The dependability of behavioral measurements: Theory of generalizability for scores and profiles. John Wiley &amp; Sons. https://www.amazon.com/Dependability-Behavioral-Measurements-Generalizability-Profiles/dp/0471188506 Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100 Kuhn, M., Jackson, S., &amp; Cimentada, J. (2020). corrr: Correlations in R [Manual]. https://CRAN.R-project.org/package=corrr LoPilato, A. C., Carter, N. T., &amp; Wang, M. (2015). Updating generalizability theory in management research: Bayesian estimation of variance components. Journal of Management, 41(2), 692–717. https://doi.org/10.1177/0149206314554215 Pedersen, T. L. (2022). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork Peng, R. D. (2019). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/ Raudenbush, S. W., &amp; Chan, W.-S. (2016). Growth curve analysis in accelerated longitudinal designs. Journal of Research in Crime and Delinquency, 29(4), 387–411. https://doi.org/10.1177/0022427892029004001 Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych Shrout, P. E., &amp; Lane, S. P. (2012). Psychometrics. In M. R. Mehl &amp; T. S. Conner (Eds.), Handbook of research methods for studying daily life (pp. 302–320). The Guilford Press. https://www.guilford.com/books/Handbook-of-Research-Methods-for-Studying-Daily-Life/Mehl-Conner/9781462513055 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 "],["introducing-the-multilevel-model-for-change.html", "3 Introducing the Multilevel Model for Change 3.1 What is the purpose of the multilevel model for change? 3.2 The level-1 submodel for individual change 3.3 The level-2 submodel for systematic interindividual differences in change 3.4 Fitting the multilevel model for change to data 3.5 Examining estimated fixed effects 3.6 Examining estimated variance components 3.7 Bonus: How did you simulate that data? Session info", " 3 Introducing the Multilevel Model for Change In this chapter [Singer and Willett introduced] the multilevel model for change, demonstrating how it allows us to address within-person and between-person questions about change simultaneously. Although there are several ways of writing the statistical model, here we adopt a simple and common approach that has much substantive appeal. We specify the multilevel model for change by simultaneously postulating a pair of subsidiary models—a level-1 submodel that describes how each person changes over time, and a level-2 model that describes how these changes differ across people (Bryk &amp; Raudenbush, 1987; D. R. Rogosa &amp; Willett, 1985). (Singer &amp; Willett, 2003, p. 3) 3.1 What is the purpose of the multilevel model for change? Unfortunately, we do not have access to the full data set Singer and Willett used in this chapter. For details, go here. However, I was able to use the data provided in Table 3.1 and the model results in Table 3.3 to simulate data with similar characteristics as the original. To see how I did it, look at the section at the end of the chapter. Anyway, here are the data in Table 3.1. library(tidyverse) early_int &lt;- tibble(id = rep(c(68, 70:72, 902, 904, 906, 908), each = 3), age = rep(c(1, 1.5, 2), times = 8), cog = c(103, 119, 96, 106, 107, 96, 112, 86, 73, 100, 93, 87, 119, 93, 99, 112, 98, 79, 89, 66, 81, 117, 90, 76), program = rep(1:0, each = 12)) print(early_int) ## # A tibble: 24 × 4 ## id age cog program ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 68 1 103 1 ## 2 68 1.5 119 1 ## 3 68 2 96 1 ## 4 70 1 106 1 ## 5 70 1.5 107 1 ## 6 70 2 96 1 ## 7 71 1 112 1 ## 8 71 1.5 86 1 ## 9 71 2 73 1 ## 10 72 1 100 1 ## # ℹ 14 more rows Later on, we also fit models using \\(age - 1\\). Here we’ll compute that and save it as age_c. early_int &lt;- early_int %&gt;% mutate(age_c = age - 1) head(early_int) ## # A tibble: 6 × 5 ## id age cog program age_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 68 1 103 1 0 ## 2 68 1.5 119 1 0.5 ## 3 68 2 96 1 1 ## 4 70 1 106 1 0 ## 5 70 1.5 107 1 0.5 ## 6 70 2 96 1 1 Here we’ll load our simulation of the full \\(n = 103\\) data set. load(&quot;data/early_int_sim.rda&quot;) 3.2 The level-1 submodel for individual change This part of the model is also called the individual growth model. Remember how in last chapter we fit a series of participant-specific models? That’s the essence of this part of the model. Here’s our version of Figure 3.1. Note that here we’re being lazy and just using OLS estimates. early_int %&gt;% ggplot(aes(x = age, y = cog)) + stat_smooth(method = &quot;lm&quot;, se = F) + geom_point() + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(panel.grid = element_blank()) + facet_wrap(~ id, ncol = 4) Based on these data, we postulate our level-1 submodel to be \\[ \\text{cog}_{ij} = [ \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1) ] + [\\epsilon_{ij}]. \\] 3.2.1 The structural part of the level-1 submodel. As far as I can tell, the data for Figure 3.2 are something like this. d &lt;- tibble(id = &quot;i&quot;, age = c(1, 1.5, 2), cog = c(95, 100, 135)) d ## # A tibble: 3 × 3 ## id age cog ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 i 1 95 ## 2 i 1.5 100 ## 3 i 2 135 To add in the horizontal dashed lines in Figure 3.2, we’ll need to fit a model. Let’s be lazy and use OLS. Don’t worry, we’ll use Bayes in a bit. fit3.1 &lt;- lm(data = d, cog ~ age) summary(fit3.1) ## ## Call: ## lm(formula = cog ~ age, data = d) ## ## Residuals: ## 1 2 3 ## 5 -10 5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.00 26.93 1.857 0.314 ## age 40.00 17.32 2.309 0.260 ## ## Residual standard error: 12.25 on 1 degrees of freedom ## Multiple R-squared: 0.8421, Adjusted R-squared: 0.6842 ## F-statistic: 5.333 on 1 and 1 DF, p-value: 0.2601 We can use the fitted() function to compute the model-implied fitted values for cog based on the age values in the data. We’ll then save those in a sensibly-named vector which we’ll attach to the rest of the data. f &lt;- fitted(fit3.1) %&gt;% data.frame() %&gt;% set_names(&quot;fitted&quot;) %&gt;% bind_cols(d) print(f) ## fitted id age cog ## 1 90 i 1.0 95 ## 2 110 i 1.5 100 ## 3 130 i 2.0 135 To make all the dashed lines and arrows in the figure, we’ll want a few specialty tibbles. path &lt;- tibble(age = c(1, 2, 2), cog = c(90, 90, 130)) text &lt;- tibble(age = c(1.2, 1.65, 2.15, 1.125, 2.075), cog = c(105, 101, 137, 75, 110), label = c(&quot;epsilon[italic(i)][1]&quot;, &quot;epsilon[italic(i)][2]&quot;, &quot;epsilon[italic(i)][3]&quot;, &quot;pi[0][italic(i)]&quot;, &quot;pi[1][italic(i)]&quot;)) arrow &lt;- tibble(age = c(1.15, 1.6, 2.1, 1.1), xend = c(1.01, 1.51, 2.01, 1.01), cog = c(103, 101, 137, 78), yend = c(92.5, 105, 132.5, 89)) # we&#39;re finally ready to plot! f %&gt;% ggplot(aes(x = age, y = cog)) + geom_point() + # the main fitted trajectory geom_line(aes(y = fitted)) + # the thick dashed line bending upward at age == 2 geom_path(data = path, linetype = 2, size = 1/2) + # the thin dashed vertical lines extending from the data dots to the fitted line geom_segment(aes(xend = age, y = cog, yend = fitted), linetype = 3, size = 1/4) + # the arrows geom_segment(data = arrow, aes(xend = xend, yend = yend), arrow = arrow(length = unit(0.1, &quot;cm&quot;)), size = 1/4) + # the statistical notation geom_text(data = text, aes(label = label), size = c(4, 4, 4, 5, 5), parse = T) + # &quot;1 year&quot; annotate(geom = &quot;text&quot;, x = 1.5, y = 86, label = &quot;1 year&quot;) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(panel.grid = element_blank()) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. In-specifying a level-1 submodel that attempts to describe everyone (all the \\(i\\)’s) in the population, we implicitly assume that all the true individual change trajectories have a common algebraic form. But we do not assume that everyone has the same exact trajectory. Because each person has his or her own individual growth parameters (intercepts and slopes), different people can have their own distinct change trajectories. (pp. 53–54) In this way, the multilevel model’s level-1 submodel is much like an interaction/moderation model with interaction terms for each level of \\(i\\). 3.2.2 The stochastic part of the level-1 submodel. The last term in our level-1 equation from above was \\([\\epsilon_{ij}]\\). This is the residual variance left in the criterion after accounting for the predictor(s) in the model. It is a mixture of systemic variation that could be accounted for by adding covariates to the model as well as measurement error. The typical assumption is \\[ \\epsilon_{ij} \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2). \\] We should point out that another way to express this is \\[\\begin{align*} \\text{cog} &amp; \\sim \\operatorname{Normal} (\\mu_{ij}, \\sigma_\\epsilon^2) \\\\ \\mu_{ij} &amp; = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1). \\end{align*}\\] This won’t be a huge deal in the context of the models Singer and Willett presented in the initial chapters of the text but expressing the models this way can help one think in terms of likelihood functions. That’s a major advantage when you start working with data which are natural to model using other distributions (e.g., count data and the Poisson, binary data and the binomial). For more on this approach, check out either edition of McElreath’s (2020a, 2015) Statistical Rethinking and my (2021, 2020) companion ebook(here and here) translating his work into brms and tidyverse code. Also, thinking in terms of likelihoods will pay off starting around Chapter 10 when we start fitting discrete-time survival models. 3.2.3 Relating the level-1 submodel to the OLS exploratory methods of chapter 2. To get the top panel in Figure 3.3, we’ll use stat_smooth() to get the OLS trajectories. early_int_sim %&gt;% ggplot(aes(x = age, y = cog)) + stat_smooth(aes(group = id), method = &quot;lm&quot;, se = F, size = 1/6) + stat_smooth(method = &quot;lm&quot;, se = F, size = 2) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(panel.grid = element_blank()) Note that now we’re working with our early_int_sim data, the one where we added the data of 95 simulated individuals to the real data of 8 id levels from Table 3.1. As such, our results will deviate a bit from those in the text. But anyways, here we go on to fit 103 individual OLS models, one for each of the id levels. Don’t worry; we’ll depart from this madness shortly. by_id &lt;- early_int_sim %&gt;% mutate(age_c = age - 1) %&gt;% group_by(id) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(data = ., cog ~ age_c))) head(by_id) ## # A tibble: 6 × 3 ## # Groups: id [6] ## id data model ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 2 2 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 3 3 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 4 4 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 5 5 &lt;tibble [3 × 4]&gt; &lt;lm&gt; ## 6 6 &lt;tibble [3 × 4]&gt; &lt;lm&gt; Now we’ll use the great helper functions from the broom package (Robinson et al., 2022), tidy() and glance(), to store the coefficient information and model fit information, respectively, in a tidy data format (see here and also here). # install.packages(&quot;broom&quot;) library(broom) by_id &lt;- by_id %&gt;% mutate(tidy = map(model, tidy), glance = map(model, glance)) Here’s what our by_id object now looks like: by_id %&gt;% head() ## # A tibble: 6 × 5 ## # Groups: id [6] ## id data model tidy glance ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt; ## 2 2 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt; ## 3 3 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt; ## 4 4 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt; ## 5 5 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt; ## 6 6 &lt;tibble [3 × 4]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt; If you want to extract the intercepts from the tidy column, you might execute code like this. unnest(by_id, tidy) %&gt;% filter(term == &quot;(Intercept)&quot;) ## # A tibble: 103 × 9 ## # Groups: id [103] ## id data model term estimate std.error statistic p.value glance ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 1 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 117 2.43e-14 4.81e15 1.32e-16 &lt;tibble [1 × 12]&gt; ## 2 2 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 110. 5.22e+ 0 2.11e 1 3.01e- 2 &lt;tibble [1 × 12]&gt; ## 3 3 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 117. 1.08e+ 1 1.08e 1 5.87e- 2 &lt;tibble [1 × 12]&gt; ## 4 4 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 136. 5.59e+ 0 2.42e 1 2.62e- 2 &lt;tibble [1 × 12]&gt; ## 5 5 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 108. 3.35e+ 0 3.21e 1 1.99e- 2 &lt;tibble [1 × 12]&gt; ## 6 6 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 110. 2.61e+ 0 4.21e 1 1.51e- 2 &lt;tibble [1 × 12]&gt; ## 7 7 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 124. 7.45e- 1 1.66e 2 3.84e- 3 &lt;tibble [1 × 12]&gt; ## 8 8 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 97 6.71e+ 0 1.45e 1 4.40e- 2 &lt;tibble [1 × 12]&gt; ## 9 9 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 121. 7.08e+ 0 1.71e 1 3.73e- 2 &lt;tibble [1 × 12]&gt; ## 10 10 &lt;tibble [3 × 4]&gt; &lt;lm&gt; (Intercept) 109. 3.73e- 1 2.92e 2 2.18e- 3 &lt;tibble [1 × 12]&gt; ## # ℹ 93 more rows This first line took the model coefficients and their respective statistics (e.g., standard errors) and unnested them (i.e., took them out of the list of data frames and converted the data to a longer structure). The second line filtered out any coefficients that were not the intercept. In this case, there are just two coefficients, the intercept and the slope for age_c. With that, we can make the leftmost stem and leaf plot from Figure 3.3. unnest(by_id, tidy) %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) %&gt;% stem() ## ## The decimal point is 1 digit(s) to the right of the | ## ## 8 | 3 ## 8 | 5 ## 9 | 000124 ## 9 | 5666677779999 ## 10 | 000111223334444 ## 10 | 555678888889 ## 11 | 00000223444 ## 11 | 55667777788 ## 12 | 001112222344 ## 12 | 556666889 ## 13 | 011223444 ## 13 | 56 ## 14 | ## 14 | 8 Here’s the stem and leaf plot in the middle. unnest(by_id, tidy) %&gt;% filter(term == &quot;age_c&quot;) %&gt;% pull(estimate) %&gt;% stem() ## ## The decimal point is 1 digit(s) to the right of the | ## ## -4 | 7 ## -4 | 111 ## -3 | 9886666655 ## -3 | 322111000 ## -2 | 9877776666555 ## -2 | 44443321111110000 ## -1 | 999999888665 ## -1 | 433322211100000 ## -0 | 988877776 ## -0 | 33211 ## 0 | 124 ## 0 | 57 ## 1 | 1113 If you want the residual variances (i.e., \\(\\sigma_\\epsilon^2\\)), you’d unnest() the glance column. They’ll be listed in the sigma column. unnest(by_id, glance) %&gt;% pull(sigma) %&gt;% stem(scale = 1) ## ## The decimal point is at the | ## ## 0 | 0044444488888 ## 1 | 22 ## 2 | 000000444499999 ## 3 | 3377777 ## 4 | 11111599999 ## 5 | 33377777 ## 6 | 111111559 ## 7 | 3333888 ## 8 | 26 ## 9 | 0004488 ## 10 | 22 ## 11 | 0088 ## 12 | 777 ## 13 | 115 ## 14 | 3 ## 15 | 59 ## 16 | 3 ## 17 | 16 ## 18 | 0 ## 19 | ## 20 | ## 21 | 266 3.3 The level-2 submodel for systematic interindividual differences in change Here are the top panels of Figure 3.4. early_int_sim &lt;- early_int_sim %&gt;% mutate(label = str_c(&quot;program = &quot;, program)) early_int_sim %&gt;% ggplot(aes(x = age, y = cog, color = label)) + stat_smooth(aes(group = id), method = &quot;lm&quot;, se = F, size = 1/6) + stat_smooth(method = &quot;lm&quot;, se = F, size = 2) + scale_color_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ label) Given the simplicity of the shapes, the bottom panels of Figure 3.4 will take a bit of preparatory work. First, we’ll need to wrangle the data a bit to get the necessary points. If we were working with a Bayesian model fit with brms, we’d use the fitted() function. But since we’re working with models fit with base R’s OLS estimator, lm(), we’ll use predict(), which accommodates a newdata argument. That’ll be crucial because in order to get the shapes correct, we’ll need to evaluate the minimum and maximum values across the fitted lines across a densely-packed sequence of age_c values. # how may `age_c` values do we need? n &lt;- 30 # define the specific `age_c` values nd &lt;- tibble(age_c = seq(from = 0, to = 1, length.out = n)) # wrangle p &lt;- by_id %&gt;% mutate(fitted = map(model, ~predict(., newdata = nd))) %&gt;% unnest(fitted) %&gt;% mutate(age = seq(from = 1, to = 2, length.out = n), program = ifelse(id &lt; 900, 1, 0)) %&gt;% group_by(program, age) %&gt;% summarise(min = min(fitted), max = max(fitted)) %&gt;% mutate(label = str_c(&quot;program = &quot;, program)) # what did we do? head(p) ## # A tibble: 6 × 5 ## # Groups: program [1] ## program age min max label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0 1 82.7 133. program = 0 ## 2 0 1.03 82.4 132. program = 0 ## 3 0 1.07 82.1 131. program = 0 ## 4 0 1.10 81.8 131. program = 0 ## 5 0 1.14 81.6 130. program = 0 ## 6 0 1.17 81.3 129. program = 0 Before we plot, we’ll need a couple tibbles for the annotation. text &lt;- tibble(age = 1.01, cog = c(101.5, 110), label = c(&quot;program = 0&quot;, &quot;program = 1&quot;), text = c(&quot;Average population trajectory,&quot;, &quot;Average population trajectory,&quot;), angle = c(345.7, 349)) math &lt;- tibble(age = 1.01, cog = c(94.5, 103), label = c(&quot;program = 0&quot;, &quot;program = 1&quot;), text = c(&quot;gamma[0][0] + gamma[10](italic(age) - 1)&quot;, &quot;(gamma[0][0] + gamma[10]) + (gamma[10] + gamma[11]) (italic(age) - 1)&quot;), angle = c(345.7, 349)) Finally, we’re ready for the bottom panels of Figure 3.4. p %&gt;% ggplot(aes(x = age)) + geom_ribbon(aes(ymin = min, ymax = max, fill = label), alpha = 1/3) + stat_smooth(data = early_int_sim, aes(y = cog, color = label), method = &quot;lm&quot;, se = F, size = 1) + geom_text(data = text, aes(y = cog, label = text, angle = angle), hjust = 0) + geom_text(data = math, aes(y = cog, label = text, angle = angle), hjust = 0, parse = T) + scale_fill_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) + scale_color_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) + scale_x_continuous(breaks = c(1, 1.5, 2)) + scale_y_continuous(&quot;cog&quot;, limits = c(50, 150)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ label) Returning to the text (p. 58), Singer and Willett asked: “What kind [of] population model might have given rise to these patterns?” Their answer is the level-2 model should have 4 specific features: “Its outcomes must be the individual growth parameters.” “The level-2 submodel must be written in separate parts, one for each level-1 growth parameter.” “Each part must specify a relationship between an individual growth parameter and the predictor.” “Each model must allow individuals who share common predictor values to vary in their individual change trajectories.” Given the current model, the level-2 submodel of change may be expressed as \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{program} + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{program} + \\zeta_{1i}. \\end{align*}\\] We’ll discuss the details about the \\(\\zeta\\) terms in a bit. 3.3.1 Structural components of the level-2 submodel. The structural parts of the level-2 submodel contain four level-2 parameters–\\(\\gamma_{00}\\), \\(\\gamma_{01}\\), \\(\\gamma_{10}\\), and \\(\\gamma_{11}\\)–known collectively as the fixed effects. The fixed effects capture systematic interindividual differences in change trajectory according to values of the level-2 predictors. (p. 60, emphasis in the original) 3.3.2 Stochastic components of the level-2 submodel. Each part of the level-2 submodel contains a residual that allows the value of each person’s growth parameters to be scattered around the relevant population averages. These residuals, \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) in [the] equation [above], represent those portions of the level-2 outcomes–the individual growth parameters–that remain unexplained by the level-2 predictor(s). (p. 61) We often summarize the \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) deviations as \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\), respectively. And importantly, these variance parameters have a covariance \\(\\sigma_{01}\\). However, and this next part is quite important, brms users should know that unlike the convention in many frequentist software packages (Bates et al., 2015; e.g., lme4, Bates et al., 2022) and in the text, brms parameterizes these in the standard-deviation metric. That is, in brms, these are expressed as \\(\\sigma_0\\) and \\(\\sigma_1\\). Similarly, the \\(\\sigma_{01}\\) presented in brms output is in a correlation metric, rather than a covariance. There are technical reasons for this are outside of the scope of the present situation (see Bürkner, 2017). The consequences is that we’ll make frequent use of squares and square roots in this project when comparing our brms::brm() results to those in the text. As on page 63 of the text, the typical way to express the multivariate distribution of the \\(\\zeta\\) parameters would be \\[\\begin{align*} \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{N} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01}\\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}, \\end{align*}\\] where the bracketed matrix on the right part of the equation is the variance/covariance matrix. If we summarize the vector of \\(\\zeta\\) terms as \\(u\\) and so on, we can re-express the above equation as \\[ u \\sim \\operatorname N (\\mathbf 0, \\mathbf \\Sigma), \\] where \\(\\mathbf 0\\) is the vector of 0 means and \\(\\mathbf \\Sigma\\) is the variance/covariance matrix. In Stan, and thus brms, we typically decompose \\(\\mathbf \\Sigma\\) as \\[\\begin{align*} \\mathbf \\Sigma &amp; = \\mathbf D \\mathbf \\Omega \\mathbf D, \\text{where} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}. \\end{align*}\\] Thus \\(\\mathbf D\\) is the diagonal matrix of standard deviations and \\(\\mathbf \\Omega\\) is the correlation matrix. 3.4 Fitting the multilevel model for change to data Singer and Willett discussed how in the 90s, we saw a bloom of software for fitting multilevel models. Very notably, they mentioned BUGS (Gilks et al., 1995) which stands for ‘Bayesian inference using Gibbs sampling’ and was a major advance in Bayesian software. As we learn in Kruschke (2015): In 1997, BUGS had a Windows-only version called WinBUGS, and later it was reimplemented in OpenBUGS which also runs best on Windows operating systems. JAGS (Plummer, 2003, 2012) retained many of the design features of BUGS, but with different samplers under the hood and better usability across different computer-operating systems (Windows, MacOS, and other forms of Linux/Unix). (pp. 193–194) There’s also Stan. From their homepage, https://mc-stan.org, we read “Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation.” Stan is free and open-source and you can find links to various documentation resources, such as the current Stan user’s guide (Stan Development Team, 2021b) and Stan reference manual (Stan Development Team, 2021a), at https://mc-stan.org/users/documentation/. Unlike BUGS and JAGS, Stan samples from the posterior via Hamiltonian Monte Carlo, which tends to scale particularly well for complex multilevel models. However, in this project we won’t be working with Stan directly. Rather, we’ll interface with it indirectly through brms. To my knowledge, brms is the most flexible and user-friendly interface for Stan within the R ecosystem. Talking about the various software options, Singer and Willett wrote: All have their strengths, and we use many of them in our research and in this book. At their core, each program does the same job; it fits the multilevel model for change to data and provides parameter estimates, measures of precision, diagnostics, and so on. There is also some evidence that all the different packages produce the same, or similar, answers to a given problem (I. G. G. Kreft &amp; de Leeuw, 1990). So, in one sense, it does not matter which program you choose. (p. 64) But importantly, in the next paragraph the authors clarified their text focused on “one particular method of estimation–maximum likelihood” (p. 64, emphasis in the original). This is quite important because, whereas we might expect various maximum-likelihood-based packages to yield the same or similar results, this will not necessarily hold when working with Bayesian software which, in addition to point estimates and expressions of uncertainty, yields an entire posterior distribution as a consequence of Bayes’ theorem, \\[ p(\\theta | d) = \\frac{p(d \\mid \\theta)\\ p(\\theta)}{p(d)}, \\] where \\(p(\\theta \\mid d)\\) is the posterior distribution, \\(p(d \\mid \\theta)\\) is the likelihood (i.e., the star of maximum likelihood), \\(p(\\theta)\\) are the priors, and \\(p(d)\\) is the normalizing constant, the probability of the data which transforms the numerator to a probability metric. Given that multilevel models are a fairly advanced topic, it is not my goal in this project to offer a tutorial on the foundations of applied Bayesian statistics. I’m taking it for granted you are familiar with the basics. But if you’re very ambitious and this is new or if you’re just rusty, I recommend you back up and lean the ropes with Richard McElreath’s excellent introductory text, Statistical Rethinking. He has great freely-available lectures that augment the text and I also have ebooks (here and here) translating both editions of his text into brms and tidyverse-style code. 3.4.1 The advantages of maximum likelihood Bayesian estimation. I just don’t think I have the strength to evangelize Bayes, at the moment. McElreath covered that a little bit in Statistical Rethinking, but he mainly took it for granted. Kruschke has been more of a Bayesian evangelist, examples of which are his 2015 text or his (2018) coauthored paper with Torrin Liddell, The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. My assumption is if you’re reading this, you’re already interested. 3.4.2 Using maximum likelihood modern Bayesian methods to fit a multilevel model. Just as with maximum likelihood, you have to specify a likelihood function with Bayes, too. The likelihood, \\(p(d | \\theta)\\), is half of the numerator of Bayes’ theorem and its meaning in Bayes is that same as with maximum likelihood estimation. The likelihood “describes the probability of observing the sample data as a function of the model’s unknown parameters” (p. 66). But unlike with maximum likelihood, we multiply the likelihood with the prior(s) and normalize the results so they’re in a probability metric. 3.5 Examining estimated fixed effects Singer and Willett discussed hypothesis testing in this section. The Bayesian paradigm can be used for hypothesis testing. For an introduction to this approach, check out Chapters 11 and 12 of Kruschke’s (2015) text. This will not be our approach in this project. My perspective on Bayesian modeling is more influenced by McElreath’s text and by Andrew Gelman’s various works. I like fitting models, inspecting their parameters, interpreting them from an effect-size perspective, and considering posterior predictions. You’ll see plenty of examples of this approach in the examples to come. 3.5.1 Interpreting estimated fixed effects. Singer and Willett presented the maximum likelihood results of our multilevel model in Table 3.3. Before we present ours, we’ll need to fit our corresponding Bayesian model. Let’s fire up brms. library(brms) Just like we did with the single-level models in the last chapter, we’ll fit our Bayesian multilevel models with the brm() function. fit3.2 &lt;- brm(data = early_int_sim, family = gaussian, formula = cog ~ 0 + Intercept + age_c + program + age_c:program + (1 + age_c | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99), seed = 3, file = &quot;fits/fit03.02&quot;) Compared to last chapter, we’ve added a few arguments. Notice the second line, family = gaussian. Remember all that likelihood talk from the last few sections? Well, with family = gaussian we’ve indicated we want to use the Gaussian likelihood function. As this is the brms default, we didn’t actually need to type out the argument. But we’ll follow this convention for the remainder of the text for two reasons. First, I hope it’s pedagogically useful to remind you of what likelihood you’re working with. Second, I think it’s generally a good idea to explicate your likelihood. In the context of the initial chapters of this text, this might seem unnecessary. We’ll constantly be using the Gaussian. But that’s largely a pedagogical decision made by the authors. There are lots of every-day applications for multilevel models with other likelihood functions, such as those suited for discrete data. And when the day comes you’ll need to fit a multilevel logistic regression model, you’ll need to use a different setting in the family argument. Plus, we will have some practice using other likelihood functions in the survival chapters later in the text. Here’s the big new thing: our formula line specified a multilevel model! Check out the (1 + age_c | id) syntax on the right. This syntax is designed to be similar to the that of the widely-used frequentist lme4 package. In the brms reference manual (Bürkner, 2021d), we lean this syntax follows the generic form (gterms | group) where “the optional gterms part may contain effects that are assumed to vary across grouping variables specified in group. We call them ‘group-level’ or ‘varying’ effects, or (adopting frequentist vocabulary) ‘random’ effects, although the latter name is misleading in a Bayesian context” (p. 35). And like with base R’s lm() function or with lme4, the 1 portion is a stand-in for the intercept. Thus, with 1 + age_c, we indicated we wanted the intercept and age_c slope to vary across groups. On the right side of the |, we defined our grouping variable as id. Another important part of the formula syntax concerns the intercept for the fixed effects. See the 0 + Intercept part? Here’s the deal: If we were using default behavior, we’d have coded either 1 + ... or just left that part out entirely. Both would have estimated the fixed intercept according to brms default behavior. But that’s the issue. By default, brms::brm() presumes your predictors are mean centered. This is critical because the default priors set by brms::brm() are also set based on this assumption. As it turns out, neither our age_c nor program variables are centered that way. program is a dummy variable and age_c is centered on 1, not the mean. Now since the brm() default priors are rather broad and uninformative, This probably wouldn’t have made much of a difference, here. However, we may as well address this issue now and avoid bad practices. So, with our 0 + Intercept solution, we told brm() to suppress the default intercept and replace it with our smartly-named Intercept parameter. This is our fixed effect for the population intercept and, importantly, brms() will assign default priors to it based on the data themselves without assumptions about centering. I’d like to acknowledge at this point that if brms and/or the multilevel model are new to you, this can be disorienting and confusing. I’m sorry. The world is unfair and Singer and Willett didn’t write their text with brms in mind. Had they done so, they’d have used mean-centered predictors for the first few models to put off this technical issue until later chapters. Yet here we are. So if you’re feeling frustrated, that mainly just means you’re paying attention. Good job! Forge on, friends. It’ll get better. The line starting with iter largely explicates brms::brm() default settings. The only change from the defaults is cores = 4, which allow you to sample from all four chains simultaneously. The control line opens up a can of worms I just don’t want to address at this point in the text. It’s a technical setting that helped us do a better job sampling form the posterior. We’ll have more opportunities to talk about it later. For now, know that the default setting for adapt_delta is 0.8. The value ranges from 0 to 1. The last line of interest is seed = 3. Markov chain Monte Carlo methods use pseudo-random number generators to sample from the posterior. To make the results of a pseudo-random process reproducible, you set the seed. There’s nothing special about setting it to 3. I just did so because that’s the chapter we’re on. Play around with other values and see what happens. Okay, that’s a lot of boring technical talk. Let’s use print() to see what we did! print(fit3.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cog ~ 0 + Intercept + age_c + program + age_c:program + (1 + age_c | id) ## Data: early_int_sim (Number of observations: 309) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 103) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9.71 1.18 7.54 12.19 1.00 717 1723 ## sd(age_c) 3.88 2.38 0.17 8.82 1.01 269 733 ## cor(Intercept,age_c) -0.47 0.37 -0.96 0.59 1.01 2390 1423 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 106.57 1.89 102.97 110.32 1.00 995 1608 ## age_c -20.53 1.92 -24.35 -16.81 1.00 2192 2784 ## program 9.13 2.52 4.17 14.02 1.00 956 1625 ## age_c:program 3.22 2.59 -1.80 8.29 1.00 1937 2971 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.58 0.52 7.61 9.60 1.00 630 1466 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’re in multilevel-model land, we have three main sections in the output. Let’s start in the middle. The ‘Population-Level Effects:’ section contains our brms analogues to the “Fixed Effects” portion of Singer and Willett’s Table 3.3. These are our \\(\\gamma\\) parameter summaries. Like we briefly covered in the last chapter, brms does not give \\(z\\)- or \\(p\\)-values. But we do get high-quality percentile-based Bayesian 95% intervals. Perhaps somewhat frustratingly, our ‘Estimate’ values are a little different from those in the text. In this instance, this is more due to us not having access to the original data than differences between Bayesian and maximum likelihood estimation. They’ll be closer in many other examples. The bottom section, ‘Family Specific Parameters:’, is our analogue to the first line in Table 3.3’s “Variance Components” section. What Singer and Willett referred to as \\(\\epsilon_{ij}\\), the brms package calls sigma. But importantly, do recall that Stan and brms parameterize variance components in the standard-deviation metric. You’d have to square our sigma to put it in a similar metric to the estimate in the text. This will be the case throughout this project. But why call this section ‘Family Specific Parameters’? Well, not all likelihoods have a \\(\\sigma\\) parameter. In the Poisson likelihood, for example, the mean and variance scale together as one parameter called \\(\\lambda\\). Since brms is designed to handle a whole slew of likelihood functions (see the Parameterization of response distributions in brms vignette, Bürkner, 2021c) it behooved Bürkner to give this section a generic name. Now we’re ready to draw our attention to the topmost section. The ‘Group-Level Effects:’ are our brms variants of the Level 2 section of Singer and Willett’s “Variance Components” section in Table 3.3. Our sd(Intercept) corresponds to their \\(\\sigma_0^2\\) and our sd(age_c) corresponds to their \\(\\sigma_1^2\\). But recall, ours are in a standard-deviation metric. The estimates in the book are expressed as variances. Finally, our cor(Intercept,age_c) parameter is a correlation among the varying-effects, whereas Singer and Willett’s \\(\\sigma_{01}\\) is a covariance. In addition to print(), a handy way to pull the fixed effects from a brm() model is with the fixef() function. fixef(fit3.2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 106.574645 1.892751 102.973171 110.321036 ## age_c -20.525399 1.923034 -24.346870 -16.813264 ## program 9.129629 2.520420 4.167805 14.023563 ## age_c:program 3.219506 2.592880 -1.802804 8.293352 You can subset its components with [] syntax. E.g., here we’ll pull the posterior mean for the overall intercept and round to two decimal places. fixef(fit3.2)[1, 1] %&gt;% round(digits = 2) ## [1] 106.57 Thus, we can write our version of the equations atop page 70 as \\(\\hat{\\pi}_{0i} =\\) 106.57 \\(+\\) 9.13\\(\\text{program}_i\\) and \\(\\hat{\\pi}_{1i} =\\) -20.53 \\(+\\) 3.22\\(\\text{program}_i\\). Again, our results differ largely because they’re based on simulated data rather than the real data in the text. We’ll be able to work with the original data in the later chapters. Anyway, here are the results of those two equations. fixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] ## [1] 115.7043 fixef(fit3.2)[2, 1] + fixef(fit3.2)[4, 1] ## [1] -17.30589 Here’s how to get our estimates corresponding to the values at the bottom of page 70. # when `program` is 0 fixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] * 0 ## [1] 106.5746 fixef(fit3.2)[2, 1] * 1 + fixef(fit3.2)[4, 1] * 1 * 0 ## [1] -20.5254 # when `program` is 1 fixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] * 1 ## [1] 115.7043 fixef(fit3.2)[2, 1] * 1 + fixef(fit3.2)[4, 1] * 1 * 1 ## [1] -17.30589 To make our version of Figure 3.5, we’ll pump the necessary age_c and program values into the full formula of the fixed effects. # specify the values for our covariates `age_c` and `program` crossing(age_c = 0:1, program = 0:1) %&gt;% # push those values through the fixed effects mutate(cog = fixef(fit3.2)[1, 1] + fixef(fit3.2)[2, 1] * age_c + fixef(fit3.2)[3, 1] * program + fixef(fit3.2)[4, 1] * age_c * program, # wrangle a bit age = age_c + 1, size = ifelse(program == 1, 1/5, 3), program = factor(program, levels = c(&quot;0&quot;, &quot;1&quot;))) %&gt;% # plot! ggplot(aes(x = age, y = cog, group = program)) + geom_line(aes(size = program)) + scale_size_manual(values = c(1, 1/2)) + scale_x_continuous(breaks = c(1, 1.5, 2)) + ylim(50, 150) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) 3.5.2 Single parameter tests for the fixed effects. As in regular regression, you can conduct a hypothesis test on each fixed effect (each \\(\\gamma\\)) using a single parameter text. Although you can equate the parameter value to any pre-specified value in your hypothesis text, most commonly you examine the null hypothesis that, controlling for all other predictors in the model, the population value of the parameter is 0, \\(H_0: \\gamma = 0\\), against the two-sided alternative that it is not, \\(H_1: \\gamma \\neq 0\\). (p. 71) You can do this with brms with the hypothesis() function. I’m not a fan of this method and am not going to showcase it in this ebook If you insist on the NHST paradigm, you’ll have to go that alone. Within the Bayesian paradigm, we have an entire posterior distribution. So let’s just look at that. Recall that in the print() and fixef() outputs, we get the parameter estimates for our \\(\\gamma\\)’s summarized in terms of the posterior mean (i.e., ‘Estimate’), the posterior standard deviation (i.e., ‘Est.error’), and the percentile-based 95% credible intervals (i.e., ‘Q2.5’ and ‘Q97.5’). But we can get much richer output with the as_draws_df() function. draws &lt;- as_draws_df(fit3.2) Here’s a look at the first 10 columns. draws[, 1:10] %&gt;% glimpse() ## Warning: Dropping &#39;draws_df&#39; class as required metadata was removed. ## Rows: 4,000 ## Columns: 10 ## $ b_Intercept &lt;dbl&gt; 105.0812, 107.2022, 107.5520, 112.0641, 102.0589, 105.3338, 106.0783, 104.6… ## $ b_age_c &lt;dbl&gt; -19.58419, -18.57810, -19.52492, -26.15283, -15.74047, -17.71064, -21.51444… ## $ b_program &lt;dbl&gt; 10.127459, 10.151319, 6.437316, 1.290667, 12.858437, 9.999833, 8.935408, 11… ## $ `b_age_c:program` &lt;dbl&gt; 2.0817137, 0.1803538, 3.4146138, 10.7849733, -3.3919367, -0.5185154, 5.6761… ## $ sd_id__Intercept &lt;dbl&gt; 8.826271, 8.861601, 10.498425, 10.549804, 8.722583, 9.919061, 9.365331, 11.… ## $ sd_id__age_c &lt;dbl&gt; 1.8588494, 0.3698994, 1.8249990, 3.2982064, 3.7854115, 5.3781747, 4.6008309… ## $ cor_id__Intercept__age_c &lt;dbl&gt; -0.353411206, -0.885412900, -0.487184296, -0.646354120, -0.444898699, -0.46… ## $ sigma &lt;dbl&gt; 8.958198, 8.592231, 9.038327, 8.787568, 8.137893, 8.075000, 8.926294, 8.142… ## $ `r_id[1,Intercept]` &lt;dbl&gt; 10.8668848, 3.2803299, 7.9896124, 4.5510258, 7.0021341, 8.0422714, 5.548063… ## $ `r_id[2,Intercept]` &lt;dbl&gt; 2.4655328, 1.9958192, -1.6457171, -2.7461001, 1.9902722, -3.4767945, -5.261… Note the warning message. The as_draws_df() function returns a special kind of data frame, which includes 3 metadata columns, which are typically hidden from view. Sometimes we want those metadata columns, sometimes we don’t. You’ll see. Anyway, here are the dimensions of our draws data frame. draws %&gt;% dim() ## [1] 4000 219 We saved our results as draws, which is a data frame with 4,000 rows (i.e., 1,000 post-warmup posterior draws times 4 chains) and columns depicting the model parameters, as well as the metadata values. With brms, the \\(\\gamma\\) parameters (i.e., the fixed effects or population parameters) get b_ prefixes in the as_draws_df() output. So we can isolate them like so. draws %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% head() ## # A tibble: 6 × 4 ## b_Intercept b_age_c b_program `b_age_c:program` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105. -19.6 10.1 2.08 ## 2 107. -18.6 10.2 0.180 ## 3 108. -19.5 6.44 3.41 ## 4 112. -26.2 1.29 10.8 ## 5 102. -15.7 12.9 -3.39 ## 6 105. -17.7 10.0 -0.519 Just a little more data wrangling will put draws in a format suitable for plotting. draws %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% ggplot(aes(x = value)) + geom_density(color = &quot;transparent&quot;, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Sure, you could fixate on zero if you wanted to. But of more interest is the overall shape of each parameter’s posterior distribution. Look at the central tendency and spread for each. Look at where each is in the parameter space. To my mind, that story is so much richer than fixating on zero. We’ll have more to say along these lines in subsequent chapters. 3.6 Examining estimated variance components Estimated variance and covariance components are trickier to interpret as their numeric values have little absolute meaning and there are no graphic aids to fall back on. Interpretation for a single fitted model is especially difficult as you lack benchmarks for evaluating the components’ magnitudes. This increases the utility of hypothesis testing, for at least the tests provide some benchmark (against the null value of 0) for comparison. (p. 72) No, no, no! I do protest. No! As I hope to demonstrate, our Bayesian brms paradigm offers rich and informative alternatives to the glib picture Singer and Willett painted back in 2003. Nowadays, we have full Bayesian estimation with Stan. Rejoice, friends. Rejoice. 3.6.1 Interpreting the estimated variance components. To extract just the variance components of a brm() model, use the VarCorr() function. VarCorr(fit3.2) ## $id ## $id$sd ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 9.711901 1.182262 7.5388995 12.189325 ## age_c 3.878702 2.378860 0.1741854 8.817749 ## ## $id$cor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 1.0000000 0.0000000 1.0000000 1.0000000 ## age_c -0.4681918 0.3663352 -0.9583428 0.5876137 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.4681918 0.3663352 -0.9583428 0.5876137 ## age_c 1.0000000 0.0000000 1.0000000 1.0000000 ## ## ## $id$cov ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 95.71841 23.37401 56.83501 148.579655 ## age_c -21.52791 19.08353 -65.88728 4.381725 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -21.52791 19.08353 -65.88728147 4.381725 ## age_c 20.70189 21.58027 0.03034056 77.752702 ## ## ## ## $residual__ ## $residual__$sd ## Estimate Est.Error Q2.5 Q97.5 ## 8.581649 0.5217765 7.610707 9.60182 In case that output is confusing, VarCorr() returned a 2-element list of lists. We can use the [[]] subsetting syntax to isolate the first list of lists. VarCorr(fit3.2)[[1]] %&gt;% str() ## List of 3 ## $ sd : num [1:2, 1:4] 9.71 3.88 1.18 2.38 7.54 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## $ cor: num [1:2, 1:4, 1:2] 1 -0.468 0 0.366 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## $ cov: num [1:2, 1:4, 1:2] 95.7 -21.5 23.4 19.1 56.8 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_c&quot; If you just want the \\(\\zeta\\)’s, subset the first list of the first list. VarCorr(fit3.2)[[1]][[1]] ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 9.711901 1.182262 7.5388995 12.189325 ## age_c 3.878702 2.378860 0.1741854 8.817749 Here’s how to get their correlation matrix. VarCorr(fit3.2)[[1]][[2]] ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 1.0000000 0.0000000 1.0000000 1.0000000 ## age_c -0.4681918 0.3663352 -0.9583428 0.5876137 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.4681918 0.3663352 -0.9583428 0.5876137 ## age_c 1.0000000 0.0000000 1.0000000 1.0000000 And perhaps of great interest, here’s how to get their variance/covariance matrix. VarCorr(fit3.2)[[1]][[3]] ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 95.71841 23.37401 56.83501 148.579655 ## age_c -21.52791 19.08353 -65.88728 4.381725 ## ## , , age_c ## ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -21.52791 19.08353 -65.88728147 4.381725 ## age_c 20.70189 21.58027 0.03034056 77.752702 You can also use the appropriate algebraic operations to transform some of the columns in the as_draws_df() output into the variance metric used in the text. Here we’ll do so for the elements in the variance/covariance matrix and \\(\\sigma_\\epsilon^2\\), too. as_draws_df(fit3.2) %&gt;% mutate(`sigma[0]^2` = sd_id__Intercept^2, `sigma[1]^2` = sd_id__age_c^2, `sigma[0][1]` = sd_id__Intercept * cor_id__Intercept__age_c * sd_id__age_c, `sigma[epsilon]^2` = sigma^2) %&gt;% pivot_longer(starts_with(&quot;sigma[&quot;), values_to = &quot;posterior&quot;) %&gt;% ggplot(aes(x = posterior)) + geom_density(color = &quot;transparent&quot;, fill = &quot;grey33&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank(), strip.text = element_text(size = 12)) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) As it turns out, the multilevel variance components are often markedly non-Gaussian. This is important for the next section. 3.6.2 Single parameter tests for the variance components. Statisticians disagree as to the nature, form, and effectiveness of these tests. Rupert G. Miller (1997), Raudenbush &amp; Bryk (2002), and others have long questioned their utility because of their sensitivity to departures from normality. Longford (1999) describes their sensitivity to sample size and imbalance (unequal numbers of observations per person) and argues that they are so misleading that they should be abandoned completely. (p. 73) This reminds me of parts from Gelman and Hill’s (2006) text on multilevel models. In Section 12.7 on the topic of model building and statistical significance, they wrote: It is not appropriate to use statistical significance as a criterion for including particular group indicators in a multilevel model…. [They went on to discuss a particular example from the text, regarding radon levels in housed in various counties.] However, we should include all 85 counties in the model, and nothing is lost by doing so. The purpose of the multilevel model is not to use whether radon levels in county 1 are statistically significantly different from those in county 2, or from the Minnesota average. Rather, we seek the best possible estimate in each county, with appropriate accounting for uncertainty. Rather that make some significance threshold, we allow all the intercepts to vary and recognize that we may not have much precision in many of the individual groups…. The same principle holds for the models discussed in the following chapters, which include varying slopes, non-nested levels, discrete data, and other complexities. Once we have included a source of variation, we do not use statistical significance to pick and choose indicators to include or exclude from the model. In practice, our biggest constraints–the main reasons we do not use extremely elaborate models in which all coefficients can vary with respect to all grouping factors–are fitting and understanding complex models. The lmer() function works well when it works, but it can break down for models with many groping factors. (p. 272, emphasis in the original) For context, lmer() is the primary function in the frequentist lme4 package. After pointing out difficulties with lmer(), they went on to point out how the Bayesian Bugs software can often overcome limitations in frequentist packages. We now have the benefit of Stan and brms. My general recommendation is if your theory suggests there should be group-level variability and you’ve collected the necessary data to fit that model, fit the full model. 3.7 Bonus: How did you simulate that data? What makes our task difficult is the multilevel model we’d like to simulate our data for has both varying intercepts and slopes. And worst yet, those varying intercepts and slopes have a correlation structure. Also of note, Singer and Willett presented their summary statistics in the form of a variance/covariance matrix in Table 3.3. As it turns out, the mvnorm() function from the MASS package (Ripley, 2022; Venables &amp; Ripley, 2002) will allow us to simulate multivariate normal data from a given mean structure and variance/covariance matrix. So our first step in simulating our data is to simulate the \\(103 – 8 = 95\\) \\(\\zeta\\) values. We’ll name the results z. # how many people are we simulating? n &lt;- 103 - 8 # what&#39;s the variance/covariance matrix? sigma &lt;- matrix(c(124.64, -36.41, -36.41, 12.29), ncol = 2) # what&#39;s our mean structure? mu &lt;- c(0, 0) # set the seed and simulate! set.seed(3) z &lt;- MASS::mvrnorm(n = n, mu = mu, Sigma = sigma) %&gt;% data.frame() %&gt;% set_names(&quot;zeta_0&quot;, &quot;zeta_1&quot;) head(z) ## zeta_0 zeta_1 ## 1 10.7586672 -3.0908765 ## 2 3.4258938 -0.4186497 ## 3 -3.0770183 0.2140130 ## 4 12.5303603 -4.9043416 ## 5 -2.1114641 0.8936950 ## 6 -0.5521597 -0.6310265 For our next step, we’ll define our \\(\\gamma\\) parameters. These are also taken from Table 3.3. g &lt;- tibble(id = 1:n, gamma_00 = 107.84, gamma_01 = 6.85, gamma_10 = -21.13, gamma_11 = 5.27) head(g) ## # A tibble: 6 × 5 ## id gamma_00 gamma_01 gamma_10 gamma_11 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 108. 6.85 -21.1 5.27 ## 2 2 108. 6.85 -21.1 5.27 ## 3 3 108. 6.85 -21.1 5.27 ## 4 4 108. 6.85 -21.1 5.27 ## 5 5 108. 6.85 -21.1 5.27 ## 6 6 108. 6.85 -21.1 5.27 Note how they’re the same for each row. That’s the essence of the meaning of a fixed effect. Anyway, this next block is a big one. After we combine g and z, we add in the appropriate program and age_c values. You can figure out those from pages 46 and 47. We then insert our final model parameter, \\(\\epsilon\\), and combine the \\(\\gamma\\)’s and \\(\\zeta\\)’s to make our two \\(\\pi\\) parameters (see page 60). Once that’s all in place, we’re ready to use the model formula to calculate the expected cog values from the \\(\\pi\\)’s, age_c, and \\(\\epsilon\\). # set the seed for the second `mutate()` line set.seed(3) early_int_sim &lt;- bind_cols(g, z) %&gt;% mutate(program = rep(1:0, times = c(54, 41))) %&gt;% expand(nesting(id, gamma_00, gamma_01, gamma_10, gamma_11, zeta_0, zeta_1, program), age_c = c(0, 0.5, 1)) %&gt;% mutate(epsilon = rnorm(n(), mean = 0, sd = sqrt(74.24))) %&gt;% mutate(pi_0 = gamma_00 + gamma_01 * program + zeta_0, pi_1 = gamma_10 + gamma_11 * program + zeta_1) %&gt;% mutate(cog = pi_0 + pi_1 * age_c + epsilon) head(early_int_sim) ## # A tibble: 6 × 13 ## id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c epsilon pi_0 pi_1 cog ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0 -8.29 125. -19.0 117. ## 2 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0.5 -2.52 125. -19.0 113. ## 3 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 1 2.23 125. -19.0 109. ## 4 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0 -9.93 118. -16.3 108. ## 5 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0.5 1.69 118. -16.3 112. ## 6 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 1 0.260 118. -16.3 102. But before we do, we’ll want to wrangle a little. We need an age column. If you look closely at Table 3.3, you’ll see all the cog values are integers. So we’ll round ours to match. Finally, we’ll want to renumber our id values to match up better with those in Table 3.3. early_int_sim &lt;- early_int_sim %&gt;% mutate(age = age_c + 1, cog = round(cog, digits = 0), id = ifelse(id &gt; 54, id + 900, id)) head(early_int_sim) ## # A tibble: 6 × 14 ## id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c epsilon pi_0 pi_1 cog age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0 -8.29 125. -19.0 117 1 ## 2 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 0.5 -2.52 125. -19.0 113 1.5 ## 3 1 108. 6.85 -21.1 5.27 10.8 -3.09 1 1 2.23 125. -19.0 109 2 ## 4 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0 -9.93 118. -16.3 108 1 ## 5 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 0.5 1.69 118. -16.3 112 1.5 ## 6 2 108. 6.85 -21.1 5.27 3.43 -0.419 1 1 0.260 118. -16.3 102 2 Finally, now we just need to prune the columns with the model parameters, rearrange the order of the columns we’d like to keep, and join these data with those from Table 3.3. early_int_sim &lt;- early_int_sim %&gt;% select(id, age, cog, program, age_c) %&gt;% full_join(early_int, by = c(&quot;id&quot;, &quot;age&quot;, &quot;cog&quot;, &quot;program&quot;, &quot;age_c&quot;)) %&gt;% arrange(id, age) glimpse(early_int_sim) ## Rows: 309 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10,… ## $ age &lt;dbl&gt; 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.… ## $ cog &lt;dbl&gt; 117, 113, 109, 108, 112, 102, 112, 113, 85, 138, 110, 97, 106, 107, 99, 111, 98, 92, 124, 10… ## $ program &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ age_c &lt;dbl&gt; 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.… Here we save our results in an external file for use later. save(early_int_sim, file = &quot;data/early_int_sim.rda&quot;) Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.19.0 Rcpp_1.0.10 broom_1.0.4 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 ## [7] dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 ggplot2_3.4.2 ## [13] tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] gridExtra_2.3 inline_0.3.19 sandwich_3.0-2 rlang_1.1.1 magrittr_2.0.3 ## [6] multcomp_1.4-23 matrixStats_0.63.0 compiler_4.3.0 mgcv_1.8-42 loo_2.6.0 ## [11] callr_3.7.3 vctrs_0.6.2 reshape2_1.4.4 pkgconfig_2.0.3 crayon_1.5.2 ## [16] fastmap_1.1.1 backports_1.4.1 ellipsis_0.3.2 labeling_0.4.2 utf8_1.2.3 ## [21] threejs_0.3.3 promises_1.2.0.1 rmarkdown_2.21 markdown_1.7 tzdb_0.4.0 ## [26] nloptr_2.0.3 ps_1.7.5 xfun_0.39 cachem_1.0.8 jsonlite_1.8.4 ## [31] highr_0.10 later_1.3.1 prettyunits_1.1.1 parallel_4.3.0 R6_2.5.1 ## [36] dygraphs_1.1.1.6 StanHeaders_2.26.25 bslib_0.4.2 stringi_1.7.12 boot_1.3-28.1 ## [41] estimability_1.4.1 jquerylib_0.1.4 bookdown_0.34 rstan_2.21.8 knitr_1.42 ## [46] zoo_1.8-12 base64enc_0.1-3 bayesplot_1.10.0 httpuv_1.6.11 Matrix_1.5-4 ## [51] splines_4.3.0 igraph_1.4.2 timechange_0.2.0 tidyselect_1.2.0 rstudioapi_0.14 ## [56] abind_1.4-5 codetools_0.2-19 miniUI_0.1.1.1 processx_3.8.1 pkgbuild_1.4.0 ## [61] lattice_0.21-8 plyr_1.8.8 shiny_1.7.4 withr_2.5.0 bridgesampling_1.1-2 ## [66] posterior_1.4.1 coda_0.19-4 evaluate_0.21 survival_3.5-5 RcppParallel_5.1.7 ## [71] xts_0.13.1 pillar_1.9.0 tensorA_0.36.2 checkmate_2.2.0 DT_0.27 ## [76] stats4_4.3.0 shinyjs_2.1.0 distributional_0.3.2 generics_0.1.3 hms_1.1.3 ## [81] rstantools_2.3.1 munsell_0.5.0 scales_1.2.1 minqa_1.2.5 gtools_3.9.4 ## [86] xtable_1.8-4 gamm4_0.2-6 glue_1.6.2 emmeans_1.8.6 projpred_2.5.0 ## [91] tools_4.3.0 shinystan_2.6.0 lme4_1.1-33 colourpicker_1.2.0 mvtnorm_1.1-3 ## [96] grid_4.3.0 crosstalk_1.2.0 colorspace_2.1-0 nlme_3.1-162 cli_3.6.1 ## [101] fansi_1.0.4 viridisLite_0.4.2 Brobdingnag_1.2-9 gtable_0.3.3 sass_0.4.6 ## [106] digest_0.6.31 TH.data_1.1-2 htmlwidgets_1.6.2 farver_2.1.1 htmltools_0.5.5 ## [111] lifecycle_1.0.3 mime_0.12 MASS_7.3-58.4 shinythemes_1.2.0 References Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Bates, D., Maechler, M., Bolker, B., &amp; Steven Walker. (2022). lme4: Linear mixed-effects models using Eigen’ and S4. https://CRAN.R-project.org/package=lme4 Bryk, A. S., &amp; Raudenbush, S. W. (1987). Application of hierarchical linear models to assessing change. Psychological Bulletin, 101(1), 147. https://doi.org/10.1037/0033-2909.101.1.147 Bürkner, P.-C. (2021c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2021d). brms reference manual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942 Gilks, W. R., Richardson, S., &amp; Spiegelhalter, D. (1995). Markov chain Monte Carlo in practice. Chapman and Hall/CRC. https://www.routledge.com/Markov-Chain-Monte-Carlo-in-Practice/Gilks-Richardson-Spiegelhalter/p/book/9780412055515 Kreft, I. G. G., &amp; de Leeuw, J. (1990). Comparing four different statistical packages for hierarchical linear regression: GENMOD, HLM, ML2, and VARCL. CSE Dissemination Office, UCLA Graduate School of Education, 405 Hilgard Avenue, Los Angeles, CA 90024-1521. https://files.eric.ed.gov/fulltext/ED340731.pdf Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kruschke, J. K., &amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin &amp; Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4 Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 McElreath, R. (2020a). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ Plummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. Proceedings of the 3rd International Workshop on Distributed Statistical Computing, 124, 1–10. http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf Plummer, M. (2012). JAGS Version 3.3.0 user manual. http://www.stat.cmu.edu/~brian/463-663/week10/articles,%20manuals/jags_user_manual.pdf Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (Second Edition). SAGE Publications, Inc. https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230 Ripley, B. (2022). MASS: Support functions and datasets for venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS Robinson, D., Hayes, A., &amp; Couch, S. (2022). broom: Convert statistical objects into tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom Rogosa, D. R., &amp; Willett, J. B. (1985). Understanding correlates of change by modeling individual differences in growth. Psychometrika, 50(2), 203–228. https://doi.org/10.1007/BF02294247 Rupert G. Miller, Jr. (1997). Beyond ANOVA: Basics of applied statistics. Chapman and Hall/CRC. https://www.routledge.com/Beyond-ANOVA-Basics-of-Applied-Statistics/Jr/p/book/9780412070112 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Stan Development Team. (2021a). Stan reference manual, Version 2.27. https://mc-stan.org/docs/2_27/reference-manual/ Stan Development Team. (2021b). Stan user’s guide, Version 2.26. https://mc-stan.org/docs/2_26/stan-users-guide/index.html Venables, W. N., &amp; Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4 "],["doing-data-analysis-with-the-multilevel-model-for-change.html", "4 Doing Data Analysis with the Multilevel Model for Change 4.1 Example: Changes in adolescent alcohol use 4.2 The composite specification of the multilevel model for change 4.3 Methods of estimation, revisited 4.4 First steps: Fitting two unconditional multilevel models for change 4.5 Practical data analytic strategies for model building 4.6 Comparing models using deviance statistics 4.7 Using Wald statistics to test composite hypotheses about fixed effects 4.8 Evaluating the tenability of a model’s assumptions 4.9 Model-based (Empirical Bayes) estimates of the individual growth parameters Session info", " 4 Doing Data Analysis with the Multilevel Model for Change “We now delve deeper into the specification, estimation, and interpretation of the multilevel model for change” (Singer &amp; Willett, 2003, p. 75). 4.1 Example: Changes in adolescent alcohol use Load the data. library(tidyverse) alcohol1_pp &lt;- read_csv(&quot;data/alcohol1_pp.csv&quot;) head(alcohol1_pp) ## # A tibble: 6 × 9 ## id age coa male age_14 alcuse peer cpeer ccoa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 14 1 0 0 1.73 1.26 0.247 0.549 ## 2 1 15 1 0 1 2 1.26 0.247 0.549 ## 3 1 16 1 0 2 2 1.26 0.247 0.549 ## 4 2 14 1 1 0 0 0.894 -0.124 0.549 ## 5 2 15 1 1 1 0 0.894 -0.124 0.549 ## 6 2 16 1 1 2 1 0.894 -0.124 0.549 Do note we already have an \\((\\text{age} - 14)\\) variable in the data, age_14. Here’s our version of Figure 4.1, using stat_smooth() to get the exploratory OLS trajectories. alcohol1_pp %&gt;% filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% ggplot(aes(x = age, y = alcuse)) + stat_smooth(method = &quot;lm&quot;, se = F) + geom_point() + coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ id, ncol = 4) By this figure, Singer and Willett suggested the simple linear level-1 submodel following the form \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 14) + \\epsilon_{ij}\\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon^2), \\end{align*}\\] where \\(\\pi_{0i}\\) is the initial status of participant \\(i\\), \\(\\pi_{1i}\\) is participant \\(i\\)’s rate of change, and \\(\\epsilon_{ij}\\) is the variation in participant \\(i\\)’s data not accounted for in the model. Singer and Willett made their Figure 4.2 “with a random sample of 32 of the adolescents” (p. 78). If we just wanted a random sample of rows, the sample_n() function would do the job. But since we’re working with long data, we’ll need some group_by() + nest() mojo. I got the trick from Jenny Bryan’s vignette, Sample from groups, n varies by group. Setting the seed makes the results from sample_n() reproducible. Here are the top panels. set.seed(4) alcohol1_pp %&gt;% group_by(id) %&gt;% nest() %&gt;% sample_n(size = 32, replace = T) %&gt;% unnest(data) %&gt;% mutate(coa = ifelse(coa == 0, &quot;coa = 0&quot;, &quot;coa = 1&quot;)) %&gt;% ggplot(aes(x = age, y = alcuse, group = id)) + stat_smooth(method = &quot;lm&quot;, se = F, linewidth = 1/4) + coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ coa) We have similar data wrangling needs for the bottom panels. set.seed(4) alcohol1_pp %&gt;% group_by(id) %&gt;% nest() %&gt;% ungroup() %&gt;% sample_n(size = 32, replace = T) %&gt;% unnest(data) %&gt;% mutate(hp = ifelse(peer &lt; mean(peer), &quot;low peer&quot;, &quot;high peer&quot;)) %&gt;% mutate(hp = factor(hp, levels = c(&quot;low peer&quot;, &quot;high peer&quot;))) %&gt;% ggplot(aes(x = age, y = alcuse, group = id)) + stat_smooth(method = &quot;lm&quot;, se = F, linewidth = 1/4) + coord_cartesian(xlim = c(13, 17), ylim = c(-1, 4)) + theme(panel.grid = element_blank()) + facet_wrap(~ hp) Based on the exploratory analyses, Singer and Willett posited the initial level-2 submodel might take the form $$ \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\zeta_{0i}\\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{coa}_i + \\zeta_{1i} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01}\\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}, \\end{align*}\\] $$ where \\(\\gamma_{00}\\) and \\(\\gamma_{10}\\) are the level-2 intercepts, the population averages when \\(\\text{coa} = 0\\), \\(\\gamma_{10}\\) and \\(\\gamma_{11}\\) are the level-2 slopes expressing the difference when \\(\\text{coa} = 1\\) and \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) are the unexplained variation across the \\(\\text{id}\\)-level intercepts and slopes. Since we’ll be fitting the model with brms::brm(), the \\(\\Sigma\\) matrix will be parameterized in terms of standard deviations and their correlation. So we might reexpress the model as $$ \\[\\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\zeta_{0i}\\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{coa}_i + \\zeta_{1i} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0 &amp; \\rho_{01}\\\\ \\rho_{01} &amp; \\sigma_1 \\end{bmatrix} \\end{pmatrix}. \\end{align*}\\] $$ 4.2 The composite specification of the multilevel model for change With a little algebra, we can combine the level-1 and level-2 submodels into the composite multilevel model for change, which follows the form $$ \\[\\begin{align*} \\text{alcuse}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{10} \\text{age_14}_{ij} + \\gamma_{01} \\text{coa}_i + \\gamma_{11} (\\text{coa}_i \\times \\text{age_14}_{ij}) \\big ] \\\\ &amp; \\;\\;\\;\\;\\; + [ \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}, \\end{align*}\\] $$ where the brackets in the first line partition the structural model (i.e., the model for \\(\\mu\\)) and the stochastic components (i.e., the \\(\\sigma\\) terms). We should note that this is the format that most closely mirrors what we use in the formula argument in brms::brm(). As long as age is not centered on the mean, our brms syntax would be: formula = alcuse ~ 0 + Intercept + age_c + coa + age_c:coa + (1 + age_c | id). 4.2.1 The structural component of the composite model. Although their interpretation is identical, the \\(\\gamma\\)s in the composite model describe patterns of change in a different way. Rather than postulating first how ALCUSE is related to TIME and the individual growth parameters, and second how the individual growth parameters are related to COA, the composite specification in equation 4.3 postulates that ALCUSE depends simultaneously on: (1) the level-1 predictor, TIME; (2) the level-2 predictor, COA; and (3) the cross-level interaction, COA by TIME. From this perspective, the composite model’s structural portion strongly resembles a regular regression model with predictors, TIME and COA, appearing as main effects (associated with \\(\\gamma_{10}\\) and \\(\\gamma_{01}\\), respectively) and in a cross-level interaction (associated with \\(\\gamma_{11}\\)). (p. 82, emphasis in the original) 4.2.2 The stochastic component of the composite model. A distinctive feature of the composite multilevel model is its composite residual, the three terms in the second set of brackets on the right of equation 4.3 that combine together the level-1 residual and the two level-2 residuals: \\[\\text{Composite residual: } [ \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} ].\\] The composite residual is not a simple sum. Instead, the second level-2 residual, \\(\\zeta_{1i}\\), is multiplied by the level-1 predictor, \\([\\text{age_14}_{ij}]\\), before joining its siblings. Despite its unusual construction, the interpretation of the composite residual is straightforward: it describes the difference between the observed and expected value of \\([\\text{alcuse}]\\) for individual \\(i\\) on occasion \\(j\\). The mathematical form of the composite residual reveals two important properties about the occasion-specific residuals not readily apparent in the level-1/level-2 specification: they can be both autocorrelated and heteroscedastic within person. (p. 84, emphasis in the original) 4.3 Methods of estimation, revisited In this section, the authors introduced generalized least squares (GLS) estimation and iterative generalized least squares (IGLS) estimation and then distinguished between full and restricted maximum likelihood estimation. Since our goal is to fit these models as Bayesians, we won’t be using or discussing any of these in this project. There are, of course, different ways to approach Bayesian estimation. Though we’re using Hamiltonian Monte Carlo, we could use other algorithms, such as the Gibbs sampler. However, all that is outside of the scope of this project. I suppose the only thing to add is that whereas GLS estimates come from minimizing a weighted function of the residuals and maximum likelihood estimates come from maximizing the log-likelihood function, the results of our Bayesian analyses (i.e., the posterior distribution) come from the consequences of Bayes’ theorem, \\[ p(\\theta \\mid d) = \\frac{p(d \\mid \\theta)\\ p(\\theta)}{p(d)}. \\] If you really want to dive into the details of this, I suggest referencing a proper introductory Bayesian textbook, such as McElreath (2020a, 2015), Kruschke (2015), or Gelman et al. (2013). I haven’t had time to check it out, but I’ve heard Labmert’s (2018) text is good, too. And for details specific to Stan, and thus brms, you might check out the documentation resources at https://mc-stan.org/users/documentation/. 4.4 First steps: Fitting two unconditional multilevel models for change Singer and Willett recommended that before you fit your full theoretical multilevel model of change–the one with all the interesting covariates–you should fit two simpler preliminary models. The first is the unconditional means model. The second is the unconditional growth model. I agree. In addition to the reasons they cover in the text, this is just good pragmatic data analysis. Start simple and build up to the more complicated models only after you’re confident you understand what’s going on with the simpler ones. And if you’re new to them, you’ll discover this is especially so with Bayesian methods. 4.4.1 The unconditional means model. The likelihood for the unconditional means model follows the formula \\[ \\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\zeta_{0i} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon^2) \\\\ \\zeta_{0i} &amp; \\sim \\operatorname{Normal}(0, \\sigma_0^2). \\end{align*} \\] Let’s open brms. library(brms) Up till this point, we haven’t focused on priors. It would have been reasonable to wonder if we’d been using them at all. Yes, we have. Even if you don’t specify priors in the brm() function, it’ll compute default weakly-informative priors for you. You might be wondering, What might these default priors look like? The get_prior() function let us take a look. get_prior(data = alcohol1_pp, family = gaussian, alcuse ~ 1 + (1 | id)) ## prior class coef group resp dpar nlpar lb ub source ## student_t(3, 1, 2.5) Intercept default ## student_t(3, 0, 2.5) sd 0 default ## student_t(3, 0, 2.5) sd id 0 (vectorized) ## student_t(3, 0, 2.5) sd Intercept id 0 (vectorized) ## student_t(3, 0, 2.5) sigma 0 default For this model, all three priors are based on Student’s \\(t\\)-distribution. In case you’re rusty, the normal distribution is just a special case of Student’s \\(t\\)-distribution. Whereas the normal is defined by two parameters (\\(\\mu\\) and \\(\\sigma\\)), the \\(t\\) distribution is defined by \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\). In frequentist circles, \\(\\nu\\) is often called the degrees of freedom. More generally, it’s also referred to as a normality parameter. We’ll examine the prior more closely in a bit. For now, let’s practice setting our priors by manually specifying them within brm(). You do with the prior argument. There are actually several ways to do this. To explore all the options, check out the set_prior section of the brms reference manual (Bürkner, 2021d). I typically define my individual priors with the prior() function. When there are more than one priors to define, I typically bind them together within c(...). Other than the addition of our fancy prior statement, the rest of the settings within brm() are much like those in prior chapters. Let’s fit the model. fit4.1 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 1 + (1 | id), prior = c(prior(student_t(3, 1, 2.5), class = Intercept), prior(student_t(3, 0, 2.5), class = sd), prior(student_t(3, 0, 2.5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.01&quot;) Here are the results. print(fit4.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 1 + (1 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.77 0.08 0.61 0.94 1.00 1513 2071 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.92 0.10 0.73 1.12 1.00 2194 2304 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.75 0.04 0.68 0.84 1.00 3220 3037 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Compare the results to those listed under “Model A” in Table 4.1. It’s important to keep in mind that brms returns ‘sigma’ and ‘sd(Intercept)’ in the standard deviation metric rather than the variance metric. “But I want them in the variance metric like in the text!”, you say. Okay fine. The best way to do the transformations is after saving the results from as_draws_df(). draws &lt;- as_draws_df(fit4.1) # first 12 columns glimpse(draws[, 1:12]) ## Rows: 4,000 ## Columns: 12 ## $ b_Intercept &lt;dbl&gt; 0.9406565, 0.9862519, 0.8047797, 0.9481130, 0.8541637, 0.8386853, 0.92… ## $ sd_id__Intercept &lt;dbl&gt; 0.7185852, 0.6471299, 0.7926978, 0.7718956, 0.7410648, 0.6956998, 0.78… ## $ sigma &lt;dbl&gt; 0.7373521, 0.8087970, 0.8117765, 0.8095093, 0.8179681, 0.7830781, 0.77… ## $ `r_id[1,Intercept]` &lt;dbl&gt; 0.9163717, 0.6145222, 0.2345081, 0.7683651, 0.3509155, 0.6096043, 1.01… ## $ `r_id[2,Intercept]` &lt;dbl&gt; -1.009483116, 0.249090063, -0.562924108, 0.203001424, 0.173337825, -0.… ## $ `r_id[3,Intercept]` &lt;dbl&gt; 0.1852035, 1.6224810, -0.4611725, 1.1150763, 0.6648818, 0.7228837, 1.0… ## $ `r_id[4,Intercept]` &lt;dbl&gt; 0.35781229, -0.01697348, 0.83689995, 0.89752770, 0.88948250, 0.6430061… ## $ `r_id[5,Intercept]` &lt;dbl&gt; -0.68773929, -0.65962658, -1.06090865, -0.71889869, -0.55364888, 0.159… ## $ `r_id[6,Intercept]` &lt;dbl&gt; 1.6937918, 1.5190274, 1.1928640, 1.3234571, 1.8810541, 1.9431236, 1.49… ## $ `r_id[7,Intercept]` &lt;dbl&gt; 0.82085231, 0.43493772, 0.73868701, 0.06875076, 0.91568144, 0.16150059… ## $ `r_id[8,Intercept]` &lt;dbl&gt; -0.44182207, -0.82966404, -0.29547227, -0.84191779, -0.45499854, -0.56… ## $ `r_id[9,Intercept]` &lt;dbl&gt; 0.316124825, 0.621843617, -0.346237928, 0.162988921, 0.530201557, 0.30… Since all we’re interested in are the variance components, we’ll select() out the relevant columns from draws, compute the squared versions, and save the results in a mini data frame, v. v &lt;- draws %&gt;% select(sigma, sd_id__Intercept) %&gt;% mutate(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2) head(v) ## # A tibble: 6 × 4 ## sigma sd_id__Intercept sigma_2_epsilon sigma_2_0 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.737 0.719 0.544 0.516 ## 2 0.809 0.647 0.654 0.419 ## 3 0.812 0.793 0.659 0.628 ## 4 0.810 0.772 0.655 0.596 ## 5 0.818 0.741 0.669 0.549 ## 6 0.783 0.696 0.613 0.484 We can view their distributions like this. v %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_vline(xintercept = c(.25, .5, .75, 1), color = &quot;white&quot;) + geom_density(size = 0, fill = &quot;black&quot;) + scale_x_continuous(NULL, limits = c(0, 1.25), breaks = seq(from = 0, to = 1.25, by = .25)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) In case it’s hard to follow what just happened, the estimates in the brms-default standard-deviation metric are the two panels on the top. Those on the bottom are in the Singer-and-Willett style variance metric. Like we discussed toward the end of last chapter, the variance parameters won’t often be Gaussian. In my experience, they’re typically skewed to the right. There’s nothing wrong with that. This is a recurrent pattern among distributions that are constrained to be zero and above. If you’re interested, you can summarize those posteriors like so. v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% # this last bit just rounds the output mutate_if(is.double, round, digits = 3) ## # A tibble: 4 × 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sd_id__Intercept 0.766 0.761 0.084 0.612 0.945 ## 2 sigma 0.755 0.752 0.042 0.678 0.841 ## 3 sigma_2_0 0.593 0.58 0.131 0.375 0.893 ## 4 sigma_2_epsilon 0.571 0.566 0.064 0.459 0.707 For this model, our posterior medians are closer to the estimates in the text (Table 4.1) than the means. However, our posterior standard deviations are pretty close to the standard errors in the text. One of the advantages of our Bayesian method is that when we compute something like the intraclass correlation coefficient \\(\\rho\\), we get an entire distribution for the parameter rather than a measly point estimates. This is always the case with Bayes. The algebraic transformations of the posterior distribution are themselves distributions. Before we compute \\(\\rho\\), do pay close attention to the formula, \\[ \\rho = \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_\\epsilon^2}. \\] Even though our brms output yields the variance parameters in the standard-deviation metric, the formula for \\(\\rho\\) demands we use variances. That’s nothing a little squaring can’t fix. Here’s what our \\(\\rho\\) looks like. v %&gt;% transmute(rho = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% ggplot(aes(x = rho)) + geom_density(size = 0, fill = &quot;black&quot;) + scale_x_continuous(expression(rho), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Though the posterior for \\(\\rho\\) is indeed centered around .5, look at how wide and uncertain that distribution is. The bulk of the posterior mass takes up almost half of the parameter space. If you wanted the summary statistics, you might do what we did for the variance parameters, above. v %&gt;% transmute(rho = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% summarise(mean = mean(rho), median = median(rho), sd = sd(rho), ll = quantile(rho, prob = .025), ul = quantile(rho, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 1 × 5 ## mean median sd ll ul ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.505 0.507 0.065 0.371 0.624 Concerning \\(\\rho\\), Singer and Willett pointed out it summarizes the size of the residual autocorrelation in the composite unconditional means mode…. Each person has a different composite residual on each occasion of measurement. But notice the difference in the subscripts of the pieces of the composite residual: while the level-1 residual, \\(\\epsilon_{ij}\\) has two subscripts (\\(i\\) and \\(j\\)), the level-2 residual, \\(\\zeta_{0i}\\), has only one (\\(i\\)). Each person can have a different \\(\\epsilon_{ij}\\) on each occasion, but has only one \\(\\zeta_{0i}\\) across every occasion. The repeated presence of \\(\\zeta_{0i}\\) in individual \\(i\\)’s composite residual links his or her composite residuals across occasions. The error autocorrelation coefficient quantifies the magnitude of this linkage; in the unconditional means model, the error autocorrelation coefficient is the intraclass correlation coefficient. Thus, we estimate that, for each person, the average correlation between any pair of composite residuals–between occasions 1 and 2, or 2 and 3, or 1 and 3–is [.5]. (pp. 96–97, emphasis in the original) Because of the differences in how they’re estimated with and presented by brm(), we focused right on the variance components. But before we move on to the next section, we should back up a bit. On page 93, Singer and Willett discussed their estimate for \\(\\gamma_{00}\\). Here’s ours. fixef(fit4.1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.9226063 0.09813736 0.728995 1.119117 They talked about how squaring that value puts it back to the natural metric the data were originally collected in. [Recall that as discussed earlier in the text the alcuse variable was square-root transformed because of excessive skew.] If you want a quick and dirty look, you can square our results, too. fixef(fit4.1)^2 ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.8512023 0.009630941 0.5314337 1.252424 However, I do not recommend this method. Though it did okay at transforming the posterior mean (i.e., Estimate), it’s not a great way to get the summary statistics correct. To do that, you’ll need to work with the posterior samples themselves. Remember how we saved them as draws? Let’s refresh ourselves and look at the first few columns. draws %&gt;% select(b_Intercept:sigma) %&gt;% head() ## # A tibble: 6 × 3 ## b_Intercept sd_id__Intercept sigma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.941 0.719 0.737 ## 2 0.986 0.647 0.809 ## 3 0.805 0.793 0.812 ## 4 0.948 0.772 0.810 ## 5 0.854 0.741 0.818 ## 6 0.839 0.696 0.783 See that b_Intercept column there? That contains our posterior draws from \\(\\gamma_{00}\\). If you want proper summary statistics from the transformed estimate, get them after transforming that column. draws %&gt;% transmute(gamma_00_squared = b_Intercept^2) %&gt;% summarise(mean = mean(gamma_00_squared), median = median(gamma_00_squared), sd = sd(gamma_00_squared), ll = quantile(gamma_00_squared, prob = .025), ul = quantile(gamma_00_squared, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) %&gt;% pivot_longer(everything()) ## # A tibble: 5 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 mean 0.861 ## 2 median 0.849 ## 3 sd 0.181 ## 4 ll 0.531 ## 5 ul 1.25 And one last bit before we move on to the next section. Remember how we discovered what the brm() default priors were for our model with the handy get_prior() function? Let’s refresh ourselves on how that worked. get_prior(data = alcohol1_pp, family = gaussian, alcuse ~ 1 + (1 | id)) ## prior class coef group resp dpar nlpar lb ub source ## student_t(3, 1, 2.5) Intercept default ## student_t(3, 0, 2.5) sd 0 default ## student_t(3, 0, 2.5) sd id 0 (vectorized) ## student_t(3, 0, 2.5) sd Intercept id 0 (vectorized) ## student_t(3, 0, 2.5) sigma 0 default We inserted the data and the model and get_prior() returned the default priors. Especially for new Bayesians, or even for experienced Bayesians working with unfamiliar models, it can be handy to plot your priors to get a sense of them. Base R has an array of functions based on the \\(t\\) distribution (e.g., rt(), dt()). These functions are limited in that while they allow users to select the desired \\(\\nu\\) values (i.e., degrees of freedom), they fix \\(\\mu = 0\\) and \\(\\sigma = 1\\). If you want to stick with the base R functions, you can find tricky ways around this. To avoid overwhelming anyone new to Bayes or the multilevel model or R or some exasperating combination, let’s just make things simpler and use a couple convenience functions from the ggdist package (Kay, 2021). We’ll start with the default intercept prior, \\(t(\\nu = 3, \\mu = 1, \\sigma = 2.5)\\). Here’s the density in the range \\([-20, 20]\\). library(ggdist) ## ## Attaching package: &#39;ggdist&#39; ## The following objects are masked from &#39;package:brms&#39;: ## ## dstudent_t, pstudent_t, qstudent_t, rstudent_t prior(student_t(3, 1, 2.5)) %&gt;% parse_dist() %&gt;% ggplot(aes(xdist = .dist_obj, y = prior)) + stat_halfeye(.width = .95, p_limits = c(.001, .999)) + scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) + labs(title = expression(paste(&quot;prior for &quot;, gamma[0][0])), x = &quot;parameter space&quot;) + theme(panel.grid = element_blank()) + coord_cartesian(xlim = c(-20, 20)) Though it’s centered on 1, the inner 95% of the density is well between -10 and 10. Given the model estimate ended up about 0.9, it looks like that was a pretty broad and minimally-informative prior. However, the prior isn’t flat and it does help guard against wasting time and HMC iterations sampling from ridiculous regions of the parameter space such as -10,000 or +500,000,000. No adolescent is drinking that much (or that little–how does one drink a negative value?). Here’s the shape of the variance priors. prior(student_t(3, 0, 2.5), lb = 0) %&gt;% parse_dist() %&gt;% ggplot(aes(xdist = .dist_obj, y = prior)) + stat_halfeye(.width = .95, p_limits = c(.001, .999)) + scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) + labs(title = expression(paste(&quot;prior for both &quot;, sigma[0], &quot; and &quot;, sigma[epsilon])), x = &quot;parameter space&quot;) + coord_cartesian(xlim = c(0, 20)) + theme(panel.grid = element_blank()) Recall that by brms default, the variance parameters have a lower-limit of 0. So specifying a Student’s \\(t\\) or other Gaussian-like prior on them ends up cutting the distribution off at 0. Given that our estimates were both below 1, it appears that these priors were minimally informative. But again, they did help prevent brm() from sampling from negative values or from obscenely-large values. These priors look kinda silly, you might say. Anyone with a little common sense can do better. Well, sure. Probably. Maybe. But keep in mind we’re still getting the layout of the land. And plus, this was a pretty simple model. Selecting high-quality priors gets tricky as the models get more complicated. In other chapters, we’ll explore other ways to specify priors for our multilevel models. But to keep things simple for now, let’s keep practicing inspecting and using the defaults with get_prior() and so on. 4.4.2 The unconditional growth model. Using the composite formula, our next model, the unconditional growth model, follows the form \\[ \\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{age_14}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}. \\end{align*} \\] With it, we now have a full composite stochastic model. Let’s query the brms::brm() default priors when we apply this model to our data. get_prior(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id)) ## prior class coef group resp dpar nlpar lb ub source ## (flat) b default ## (flat) b age_14 (vectorized) ## (flat) b Intercept (vectorized) ## lkj(1) cor default ## lkj(1) cor id (vectorized) ## student_t(3, 0, 2.5) sd 0 default ## student_t(3, 0, 2.5) sd id 0 (vectorized) ## student_t(3, 0, 2.5) sd age_14 id 0 (vectorized) ## student_t(3, 0, 2.5) sd Intercept id 0 (vectorized) ## student_t(3, 0, 2.5) sigma 0 default Several things of note: First, notice how we continue to use the student_t(3, 0, 2.5) for all three of our standard-deviation-metric variance parameters. Since we’re now estimating \\(\\sigma_0\\) and \\(\\sigma_1\\), which themselves have a correlation, \\(\\rho_{01}\\), we have a prior of class = cor. I’m going to put off what is meant by the name lkj, but for the moment just realize that this prior is essentially noninformative within this context. There’s a major odd development with this output. Notice how the prior column is (flat) for the rows for our two coefficients of class b. And if you’re a little confused, recall that because our predictor age_14 is not mean-centered, we’ve used the 0 + Intercept syntax, which switches the model intercept parameter to the class of b. From the set_prior section of the reference manual for brms version 2.12.0, we read: “The default prior for population-level effects (including monotonic and category specific effects) is an improper flat prior over the reals” (p. 179). At present, these priors are uniform across the entire parameter space. They’re not just weak, their entirely noninformative. That is, the likelihood dominates the posterior for those parameters. Here’s how to fit the model with these priors. fit4.2 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 2.5), class = sd), prior(student_t(3, 0, 2.5), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .9), file = &quot;fits/fit04.02&quot;) How did we do? print(fit4.2, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.792 0.103 0.595 0.998 1.001 803 1574 ## sd(age_14) 0.368 0.092 0.155 0.529 1.007 360 314 ## cor(Intercept,age_14) -0.120 0.263 -0.504 0.583 1.004 499 379 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.650 0.107 0.438 0.865 1.003 2078 2594 ## age_14 0.272 0.065 0.147 0.400 1.001 3540 2850 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.604 0.051 0.515 0.715 1.003 435 500 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If your compare our results with those in the “Model B” column in Table 4.1, you’ll see our summary results match well with those in the text. Our \\(\\gamma\\)’s (i.e., ‘Population-Level Effects:’) are near identical. The leftmost panel in Figure 4.3 shows the prototypical trajectory, based on the \\(\\gamma\\)s. A quick way to get that within our brms framework is with the conditional_effects() function. Here’s the default output. conditional_effects(fit4.2) Staying with conditional_effects() allows users some flexibility for customizing the plot(s). For example, the default behavior is to depict the trajectory in terms of its 95% intervals and posterior median. If you’d prefer the 80% intervals and the posterior mean, customize it like so. conditional_effects(fit4.2, robust = F, prob = .8) We’ll explore more options with brms::conditional_effects() with Model C. For now, let’s turn our focus on the stochastic elements in the model. Here we extract the posterior samples and do the conversions to see how they compare with Singer and Willett’s. draws &lt;- as_draws_df(fit4.2) v &lt;- draws %&gt;% transmute(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__age_14^2, sigma_01 = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14) head(v) ## # A tibble: 6 × 4 ## sigma_2_epsilon sigma_2_0 sigma_2_1 sigma_01 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.316 0.568 0.104 -0.0409 ## 2 0.333 0.751 0.0648 0.0311 ## 3 0.429 0.543 0.0544 0.0210 ## 4 0.505 0.408 0.106 0.0311 ## 5 0.450 0.694 0.0122 -0.00762 ## 6 0.454 0.501 0.0177 0.0522 This time, our v object only contains the stochastic components in the variance metric. Let’s plot. v %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) For each, their posterior mass is centered near the point estimates Singer and Willet reported in the text. Here are the summary statistics. v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 4 × 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sigma_01 -0.053 -0.047 0.076 -0.222 0.078 ## 2 sigma_2_0 0.637 0.62 0.165 0.354 0.997 ## 3 sigma_2_1 0.144 0.14 0.065 0.024 0.28 ## 4 sigma_2_epsilon 0.367 0.359 0.063 0.265 0.511 Happily, they’re quite comparable to those in the text. We’ve been pulling the posterior samples for all parameters with as_draws_df() and subsetting to a few variables of interest, such as the variance parameters. But it our primary interest is just the iterations for the variance parameters, we can extract them in a more focused way with the VarCorr() function. Here’s how we’d do so for fit4.2. VarCorr(fit4.2, summary = F) %&gt;% str() ## List of 2 ## $ id :List of 3 ## ..$ sd : num [1:4000, 1:2] 0.754 0.866 0.737 0.639 0.833 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ draw : chr [1:4000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ variable: chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## .. ..- attr(*, &quot;nchains&quot;)= int 4 ## ..$ cor: num [1:4000, 1:2, 1:2] 1 1 1 1 1 1 1 1 1 1 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## ..$ cov: num [1:4000, 1:2, 1:2] 0.568 0.751 0.543 0.408 0.694 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## .. .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; ## $ residual__:List of 1 ## ..$ sd: num [1:4000, 1] 0.562 0.577 0.655 0.71 0.671 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ draw : chr [1:4000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ variable: chr &quot;&quot; ## .. ..- attr(*, &quot;nchains&quot;)= int 4 That last part, the contents of the second higher-level list indexed by $ residual, contains the contents for \\(\\sigma_\\epsilon\\). On page 100 in the text, Singer and Willett compared \\(\\sigma_\\epsilon^2\\) from the first model to that from the second. We might do that like so. cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~.^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(`fit4.1 - fit4.2` = fit4.1 - fit4.2) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;fit4.1&quot;, &quot;fit4.2&quot;, &quot;fit4.1 - fit4.2&quot;))) %&gt;% ggplot(aes(x = value)) + geom_vline(xintercept = .5, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[epsilon]^2)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;, ncol = 3) To compute a formal summary of the decline in \\(\\sigma_\\epsilon^2\\) after adding time to the model, we might summarize like before. cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~ .^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(proportion_decline = (fit4.1 - fit4.2) / fit4.1) %&gt;% summarise(mean = mean(proportion_decline), median = median(proportion_decline), sd = sd(proportion_decline), ll = quantile(proportion_decline, prob = .025), ul = quantile(proportion_decline, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## mean median sd ll ul ## 1 0.35 0.365 0.132 0.051 0.569 In case it wasn’t clear, when we presented fit4.1 – fit4.2 in the density plot, that was a simple difference score. However, we computed proportion_decline above by dividing that difference score by fit4.1; that’s what put the difference in a proportion metric. Anyway, Singer and Willett’s method led them to summarize the decline as .40. Our method was a more conservative .34-ish. And very happily, our method allows us to describe the proportion decline with summary statistics for the full posterior, such as with the \\(\\textit{SD}\\) and the 95% intervals. draws %&gt;% ggplot(aes(x = cor_id__Intercept__age_14)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(expression(rho[0][1]), limits = c(-1, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) The estimate Singer and Willett hand-computed in the text, -.22, is near the mean of our posterior distribution for \\(\\rho_{01}\\). However, our distribution provides a full expression of the uncertainty in the parameter. As are many other values within the parameter space, zero is indeed a credible value for \\(\\rho_{01}\\). On page 101, we get the generic formula for computing the residual variance for a given occasion \\(j\\), \\[ \\sigma_{\\text{Residual}_j}^2 = \\sigma_0^2 + \\sigma_1^2 \\text{time}_j + 2 \\sigma_{01} \\text{time}_j + \\sigma_\\epsilon^2. \\] If we were just interested in applying it to one of our age values, say 14, we might apply the formula to the posterior like this. draws %&gt;% transmute(sigma_2_residual_j = sd_id__Intercept^2 + sd_id__age_14^2 * 0 + 2 * sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * 0 + sigma^2) %&gt;% head() ## # A tibble: 6 × 1 ## sigma_2_residual_j ## &lt;dbl&gt; ## 1 0.884 ## 2 1.08 ## 3 0.972 ## 4 0.913 ## 5 1.14 ## 6 0.955 But given we’d like to do so over several values of age, it might be better to wrap the equation in a custom function. Let’s call it make_s2rj(). make_s2rj &lt;- function(x) { draws %&gt;% transmute(sigma_2_residual_j = sd_id__Intercept^2 + sd_id__age_14^2 * x + 2 * sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * x + sigma^2) %&gt;% pull() } Now we can put our custom make_s2rj() function to work within the purrr::map() paradigm. We’ll plot the results. tibble(age = 14:16) %&gt;% mutate(age_c = age - 14) %&gt;% mutate(s2rj = map(age_c, make_s2rj)) %&gt;% unnest(s2rj) %&gt;% mutate(label = str_c(&quot;age = &quot;, age)) %&gt;% ggplot(aes(x = s2rj)) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + # just for reference geom_vline(xintercept = 1, color = &quot;grey92&quot;, linetype = 2) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Behold the shape of longitudinal heteroscedasticity.&quot;, x = expression(sigma[italic(Residual[j])]^2)) + theme(panel.grid = element_blank()) + facet_wrap(~ label, scales = &quot;free_y&quot;, ncol = 1) We see a subtle increase over time, particularly from age = 15 to age = 16. Yep, that’s heteroscedasticity. It is indeed “beyond the bland homoscedasticity we assume of residuals in cross-sectional data” (p. 101). We might also be interested in computing the autocorrelation between the composite residuals on occasions \\(j\\) and \\(j&#39;\\), which follows the formula \\[ \\rho_{\\text{Residual}_j, \\text{Residual}_{j&#39;}} = \\frac{\\sigma_0^2 + \\sigma_{01} (\\text{time}_j + \\text{time}_{j&#39;}) + \\sigma_1^2 \\text{time}_j \\text{time}_{j&#39;}} {\\sqrt{\\sigma_{\\text{Residual}_j}^2 \\sigma_{\\text{Residual}_{j&#39;}}^2 }}. \\] We only want to do that by hand once. Let’s make a custom function following the formula. make_rho_rj_rjp &lt;- function(j, jp) { # define the elements in the denominator s2rj_j &lt;- make_s2rj(j) s2rj_jp &lt;- make_s2rj(jp) # compute draws %&gt;% transmute(r = (sd_id__Intercept^2 + sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * (j + jp) + sd_id__age_14^2 * j * jp) / sqrt(s2rj_j * s2rj_jp)) %&gt;% pull() } If you only cared about measures of central tendency, such as the posterior median, you could use the function like this. make_rho_rj_rjp(0, 1) %&gt;% median() ## [1] 0.5705756 make_rho_rj_rjp(1, 2) %&gt;% median() ## [1] 0.7269452 make_rho_rj_rjp(0, 2) %&gt;% median() ## [1] 0.5121775 Here are the full posteriors. tibble(occasion = 1:3) %&gt;% mutate(age_c = occasion - 1, j = c(1, 2, 1) - 1, jp = c(2, 3, 3) - 1) %&gt;% mutate(r = map2(j, jp, make_rho_rj_rjp)) %&gt;% unnest(r) %&gt;% mutate(label = str_c(&quot;occasions &quot;, j + 1, &quot; and &quot;, jp + 1)) %&gt;% ggplot(aes(x = r)) + # just for reference geom_vline(xintercept = c(.5, .75), color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(expression(rho[Residual[italic(j)]][Residual[italic(j*minute)]]), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Behold the shapes of our autocorrelations!&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ label, scales = &quot;free_y&quot;, ncol = 1) 4.4.3 Quantifying the proportion of outcome variation “explained.” Because of the way the multilevel model partitions off variance into different sources (e.g., \\(\\sigma_0^2\\), \\(\\sigma_1^2\\), and \\(\\sigma_\\epsilon^2\\) in the unconditional growth model), the conventional \\(R^2\\) is not applicable for evaluating models in the traditional OLS sense of percent of variance explained. Several pseudo \\(R^2\\) statistics are frequently used instead. Be warned, “statisticians have yet to agree on appropriate summaries (I. G. Kreft &amp; de Leeuw, 1998; Snijders &amp; Bosker, 1994)” (p. 102). See also Jaeger et al. (2017), Jason D. Rights &amp; Cole (2018), and Jason D. Rights &amp; Sterba (2020). To my eye, none of the solutions presented in this section are magic bullets. 4.4.3.1 An overall summary of total outcome variability explained. In multiple regression, one simple way of computing a summary \\(R^2\\) statistic is to square the sample correlation between observed and predicted values of the outcome. The same approach can be used in the multilevel model for change. All you need to do is: (1) compute the predicted outcome value for each person on each occasion of measurement; and (2) square the sample correlation between observed and predicted values. The resultant pseudo-\\(R^2\\) statistic assesses the proportion of total outcome variation “explained” by the multilevel model’s specific contribution of predictors. (p. 102, emphasis added) Singer and Willett called this \\(R_{y, \\hat y}^2\\). They then walked through an example with their Model B (fit4.2), the unconditional growth model. Within our brms paradigm, we typically use the fitted() function to return predicted outcome values for cases within the data. The default option for the fitted() function is to return these predictions after accounting for the level-2 clustering. As we will see, Singer and Willett’s \\(R_{y, \\hat y}^2\\) statistic only accounts for predictors (i.e., age_14, in this case), not clustering variables (i.e., id, in this case). To follow Singer and Willett’s specification, we need to set re_formula = NA, which will instruct fitted() to return the expected values without reference to the level-2 clustering. Here’s a look at the first six rows of that output. fitted(fit4.2, re_formula = NA) %&gt;% head() ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.6497368 0.10745950 0.4380402 0.8652803 ## [2,] 0.9218824 0.09834597 0.7247202 1.1173032 ## [3,] 1.1940281 0.12716581 0.9444807 1.4488893 ## [4,] 0.6497368 0.10745950 0.4380402 0.8652803 ## [5,] 0.9218824 0.09834597 0.7247202 1.1173032 ## [6,] 1.1940281 0.12716581 0.9444807 1.4488893 Within our Bayesian/brms paradigm, out expected values come with expressions of uncertainty in terms of the posterior standard deviation and percentile-based 95% intervals. If we followed Singer and Willett’s method in the text, we’d only work with the posterior means as presented within the Estimate column. But since we’re Bayesians, we should attempt to work with the model uncertainty. One approach is to set summary = F. f &lt;- fitted(fit4.2, summary = F, re_formula = NA) %&gt;% data.frame() %&gt;% set_names(1:ncol(.)) %&gt;% rownames_to_column(&quot;draw&quot;) head(f) ## draw 1 2 3 4 5 6 7 8 9 ## 1 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 10 11 12 13 14 15 16 17 18 19 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 20 21 22 23 24 25 26 27 28 29 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 30 31 32 33 34 35 36 37 38 39 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 40 41 42 43 44 45 46 47 48 49 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 50 51 52 53 54 55 56 57 58 59 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 60 61 62 63 64 65 66 67 68 69 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 70 71 72 73 74 75 76 77 78 79 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 80 81 82 83 84 85 86 87 88 89 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 90 91 92 93 94 95 96 97 98 99 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 100 101 102 103 104 105 106 107 108 109 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 110 111 112 113 114 115 116 117 118 119 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 120 121 122 123 124 125 126 127 128 129 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 130 131 132 133 134 135 136 137 138 139 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 140 141 142 143 144 145 146 147 148 149 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 150 151 152 153 154 155 156 157 158 159 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 160 161 162 163 164 165 166 167 168 169 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 170 171 172 173 174 175 176 177 178 179 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 180 181 182 183 184 185 186 187 188 189 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 190 191 192 193 194 195 196 197 198 199 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 200 201 202 203 204 205 206 207 208 209 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 210 211 212 213 214 215 216 217 218 219 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 ## 220 221 222 223 224 225 226 227 228 229 ## 1 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 ## 2 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 ## 3 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 ## 4 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 ## 5 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 ## 6 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 ## 230 231 232 233 234 235 236 237 238 239 ## 1 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 ## 2 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 ## 3 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 ## 4 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 ## 5 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 ## 6 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 ## 240 241 242 243 244 245 246 ## 1 1.245269 0.5987915 0.9220302 1.245269 0.5987915 0.9220302 1.245269 ## 2 1.276545 0.8109860 1.0437653 1.276545 0.8109860 1.0437653 1.276545 ## 3 1.208412 0.7543876 0.9813998 1.208412 0.7543876 0.9813998 1.208412 ## 4 1.272410 0.6657516 0.9690807 1.272410 0.6657516 0.9690807 1.272410 ## 5 1.321353 0.8239123 1.0726329 1.321353 0.8239123 1.0726329 1.321353 ## 6 1.215989 0.6303015 0.9231454 1.215989 0.6303015 0.9231454 1.215989 With those settings, fitted() returned a \\(4,000 \\times 246\\) numeric array. The 4,000 rows corresponded to the 4,000 post-warmup HMC draws. Each of the 246 columns corresponded to one of the 246 rows in the original alcohol1_pp data. To make the output more useful, we converted it to a data frame, named the columns by the row numbers corresponding to the original alcohol1_pp data, and converted the row names to an draw column. In the next code block, we’ll convert f to the long format and use left_join() to join it with the relevant subset of the alcohol1_pp data. f &lt;- f %&gt;% pivot_longer(-draw, names_to = &quot;row&quot;, values_to = &quot;fitted&quot;) %&gt;% mutate(row = row %&gt;% as.integer()) %&gt;% left_join( alcohol1_pp %&gt;% mutate(row = 1:n()) %&gt;% select(row, alcuse), by = &quot;row&quot; ) f ## # A tibble: 984,000 × 4 ## draw row fitted alcuse ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.599 1.73 ## 2 1 2 0.922 2 ## 3 1 3 1.25 2 ## 4 1 4 0.599 0 ## 5 1 5 0.922 0 ## 6 1 6 1.25 1 ## 7 1 7 0.599 1 ## 8 1 8 0.922 2 ## 9 1 9 1.25 3.32 ## 10 1 10 0.599 0 ## # ℹ 983,990 more rows If we collapse the distinction across the 4,000 HMC draws, here is the squared correlation between fitted and alcuse. f %&gt;% summarise(r = cor(fitted, alcuse), r2 = cor(fitted, alcuse)^2) ## # A tibble: 1 × 2 ## r r2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.186 0.0346 This is close to the \\(R_{y, \\hat y}^2 = .043\\) Singer and Willett reported in the text. It might seem unsatisfying how this seemingly ignores model uncertainty by collapsing across HMC draws. Here’s a look at what happens is we compute the \\(R_{y, \\hat y}^2\\) separately for each iteration. f %&gt;% mutate(draw = draw %&gt;% as.double()) %&gt;% group_by(draw) %&gt;% summarise(r = cor(fitted, alcuse), r2 = cor(fitted, alcuse)^2) ## # A tibble: 4,000 × 3 ## draw r r2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.208 0.0434 ## 2 2 0.208 0.0434 ## 3 3 0.208 0.0434 ## 4 4 0.208 0.0434 ## 5 5 0.208 0.0434 ## 6 6 0.208 0.0434 ## 7 7 0.208 0.0434 ## 8 8 0.208 0.0434 ## 9 9 0.208 0.0434 ## 10 10 0.208 0.0434 ## # ℹ 3,990 more rows Now for every level of draw, \\(R_{y, \\hat y}^2 = .0434\\), which matches up nicely with the text. But it seems odd that the value should be the same for each of the 4,000 HMC draws. Sadly, my efforts to debug my workflow have been unsuccessful. If you see a flaw in this method, please share on GitHub. Just for kicks, here’s a more compact alternative to our fitted() + left_join() approach that more closely resembles the work flow Singer and Willett showed on pages 102 and 103. tibble(age_14 = 0:2) %&gt;% mutate(fitted = map(age_14, ~ draws$b_Intercept + draws$b_age_14 * .)) %&gt;% full_join(alcohol1_pp %&gt;% select(id, age_14, alcuse), by = &quot;age_14&quot;) %&gt;% mutate(row = 1:n()) %&gt;% unnest(fitted) %&gt;% mutate(draw = rep(1:4000, times = alcohol1_pp %&gt;% nrow())) %&gt;% group_by(draw) %&gt;% summarise(r = cor(fitted, alcuse), r2 = cor(fitted, alcuse)^2) ## # A tibble: 4,000 × 3 ## draw r r2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.208 0.0434 ## 2 2 0.208 0.0434 ## 3 3 0.208 0.0434 ## 4 4 0.208 0.0434 ## 5 5 0.208 0.0434 ## 6 6 0.208 0.0434 ## 7 7 0.208 0.0434 ## 8 8 0.208 0.0434 ## 9 9 0.208 0.0434 ## 10 10 0.208 0.0434 ## # ℹ 3,990 more rows Either way, our results agree with those in the text: about “4.3% of the total variability in ALCUSE is associated with linear time” (p. 103, emphasis in the original). 4.4.3.2 Pseudo-\\(R^2\\) statistics computed from the variance components. Residual variation–that portion of the outcome variation unexplained by a model’s predictors–provides another criterion for comparison. When you fit a series of models, you hope that added predictors further explain unexplained outcome variation, causing residual variation to decline. The magnitude of this decline quantifies the improvement in fit. A large decline suggests that the predictors make a big difference; a small, or zero, decline suggests that they do not. To assess these declines on a common scale, we compute the proportional reduction in residual variance as we add predictors. Each unconditional model yields residual variances that serve as yardsticks for comparison. The unconditional means model provides a baseline estimate of \\(\\sigma_\\epsilon^2\\); the unconditional growth model provides baseline estimates of \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\). Each leads to its own pseudo-\\(R^2\\) statistic. (p. 103, emphasis in the original) This provides three more pseudo-\\(R^2\\) statistics: \\(R_\\epsilon^2\\), \\(R_0^2\\), and \\(R_1^2\\). The formula for the first is \\[ R_\\epsilon^2 = \\frac{\\sigma_\\epsilon^2 (\\text{unconditional means model}) - \\sigma_\\epsilon^2 (\\text{unconditional growth model})}{\\sigma_\\epsilon^2 (\\text{unconditional means model})}. \\] We’ve actually already computed this one, above, under the name where we referred to it as the decline in \\(\\sigma_\\epsilon^2\\) after adding time to the model. Here it is again. cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~ .^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(r_2_epsilon = (fit4.1 - fit4.2) / fit4.1) %&gt;% summarise(mean = mean(r_2_epsilon), median = median(r_2_epsilon), sd = sd(r_2_epsilon), ll = quantile(r_2_epsilon, prob = .025), ul = quantile(r_2_epsilon, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## mean median sd ll ul ## 1 0.35 0.365 0.132 0.051 0.569 Here’s a look at the full distribution for our \\(\\sigma_\\epsilon^2\\). cbind(VarCorr(fit4.1, summary = F)[[2]][[1]], VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% mutate_all(~ .^2) %&gt;% set_names(str_c(&quot;fit4.&quot;, 1:2)) %&gt;% mutate(r_2_epsilon = (fit4.1 - fit4.2) / fit4.1) %&gt;% ggplot(aes(x = r_2_epsilon)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(expression(Pseudo~italic(R)[epsilon]^2), limits = c(-1, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) When we use the full posteriors of our two \\(\\epsilon_\\epsilon^2\\) parameters, we end up with a slightly smaller statistic than the one in the text. So our conclusion is about 35% of the intraindividual variance is accounted for by time. If we consider additional models with predictors for the \\(\\zeta\\)s, we can examine similar pseudo \\(R^2\\) statistics following the generic form \\[ R_\\zeta^2 = \\frac{\\sigma_\\zeta^2 (\\text{unconditional growth model}) - \\sigma_\\zeta^2 (\\text{subsequent model})}{\\sigma_\\zeta^2 (\\text{unconditional growth model})}, \\] where \\(\\zeta\\) could refer to \\(\\zeta_{0i}\\), \\(\\zeta_{1i}\\), and so on. If you look back up at the shape of the full posterior of \\(R_\\epsilon^2\\), you’ll notice part of the left tail crosses zero. “Unlike traditional \\(R^2\\) statistics, which will always be positive (or zero), some of these statistics can be negative” (p. 104)! If you compute them, interpret pseudo-\\(R^2\\) statistics with a grain of salt. 4.5 Practical data analytic strategies for model building A sound statistical model includes all necessary predictors and no unnecessary ones. But how do you separate the wheat from the chaff? We suggest you rely on a combination of substantive theory, research questions, and statistical evidence. Never let a computer select predictors mechanically. (pp. 104–105, emphasis in the original) 4.5.1 A taxonomy of statistical models. We suggest that you base decisions to enter, retain, and remove predictors on a combination of logic, theory, and prior research, supplemented by judicious [parameter evaluation] and comparison of model fit. At the outset, you might examine the effect of each predictor individually. You might then focus on predictors of primary interest (while including others whose effects you want to control). As in regular regression, you can add predictors singly or in groups and you can address issues of functional form using interactions and transformations. As you develop the taxonomy, you will progress toward a “final model” whose interpretation addresses your research questions. We place quotes around this term to emphasize that we believe no statistical model is ever final; it is simply a placeholder until a better model is found. (p. 105, emphasis in the original) 4.5.2 Interpreting fitted models. You need not interpret every model you fit, especially those designed to guide interim decision making. When writing up findings for presentation and publication, we suggest that you identify a manageable subset of models that, taken together, tells a persuasive story parsimoniously. At a minimum, this includes the unconditional means model, the unconditional growth model, and a “final model”. You may also want to present intermediate models that either provide important building blocks or tell interesting stories in their own right. (p. 106) In the dawn of the post-replication crisis era, it’s astonishing to reread and transcribe this section and the one above. I like a lot of what the authors had to say. Much of it seems like good pragmatic advice. But if they were to rewrite these sections again, I wonder what changes they’d make. Would they recommend researchers preregister their primary hypothesis, variables of interest, and perhaps their model building strategy (Nosek et al., 2018)? Would they be interested in a multiverse analysis (Steegen et al., 2016)? Would they still recommend sharing only a subset of one’s analyses in the era of sharing platforms like GitHub and the Open Science Framework? Would they weigh in on developments in causal inference (Pearl et al., 2016)? 4.5.2.1 Model C: The uncontrolled effects of COA. The default priors for Model C are the same as for the unconditional growth model. All we’ve done is add parameters of class = b. As these default to improper flat priors, we have nothing to add to the prior argument to include them. Feel free to check with get_prior(). For the sake of practice, this model follows the form \\[ \\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\gamma_{10} \\text{age_14}_{ij} + \\gamma_{11} \\text{coa}_i \\times \\text{age_14}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\text{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\text{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}. \\end{align*} \\] Fit the model. fit4.3 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + age_14:coa + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 2.5), class = sd), prior(student_t(3, 0, 2.5), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.03&quot;) Check the summary. print(fit4.3, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + coa + age_14:coa + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.695 0.101 0.500 0.899 1.001 809 1602 ## sd(age_14) 0.369 0.092 0.166 0.535 1.004 411 526 ## cor(Intercept,age_14) -0.103 0.279 -0.509 0.622 1.002 609 557 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.320 0.134 0.052 0.582 1.000 2091 2190 ## age_14 0.288 0.086 0.119 0.458 1.000 2671 2703 ## coa 0.736 0.198 0.349 1.126 1.001 1980 2402 ## age_14:coa -0.044 0.128 -0.292 0.210 1.002 2698 2701 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.607 0.051 0.516 0.716 1.002 456 1041 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our \\(\\gamma\\)’s are quite similar to those presented in the text. Our \\(\\sigma_\\epsilon\\) for this model is about the same as with fit4.2. Let’s practice with conditional_effects() to plot the consequences of this model. conditional_effects(fit4.3) This time we got back three plots. The first two were of the lower-order parameters \\(\\gamma_{10}\\) and \\(\\gamma_{01}\\). Note how the plot for coa treated it as a continuous variable. This is because the variable was saved as an integer in the original data set. fit4.3$data %&gt;% glimpse() ## Rows: 246 ## Columns: 5 ## $ alcuse &lt;dbl&gt; 1.732051, 2.000000, 2.000000, 0.000000, 0.000000, 1.000000, 1.000000, 2.000000, … ## $ Intercept &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9,… Coding it as an integer further complicated things for the third plot returned by conditional_effects(), the one for the interaction of age_14 and coa, \\(\\gamma_{11}\\). Since coa is binary, the natural way to express its interaction with age_14 would be with age_14 on the \\(x\\)-axis and two separate trajectories, one for each value of coa. That’s what Singer and Willett very sensibly did with the middle panel of Figure 4.3. However, the conditional_effects()function defaults to expressing interactions such that the first variable in the term–in this case,age_14–is on the \\(x\\)-axis and the second variable in the term–coa, treated as an integer–is depicted in three lines corresponding its mean and its mean \\(\\pm\\) one standard deviation. This is great for continuous variables, but incoherent for categorical ones. The fix is to adjust the data and refit the model. fit4.4 &lt;- update(fit4.3, newdata = alcohol1_pp %&gt;% mutate(coa = factor(coa)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.04&quot;) We might compare the updated model with its predecessor. To get a focused look, we can use the posterior_summary() function with a little subsetting. posterior_summary(fit4.3)[1:4, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.320 0.134 0.052 0.582 ## b_age_14 0.288 0.086 0.119 0.458 ## b_coa 0.736 0.198 0.349 1.126 ## b_age_14:coa -0.044 0.128 -0.292 0.210 posterior_summary(fit4.4)[1:4, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.320 0.134 0.052 0.582 ## b_age_14 0.288 0.086 0.119 0.458 ## b_coa1 0.736 0.198 0.349 1.126 ## b_age_14:coa1 -0.044 0.128 -0.292 0.210 The results are about the same. The payoff comes when we try again with conditional_effects(). conditional_effects(fit4.4) Much better. Now the plot for \\(\\gamma_{01}\\) treats coa as binary and our plot for the interaction between age_14 and coa is much close to the one in Figure 4.3. Since we’re already on a conditional_effects() tangent, we may as well go further. When working with models like fit4.3 where you have multiple fixed effects, sometimes you only want the plots for a subset of those effects. For example, if our main goal is to do a good job tastefully reproducing the middle plot in Figure 4.3, we only need the interaction plot. In such a case, use the effects argument. conditional_effects(fit4.4, effects = &quot;age_14:coa&quot;) Earlier we discussed how conditional_effects() lets users adjust some of the output. But if you want an extensive overhaul, it’s better to save the output of conditional_effects() as an object and manipulate that object with the plot() function. ce &lt;- conditional_effects(fit4.4, effects = &quot;age_14:coa&quot;) str(ce) ## List of 1 ## $ age_14:coa:&#39;data.frame&#39;: 200 obs. of 12 variables: ## ..$ age_14 : num [1:200] 0 0 0.0202 0.0202 0.0404 ... ## ..$ coa : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## ..$ alcuse : num [1:200] 0.922 0.922 0.922 0.922 0.922 ... ## ..$ Intercept : num [1:200] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ id : logi [1:200] NA NA NA NA NA NA ... ## ..$ cond__ : Factor w/ 1 level &quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## ..$ effect1__ : num [1:200] 0 0 0.0202 0.0202 0.0404 ... ## ..$ effect2__ : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## ..$ estimate__: num [1:200] 0.322 1.056 0.328 1.061 0.333 ... ## ..$ se__ : num [1:200] 0.133 0.143 0.133 0.143 0.133 ... ## ..$ lower__ : num [1:200] 0.0519 0.7644 0.0591 0.7715 0.0667 ... ## ..$ upper__ : num [1:200] 0.582 1.347 0.587 1.349 0.591 ... ## ..- attr(*, &quot;effects&quot;)= chr [1:2] &quot;age_14&quot; &quot;coa&quot; ## ..- attr(*, &quot;response&quot;)= chr &quot;alcuse&quot; ## ..- attr(*, &quot;surface&quot;)= logi FALSE ## ..- attr(*, &quot;categorical&quot;)= logi FALSE ## ..- attr(*, &quot;ordinal&quot;)= logi FALSE ## ..- attr(*, &quot;points&quot;)=&#39;data.frame&#39;: 246 obs. of 6 variables: ## .. ..$ age_14 : num [1:246] 0 1 2 0 1 2 0 1 2 0 ... ## .. ..$ coa : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## .. ..$ resp__ : num [1:246] 1.73 2 2 0 0 ... ## .. ..$ cond__ : Factor w/ 1 level &quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## .. ..$ effect1__: num [1:246] 0 1 2 0 1 2 0 1 2 0 ... ## .. ..$ effect2__: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## - attr(*, &quot;class&quot;)= chr &quot;brms_conditional_effects&quot; Our ce is an object of class “’brms_conditional_effects” which contains a list of a single data frame. Had we omitted our effects argument, above, we’d have a list of 3 instead. Anyway, these data frames contain the necessary information to produce the plot. The advantage of saving ce this way is we can now insert it into the plot() function. The simple output is the same as before. plot(ce) The plot() function will allow us to do other things, like add in the original data or omit the white grid lines. ce %&gt;% plot(points = T, point_args = list(size = 1/4, alpha = 1/4, width = .05, height = .05, color = &quot;black&quot;), theme = theme(panel.grid = element_blank())) And for even more control, you can tack on typical ggplot2 functions. But when you want to do so, make sure to set the plot = FALSE argument and then subset after the right parenthesis of the plot() function. plot(ce, theme = theme(legend.position = &quot;none&quot;, panel.grid = element_blank()), plot = FALSE)[[1]] + annotate(geom = &quot;text&quot;, x = 2.1, y = c(.95, 1.55), label = str_c(&quot;coa = &quot;, 0:1), hjust = 0, size = 3.5) + scale_fill_brewer(type = &quot;qual&quot;) + scale_color_brewer(type = &quot;qual&quot;) + scale_x_continuous(&quot;age&quot;, limits = c(-1, 3), labels = 13:17) + scale_y_continuous(limits = c(0, 2), breaks = 0:2) But anyway, let’s get back on track and talk about the variance components. Singer and Willett contrasted \\(\\sigma_\\epsilon^2\\) from Model B to the new one from Model C. We might use VarCorr() to do the same. VarCorr(fit4.2)[[2]] ## $sd ## Estimate Est.Error Q2.5 Q97.5 ## 0.6035258 0.05121693 0.5146678 0.7146179 VarCorr(fit4.3)[[2]] ## $sd ## Estimate Est.Error Q2.5 Q97.5 ## 0.6073666 0.05108632 0.5161462 0.7164554 We could have also extracted that information by subsetting posterior_summary(). posterior_summary(fit4.2)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.60352584 0.05121693 0.51466784 0.71461791 posterior_summary(fit4.3)[&quot;sigma&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.60736661 0.05108632 0.51614618 0.71645542 Anyway, to get these in a variance metric, just square their posterior samples and summarize. Our next task is to formally compare fit4.2 and fit4.3 in terms of declines in \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\). bind_cols( as_draws_df(fit4.2) %&gt;% transmute(fit2_sigma_2_0 = sd_id__Intercept^2, fit2_sigma_2_1 = sd_id__age_14^2), as_draws_df(fit4.3) %&gt;% transmute(fit3_sigma_2_0 = sd_id__Intercept^2, fit3_sigma_2_1 = sd_id__age_14^2) ) %&gt;% mutate(`decline~&#39;in&#39;~sigma[0]^2` = (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0, `decline~&#39;in&#39;~sigma[1]^2` = (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) %&gt;% pivot_longer(contains(&quot;decline&quot;)) %&gt;% ggplot(aes(x = value)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_density(fill = &quot;grey25&quot;, color = &quot;transparent&quot;) + scale_x_continuous(NULL, limits = c(-5, 2)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, labeller = label_parsed, ncol = 1) Here are the percents of variance declined from fit4.2 to fit4.3. bind_cols( as_draws_df(fit4.2) %&gt;% transmute(fit2_sigma_2_0 = sd_id__Intercept^2, fit2_sigma_2_1 = sd_id__age_14^2), as_draws_df(fit4.3) %&gt;% transmute(fit3_sigma_2_0 = sd_id__Intercept^2, fit3_sigma_2_1 = sd_id__age_14^2) ) %&gt;% mutate(`decline~&#39;in&#39;~sigma[0]^2` = (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0, `decline~&#39;in&#39;~sigma[1]^2` = (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) %&gt;% pivot_longer(contains(&quot;decline&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) ## # A tibble: 2 × 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 decline~&#39;in&#39;~sigma[0]^2 0.170 0.229 0.347 -0.672 0.657 ## 2 decline~&#39;in&#39;~sigma[1]^2 -0.736 0.00344 7.07 -5.26 0.809 In this case, we end up with massive uncertainty when working with the full posteriors. This is particularly the case with the difference in \\(\\sigma_1^2\\), which is left skewed for days. Here are the results when we only use point estimates. bind_cols( as_draws_df(fit4.2) %&gt;% transmute(fit2_sigma_2_0 = sd_id__Intercept^2, fit2_sigma_2_1 = sd_id__age_14^2), as_draws_df(fit4.3) %&gt;% transmute(fit3_sigma_2_0 = sd_id__Intercept^2, fit3_sigma_2_1 = sd_id__age_14^2) ) %&gt;% summarise_all(median) %&gt;% transmute(`% decline in sigma_2_0` = 100 * (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0, `% decline in sigma_2_1` = 100 * (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) ## # A tibble: 1 × 2 ## `% decline in sigma_2_0` `% decline in sigma_2_1` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 22.4 0.604 “These variance components are now called partial or conditional variances because they quantify the interindividual differences in change that remain unexplained by the model’s predictors” (p. 108, emphasis in the original). 4.5.2.2 Model D: The controlled effects of COA. This model follows the form \\[ \\begin{align*} \\text{alcuse}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\gamma_{02} \\text{peer}_i + \\gamma_{10} \\text{age_14}_{ij} \\\\ &amp; \\;\\;\\; + \\gamma_{11} \\text{coa}_i \\times \\text{age_14}_{ij} + \\gamma_{12} \\text{peer}_i \\times \\text{age_14}_{ij} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{age_14}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\text{Normal} (0, \\sigma_\\epsilon^2) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\text{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 &amp; \\sigma_{01} \\\\ \\sigma_{01} &amp; \\sigma_1^2 \\end{bmatrix} \\end{pmatrix}. \\end{align*} \\] Fit that joint. fit4.5 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:coa + age_14:peer + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 2.5), class = sd), prior(student_t(3, 0, 2.5), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.05&quot;) print(fit4.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:coa + age_14:peer + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.485 0.101 0.279 0.681 1.006 580 1272 ## sd(age_14) 0.364 0.080 0.202 0.519 1.018 346 510 ## cor(Intercept,age_14) 0.117 0.342 -0.407 0.914 1.016 258 214 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.316 0.148 -0.605 -0.025 1.002 1890 2347 ## age_14 0.428 0.115 0.199 0.654 1.000 1925 1877 ## coa 0.581 0.165 0.251 0.901 1.000 2389 2662 ## peer 0.693 0.115 0.472 0.925 1.004 1747 2176 ## age_14:coa -0.013 0.127 -0.261 0.238 1.001 2622 2667 ## age_14:peer -0.150 0.088 -0.323 0.022 1.001 1803 2350 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.606 0.047 0.519 0.704 1.011 480 923 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). All our \\(\\gamma\\) estimates are similar to those presented in Table 4.1. Let’s compute the variances and the covariance, \\(\\sigma_{01}^2\\). Here are the plots. v &lt;- as_draws_df(fit4.5) %&gt;% transmute(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__age_14^2, sigma_01 = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14) v %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) And now we compute the summary statistics. v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 4 × 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sigma_01 0.005 0.011 0.054 -0.113 0.091 ## 2 sigma_2_0 0.245 0.234 0.098 0.078 0.464 ## 3 sigma_2_1 0.139 0.134 0.058 0.041 0.27 ## 4 sigma_2_epsilon 0.369 0.365 0.057 0.27 0.495 Like the \\(\\gamma\\)’s, our variance components are all similar to those in the text. bind_cols( as_draws_df(fit4.2) %&gt;% transmute(fit4.2_sigma_2_epsilon = sigma^2, fit4.2_sigma_2_0 = sd_id__Intercept^2, fit4.2_sigma_2_1 = sd_id__age_14^2), as_draws_df(fit4.5) %&gt;% transmute(fit4.5_sigma_2_epsilon = sigma^2, fit4.5_sigma_2_0 = sd_id__Intercept^2, fit4.5_sigma_2_1 = sd_id__age_14^2) ) %&gt;% summarise_all(median) %&gt;% mutate(`% decline in sigma_2_epsilon` = 100 * (fit4.2_sigma_2_epsilon - fit4.5_sigma_2_epsilon) / fit4.2_sigma_2_epsilon, `% decline in sigma_2_0` = 100 * (fit4.2_sigma_2_0 - fit4.5_sigma_2_0) / fit4.2_sigma_2_0, `% decline in sigma_2_1` = 100 * (fit4.2_sigma_2_1 - fit4.5_sigma_2_1) / fit4.2_sigma_2_1) %&gt;% pivot_longer(contains(&quot;%&quot;)) %&gt;% select(name, value) ## # A tibble: 3 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 % decline in sigma_2_epsilon -1.71 ## 2 % decline in sigma_2_0 62.2 ## 3 % decline in sigma_2_1 4.53 The percentages in which our variance componence declined relative to the unconditional growth model are of similar orders of magnitude as those presented in the text. 4.5.2.3 Model E: A tentative “final model” for the controlled effects of coa. This model is just like the last, but with the simple omission of the \\(\\gamma_{12}\\) parameter. fit4.6 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 2.5), class = sd), prior(student_t(3, 0, 2.5), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.06&quot;) print(fit4.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id) ## Data: alcohol1_pp (Number of observations: 246) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 82) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.485 0.102 0.281 0.675 1.009 547 1348 ## sd(age_14) 0.358 0.078 0.198 0.505 1.028 216 673 ## cor(Intercept,age_14) 0.136 0.340 -0.393 0.903 1.026 228 366 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.314 0.151 -0.608 -0.021 1.000 1910 2692 ## age_14 0.425 0.105 0.223 0.630 1.000 2166 2657 ## coa 0.573 0.148 0.284 0.859 1.001 2198 2861 ## peer 0.694 0.113 0.475 0.915 1.000 2105 2784 ## age_14:peer -0.152 0.085 -0.314 0.015 1.000 2124 2495 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.606 0.047 0.514 0.700 1.017 391 1483 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The \\(\\gamma\\)’s all look well behaved. Here are the variance component summaries. v &lt;- as_draws_df(fit4.6) %&gt;% transmute(sigma_2_epsilon = sigma^2, sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__age_14^2, sigma_01 = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14) v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), median = median(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 4 × 6 ## name mean median sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sigma_01 0.008 0.014 0.053 -0.108 0.094 ## 2 sigma_2_0 0.245 0.236 0.099 0.079 0.456 ## 3 sigma_2_1 0.134 0.129 0.056 0.039 0.255 ## 4 sigma_2_epsilon 0.369 0.365 0.057 0.264 0.489 4.5.3 Displaying prototypical change trajectories. On page 111, Singer and Willett computed the various levels of the \\(\\pi\\) coefficients when coa == 0 or coa == 1. To follow along, we’ll want to work directly with the posterior draws from fit4.3. draws &lt;- as_draws_df(fit4.3) draws %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% head() ## # A tibble: 6 × 4 ## b_Intercept b_age_14 b_coa `b_age_14:coa` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.270 0.198 0.979 -0.0312 ## 2 0.517 0.245 0.694 -0.127 ## 3 0.557 0.233 0.712 -0.0784 ## 4 0.218 0.332 0.650 -0.0852 ## 5 0.458 0.312 0.595 0.0750 ## 6 0.195 0.316 0.790 -0.0752 Here we apply the formulas to the posterior draws and then summarize with posterior means. draws %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% transmute(pi_0_coa0 = b_Intercept + b_coa * 0, pi_1_coa0 = b_age_14 + `b_age_14:coa` * 0, pi_0_coa1 = b_Intercept + b_coa * 1, pi_1_coa1 = b_age_14 + `b_age_14:coa` * 1) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(posterior_mean = mean(value) %&gt;% round(digits = 3)) ## # A tibble: 4 × 2 ## name posterior_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 pi_0_coa0 0.32 ## 2 pi_0_coa1 1.06 ## 3 pi_1_coa0 0.288 ## 4 pi_1_coa1 0.244 We already plotted these trajectories and their 95% intervals a few sections up. If we want to work with the full composite model to predict \\(Y_{ij}\\) (i.e., alcuse) directly, we multiply the b_coa, b_age_14, and b_age_14:coa vectors by the appropriate values of coa and peer. For example, here’s what you’d code if you wanted the initial alcuse status for when coa == 1. draws %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(y = b_Intercept + b_coa * 1 + b_age_14 * 0 + `b_age_14:coa` * 0 * 1) %&gt;% head() ## # A tibble: 6 × 5 ## b_Intercept b_age_14 b_coa `b_age_14:coa` y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.270 0.198 0.979 -0.0312 1.25 ## 2 0.517 0.245 0.694 -0.127 1.21 ## 3 0.557 0.233 0.712 -0.0784 1.27 ## 4 0.218 0.332 0.650 -0.0852 0.868 ## 5 0.458 0.312 0.595 0.0750 1.05 ## 6 0.195 0.316 0.790 -0.0752 0.985 If you were to take the mean of that new y column, you’d discover it’s the same as the mean of our pi_0_coa1, above. draws %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(y = b_Intercept + b_coa * 1 + b_age_14 * 0 + `b_age_14:coa` * 0 * 1) %&gt;% summarise(pi_0_coa1 = mean(y)) ## # A tibble: 1 × 1 ## pi_0_coa1 ## &lt;dbl&gt; ## 1 1.06 Singer and Willett suggested four strategies to help researchers pick the prototypical values of the predictors to focus on: Substantively interesting values (e.g., typical ages, values corresponding to transition points) A range of percentiles (e.g., 25th, 50th, and 75th) Just the sample mean The sample mean \\(\\pm\\) something like 1 standard deviation They next discuss the right panel of Figure 4.3. We could continue to work directly with the as_draws_df() to make our version of that figure. But it you want to accompany the posterior mean trajectories with their 95% intervals, and I hope you do, the as_draws_df() method will get tedious. Happily, brms offers users and alternative with the fitted() function. Since the right panel is somewhat complicated, it’ll behoove us to practice with the simpler left panel, first. In fit4.2 (i.e., Model C), age_14 is the only predictor. Here we’ll specify the values along the range in the original data, ranging from 0 to 2. However, we end up specifying a bunch of values within that range in addition to the two endpoints. This is because the 95% intervals typically have a bow tie shape. To depict that shape well, we need more than a couple values. We save those values as a tibble called nd (i.e., new data). We make use of them within fitted with the newdata = nd argument. Since we’re only interested in the general trajectory, the consequence of the \\(\\gamma\\)’s, we end up coding re_formula = NA. In so doing, we ask fitted() to ignore the group-level effects. In this example, that means we are ignoring the id-level deviations from the overall trajectories. If you’re confused by that that means, don’t worry. That part of the model should become more clear as we go along in the text. Since fitted() returns an array, we then convert the results into a data frame for use within the tidyverse framework. For plotting, it’s handy to bind those results together with the nd, the predictor values we used to compute the fitted values with. In the final wrangling step, we use our age_14 values to compute the age values. nd &lt;- tibble(age_14 = seq(from = 0, to = 2, length.out = 30)) f &lt;- fitted(fit4.2, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(age = age_14 + 14) head(f) ## Estimate Est.Error Q2.5 Q97.5 age_14 age ## 1 0.6497368 0.10745950 0.4380402 0.8652803 0.00000000 14.00000 ## 2 0.6685054 0.10559034 0.4614879 0.8822467 0.06896552 14.06897 ## 3 0.6872741 0.10387940 0.4834844 0.8973132 0.13793103 14.13793 ## 4 0.7060428 0.10233461 0.5033354 0.9122759 0.20689655 14.20690 ## 5 0.7248114 0.10096359 0.5248719 0.9276806 0.27586207 14.27586 ## 6 0.7435801 0.09977351 0.5474076 0.9428426 0.34482759 14.34483 Since we only had one predictor, age_14, for which we specified 30 specific values, we ended up with 30 rows in our output. By default, fitted() summarized the fitted values with posterior means (Estimate), standard deviations (Est.Error), and percentile-based 95% intervals (Q2.5 and Q97.5). The other columns are the values we bound to them. Here’s how we might use these to make our fitted() version of the leftmost panel of Figure 4.3. f %&gt;% ggplot(aes(x = age)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;, alpha = 3/4) + geom_line(aes(y = Estimate)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2, limits = c(0, 2)) + coord_cartesian(xlim = c(13, 17)) + theme(panel.grid = element_blank()) With fit4.6 (i.e., Model E), we now have three predictors. We’d like to see the full range across age_14 for four combinations of coa and peer values. To my mind, the easiest way to get those values right is with a little crossing() and expand(). nd &lt;- crossing(coa = 0:1, peer = c(.655, 1.381)) %&gt;% expand_grid(age_14 = seq(from = 0, to = 2, length.out = 30)) head(nd, n = 10) ## # A tibble: 10 × 3 ## coa peer age_14 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.655 0 ## 2 0 0.655 0.0690 ## 3 0 0.655 0.138 ## 4 0 0.655 0.207 ## 5 0 0.655 0.276 ## 6 0 0.655 0.345 ## 7 0 0.655 0.414 ## 8 0 0.655 0.483 ## 9 0 0.655 0.552 ## 10 0 0.655 0.621 Now we use fitted() much like before. f &lt;- fitted(fit4.6, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # a little wrangling will make plotting much easier mutate(age = age_14 + 14, coa = ifelse(coa == 0, &quot;coa = 0&quot;, &quot;coa = 1&quot;), peer = factor(peer)) glimpse(f) ## Rows: 120 ## Columns: 8 ## $ Estimate &lt;dbl&gt; 0.1401928, 0.1626439, 0.1850951, 0.2075462, 0.2299973, 0.2524485, 0.2748996, 0.2… ## $ Est.Error &lt;dbl&gt; 0.1107089, 0.1091486, 0.1077753, 0.1065962, 0.1056179, 0.1048459, 0.1042849, 0.1… ## $ Q2.5 &lt;dbl&gt; -7.684122e-02, -4.981252e-02, -2.654313e-02, 7.571781e-05, 2.576044e-02, 4.94618… ## $ Q97.5 &lt;dbl&gt; 0.3567606, 0.3752108, 0.3950149, 0.4142247, 0.4357135, 0.4581377, 0.4791907, 0.5… ## $ coa &lt;chr&gt; &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;coa = 0&quot;, &quot;co… ## $ peer &lt;fct&gt; 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.6… ## $ age_14 &lt;dbl&gt; 0.00000000, 0.06896552, 0.13793103, 0.20689655, 0.27586207, 0.34482759, 0.413793… ## $ age &lt;dbl&gt; 14.00000, 14.06897, 14.13793, 14.20690, 14.27586, 14.34483, 14.41379, 14.48276, … For our version of the right panel of Figure 4.3, most of the action is in ggplot(), geom_ribbon(), geom_line(), and facet_wrap(). All the rest is cosmetic. f %&gt;% ggplot(aes(x = age, color = peer, fill = peer)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate, size = peer)) + scale_size_manual(values = c(1/2, 1)) + scale_fill_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + scale_color_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2) + labs(subtitle = &quot;High peer values are in red; low ones are in blue.&quot;) + coord_cartesian(xlim = c(13, 17)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ coa) In my opinion, it works better to split the plot into two when you include the 95% intervals. 4.5.4 Recentering predictors to improve interpretation. The easiest strategy for recentering a time-invariant predictor is to subtract its sample mean from each observed value. When we center a predictor on its sample mean, the level-2 fitted intercepts represent the average fitted values of initial status (or rate of change). We can also recenter a time-invariant predictor by subtracting another meaningful value… Recentering works best when the centering constant is substantively meaningful. (pp. 113–114) As we’ll see later, centering can also make it easier to select meaningful priors on the model intercept. If you look at our alcohol1_pp data, you’ll see we already have centered versions of our time-invariant predictors. They’re the last two columns, cpeer and ccoa. alcohol1_pp %&gt;% glimpse() ## Rows: 246 ## Columns: 9 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16,… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ male &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,… ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,… ## $ alcuse &lt;dbl&gt; 1.732051, 2.000000, 2.000000, 0.000000, 0.000000, 1.000000, 1.000000, 2.000000, 3.3… ## $ peer &lt;dbl&gt; 1.2649111, 1.2649111, 1.2649111, 0.8944272, 0.8944272, 0.8944272, 0.8944272, 0.8944… ## $ cpeer &lt;dbl&gt; 0.2469111, 0.2469111, 0.2469111, -0.1235728, -0.1235728, -0.1235728, -0.1235728, -0… ## $ ccoa &lt;dbl&gt; 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549,… If you wanted to center them by hand, you’d just execute something like this. alcohol1_pp %&gt;% mutate(peer_c = peer - mean(peer)) ## # A tibble: 246 × 10 ## id age coa male age_14 alcuse peer cpeer ccoa peer_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 14 1 0 0 1.73 1.26 0.247 0.549 0.247 ## 2 1 15 1 0 1 2 1.26 0.247 0.549 0.247 ## 3 1 16 1 0 2 2 1.26 0.247 0.549 0.247 ## 4 2 14 1 1 0 0 0.894 -0.124 0.549 -0.123 ## 5 2 15 1 1 1 0 0.894 -0.124 0.549 -0.123 ## 6 2 16 1 1 2 1 0.894 -0.124 0.549 -0.123 ## 7 3 14 1 1 0 1 0.894 -0.124 0.549 -0.123 ## 8 3 15 1 1 1 2 0.894 -0.124 0.549 -0.123 ## 9 3 16 1 1 2 3.32 0.894 -0.124 0.549 -0.123 ## 10 4 14 1 1 0 0 1.79 0.771 0.549 0.771 ## # ℹ 236 more rows Did you notice how our peer_c values, above, deviated slightly from those in cpeer? That’s because peer_c was based on the exact sample mean. Those in cpeer are based on the sample mean as provided in the text, 1.018, which is introduces rounding error. For the sake of simplicity, we’ll go with centered variables matching up with the text. Here we’ll hastily fit the models with help from the update() function. fit4.7 &lt;- update(fit4.6, newdata = alcohol1_pp, alcuse ~ 0 + Intercept + age_14 + coa + cpeer + age_14:cpeer + (1 + age_14 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.07&quot;) fit4.8 &lt;- update(fit4.6, newdata = alcohol1_pp, alcuse ~ 0 + Intercept + age_14 + ccoa + peer + age_14:peer + (1 + age_14 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.08&quot;) Here we reproduce the \\(\\gamma\\)s from fit4.6 and compare the to the updates from fit4.7 and fit4.8. fixef(fit4.6) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.314 0.151 -0.608 -0.021 ## age_14 0.425 0.105 0.223 0.630 ## coa 0.573 0.148 0.284 0.859 ## peer 0.694 0.113 0.475 0.915 ## age_14:peer -0.152 0.085 -0.314 0.015 fixef(fit4.7) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.392 0.105 0.187 0.598 ## age_14 0.272 0.063 0.145 0.391 ## coa 0.572 0.150 0.274 0.859 ## cpeer 0.698 0.114 0.480 0.920 ## age_14:cpeer -0.152 0.085 -0.318 0.012 fixef(fit4.8) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.057 0.140 -0.328 0.210 ## age_14 0.425 0.107 0.223 0.630 ## ccoa 0.573 0.148 0.277 0.866 ## peer 0.694 0.113 0.472 0.920 ## age_14:peer -0.152 0.086 -0.321 0.013 4.6 Comparing models using deviance statistics As you will see, we will also make use of deviance within our Bayesian Stan-based paradigm. But we’ll do so a little differently than what Singer and Willett presented. 4.6.1 The deviance statistic. As it turns out, we Bayesians use the log-likelihood (LL), too. Recall how the numerator in the right-hand side of Bayes’ Theorem was \\(p(\\text{data} \\mid \\theta) p(\\theta)\\)? That first part, \\(p(\\text{data} \\mid \\theta)\\), is the likelihood. In words, the likelihood is the probability of the data given the parameters. We generally work with the log of the likelihood rather than the likelihood itself because it’s easier to work with statistically. When you’re working with brms, you can extract the LL with the log_lik() function. Here’s an example with fit4.1, our unconditional means model. log_lik(fit4.1) %&gt;% str() ## num [1:4000, 1:246] -0.629 -0.72 -1.075 -0.708 -0.926 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL You may have noticed we didn’t just get a single value back. Rather, we got an array of 4,000 rows and 246 columns. The reason we got 4,000 rows is because that’s how many post-warmup iterations we drew from the posterior. I.e., we set brm(..., iter = 2000, warmup = 1000, chains = 4). With respect to the 246 columns, that’s how many rows there are in the alcohol1_pp data. So for each case in the data, we get an entire posterior distribution of LL values. With the multilevel model, we can define deviance for a given model as its LL times -2, \\[ \\text{Deviance} = -2 LL_\\text{current model}. \\] Here that is in code for fit4.1. ll &lt;- log_lik(fit4.1) %&gt;% data.frame() %&gt;% mutate(ll = rowSums(.)) %&gt;% mutate(deviance = -2 * ll) %&gt;% select(ll, deviance, everything()) dim(ll) ## [1] 4000 248 Because we used HMC, deviance is a distribution rather than a single number. Here’s what it looks like for fit4.1. ll %&gt;% ggplot(aes(x = deviance)) + geom_density(fill = &quot;grey25&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Much like the frequentists, we Bayesian generally prefer models with smaller deviance distributions. The reasons frequentists multiply the LL by -2 is because after doing so, the difference in deviance values between two models follows a \\(\\chi^2\\) distribution and the old \\(\\chi^2\\)-difference test is widely-used in frequentist statistics. Bayesians often just go ahead and use the -2 multiplication, too. It’s largely out of tradition. But as we’ll see, some contemporary Bayesians are challenging that tradition. 4.6.2 When and how can you compare deviance statistics? As for the frequentists, deviance values/distributions in the Bayesian context are only meaningful in the relative sense. You cannot directly interpret a single models deviance distribution by the magnitude or sign of its central tendency. But you can compare two or more models by the relative locations of their deviance distributions. When doing so, they must have been computed using the same data (i.e., no differences in missingness in the predictors) and the models must be nested. However, in contemporary Bayesian practice we don’t tend to compare models with deviance. For details on why, check out Chapter 6 in McElreath’s Statistical Rethinking. McElreath also covered the topic in several online lectures (e.g., here and here). 4.6.3 Implementing deviance-based hypothesis tests. In this project, we are not going to practice comparing deviance values using frequentist \\(\\chi^2\\) tests. We will, however, cover Bayesian information criteria. 4.6.4 AIC and BIC WAIC and LOO statistics: Comparing nonnested models using information criteria [and cross validation]. We do not use the AIC or the BIC within the Stan ecosystem. The AIC is frequentist and cannot handle models with priors. The BIC is interesting in it’s a double misnomer. It is neither Bayesian nor is it a proper information criterion–though it does scale like one. However, it might be useful for our purposes to walk out the AIC a bit. It’ll ground our discussion of the WAIC and LOO. From Spiegelhalter, Best, Carlin and van der Linde [@-spiegelhalterDevianceInformationCriterion2014], we read: Suppose that we have a given set of candidate models, and we would like a criterion to assess which is ‘better’ in a defined sense. Assume that a model for observed data \\(y\\) postulates a density \\(p(y \\mid \\theta)\\) (which may include covariates etc.), and call \\(D(\\theta) = -2 \\log {p(y \\mid \\theta)}\\) the deviance, here considered as a function of \\(\\theta\\). Classical model choice uses hypothesis testing for comparing nested models, e.g. the deviance (likelihood ratio) test in generalized linear models. For non nested models, alternatives include the Akaike information criterion \\[AIC = -2 \\log {p(y \\mid \\hat{\\theta})} + 2k\\] where \\(\\hat{\\theta}\\) is the maximum likelihood estimate and \\(k\\) is the number of parameters in the model (dimension of \\(\\Theta\\)). AIC is built with the aim of favouring models that are likely to make good predictions. Since we generally do not have independent validation data, we can assess which model best predicts the observed data by using the deviance, but if parameters have been estimated we need some penalty for this double use of the data. AIC’s penalty of 2k has been shown to be asymptotically equivalent to leave-one-out cross-validation. However, AIC does not work in models with informative prior information, such as hierarchical models, since the prior effectively acts to ‘restrict’ the freedom of the model parameters, so the appropriate ‘number of parameters’ is generally unclear. (pp. 485–486, emphasis in the original) For the past two decades, the Deviance Information Criterion [DIC; Spiegelhalter et al. (2002)] has been a popular information criterion among Bayesians. Let’s define \\(D\\) as the posterior distribution of deviance values and \\(\\bar D\\) as its mean. If you compute deviance based on the posterior mean, you have \\(\\hat D\\). Within a multi-parameter model, this would be the deviance based on the collection of the posterior mean of each parameter. With these, we define the DIC as \\[\\text{DIC} = \\bar D + (\\bar D + \\hat D) + \\bar D + p_D,\\] where \\(p_D\\) is the number of effective parameters in the model, which is also sometimes referred to as the penalty term. As McElreath pointed out in Statistical Rethinking, the \\(p_D\\) is just the expected distance between the deviance in-sample and the deviance out-of-sample. In the case of flat priors, DIC reduces directly to AIC, because the expected distance is just the number of parameters. But more generally, \\(p_D\\) will be some fraction of the number of parameters, because regularizing priors constrain a model’s flexibility. (p. 191) As you’ll see, you can get the \\(p_D\\) for brms::brm() models. However, the DIC is limited in that it requires a multivariate Gaussian posterior and I’m not aware of a convenience function within brms that will compute the DIC. Which is fine. The DIC has been overshadowed in recent years by newer methods. But for a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. 4.6.4.1 The Widely Applicable Information Criterion (WAIC). The main information criterion within our Stan ecosystem paradigm is the Widely Applicable Information Criterion [WAIC; Watanabe (2010)]. From McElreath, again, we read: It does not require a multivariate Gaussian posterior, and it is often more accurate than DIC. There are types of models for which it is hard to define at all, however. We’ll discuss that issue more, after defining WAIC. The distinguishing feature of WAIC is that it is pointwise. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data. This is useful, because some observations are much harder to predict than others and may also have different uncertainty… You can think of WAIC as handling uncertainty where it actually matters: for each independent observation. Define \\(\\Pr (y_i)\\) as the average likelihood of observation \\(i\\) in the training sample. This means we compute the likelihood of \\(y_i\\) for each set of parameters samples from the posterior distribution. Then we average the likelihoods for each observation \\(i\\) and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density, \\[\\text{lppd} = \\sum_{i = 1}^N \\log \\Pr (y_i)\\] You might say this out loud as: The log-pointwise-predictive-density is the total across observations of the logarithm of the average likelihood of each observation. The lppd is just a pointwise analog of deviance, averaged over the posterior distribution. If you multiplied it by -2, it’d be similar to the deviance, in fact. The second piece of WAIC is the effective number of parameters \\(p_\\text{WAIC}\\). Define \\(V(y_i)\\) as the variance in log-likelihood for observation \\(i\\) in the training sample. This means we compute the log-likelihood of \\(y_i\\) for each sample from the posterior distribution. Then we take the variance of those values. This is \\(V(y_i)\\). Now \\(p_\\text{WAIC}\\) is defined as: \\[p_\\text{WAIC} = \\sum_{i=1}^N V(y_i)\\] Now WAIC is defined as: \\[\\text{WAIC} = -2(\\text{lppd} - p_\\text{WAIC})\\] And this value is yet another estimate of out-of-sample deviance. (pp. 191–192, emphasis in the original) In Chapter 6 of my ebook translating McElreath’s Statistical Rethinking into brms and tidyverse code, I walk out how to hand compute the WAIC for a brm() fit. I’m not going to repeat the exercise, here. But do see the project and McElreath’s text if you’re interested. Rather, I’d like to get down to business. In brms, you can get a model’s WAIC with the waic() function. waic(fit4.1) ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_waic -312.4 12.1 ## p_waic 55.4 4.8 ## waic 624.7 24.3 ## ## 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying loo instead. We’ll come back to that warning message, later. For now, notice the main output is a \\(3 \\times 2\\) data frame with named rows. For the statistic in each row, you get a point estimate and a standard error. The WAIC is on the bottom. The effective number of parameters, the \\(p_\\text{WAIC}\\), is in the middle. Notice the elpd_waic on the top. That’s what you get without the \\(-2 \\times \\dots\\) in the formula. Remember how that part is just to put things in a metric amenable to \\(\\chi^2\\)-difference testing? Well, not all Bayesians like that and within the Stan ecosystem you’ll also see the WAIC expressed instead as the \\(\\text{elpd}_\\text{WAIC}\\). The current recommended workflow within brms is to attach the WAIC information to the model fit. You do it with the add_criterion() function. fit4.1 &lt;- add_criterion(fit4.1, criterion = &quot;waic&quot;) And now you can access that information directly with good-old $ indexing. fit4.1$criteria$waic ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_waic -312.4 12.1 ## p_waic 55.4 4.8 ## waic 624.7 24.3 ## ## 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying loo instead. You might notice how that value is similar to the AIC and BIC values for Model A in Table 4.1. But it’s not identical and we shouldn’t expect it to be. It was computed by a different formula that accounts for priors. For our purposes, this is much better than the frequentist AIC and BIC. We need statistics that can handle priors. 4.6.4.2 Leave-one-out cross-validation (LOO-CV). We have another big option for model comparison within the Stan ecosystem. It involves leave-one-out cross-validation (LOO-CV). It’s often the case that we aren’t just interested in modeling the data we have in hand. The hope is our findings would generalize to other data we could have collected or may collect in the future. We’d like our findings to tell us something more general about the world at large. But unless you’re studying something highly uniform like the weights of hydrogen atoms, chances are your data have idiosyncrasies that won’t generalize well to other data. Sure, if we had all the information on all the relevant variables, we could explain the discrepancies across samples with hidden moderators and such. But we don’t have all the data and we typically don’t even know what all the relevant variables are. Welcome to science. To address this problem, you might recommend we collect data from two samples for each project. Starting with sample A, we’d fit a series of models and settle on one or a small subset that both speak to our scientific hypothesis and seem to fit the sample A data well. Then we’d switch to sample B and rerun our primary model(s) from A to make sure our findings generalize. In this paradigm, we might call the A data in sample and the B data out of sample–or out of sample A, anyways. The problem is we often have time and funding constraints. We only have sample A and we may never collect sample B. So we’ll need to make the most out of A. Happily, tricky statisticians have our back. Instead, what we might do is divide our data into \\(k\\) equally-sized subsets. Call those subsets folds. If we leave one of the folds out, we can fit the model with the remaining data and then see how well that model speaks to the left-out fold. After doing this for every fold, we can get an average performance across folds. Note how as \\(k\\) increases, the number of cases with a fold get smaller. In the extreme, \\(k = N\\), the number of cases within the data. At that point, \\(k\\)-fold cross-validation turns into leave-one-out cross-validation (LOO-CV). But there’s a practical difficulty with LOO-CV: it’s costly. As you may have noticed, it takes some time to fit a Bayesian multilevel model. For large data and/or complicated models, sometimes it takes hours or days. Most of us just don’t have enough time or computational resources to fit that many models. Happily, we have an approximation to pure LOO-CV. Vehtari, Gelman, and Gabry (2017) proposed Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO) as an efficient way to approximate true LOO-CV. At this point, it’s probably best to let the statisticians speak for themselves: To maintain comparability with the given dataset and to get easier interpretation of the differences in scale of effective number of parameters, we define a measure of predictive accuracy for the \\(n\\) data points taken one at a time: \\[\\begin{align*} \\text{elpd} &amp; = \\text{expected log pointwise predictive density for a new dataset} \\\\ &amp; = \\sum_{i = 1}^n \\int p_t (\\tilde{y}_i) \\log p (\\tilde{y}_i \\mid y) d \\tilde{y}_i, \\end{align*}\\] where \\(p_t (\\tilde{y}_i)\\) is the distribution representing the true data-generating process for \\(\\tilde{y}_i\\). The \\(p_t (\\tilde{y}_i)\\)’s are unknown, and we will use cross-validation or WAIC to approximate. In a regression, these distributions are also implicitly conditioned on any predictors in the model… The Bayesian LOO estimate of out-of-sample predictive fit is \\[\\text{elpd}_{\\text{loo}} = \\sum_{i = 1}^n \\log p (y_i \\mid y - _i),\\] where \\[p (y_i \\mid y - _i) = \\int p (y_i \\mid \\theta) p (\\theta \\mid y - _i) d \\theta\\] is the leave-one-out predictive density given the data without the ith data point. (pp. 2–3) For the rest of the details, check out the original paper. Our goal is to practice using the PSIS-LOO. Since this is the only version of the LOO we’ll be using in this project, I’m just going to refer to it as the LOO from here on. To use the LOO to evaluate a brm() fit, you just use the loo() function. Though you don’t have to save the results as an object, we’ll be forward thinking and do so here. l_fit4.1 &lt;- loo(fit4.1) print(l_fit4.1) ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_loo -315.8 12.5 ## p_loo 58.8 5.2 ## looic 631.5 25.0 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 218 88.6% 920 ## (0.5, 0.7] (ok) 25 10.2% 90 ## (0.7, 1] (bad) 3 1.2% 35 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## See help(&#39;pareto-k-diagnostic&#39;) for details. Remember that warning message we got from the waic() a while back? We get more information along those lines from the loo(). As it turns out, a few of the cases in the data were unduly influential in the model fit. Within the loo() paradigm, those are indexed by the pareto_k values. As it turns out, the Pareto \\(k\\) can be used as a diagnostic tool. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)’s are low. We typically get worried when those \\(k\\)’s exceed 0.7 and the loo() function spits out a warning message when they do. If you didn’t know, the brms functions like the waic() and loo() actually come from the loo package (Vehtari et al., 2017; Vehtari et al., 2022; Yao et al., 2018). Explicitly loading loo will buy us some handy convenience functions. library(loo) We’ll be leveraging those \\(k\\) values with the pareto_k_table() and pareto_k_ids() functions. Both functions take objects created by the loo() or psis() functions. Let’s take a look at the pareto_k_table() function first. pareto_k_table(l_fit4.1) ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 218 88.6% 920 ## (0.5, 0.7] (ok) 25 10.2% 90 ## (0.7, 1] (bad) 3 1.2% 35 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; This is the same table that popped out earlier after using the loo(). Recall that this data set has 246 observations (i.e., execute count(alcohol1_pp)). With pareto_k_table(), we see how the Pareto \\(k\\) values have been categorized into bins ranging from “good” to “very bad”. Clearly, we like nice and low \\(k\\)’s. In this example, most of our observations are “good” or “ok.” Two are in the “bad” \\(k\\) range. We can take a closer look by placing our loo() object into plot(). plot(l_fit4.1) We got back a nice diagnostic plot for those \\(k\\) values, ordered by row number. We can see that our three observations with the “bad” \\(k\\) values were earlier in the data and it appears their \\(k\\) values are just a smidge above the recommended threshold. If we wanted to further verify to ourselves which observations those were, we’d use the pareto_k_ids() function. pareto_k_ids(l_fit4.1, threshold = .7) ## [1] 27 132 177 Note our use of the threshold argument. Play around with it to see how it works. In case you’re curious, here are those rows. alcohol1_pp %&gt;% slice(pareto_k_ids(l_fit4.1, threshold = .7)) ## # A tibble: 3 × 9 ## id age coa male age_14 alcuse peer cpeer ccoa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 16 1 1 2 3.46 0 -1.02 0.549 ## 2 44 16 0 1 2 3 0 -1.02 -0.451 ## 3 59 16 0 1 2 2.65 0.894 -0.124 -0.451 If you want an explicit look at those \\(k\\) values, execute l_fit4.1$diagnostics$pareto_k. For the sake of space, I’m going to omit the output. l_fit4.1$diagnostics$pareto_k The pareto_k values can be used to examine cases that are overly-influential on the model parameters, something like a Cook’s \\(D_i\\). See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in Vehtari et al. (2017) and in this presentation by Aki Vehtari. Anyway, the implication of all this is these values suggest fit4.1 (i.e., Model A) might not be the best model of the data. Happily, we have other models to compare it to. That leads into the next section: 4.6.4.3 You can compare Bayesian models with the WAIC and LOO. Remember how we used the add_criterion() function, above. That’ll work for both WAIC and the LOO. Let’s do that for Models A through E. fit4.1 &lt;- add_criterion(fit4.1, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit4.2 &lt;- add_criterion(fit4.2, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit4.3 &lt;- add_criterion(fit4.3, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit4.5 &lt;- add_criterion(fit4.5, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) And to refresh, we can pull the WAIC and LOO information with $ indexing. Here’s how to get the LOO info for fit4.2. fit4.2$criteria$loo ## ## Computed from 4000 by 246 log-likelihood matrix ## ## Estimate SE ## elpd_loo -289.5 12.8 ## p_loo 95.9 7.6 ## looic 579.0 25.5 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 127 51.6% 571 ## (0.5, 0.7] (ok) 86 35.0% 47 ## (0.7, 1] (bad) 32 13.0% 21 ## (1, Inf) (very bad) 1 0.4% 15 ## See help(&#39;pareto-k-diagnostic&#39;) for details. Sigh. Turns out there are even more overly-influential cases in the unconditional growth model. In the case of a real data analysis, this might suggest we need a more robust model. One possible solution might be switching out our Gaussian likelihood for the robust Student’s \\(t\\)-distribution. For an introduction, you might check out my blog post on the topic, Robust Linear Regression with Student’s \\(t\\)-Distribution. But that’ll take us farther afield than I want to go, right now. The point to focus on, here, is we can use the loo_compare() function to compare fits by their WAIC or LOO. Let’s practice with the WAIC. ws &lt;- loo_compare(fit4.1, fit4.2, fit4.3, fit4.5, criterion = &quot;waic&quot;) print(ws) ## elpd_diff se_diff ## fit4.5 0.0 0.0 ## fit4.2 -5.3 4.3 ## fit4.3 -5.7 3.7 ## fit4.1 -41.3 8.2 Remember how we said that some contemporary Bayesians aren’t fans of putting Bayesian information criteria in a \\(\\chi^2\\) metric? Well, it turns out Aki Vehtari, of the Stan team and loo package fame–and also the primary author in that PSIS-LOO paper from before–, is one of those Bayesians. So instead of getting difference scores in the WAIC metric, we get them in the \\(\\text{elpd}_\\text{WAIC}\\) metric instead. But remember, if you prefer these estimates in the traditional metric, just multiply by -2. cbind(waic_diff = ws[, 1] * -2, se = ws[, 2] * 2) ## waic_diff se ## fit4.5 0.00000 0.000000 ## fit4.2 10.56861 8.559608 ## fit4.3 11.36732 7.363773 ## fit4.1 82.58885 16.445378 The reason we multiplied the se_diff column (i.e., the standard errors for the difference estimates) by 2 is because you can’t have negative standard errors. That’d be silly. But anyway, notice that the brm() fits have been rank ordered with the smallest differences at the top. Each row in the output is the difference of one of the fits compared to the best-fitting fit. Since fit4.5 apparently had the lowest WAIC value, it was ranked at the top. And notice how its waic_diff is 0. That, of course, is because \\(x - x = 0\\). So all the other difference scores are follow the formula \\(\\text{Difference}_x = \\text{WAIC}_\\text{fit_x} - \\text{WAIC}_\\text{fit_4.5}\\). Concerning our ws object, we can get more information on our models’ WAIC information if we include a simplify = F argument within print(). print(ws, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit4.5 0.0 0.0 -271.1 11.2 74.2 5.8 542.2 22.4 ## fit4.2 -5.3 4.3 -276.4 11.8 82.8 6.6 552.7 23.6 ## fit4.3 -5.7 3.7 -276.8 11.6 81.2 6.3 553.5 23.1 ## fit4.1 -41.3 8.2 -312.4 12.1 55.4 4.8 624.7 24.3 Their WAIC estimates and the associated standard errors are in the final two columns. In the two before that, we get the \\(p_\\text{WAIC}\\) estimates and their standard errors. We can get similar information for the LOO. loo_compare(fit4.1, fit4.2, fit4.3, fit4.5, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit4.5 0.0 0.0 -282.2 12.1 85.3 6.9 564.4 24.3 ## fit4.2 -7.3 4.6 -289.5 12.8 95.9 7.6 579.0 25.5 ## fit4.3 -7.7 4.0 -289.9 12.7 94.3 7.5 579.7 25.4 ## fit4.1 -33.6 8.4 -315.8 12.5 58.8 5.2 631.5 25.0 If you wanted a more focused comparison, say between fit1 and fit2, you’d just simplify your input. loo_compare(fit4.1, fit4.2, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit4.2 0.0 0.0 -289.5 12.8 95.9 7.6 579.0 25.5 ## fit4.1 -26.3 7.9 -315.8 12.5 58.8 5.2 631.5 25.0 We’ll get more practice with these methods as we go along. But for your own edification, you might check out the vignettes put out by the loo team. 4.7 Using Wald statistics to test composite hypotheses about fixed effects I’m not going to address issues of composite null-hypothesis tests using the Wald statistic. However, we can address some of these issues from a different more estimation-based perspective. Consider the initial question posed on page 123: Suppose, for example, you wanted to test whether the entire true change trajectory for a particular type of adolescent–say, a child of non-alcoholic parents with an average value of PEER–differs from a “null” trajectory (one with zero intercept and zero slope). This is tantamount to asking whether the average child of non-alcoholic parents drinks no alcohol at age 14 and remains abstinent over time. Singer and Willett then expressed their joint null hypothesis as \\[H_0: \\gamma_{00} = 0 \\; \\text{and} \\; \\gamma_{10} = 0.\\] This is a substantive question we can address more informatively with fitted(). First, let’s provide the necessary values for our predictor variables, coa, peer, and age_14. mu_peer &lt;- mean(alcohol1_pp$peer) nd &lt;- tibble(coa = 0, peer = mu_peer, age_14 = seq(from = 0, to = 2, length.out = 30)) head(nd) ## # A tibble: 6 × 3 ## coa peer age_14 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1.02 0 ## 2 0 1.02 0.0690 ## 3 0 1.02 0.138 ## 4 0 1.02 0.207 ## 5 0 1.02 0.276 ## 6 0 1.02 0.345 Now we use fitted() to examine the model-implied trajectory for a child of non-alcoholic parents and average peer values. f &lt;- fitted(fit4.6, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(age = age_14 + 14) f %&gt;% ggplot(aes(x = age)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2, limits = c(0, 2)) + labs(subtitle = &quot;Zero is credible for neither\\nthe intercept nor the slope.&quot;) + coord_cartesian(xlim = c(13, 17)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Recall that the result of our Bayesian analyses are the probability of the parameters given the data, \\(p(\\theta \\mid d)\\). Based on our plot, there is much less than a .05 probability either intercept or slope for teens of this demographic are zero. If you really wanted to fixate on zero, you could even use geom_hline() to insert a horizontal line at zero in the figure. But all that fixating on zero detracts from what to my mind are the more important parts of the model. The intercept at age = 14 is about 1/3 and the endpoint when age = 16 is almost at \\(1\\). Those are our effect sizes. If you wanted to quantify those effect sizes more precisely, just query our fitted() object, f. f %&gt;% select(age, Estimate, Q2.5, Q97.5) %&gt;% filter(age %in% c(14, 16)) %&gt;% mutate_all(round, digits = 2) ## age Estimate Q2.5 Q97.5 ## 1 14 0.39 0.18 0.6 ## 2 16 0.93 0.67 1.2 Works like a champ. But we haven’t fully covered part of Singer and Willett’s joint hypothesis test. They proposed a joint Null that included the \\(\\gamma_{10} = 0\\). Though it’s clear from the plot that the trajectory increases, we can address the issue more directly with a difference score. For our difference, we’ll subtract the estimate at age = 14 from the one at age = 15. But to that, we’ll have to return to fitted(). So far, we’ve been using the default output which returns summaries of the posterior. To compute a proper difference score, we’ll need to work with all the posterior draws in order to approximate the full distribution. We do that by setting summary = F. And since we’re only interested in the estimates from these two age values, we’ll streamline our nd data. nd &lt;- tibble(coa = 0, peer = mu_peer, age_14 = 0:1) f &lt;- fitted(fit4.6, newdata = nd, re_formula = NA, summary = F) %&gt;% data.frame() str(f) ## &#39;data.frame&#39;: 4000 obs. of 2 variables: ## $ X1: num 0.557 0.428 0.368 0.433 0.319 ... ## $ X2: num 0.862 0.756 0.602 0.66 0.642 ... Now our f object has 4,000 rows and 2 columns. Each of the rows corresponds to one of the 4,000 post-warmup posterior draws. The columns correspond to the two rows in our nd data. To get a slope based on this combination of predictor values, we simply subtract the first column from the second. f &lt;- f %&gt;% transmute(difference = X2 - X1) f %&gt;% ggplot(aes(x = difference)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Based on 4,000 posterior draws, not a single one\\nsuggests the slope is even close to zero. Rather, the\\nposterior mass is concentrated around 0.25.&quot;, x = expression(paste(gamma[0][1], &quot; (i.e., the difference between the two time points)&quot;))) + coord_cartesian(xlim = 0:1) + theme(panel.grid = element_blank()) Here are the posterior mean and 95% intervals. f %&gt;% summarise(mean = mean(difference), ll = quantile(difference, probs = .025), ul = quantile(difference, probs = .975)) ## mean ll ul ## 1 0.2705034 0.1463065 0.3968954 On page 125, Singer and Willett further mused: When we examined the OLS estimated change trajectories in figure 4.2, we noticed that among children of non-alcoholic parents, those with low values of CPEER tended to have a lower initial status and steeper slopes than those with high values of CPEER. We might therefore ask whether the former group “catches up” to the latter. This is a question about the “vertical” separation between these two groups[’] true change trajectories at some later age, say 16. Within their joint hypothesis testing paradigm, they pose this as testing \\[H_0: 0\\gamma_{00} + 0\\gamma_{01} + 1\\gamma_{02} + 0\\gamma_{10} + 2\\gamma_{12} = 0.\\] From our perspective, this is a differences of differences analysis. That is, first we’ll compute the model implied alcuse estimates for the four combinations of the two levels of age and peer, holding coa constant at 0. Second, we’ll compute the differences between the two peer levels at each age. Third and finally, we’ll compute a difference of those differences. For our first step, recall it was fit4.7 that used the cpeer variable. # first step nd &lt;- crossing(age_14 = c(0, 2), cpeer = c(-.363, .363)) %&gt;% mutate(coa = 0) head(nd) ## # A tibble: 4 × 3 ## age_14 cpeer coa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 -0.363 0 ## 2 0 0.363 0 ## 3 2 -0.363 0 ## 4 2 0.363 0 f &lt;- fitted(fit4.7, newdata = nd, re_formula = NA, summary = F) %&gt;% data.frame() head(f) ## X1 X2 X3 X4 ## 1 0.03788212 0.5636738 0.7855357 0.9867873 ## 2 0.14784119 0.5055112 0.7909200 0.9861349 ## 3 0.18285206 0.4843414 0.5921288 0.8070981 ## 4 0.08708519 0.4795838 0.7427321 0.9024861 ## 5 0.18469932 0.7342615 0.8319810 1.0873110 ## 6 0.06184649 0.6010113 0.5206236 0.8319538 For our initial difference scores, we’ll subtract the estimates for the lower level of cpeer from the higher ones. # step 2 f &lt;- f %&gt;% transmute(`difference at 14` = X2 - X1, `difference at 16` = X4 - X3) head(f) ## difference at 14 difference at 16 ## 1 0.5257917 0.2012516 ## 2 0.3576700 0.1952149 ## 3 0.3014894 0.2149694 ## 4 0.3924986 0.1597540 ## 5 0.5495622 0.2553300 ## 6 0.5391648 0.3113302 For our final difference score, we’ll subtract the first difference score from the second. # step 3 f &lt;- f %&gt;% mutate(`difference in differences` = `difference at 16` - `difference at 14`) head(f) ## difference at 14 difference at 16 difference in differences ## 1 0.5257917 0.2012516 -0.32454013 ## 2 0.3576700 0.1952149 -0.16245519 ## 3 0.3014894 0.2149694 -0.08652001 ## 4 0.3924986 0.1597540 -0.23274460 ## 5 0.5495622 0.2553300 -0.29423221 ## 6 0.5391648 0.3113302 -0.22783462 Here we’ll plot all three. f %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;different differences&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) Singer and Willett concluded they could “reject the null hypothesis at any conventional level of significance” (p. 126). If we must appeal to the Null, here are the posterior means and 95% intervals for our differences. f %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 3 × 4 ## name mean ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 difference at 14 0.507 0.349 0.668 ## 2 difference at 16 0.286 0.064 0.506 ## 3 difference in differences -0.22 -0.462 0.017 Our results contrast a bit from Singer and Willett’s. Though the bulk of our posterior mass is concentrated around -0.22, zero is a credible value within the difference of differences density. Our best bet is the differences begin to converge over time. However, that rate of that convergence is subtle and somewhat imprecise relative to the effect size. Interpret with caution. 4.8 Evaluating the tenability of a model’s assumptions “Whenever you fit a statistical model, you invoke assumptions” (p. 127). This is the case for multilevel Bayesian models, too. 4.8.1 Checking functional form. We’ve already checked the functional form at level-1 with our version of Figure 4.1. When we made our version of Figure 4.1, we relied on ggplot2::stat_smooth() to compute the id-level OLS trajectories. To make our variants of Figure 4.4, we’ll have to back up and compute them externally with lm(). Here we’ll do so in bulk with a nested data frame. The broom package will help us extract the results. library(broom) o &lt;- alcohol1_pp %&gt;% nest(-id, -coa, -peer) %&gt;% mutate(ols = map(data, ~lm(data = ., alcuse ~ 1 + age_14))) %&gt;% mutate(tidy = map(ols, tidy)) %&gt;% unnest(tidy) %&gt;% # this is unnecessary, but will help with plotting mutate(term = factor(term, levels = c(&quot;(Intercept)&quot;, &quot;age_14&quot;), labels = c(&quot;pi[0]&quot;, &quot;pi[1]&quot;))) head(o) ## # A tibble: 6 × 10 ## id coa peer data ols term estimate std.error statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.26 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[0] 1.78 0.0999 17.8 0.0357 ## 2 1 1 1.26 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[1] 0.134 0.0774 1.73 0.333 ## 3 2 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[0] -0.167 0.373 -0.447 0.732 ## 4 2 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[1] 0.5 0.289 1.73 0.333 ## 5 3 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[0] 0.947 0.118 8.03 0.0789 ## 6 3 1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt; pi[1] 1.16 0.0914 12.7 0.0501 Now plot. o %&gt;% select(coa:peer, term:estimate) %&gt;% pivot_longer(coa:peer) %&gt;% ggplot(aes(x = value, y = estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(alpha = 2/3) + theme(panel.grid = element_blank(), strip.text = element_text(size = 11)) + facet_grid(term ~ name, scales = &quot;free&quot;, labeller = label_parsed) With a little more wrangling, we can extract the Pearson’s correlation coefficients for each panel. o %&gt;% select(coa:peer, term:estimate) %&gt;% pivot_longer(coa:peer) %&gt;% group_by(term, name) %&gt;% nest() %&gt;% mutate(r = map_dbl(data, ~cor(.)[2, 1] %&gt;% round(digits = 2))) ## # A tibble: 4 × 4 ## # Groups: term, name [4] ## term name data r ## &lt;fct&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 pi[0] coa &lt;tibble [82 × 2]&gt; 0.39 ## 2 pi[0] peer &lt;tibble [82 × 2]&gt; 0.58 ## 3 pi[1] coa &lt;tibble [82 × 2]&gt; -0.04 ## 4 pi[1] peer &lt;tibble [82 × 2]&gt; -0.19 4.8.2 Checking normality. The basic multilevel model of change yields three variance parameters, \\(\\epsilon_{ij}\\), \\(\\zeta_{0i}\\), and \\(\\zeta_{1i}\\). Each measurement occasion in the model receives a model-implied estimate for each. Singer and Willett referred to those estimates as \\(\\hat{\\epsilon}_{ij}\\), \\(\\hat{\\zeta}_{0i}\\), and \\(\\hat{\\zeta}_{1i}\\). As with frequentist software, our Bayesian software brms will return these estimates. To extract our Bayesian draws for the \\(\\hat{\\epsilon}_{ij}\\)’s, we use the residuals() function. e &lt;- residuals(fit4.6) str(e) ## num [1:246, 1:4] 0.2865 0.2544 -0.0457 -0.3951 -0.5896 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; head(e) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.28645522 0.3235166 -0.3763283 0.88418276 ## [2,] 0.25436837 0.2945643 -0.3189651 0.82411080 ## [3,] -0.04566771 0.4349924 -0.9053165 0.80634266 ## [4,] -0.39506888 0.3398897 -1.0223290 0.30304545 ## [5,] -0.58958153 0.3060274 -1.1864493 0.01680105 ## [6,] 0.21590581 0.4420805 -0.6299085 1.10509364 For our fit5, the residuals() function returned a \\(246 \\times 4\\) numeric array. Each row corresponded to one of the rows of the original data set. The four vectors are the familiar summaries Estimate, Est.Error, Q2.5, and Q97.5. If we’d like to work with these in a ggplot2-made plot, we’ll have to convert our e object to a data frame. After we make the conversion, we then make the top left panel of Figure 4.5. e &lt;- e %&gt;% data.frame() e %&gt;% ggplot(aes(sample = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_qq() + ylim(-2, 2) + labs(x = &quot;Normal score&quot;, y = expression(hat(epsilon)[italic(ij)])) + theme(panel.grid = element_blank()) For the right plot on the top, we need to add an id index. That’s as easy as appending the one from the original data. If you followed closely with the text, you may have also noticed this panel is of the standardized residuals. That just means we’ll have to hand-standardize ours before plotting. e %&gt;% bind_cols(alcohol1_pp %&gt;% select(id)) %&gt;% mutate(z = (Estimate - mean(Estimate)) / sd(Estimate)) %&gt;% ggplot(aes(x = id, y = z)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point() + scale_y_continuous(expression(italic(std)~hat(epsilon)[italic(ij)]), limits = c(-2, 2)) + theme(panel.grid = element_blank()) We’ll need to use the ranef() function to return the estimates for the \\(\\zeta\\)’s. z &lt;- ranef(fit4.6) str(z) ## List of 1 ## $ id: num [1:82, 1:4, 1:2] 0.31 -0.484 0.329 -0.353 -0.563 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:82] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;age_14&quot; z[[1]][1:6, , &quot;Intercept&quot;] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.3096401 0.3255604 -0.2994276 0.99225330 ## 2 -0.4838875 0.3379615 -1.1779208 0.13797688 ## 3 0.3288607 0.3232582 -0.3171446 0.96187776 ## 4 -0.3533780 0.3581920 -1.1149435 0.27253190 ## 5 -0.5632002 0.3358156 -1.2455240 0.07918749 ## 6 0.8331396 0.3621270 0.1799406 1.59070253 z[[1]][1:6, , &quot;age_14&quot;] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.06708165 0.2443591 -0.42337534 0.5545886 ## 2 -0.09468276 0.2464821 -0.56047541 0.4213590 ## 3 0.45095604 0.2619440 -0.03623371 0.9944058 ## 4 0.14971138 0.2626305 -0.33000069 0.7007354 ## 5 -0.30796549 0.2554600 -0.82223987 0.1962415 ## 6 0.25007189 0.2598447 -0.28315206 0.7505450 For our fit5, the ranef() function returned a list of 1, indexed by id. Therein lay a 3-dimensional array. The first two dimensions are the same as what we got from residuals(), above. The third dimension had two levels: Intercept and age_14. In other words, the third dimension is the one that differentiated between \\(\\hat{\\zeta}_{0i}\\) and \\(\\hat{\\zeta}_{1i}\\). to make this thing a little more useful, let’s convert it to a long-formatted data frame. z &lt;- rbind(z[[1]][ , , &quot;Intercept&quot;], z[[1]][ , , &quot;age_14&quot;]) %&gt;% data.frame() %&gt;% mutate(ranef = rep(c(&quot;hat(zeta)[0][italic(i)]&quot;, &quot;hat(zeta)[1][italic(i)]&quot;), each = n() / 2)) glimpse(z) ## Rows: 164 ## Columns: 5 ## $ Estimate &lt;dbl&gt; 0.30964009, -0.48388750, 0.32886075, -0.35337801, -0.56320019, 0.83313962, 0.213… ## $ Est.Error &lt;dbl&gt; 0.3255604, 0.3379615, 0.3232582, 0.3581920, 0.3358156, 0.3621270, 0.3399342, 0.3… ## $ Q2.5 &lt;dbl&gt; -0.29942760, -1.17792082, -0.31714461, -1.11494345, -1.24552398, 0.17994060, -0.… ## $ Q97.5 &lt;dbl&gt; 0.99225330, 0.13797688, 0.96187776, 0.27253190, 0.07918749, 1.59070253, 0.913804… ## $ ranef &lt;chr&gt; &quot;hat(zeta)[0][italic(i)]&quot;, &quot;hat(zeta)[0][italic(i)]&quot;, &quot;hat(zeta)[0][italic(i)]&quot;,… Now we’re ready to plot the remaining panels on the left of Figure 4.5. z %&gt;% ggplot(aes(sample = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_qq() + ylim(-1, 1) + labs(x = &quot;Normal score&quot;, y = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ ranef, labeller = label_parsed, ncol = 1) Here are the ones on the right. z %&gt;% bind_cols( bind_rows( alcohol1_pp %&gt;% distinct(id), alcohol1_pp %&gt;% distinct(id) ) ) %&gt;% mutate(ranef = str_c(&quot;italic(std)~&quot;, ranef)) %&gt;% # note we have to group them before standardizing group_by(ranef) %&gt;% mutate(z = (Estimate - mean(Estimate)) / sd(Estimate)) %&gt;% ggplot(aes(x = id, y = z)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point() + scale_y_continuous(NULL, limits = c(-3, 3)) + theme(panel.grid = element_blank()) + facet_wrap(~ ranef, labeller = label_parsed, ncol = 1) If you were paying close attention, you may have noticed that for all three of our id-level deviation estimates, they were summarized not only by a posterior mean but by standard deviations and 95% intervals, too. To give a sense of what that means, here are those last two plots, again, but this time including vertical bars defined by the 95% intervals. z %&gt;% bind_cols( bind_rows( alcohol1_pp %&gt;% distinct(id), alcohol1_pp %&gt;% distinct(id) ) ) %&gt;% ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(shape = 20, size = 1/3) + ylab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ ranef, labeller = label_parsed, ncol = 1) When you go Bayesian, even your residuals get full posterior distributions. 4.8.3 Checking homoscedasticity. Here we examine the homoscedasticity assumption by plotting the residual estimates against our predictors. We’ll start with the upper left panel of Figure 4.6. e %&gt;% bind_cols(alcohol1_pp) %&gt;% ggplot(aes(x = age, y = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(alpha = 1/4) + ylab(expression(hat(epsilon)[italic(ij)])) + coord_cartesian(xlim = c(13, 17), ylim = c(-2, 2)) + theme(panel.grid = element_blank()) Here’s a quick way to get the remaining four panels. z %&gt;% bind_cols( bind_rows( alcohol1_pp %&gt;% distinct(id, coa, peer), alcohol1_pp %&gt;% distinct(id, coa, peer) ) ) %&gt;% select(Estimate, ranef, coa, peer) %&gt;% pivot_longer(-c(Estimate, ranef)) %&gt;% ggplot(aes(x = value, y = Estimate)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(alpha = 1/3) + ylim(-1, 1) + labs(x = &quot;covariate value&quot;, y = NULL) + theme(panel.grid = element_blank(), strip.text = element_text(size = 10)) + facet_grid(ranef ~ name, labeller = label_parsed, scales = &quot;free&quot;) 4.9 Model-based (Empirical Bayes) estimates of the individual growth parameters In this section, the authors discussed two methods for constructing id-level trajectories: a) use a weighted average of the OLS and multilevel estimates and b) rely solely on the multilevel model by making use of the three sources of residual variation. Our method will be the latter. Here are the data for id == 23. alcohol1_pp %&gt;% select(id:coa, cpeer, alcuse) %&gt;% filter(id == 23) ## # A tibble: 3 × 5 ## id age coa cpeer alcuse ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23 14 1 -1.02 1 ## 2 23 15 1 -1.02 1 ## 3 23 16 1 -1.02 1.73 draws_23 &lt;- as_draws_df(fit4.7) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% # make our pis mutate(`pi[0][&quot;,23&quot;]` = b_Intercept + b_coa * 1 + b_cpeer * -1.018, `pi[1][&quot;,23&quot;]` = b_age_14 + `b_age_14:cpeer` * -1.018) head(draws_23) ## # A tibble: 6 × 7 ## b_Intercept b_age_14 b_coa b_cpeer `b_age_14:cpeer` `pi[0][&quot;,23&quot;]` `pi[1][&quot;,23&quot;]` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.301 0.293 0.527 0.724 -0.224 0.0907 0.520 ## 2 0.327 0.281 0.576 0.493 -0.112 0.401 0.395 ## 3 0.334 0.183 0.723 0.415 -0.0596 0.634 0.244 ## 4 0.283 0.270 0.598 0.541 -0.160 0.331 0.433 ## 5 0.459 0.250 0.582 0.757 -0.203 0.271 0.456 ## 6 0.331 0.172 0.859 0.743 -0.157 0.434 0.332 It doesn’t help us much now, but the reason we’ve formatted the names for our two \\(\\pi\\) columns so oddly is because those names will work much nicer in the figure we’ll make, below. Just wait and see. Anyways, more than a couple point estimates, we returned the draws from the full posterior distribution. We might summarize them. draws_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 × 4 ## name mean ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;pi[0][\\&quot;,23\\&quot;]&quot; 0.253 -0.082 0.578 ## 2 &quot;pi[1][\\&quot;,23\\&quot;]&quot; 0.426 0.213 0.626 Or we could plot them. draws_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;participant-specific parameter estimates&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, labeller = label_parsed, scales = &quot;free_y&quot;) Yet this approach neglects the \\(\\zeta\\)’s. We’ve been extracting the \\(\\zeta\\)’s with ranef(). We also get them when we use as_draws_df(). Here we’ll extract both the \\(\\gamma\\)’s as well as the \\(\\zeta\\)’s for id == 23. draws_23 &lt;- as_draws_df(fit4.7) %&gt;% select(starts_with(&quot;b_&quot;), contains(&quot;23&quot;)) glimpse(draws_23) ## Rows: 4,000 ## Columns: 7 ## $ b_Intercept &lt;dbl&gt; 0.3007780, 0.3266762, 0.3335967, 0.2833345, 0.4594804, 0.3314289, 0.4… ## $ b_age_14 &lt;dbl&gt; 0.29269175, 0.28092561, 0.18300835, 0.26963728, 0.25008280, 0.1724299… ## $ b_coa &lt;dbl&gt; 0.5271688, 0.5760530, 0.7228979, 0.5975501, 0.5824831, 0.8585142, 0.6… ## $ b_cpeer &lt;dbl&gt; 0.7242310, 0.4926585, 0.4152746, 0.5406317, 0.7569727, 0.7426513, 0.8… ## $ `b_age_14:cpeer` &lt;dbl&gt; -0.22351248, -0.11188374, -0.05958678, -0.16029242, -0.20263927, -0.1… ## $ `r_id[23,Intercept]` &lt;dbl&gt; 0.29680214, 0.38927887, 0.84109967, 0.80968707, 0.58081307, 0.1180832… ## $ `r_id[23,age_14]` &lt;dbl&gt; 0.30387794, -0.24906958, -0.32359686, -0.23184724, -0.35871606, 0.109… With the r_id prefix, brms tells you these are residual estimates for the levels in the id grouping variable. Within the brackets, we learn these particular columns are for id == 23, the first with respect to the Intercept and second with respect to the age_14 parameter. Let’s put them to use. draws_23 &lt;- draws_23 %&gt;% mutate(`pi[0][&quot;,23&quot;]` = b_Intercept + b_coa * 1 + b_cpeer * -1.018 + `r_id[23,Intercept]`, `pi[1][&quot;,23&quot;]` = b_age_14 + `b_age_14:cpeer` * -1.018 + `r_id[23,age_14]`) glimpse(draws_23) ## Rows: 4,000 ## Columns: 9 ## $ b_Intercept &lt;dbl&gt; 0.3007780, 0.3266762, 0.3335967, 0.2833345, 0.4594804, 0.3314289, 0.4… ## $ b_age_14 &lt;dbl&gt; 0.29269175, 0.28092561, 0.18300835, 0.26963728, 0.25008280, 0.1724299… ## $ b_coa &lt;dbl&gt; 0.5271688, 0.5760530, 0.7228979, 0.5975501, 0.5824831, 0.8585142, 0.6… ## $ b_cpeer &lt;dbl&gt; 0.7242310, 0.4926585, 0.4152746, 0.5406317, 0.7569727, 0.7426513, 0.8… ## $ `b_age_14:cpeer` &lt;dbl&gt; -0.22351248, -0.11188374, -0.05958678, -0.16029242, -0.20263927, -0.1… ## $ `r_id[23,Intercept]` &lt;dbl&gt; 0.29680214, 0.38927887, 0.84109967, 0.80968707, 0.58081307, 0.1180832… ## $ `r_id[23,age_14]` &lt;dbl&gt; 0.30387794, -0.24906958, -0.32359686, -0.23184724, -0.35871606, 0.109… ## $ `pi[0][&quot;,23&quot;]` &lt;dbl&gt; 0.38748175, 0.79048181, 1.47484473, 1.14020863, 0.85217841, 0.5520073… ## $ `pi[1][&quot;,23&quot;]` &lt;dbl&gt; 0.82410540, 0.14575368, -0.07992917, 0.20096773, 0.09765351, 0.441647… Here are our updated summaries. draws_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 2 × 4 ## name mean ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;pi[0][\\&quot;,23\\&quot;]&quot; 0.565 -0.072 1.21 ## 2 &quot;pi[1][\\&quot;,23\\&quot;]&quot; 0.51 0.004 1.01 And here are the updated density plots. draws_23 %&gt;% pivot_longer(starts_with(&quot;pi&quot;)) %&gt;% ggplot(aes(x = value)) + geom_density(size = 0, fill = &quot;grey25&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;participant-specific parameter estimates&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, labeller = label_parsed, scales = &quot;free_y&quot;) We’ve been focusing on the \\(\\pi\\) parameters. Notice that when we turn our attention to Figure 4.7, we’re now shifting focus slightly to the consequences of those parameters. We’re not attending to trajectories. It’s important to pick up on this distinction because it has consequences for our programming workflow. If you wanted to keep a parameter-centric workflow, we could continue to expand on our as_draws_df() by applying the full composite formula to explicitly add in predictions for various levels of age_14. And we could do that separately or in bulk for the eight participants highlighted in the figure. However pedagogically useful that might be, it’d be very tedious. If we instead take a trajectory-centric perspective, it’ll be more natural and efficient to work with a fitted()-based workflow. Let’s define our nd data. nd &lt;- alcohol1_pp %&gt;% select(id:coa, age_14:alcuse, cpeer) %&gt;% filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% # these next two lines will make plotting easier mutate(id_label = ifelse(id &lt; 10, str_c(&quot;0&quot;, id), id)) %&gt;% mutate(id_label = str_c(&quot;id = &quot;, id_label)) glimpse(nd) ## Rows: 24 ## Columns: 7 ## $ id &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65, … ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 1… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2 ## $ alcuse &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, 1… ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1.… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 23&quot;, &quot;id … We’ve isolated the relevant predictor variables for our eight focal participants. Next we’ll pump them through fitted() and wrangle as usual. f &lt;- fitted(fit4.7, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) glimpse(f) ## Rows: 24 ## Columns: 11 ## $ Estimate &lt;dbl&gt; 1.1343091, 1.4480770, 1.7618448, 2.3722619, 2.7030545, 3.0338470, 0.5651605, 1.0… ## $ Est.Error &lt;dbl&gt; 0.3610582, 0.3038636, 0.4495878, 0.3732088, 0.3184376, 0.4531277, 0.3321347, 0.2… ## $ Q2.5 &lt;dbl&gt; 0.36536447, 0.87256097, 0.89927906, 1.68844269, 2.07745330, 2.14940681, -0.07202… ## $ Q97.5 &lt;dbl&gt; 1.788938, 2.032998, 2.623005, 3.162017, 3.326801, 3.906143, 1.213904, 1.642876, … ## $ id &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65,… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, … ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2 ## $ alcuse &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, … ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 23&quot;, &quot;id… Notice how this time we omitted the re_formula = NA argument. By default, re_formula = NULL, the consequence of which is the output is based on all the parameters in the multilevel model, not just the \\(\\gamma\\)’s. Here are what they look like. f %&gt;% ggplot(aes(x = age, y = Estimate)) + geom_line(size = 1) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ id_label, ncol = 4) Now we’ve warmed up, let’s add in the data and the other lines so make the full version of Figure 4.7. Before we do so, we’ll revisit fitted(). Notice the return of the re_formula = NA argument. The trajectories in our f_gamma_only data frame will only be sensitive to the \\(\\gamma\\)s. f_gamma_only &lt;- fitted(fit4.7, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) glimpse(f_gamma_only) ## Rows: 24 ## Columns: 11 ## $ Estimate &lt;dbl&gt; 1.5013628, 1.6559457, 1.8105286, 1.6487576, 1.7712932, 1.8938287, 0.2526139, 0.6… ## $ Est.Error &lt;dbl&gt; 0.1369209, 0.1325563, 0.1830768, 0.1517240, 0.1460509, 0.2056251, 0.1692312, 0.1… ## $ Q2.5 &lt;dbl&gt; 1.23116197, 1.39538789, 1.46190193, 1.34537844, 1.48406531, 1.50343894, -0.08231… ## $ Q97.5 &lt;dbl&gt; 1.7760108, 1.9122619, 2.1666863, 1.9522861, 2.0567260, 2.2973858, 0.5777892, 1.0… ## $ id &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65,… ## $ age &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, … ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2 ## $ alcuse &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, … ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 14&quot;, &quot;id = 23&quot;, &quot;id… Let’s plot! f %&gt;% ggplot(aes(x = age)) + # `id`-specific lines geom_line(aes(y = Estimate), size = 1) + # gamma-centric lines geom_line(data = f_gamma_only, aes(y = Estimate), size = 1/2) + # OLS lines stat_smooth(data = nd, aes(y = alcuse), method = &quot;lm&quot;, se = F, color = &quot;black&quot;, linetype = 2, size = 1/2) + # data points geom_point(data = nd, aes(y = alcuse)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ id_label, ncol = 4) Though our purpose was largely to reproduce Figure 4.7, we might push ourselves a little further. Our Bayesian estimates came with measures of uncertainty, the posterior standard deviations and the 95% intervals. Whenever possible, it’s good form to include some expression of our uncertainty in our plots. Here let’s focus on the id-specific trajectories. f %&gt;% ggplot(aes(x = age, y = Estimate)) + # `id`-specific 95% intervals geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + # `id`-specific lines geom_line(size = 1) + # data points geom_point(data = nd, aes(y = alcuse)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ id_label, ncol = 4) This also clarifies an important visualization point. If you only care about plotting straight lines, you only need two points. However, if you want to express shapes with curves, such as the typically-bowtie-shaped 95% intervals, you need estimates over a larger number of predictor values. Back to fitted()! # we need an expanded version of the `nd` nd_expanded &lt;- alcohol1_pp %&gt;% select(id, coa, cpeer) %&gt;% filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% # this part is important! expand(nesting(id, coa, cpeer), age_14 = seq(from = 0, to = 2, length.out = 30)) %&gt;% mutate(id_label = ifelse(id &lt; 10, str_c(&quot;0&quot;, id), id)) %&gt;% mutate(id_label = str_c(&quot;id = &quot;, id_label), age = age_14 + 14) # pump our `nd_expanded` into `fitted()` f &lt;- fitted(fit4.7, newdata = nd_expanded) %&gt;% data.frame() %&gt;% bind_cols(nd_expanded) glimpse(f) ## Rows: 240 ## Columns: 10 ## $ Estimate &lt;dbl&gt; 1.134309, 1.155948, 1.177587, 1.199227, 1.220866, 1.242505, 1.264144, 1.285783, … ## $ Est.Error &lt;dbl&gt; 0.3610582, 0.3507056, 0.3410701, 0.3322143, 0.3242021, 0.3170973, 0.3109623, 0.3… ## $ Q2.5 &lt;dbl&gt; 0.3653645, 0.4099417, 0.4516141, 0.4964590, 0.5495808, 0.5857319, 0.6297501, 0.6… ## $ Q97.5 &lt;dbl&gt; 1.788938, 1.786759, 1.791425, 1.797305, 1.809752, 1.817148, 1.830143, 1.847108, … ## $ id &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ coa &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ cpeer &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7… ## $ age_14 &lt;dbl&gt; 0.00000000, 0.06896552, 0.13793103, 0.20689655, 0.27586207, 0.34482759, 0.413793… ## $ id_label &lt;chr&gt; &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id = 04&quot;, &quot;id… ## $ age &lt;dbl&gt; 14.00000, 14.06897, 14.13793, 14.20690, 14.27586, 14.34483, 14.41379, 14.48276, … Notice how we now have many more rows. Let’s plot. f %&gt;% ggplot(aes(x = age, y = Estimate)) + # `id`-specific 95% intervals geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + # `id`-specific lines geom_line(size = 1) + # data points geom_point(data = nd, aes(y = alcuse)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:4, limits = c(-1, 4)) + xlim(13, 17) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ id_label, ncol = 4) Singer and Willett pointed out that one of the ways in which the multilevel model is more parsimonious than a series of id-specific single-level models is that all id levels share the same \\(\\sigma_\\epsilon\\) parameter. For now, we should just point out that it’s possible to relax this assumption with modern Bayesian software, such as brms. For ideas on how, check out Donald Williams’ work (e.g., Williams et al., 2019). Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_1.0.4 loo_2.6.0 ggdist_3.3.0 brms_2.19.0 Rcpp_1.0.10 lubridate_1.9.2 ## [7] forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [13] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] gridExtra_2.3 inline_0.3.19 sandwich_3.0-2 rlang_1.1.1 ## [5] magrittr_2.0.3 multcomp_1.4-23 matrixStats_0.63.0 compiler_4.3.0 ## [9] mgcv_1.8-42 callr_3.7.3 vctrs_0.6.2 reshape2_1.4.4 ## [13] pkgconfig_2.0.3 crayon_1.5.2 fastmap_1.1.1 backports_1.4.1 ## [17] ellipsis_0.3.2 labeling_0.4.2 utf8_1.2.3 threejs_0.3.3 ## [21] promises_1.2.0.1 rmarkdown_2.21 markdown_1.7 tzdb_0.4.0 ## [25] nloptr_2.0.3 ps_1.7.5 bit_4.0.5 xfun_0.39 ## [29] cachem_1.0.8 jsonlite_1.8.4 highr_0.10 later_1.3.1 ## [33] prettyunits_1.1.1 parallel_4.3.0 R6_2.5.1 dygraphs_1.1.1.6 ## [37] RColorBrewer_1.1-3 StanHeaders_2.26.25 bslib_0.4.2 stringi_1.7.12 ## [41] boot_1.3-28.1 estimability_1.4.1 jquerylib_0.1.4 bookdown_0.34 ## [45] rstan_2.21.8 knitr_1.42 zoo_1.8-12 base64enc_0.1-3 ## [49] bayesplot_1.10.0 httpuv_1.6.11 Matrix_1.5-4 splines_4.3.0 ## [53] igraph_1.4.2 timechange_0.2.0 tidyselect_1.2.0 rstudioapi_0.14 ## [57] abind_1.4-5 codetools_0.2-19 miniUI_0.1.1.1 processx_3.8.1 ## [61] pkgbuild_1.4.0 lattice_0.21-8 plyr_1.8.8 shiny_1.7.4 ## [65] withr_2.5.0 bridgesampling_1.1-2 posterior_1.4.1 coda_0.19-4 ## [69] evaluate_0.21 survival_3.5-5 RcppParallel_5.1.7 xts_0.13.1 ## [73] pillar_1.9.0 tensorA_0.36.2 checkmate_2.2.0 DT_0.27 ## [77] stats4_4.3.0 shinyjs_2.1.0 distributional_0.3.2 generics_0.1.3 ## [81] vroom_1.6.3 hms_1.1.3 rstantools_2.3.1 munsell_0.5.0 ## [85] scales_1.2.1 minqa_1.2.5 gtools_3.9.4 xtable_1.8-4 ## [89] gamm4_0.2-6 glue_1.6.2 emmeans_1.8.6 projpred_2.5.0 ## [93] tools_4.3.0 shinystan_2.6.0 lme4_1.1-33 colourpicker_1.2.0 ## [97] mvtnorm_1.1-3 grid_4.3.0 crosstalk_1.2.0 colorspace_2.1-0 ## [101] nlme_3.1-162 cli_3.6.1 fansi_1.0.4 Brobdingnag_1.2-9 ## [105] gtable_0.3.3 sass_0.4.6 digest_0.6.31 TH.data_1.1-2 ## [109] htmlwidgets_1.6.2 farver_2.1.1 htmltools_0.5.5 lifecycle_1.0.3 ## [113] mime_0.12 MASS_7.3-58.4 bit64_4.0.5 shinythemes_1.2.0 References Bürkner, P.-C. (2021d). brms reference manual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/ Jaeger, B. C., Edwards, L. J., Das, K., &amp; Sen, P. K. (2017). An R2 statistic for fixed effects in the generalized linear mixed model. Journal of Applied Statistics, 44(6), 1086–1105. https://doi.org/10.1080/02664763.2016.1193725 Kay, M. (2021). ggdist: Visualizations of distributions and uncertainty [Manual]. https://CRAN.R-project.org/package=ggdist Kreft, I. G., &amp; de Leeuw, J. (1998). Introducing multilevel modeling. SAGE Publications, Inc. https://doi.org/https://dx.doi.org/10.4135/9781849209366 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Lambert, B. (2018). A student’s guide to Bayesian statistics. SAGE Publications, Inc. https://ben-lambert.com/a-students-guide-to-bayesian-statistics/ McElreath, R. (2020a). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114 Pearl, J., Glymour, M., &amp; Jewell, N. P. (2016). Causal Inference in Statistics - A Primer (1st Edition). Wiley. https://www.wiley.com/en-us/Causal+Inference+in+Statistics%3A+A+Primer-p-9781119186847 Rights, Jason D., &amp; Cole, D. A. (2018). Effect size measures for multilevel models in clinical child and adolescent research: New R-squared methods and recommendations. Journal of Clinical Child &amp; Adolescent Psychology, 47(6), 863–873. https://doi.org/10.1080/15374416.2018.1528550 Rights, Jason D., &amp; Sterba, S. K. (2020). New recommendations on the use of R-squared differences in multilevel model comparisons. Multivariate Behavioral Research, 55(4), 568–599. https://doi.org/10.1080/00273171.2019.1660605 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Snijders, T. A. B., &amp; Bosker, R. J. (1994). Modeled variance in two-level models. Sociological Methods &amp; Research, 22(3), 342–363. https://doi.org/10.1177/0049124194022003004 Spiegelhalter, D. J., Best, N. G., Carlin, B. P., &amp; Linde, A. V. D. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4), 583–639. https://doi.org/10.1111/1467-9868.00353 Steegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp; Gelman, A. (2022). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/ Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4 Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html Williams, D. R., Rouder, J., &amp; Rast, P. (2019). Beneath the surface: Unearthing within-Person variability and mean relations with Bayesian mixed models. https://doi.org/10.31234/osf.io/gwatq Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091 "],["treating-time-more-flexibly.html", "5 Treating Time More Flexibly 5.1 Variably spaced measurement occasions 5.2 Varying numbers of measurement occasions 5.3 Time-varying predictors 5.4 Recentering the effect of TIME Session info", " 5 Treating Time More Flexibly All the illustrative longitudinal data sets in previous chapters share two structural features that simplify analysis. Each is: (1) balanced–everyone is assessed on the identical number of occasions; and (2) time-structured–each set of occasions is identical across individuals. Our analyses have also been limited in that we have used only: (1) time-invariant predictors that describe immutable characteristics of individuals or their environment (except for TIME itself); and (2) a representation of TIME that forces the level-1 individual growth parameters to represent “initial status” and “rate of change.” The multilevel model for change is far more flexible than these examples suggest. With little or no adjustment, you can use the same strategies to analyze more complex data sets. Not only can the waves of data be irregularly spaced, their number and spacing can vary across participants. Each individual can have his or her own data collection schedule and number of waves can vary without limit from person to person. So, too, predictors of change can be time-invariant or time-varying, and the level-1 submodel can be parameterized in a variety of interesting ways. (Singer &amp; Willett, 2003, p. 138, emphasis in the original) 5.1 Variably spaced measurement occasions Many researchers design their studies with the goal of assessing each individual on an identical set of occasions… Yet sometimes, despite a valiant attempt to collect time-structured data, actual measurement occasions will differ. Variation often results from the realities of fieldwork and data collection… So, too, many researchers design their studies knowing full well that the measurement occasions may differ across participants. This is certainly true, for example, of those who use an accelerated cohort design in which an age-heterogeneous cohort of individuals is followed for a constant period of time. Because respondents initial vary in age, and age, not wave, is usually the appropriate metric for analyses (see the discussion of time metrics in section 1.3.2), observed measurement occasions will differ across individuals. (p. 139, emphasis in the original) 5.1.1 The structure of variably spaced data sets. You can find the PIAT data from the CNLSY study in the reading_pp.csv file. library(tidyverse) reading_pp &lt;- read_csv(&quot;data/reading_pp.csv&quot;) head(reading_pp) ## # A tibble: 6 × 5 ## id wave agegrp age piat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 6.5 6 18 ## 2 1 2 8.5 8.33 35 ## 3 1 3 10.5 10.3 59 ## 4 2 1 6.5 6 18 ## 5 2 2 8.5 8.5 25 ## 6 2 3 10.5 10.6 28 On pages 141 and 142, Singer and Willett discussed the phenomena of occasion creep, which is when “the temporal separations of occasions widens as the actual ages exceed design projections”. Here’s what that might look like. reading_pp %&gt;% ggplot(aes(x = age, y = wave)) + geom_vline(xintercept = c(6.5, 8.5, 10.5), color = &quot;white&quot;) + geom_jitter(alpha = .5, height = .33, width = 0) + scale_x_continuous(breaks = c(6.5, 8.5, 10.5)) + scale_y_continuous(breaks = 1:3) + ggtitle(&quot;This is what occasion creep looks like.&quot;, subtitle = &quot;As the waves go by, the variation of the ages widens and their central tendency\\ncreeps away from the ideal point.&quot;) + theme(panel.grid = element_blank()) Here’s how we might make our version of Figure 5.1. set.seed(5) # wrangle reading_pp %&gt;% nest(data = c(wave, agegrp, age, piat)) %&gt;% sample_n(size = 9) %&gt;% unnest(data) %&gt;% # this will help format and order the facets mutate(id = ifelse(id &lt; 10, str_c(&quot;0&quot;, id), id) %&gt;% str_c(&quot;id = &quot;, .)) %&gt;% pivot_longer(contains(&quot;age&quot;)) %&gt;% # plot ggplot(aes(x = value, y = piat, color = name)) + geom_point(alpha = 2/3) + stat_smooth(method = &quot;lm&quot;, se = F, linewidth = 1/2) + scale_color_viridis_d(NULL, option = &quot;B&quot;, end = .5, direction = -1) + xlab(&quot;measure of age&quot;) + coord_cartesian(xlim = c(5, 12), ylim = c(0, 80)) + theme(panel.grid = element_blank()) + facet_wrap(~ id) Since it wasn’t clear which id values the authors used in the text, we just randomized. Change the seed to view different samples. 5.1.2 Postulating and fitting multilevel models with variably spaced waves of data. The composite formula for our first model is $$ \\[\\begin{align*} \\text{piat}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} (\\text{agegrp}_{ij} - 6.5) + \\zeta_{0i} + \\zeta_{1i} (\\text{agegrp}_{ij} - 6.5) + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma \\end{pmatrix}, \\text{where} \\\\ \\mathbf \\Sigma &amp; = \\mathbf D \\mathbf\\Omega \\mathbf D&#39;, \\text{where} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\end{align*}\\] $$ It’s the same for the twin model using age rather than agegrp. Notice how we’ve switched from Singer and Willett’s \\(\\sigma^2\\) parameterization to the \\(\\sigma\\) parameterization typical of brms. reading_pp &lt;- reading_pp %&gt;% mutate(agegrp_c = agegrp - 6.5, age_c = age - 6.5) head(reading_pp) ## # A tibble: 6 × 7 ## id wave agegrp age piat agegrp_c age_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 6.5 6 18 0 -0.5 ## 2 1 2 8.5 8.33 35 2 1.83 ## 3 1 3 10.5 10.3 59 4 3.83 ## 4 2 1 6.5 6 18 0 -0.5 ## 5 2 2 8.5 8.5 25 2 2 ## 6 2 3 10.5 10.6 28 4 4.08 In the last chapter, we began familiarizing ourselves with brms::brm() default priors. It’s time to level up. Another approach is to use domain knowledge to set weakly-informative priors. Let’s start with the PIAT. The Peabody Individual Achievement Test is a standardized individual test of scholastic achievement. It yields several subtest scores. The reading subtest is the one we’re focusing on, here. As is typical for such tests, the PIAT scores are normed to yield a population mean of 100 and a standard deviation of 15. With that information alone, even a PIAT novice should have an idea about how to specify the priors. Since our sole predictor variables are versions of age centered at 6.5, we know that the model intercept is interpreted as the expected value on the PIAT when the children are 6.5 years old. If you knew nothing else, you’d guess the mean score would be 100 with a standard deviation around 15. One way to use a weakly-informative prior on the intercept would be to multiply that \\(SD\\) by a number like 2. Next we need a prior for the time variables, age_c and agegrp_c. A one-unit increase in either of these is the expected increase in the PIAT with one year’s passage of age. Bringing in a little domain knowledge, IQ and achievement tests tend to be rather stable over time. However, we also expect children to get better as they age and we also don’t know exactly how these data have been adjusted for the children’s ages. It’s also important to know that it’s typical within the Bayesian world to place Normal priors on \\(\\beta\\) parameters. So one approach would be to center the Normal prior on 0 and put something like twice the PIAT’s standard deviation on the prior’s \\(\\sigma\\). If we were PIAT researchers, we could do much better. But with minimal knowledge of the test, this approach is certainly beats defaults. Next we have the variance parameters. Recall that brms::brm() defaults are Student’s \\(t\\)-distributions with \\(\\nu = 3\\) and \\(\\mu = 0\\). Let’s start there. Now we just need to put values on \\(\\sigma\\). Since the PIAT has a standard deviation of 15 in the population, why not just use 15? If you felt insecure about this, multiply if by a factor of 2 or so. Also recall that when Student’s \\(t\\)-distributions has a \\(\\nu = 3\\), the tails are quite fat. Within the context of Bayesian priors, those fat tails make it easy for the likelihood to dominate the prior even when it’s a good way into the tail. Finally, we have the correlation among the group-level variance parameters, \\(\\sigma_0\\) and \\(\\sigma_1\\). Recall that last chapter we learned the brms::brm() default was lkj(1). To get a sense of what the LKJ does, we’ll simulate from it. McElreath’s rethinking package contains a handy rlkjcorr() function, which will allow us to simulate n draws from a K by K correlation matrix for which \\(\\eta\\) is defined by eta. Let’s take n &lt;- 1e6 draws from two LKJ prior distributions, one with \\(\\eta = 1\\) and the other with \\(\\eta = 4\\). library(rethinking) n &lt;- 1e6 set.seed(5) lkj &lt;- tibble(eta = c(1, 4)) %&gt;% mutate(draws = purrr::map(eta, ~ rlkjcorr(n, K = 2, eta = .)[, 2, 1])) %&gt;% unnest(draws) glimpse(lkj) ## Rows: 2,000,000 ## Columns: 2 ## $ eta &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ draws &lt;dbl&gt; 0.59957109, -0.83375155, 0.79069974, -0.05591997, -0.91300025, 0.45343010, 0.3631919… Now plot that lkj. lkj %&gt;% mutate(eta = factor(eta)) %&gt;% ggplot(aes(x = draws, fill = eta, color = eta)) + geom_density(size = 0, alpha = 2/3) + geom_text(data = tibble( draws = c(.75, .35), y = c(.6, 1.05), label = c(&quot;eta = 1&quot;, &quot;eta = 4&quot;), eta = c(1, 4) %&gt;% as.factor()), aes(y = y, label = label)) + scale_fill_viridis_d(option = &quot;A&quot;, end = .5) + scale_color_viridis_d(option = &quot;A&quot;, end = .5) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(rho)) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. When we use lkj(1), the prior is flat over the parameter space. However, setting lkj(4) is tantamount to a prior with a probability mass concentrated a bit towards zero. It’s a prior that’s skeptical of extremely large or small correlations. Within the context of our multilevel model \\(\\rho\\) parameters, this will be our weakly-regularizing prior. Let’s prepare to fit our models and load brms. detach(package:rethinking, unload = T) library(brms) Fit the models. Following the same form, the differ in that the first uses agegrp_c and the second uses age_c. fit5.1 &lt;- brm(data = reading_pp, family = gaussian, piat ~ 0 + Intercept + agegrp_c + (1 + agegrp_c | id), prior = c(prior(normal(100, 30), class = b, coef = Intercept), prior(normal(0, 30), class = b, coef = agegrp_c), prior(student_t(3, 0, 15), class = sd), prior(student_t(3, 0, 15), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.01&quot;) fit5.2 &lt;- brm(data = reading_pp, family = gaussian, piat ~ 0 + Intercept + age_c + (1 + age_c | id), prior = c(prior(normal(100, 30), class = b, coef = Intercept), prior(normal(0, 30), class = b, coef = age_c), prior(student_t(3, 0, 15), class = sd), prior(student_t(3, 0, 15), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.02&quot;) Focusing first on fit5.1, our analogue to the \\((AGEGRP – 6.5)\\) model displayed in Table 5.2, here is our model summary. print(fit5.1, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: piat ~ 0 + Intercept + agegrp_c + (1 + agegrp_c | id) ## Data: reading_pp (Number of observations: 267) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 89) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 3.388 0.824 1.724 4.981 1.009 1067 1719 ## sd(agegrp_c) 2.197 0.284 1.659 2.787 1.003 1139 2070 ## cor(Intercept,agegrp_c) 0.172 0.220 -0.226 0.627 1.007 616 1068 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 21.198 0.634 19.941 22.452 1.000 4896 3166 ## agegrp_c 5.028 0.309 4.422 5.637 1.000 4121 3240 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.271 0.360 4.604 6.012 1.010 991 1720 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the age_c model. print(fit5.2, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: piat ~ 0 + Intercept + age_c + (1 + age_c | id) ## Data: reading_pp (Number of observations: 267) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 89) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 2.748 0.848 1.037 4.364 1.001 1081 1128 ## sd(age_c) 1.976 0.247 1.520 2.477 1.002 1152 2193 ## cor(Intercept,age_c) 0.244 0.229 -0.177 0.695 1.006 479 1107 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 21.103 0.605 19.937 22.305 1.001 5162 3451 ## age_c 4.537 0.283 3.973 5.078 1.001 2742 2634 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.187 0.345 4.512 5.870 1.000 1220 1896 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). For a more focused look, we can use fixef() compare our \\(\\gamma\\)’s to each other and those in the text. fixef(fit5.1) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 21.198 0.634 19.941 22.452 ## agegrp_c 5.028 0.309 4.422 5.637 fixef(fit5.2) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 21.103 0.605 19.937 22.305 ## age_c 4.537 0.283 3.973 5.078 Here are our \\(\\sigma_\\epsilon\\) summaries. VarCorr(fit5.1)$residual$sd %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## 5.271 0.36 4.604 6.012 VarCorr(fit5.2)$residual$sd %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## 5.187 0.345 4.512 5.87 From a quick glance, you can see they are about the square of the \\(\\sigma_\\epsilon^2\\) estimates in the text. Let’s go ahead and compute the LOO and WAIC. fit5.1 &lt;- add_criterion(fit5.1, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit5.2 &lt;- add_criterion(fit5.2, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) Compare the models with a WAIC difference. loo_compare(fit5.1, fit5.2, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.2 0.0 0.0 -865.6 15.3 77.4 7.2 1731.1 30.6 ## fit5.1 -5.1 3.4 -870.6 13.6 78.5 6.5 1741.3 27.2 The WAIC difference between the two isn’t that large relative to its standard error. The LOO tells a similar story. loo_compare(fit5.1, fit5.2, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit5.2 0.0 0.0 -879.9 16.0 91.7 8.3 1759.8 32.1 ## fit5.1 -4.5 3.6 -884.4 14.3 92.3 7.5 1768.9 28.5 The uncertainty in our WAIC and LOO estimates and their differences provides information that was not available for the AIC and the BIC comparisons in the text. We can also compare the WAIC and the LOO with model weights. Given the WAIC, from McElreath (2015) we learn A total weight of 1 is partitioned among the considered models, making it easier to compare their relative predictive accuracy. The weight for a model \\(i\\) in a set of \\(m\\) models is given by: \\[w_i = \\frac{\\exp(-\\frac{1}{2} \\text{dWAIC}_i)}{\\sum_{j = 1}^m \\exp(-\\frac{1}{2} \\text{dWAIC}_i)}\\] where dWAIC is the dWAIC in the compare table output. This example uses WAIC but the formula is the same for any other information criterion, since they are all on the deviance scale. (p. 199) The compare() function McElreath referenced is from his (2020b) rethinking package, which is meant to accompany his texts. We don’t have that function with brms. A rough analogue to the rethinking::compare() function is loo_compare(). We don’t quite have a dWAIC column from loo_compare(). Remember how last chapter we discussed how Aki Vehtari isn’t a fan of converting information criteria to the \\(\\chi^2\\) difference metric with that last \\(-2 \\times ...\\) step? That’s why we have an elpd_diff instead of a dWAIC. But to get the corresponding value, you just multiply those values by -2. And yet if you look closely at the formula for \\(w_i\\), you’ll see that each time the dWAIC term appears, it’s multiplied by \\(-\\frac{1}{2}\\). So we don’t really need that dWAIC value anyway. As it turns out, we’re good to go with our elpd_diff. Thus the above equation simplifies to \\[ w_i = \\frac{\\exp(\\text{elpd_diff}_i)}{\\sum_{j = 1}^m \\exp(\\text{elpd_diff}_i)} \\] But recall you don’t have to do any of this by hand. We have the brms::model_weights() function, which we can use to compute weights with the WAIC or the LOO. model_weights(fit5.1, fit5.2, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.1 fit5.2 ## 0.006 0.994 model_weights(fit5.1, fit5.2, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit5.1 fit5.2 ## 0.011 0.989 Both put the lion’s share of the weight on the age_c model. Back to McElreath McElreath (2015): But what do these weights mean? There actually isn’t a consensus about that. But here’s Akaike’s interpretation, which is common. A model’s weight is an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models considered. Here’s the heuristic explanation. First, regard WAIC as the expected deviance of a model on future data. That is to say that WAIC gives us an estimate of \\(\\text{E} (D_\\text{test})\\). Akaike weights convert these deviance values, which are log-likelihoods, to plain likelihoods and then standardize them all. This is just like Bayes’ theorem uses a sum in the denominator to standardize the produce of the likelihood and prior. Therefore the Akaike weights are analogous to posterior probabilities of models, conditional on expected future data. (p. 199, emphasis in the original) 5.2 Varying numbers of measurement occasions As Singer and Willett pointed out, once you allow the spacing of waves to vary across individuals, it is a small leap to allow their number to vary as well. Statisticians say that such data sets are unbalanced. As you would expect, balance facilitates analysis: models can be parameterized more easily, random effects can be estimated more precisely, and computer algorithms will converge more rapidly. Yet a major advantage of the multilevel model for change is that it is easily fit to unbalanced data. (p. 146, emphasis in the original) 5.2.1 Analyzing data sets in which the number of waves per person varies. Here we load the wages_pp.csv data. wages_pp &lt;- read_csv(&quot;data/wages_pp.csv&quot;) glimpse(wages_pp) ## Rows: 6,402 ## Columns: 15 ## $ id &lt;dbl&gt; 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 53, … ## $ lnw &lt;dbl&gt; 1.491, 1.433, 1.469, 1.749, 1.931, 1.709, 2.086, 2.129, 1.982, 1.798, 2.256,… ## $ exper &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040,… ## $ ged &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1… ## $ postexp &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040,… ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hispanic &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hgc &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7… ## $ hgc.9 &lt;dbl&gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -2, -2, -2… ## $ uerate &lt;dbl&gt; 3.215, 3.215, 3.215, 3.295, 2.895, 2.495, 2.595, 4.795, 4.895, 7.400, 7.400,… ## $ ue.7 &lt;dbl&gt; -3.785, -3.785, -3.785, -3.705, -4.105, -4.505, -4.405, -2.205, -2.105, 0.40… ## $ ue.centert1 &lt;dbl&gt; 0.000, 0.000, 0.000, 0.080, -0.320, -0.720, -0.620, 1.580, 0.000, 2.505, 2.5… ## $ ue.mean &lt;dbl&gt; 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 5.0965, 5.09… ## $ ue.person.cen &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0800, -0.3200, -0.7200, -0.6200, 1.5800, -0.2015, … ## $ ue1 &lt;dbl&gt; 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 4.895, 4.895, 4.895,… Here’s a more focused look along the lines of Table 5.3. wages_pp %&gt;% select(id, exper, lnw, black, hgc, uerate) %&gt;% filter(id %in% c(206, 332, 1028)) ## # A tibble: 20 × 6 ## id exper lnw black hgc uerate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 206 1.87 2.03 0 10 9.2 ## 2 206 2.81 2.30 0 10 11 ## 3 206 4.31 2.48 0 10 6.30 ## 4 332 0.125 1.63 0 8 7.1 ## 5 332 1.62 1.48 0 8 9.6 ## 6 332 2.41 1.80 0 8 7.2 ## 7 332 3.39 1.44 0 8 6.20 ## 8 332 4.47 1.75 0 8 5.60 ## 9 332 5.18 1.53 0 8 4.60 ## 10 332 6.08 2.04 0 8 4.30 ## 11 332 7.04 2.18 0 8 3.40 ## 12 332 8.20 2.19 0 8 4.39 ## 13 332 9.09 4.04 0 8 6.70 ## 14 1028 0.004 0.872 1 8 9.3 ## 15 1028 0.035 0.903 1 8 7.4 ## 16 1028 0.515 1.39 1 8 7.3 ## 17 1028 1.48 2.32 1 8 7.4 ## 18 1028 2.14 1.48 1 8 6.30 ## 19 1028 3.16 1.70 1 8 5.90 ## 20 1028 4.10 2.34 1 8 6.9 To get a sense of the diversity in the number of occasions per id, use group_by() and count(). wages_pp %&gt;% count(id) %&gt;% ggplot(aes(y = n)) + geom_bar() + scale_y_continuous(&quot;# measurement occasions&quot;, breaks = 1:13) + xlab(&quot;count of cases&quot;) + theme(panel.grid = element_blank()) The spacing of the measurement occasions also differs a lot across cases. Recall that exper “identifies the specific moment–to the nearest day–in each man’s labor force history associated with each observed value of” lnw (p. 147). Here’s a sense of what that looks like. wages_pp %&gt;% filter(id %in% c(206, 332, 1028)) %&gt;% mutate(id = factor(id)) %&gt;% ggplot(aes(x = exper, y = lnw, color = id)) + geom_point() + geom_line() + scale_color_viridis_d(option = &quot;B&quot;, begin = .35, end = .8) + theme(panel.grid = element_blank()) Uneven for dayz. Here’s the brms version of the composite formula for Model A, the unconditional growth model for lnw. $$ \\[\\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{exper}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf\\Sigma \\end{pmatrix}, \\text{where} \\\\ \\mathbf\\Sigma &amp; = \\mathbf D \\mathbf \\Omega \\mathbf D&#39;, \\text{where} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix}, \\text{and} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\end{align*}\\] $$ To attempt setting priors for this, we need to review what lnw is. From the text: “To adjust for inflation, each hourly wage is expressed in constant 1990 dollars. To address the skewness commonly found in wage data and to linearize the individual wage trajectories, we analyze the natural logarithm of wages, LNW” (p. 147). So it’s the log of participant wages in 1990 dollars. From the official US Social Secutiry website, we learn the average yearly wage in 1990 was $20,172.11. Here’s that natural log for that. log(20172.11) ## [1] 9.912056 However, that’s the yearly wage. In the text, this is conceptualized as rate per hour. If we presume a 40 hour week for 52 weeks, this translates to a little less than $10 per hour. 20172.11 / (40 * 52) ## [1] 9.69813 Here’s what that looks like in a log metric. log(20172.11 / (40 * 52)) ## [1] 2.271933 But keep in mind that “to track wages on a common temporal scale, Murnane and colleagues decided to clock time from each respondent’s first day of work” (p. 147). So the wages at one’s initial point in the study were often entry-level wages. From the official website for the US Department of Labor, we learn the national US minimum wage in 1990 was $3.80 per hour. Here’s what that looks like on the log scale. log(3.80) ## [1] 1.335001 So perhaps this is a better figure to center our prior for the model intercept on. If we stay with a conventional Gaussian prior and put \\(\\mu = 1.335\\), what value should we use for the standard deviation? Well, if that’s the log minimum and 2.27 is the log mean, then there’s less than a log value of 1 between the minimum and the mean. If we’d like to continue our practice of weakly regularizing priors a value of 1 or even 0.5 on the log scale would seem reasonable. For simplicity, we’ll use normal(1.335, 1). Next we need a prior for the expected increase over a single year’s employment. A conservative default might be to center it on zero—no change from year to year. Since as we’ve established a 1 on the log scale is more than the difference between the minimum and average hourly wages in 1990 dollars, we might just use normal(0, 0.5) as a starting point. So then what about our variance parameters? Given these are all entry-level workers and given how little we’d expect them to increase from year to year, a student_t(3, 0, 1) on the log scale would seem pretty permissive. So then here’s how we might formally specify our model priors: \\[ \\begin{align*} \\gamma_{00} &amp; \\sim \\operatorname{Normal}(1.335, 1) \\\\ \\gamma_{10} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\sigma_0 &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\sigma_1 &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4) \\end{align*} \\] For a point of comparison, here are the brms::brm() default priors. get_prior(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + (1 + exper | id)) ## prior class coef group resp dpar nlpar lb ub source ## (flat) b default ## (flat) b exper (vectorized) ## (flat) b Intercept (vectorized) ## lkj(1) cor default ## lkj(1) cor id (vectorized) ## student_t(3, 0, 2.5) sd 0 default ## student_t(3, 0, 2.5) sd id 0 (vectorized) ## student_t(3, 0, 2.5) sd exper id 0 (vectorized) ## student_t(3, 0, 2.5) sd Intercept id 0 (vectorized) ## student_t(3, 0, 2.5) sigma 0 default Even though our priors are still quite permissive on the scale of the data, they’re much more informative than the defaults. If we had formal backgrounds in the entry-level economy of the US in the early 1900s, we’d be able to specify even better priors. But hopefully this walk-through gives a sense of how to start thinking about model priors. Let’s fit the model. To keep the size of the fits/fit05.03.rds file below the 100MB GitHub limit, we’ll set chains = 3 and compensate by upping iter a little. fit5.3 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b, coef = exper), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.03&quot;) Here are the results. print(fit5.3, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.232 0.011 0.212 0.253 1.002 2004 2969 ## sd(exper) 0.041 0.003 0.037 0.047 1.001 674 1505 ## cor(Intercept,exper) -0.287 0.067 -0.410 -0.147 1.001 727 1645 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.716 0.011 1.694 1.737 1.000 2959 3329 ## exper 0.046 0.002 0.041 0.050 1.001 2821 3499 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.309 0.003 0.303 0.315 1.000 4212 3206 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since the criterion lnw is on the log scale, Singer and Willett pointed out our estimate for \\(\\gamma_{10}\\) indicates a nonlinear growth rate on the natural dollar scale. They further explicated that “if an outcome in a linear relationship, \\(Y\\), is expressed as a natural logarithm and \\(\\hat \\gamma_{01}\\) is the regression coefficient for a predictor \\(X\\), then \\(100(e^{\\hat{\\gamma}_{01}} - 1)\\) is the percentage change in \\(Y\\) per unit difference in \\(X\\)” (p. 148, emphasis in the original). Here’s how to do that conversion with our brms output. draws &lt;- as_draws_df(fit5.3) %&gt;% transmute(percent_change = 100 * (exp(b_exper) - 1)) head(draws) ## # A tibble: 6 × 1 ## percent_change ## &lt;dbl&gt; ## 1 4.88 ## 2 4.69 ## 3 5.07 ## 4 4.51 ## 5 4.53 ## 6 4.51 For our plot, let’s break out Matthew Kay’s handy tidybayes package (Kay, 2023). With the tidybayes::stat_halfeye() function, it’s easy to put horizontal point intervals beneath out parameter densities. Here we’ll use 95% intervals. library(tidybayes) draws %&gt;% ggplot(aes(x = percent_change, y = 0)) + stat_halfeye(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Percent change&quot;, x = expression(100*(italic(e)^(hat(gamma)[1][0])-1))) + theme(panel.grid = element_blank()) The tidybayes package also has a group of functions that make it easy to summarize posterior parameters with measures of central tendency (i.e., mean, median, mode) and intervals (i.e., percentile based, highest posterior density intervals). Here we’ll use median_qi() to get the posterior median and percentile-based 95% intervals. draws %&gt;% median_qi(percent_change) ## # A tibble: 1 × 6 ## percent_change .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 4.67 4.19 5.17 0.95 median qi For our next model, Model B in Table 5.4, we add two time-invariant covariates. In the data, these are listed as black and hgc.9. Before we proceed, let’s rename hgc.9 to be more consistent with tidyverse style. wages_pp &lt;- wages_pp %&gt;% rename(hgc_9 = hgc.9) There we go. Let’s take a look at the distributions of our covariates. wages_pp %&gt;% pivot_longer(c(black, hgc_9)) %&gt;% ggplot(aes(x = value)) + geom_bar() + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) We see black is a dummy variable coded “Black” = 1, “Non-black” = 0. hgc_9 is a somewhat Gaussian ordinal centered around zero. For context, it might also help to check its standard deviation. sd(wages_pp$hgc_9) ## [1] 1.347135 With a mean near 0 and an \\(SD\\) near 1, hgc_9 is almost in a standardized metric. If we wanted to keep with our weakly-regularizing approach, normal(0, 1) or even normal(0, 0.5) would be pretty permissive for both these variables. Recall that we’re predicting wage on the log scale. A \\(\\gamma\\) value of 1 or even 0.5 would be humongous for the social sciences. Since we already have the \\(\\gamma\\) for exper set to normal(0, 0.5), let’s just keep with that. Here’s how we might describe our model in statistical terms: $$ \\[\\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_{i} - 9) + \\gamma_{02} \\text{black}_{i} \\\\ &amp; \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{11} \\text{exper}_{ij} \\times (\\text{hgc}_{i} - 9) + \\gamma_{12} \\text{exper}_{ij} \\times \\text{black}_{i} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(1.335, 1) \\\\ \\gamma_{01}, \\dots, \\gamma_{12} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t} (3, 0, 1) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] $$ The top portion up through the \\(\\mathbf\\Omega\\) line is the likelihood. Starting with \\(\\gamma_{00} \\sim \\text{Normal}(1.335, 1)\\) on down, we’ve listed our priors. Here’s how to fit the model with brms. fit5.4 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.04&quot;) Let’s take a look at the results. print(fit5.4, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.227 0.011 0.207 0.249 1.002 1467 2232 ## sd(exper) 0.040 0.003 0.035 0.046 1.006 501 1219 ## cor(Intercept,exper) -0.293 0.071 -0.418 -0.144 1.006 566 1296 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.717 0.013 1.693 1.742 1.000 2367 3009 ## hgc_9 0.035 0.008 0.020 0.050 1.001 2907 3462 ## black 0.016 0.024 -0.030 0.062 1.002 2135 2973 ## exper 0.049 0.003 0.044 0.054 1.000 2349 3573 ## hgc_9:exper 0.001 0.002 -0.002 0.005 1.000 2667 3164 ## black:exper -0.018 0.005 -0.029 -0.007 1.000 2152 2714 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.309 0.003 0.303 0.315 1.001 2849 3100 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The \\(\\gamma\\)’s are on par with those in the text. When we convert the \\(\\sigma\\) parameters to the \\(\\sigma^2\\) metric, here’s what they look like. draws &lt;- as_draws_df(fit5.4) draws %&gt;% transmute(`sigma[0]^2` = sd_id__Intercept^2, `sigma[1]^2` = sd_id__exper^2, `sigma[epsilon]^2` = sigma^2) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + stat_halfeye(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + coord_cartesian(ylim = c(1.4, 3.4)) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) We might plot our \\(\\gamma\\)’s, too. Here we’ll use tidybayes::stat_pointinterval() to just focus on the points and intervals. draws %&gt;% select(b_Intercept:`b_black:exper`) %&gt;% set_names(str_c(&quot;gamma&quot;, c(&quot;[0][0]&quot;, &quot;[0][1]&quot;, &quot;[0][2]&quot;, &quot;[1][0]&quot;, &quot;[1][1]&quot;, &quot;[1][2]&quot;))) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_pointinterval(.width = .95, size = 1/2) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) As in the text, our \\(\\gamma_{02}\\) and \\(\\gamma_{11}\\) parameters hovered around zero. For our next model, Model C in Table 5.4, we’ll drop those parameters. fit5.5 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.05&quot;) Let’s take a look at the results. print(fit5.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.227 0.011 0.206 0.249 1.000 1687 2519 ## sd(exper) 0.041 0.003 0.035 0.046 1.002 651 1287 ## cor(Intercept,exper) -0.297 0.069 -0.424 -0.156 1.001 699 1767 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.722 0.011 1.701 1.743 1.001 2946 3624 ## hgc_9 0.038 0.006 0.025 0.051 1.000 2909 3307 ## exper 0.049 0.003 0.044 0.054 1.001 2656 3501 ## exper:black -0.016 0.005 -0.025 -0.007 1.001 2448 3128 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.309 0.003 0.303 0.315 1.002 3348 3526 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Perhaps unsurprisingly, the parameter estimates for fit5.5 ended up quite similar to those from fit5.4. Happily, they’re also similar to those in the text. Let’s compute the WAIC estimates. fit5.3 &lt;- add_criterion(fit5.3, criterion = &quot;waic&quot;) fit5.4 &lt;- add_criterion(fit5.4, criterion = &quot;waic&quot;) fit5.5 &lt;- add_criterion(fit5.5, criterion = &quot;waic&quot;) Compare their WAIC estimates using \\(\\text{elpd}\\) difference scores. loo_compare(fit5.3, fit5.4, fit5.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.5 0.0 0.0 -2051.2 103.8 866.6 27.0 4102.3 207.5 ## fit5.4 -0.2 1.6 -2051.4 103.5 867.8 26.9 4102.8 207.1 ## fit5.3 -2.5 4.2 -2053.7 103.5 876.2 27.0 4107.4 207.1 The differences are subtle. Here are the WAIC weights. model_weights(fit5.3, fit5.4, fit5.5, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.3 fit5.4 fit5.5 ## 0.042 0.424 0.534 When we use weights, almost all goes to fit5.4 and fit5.5. Focusing on the trimmed model, fit5.5, let’s get ready to make our version of Figure 5.2. We’ll start with fitted() work. nd &lt;- crossing(black = 0:1, hgc_9 = c(0, 3)) %&gt;% expand_grid(exper = seq(from = 0, to = 11, length.out = 30)) f &lt;- fitted(fit5.5, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) head(f) ## Estimate Est.Error Q2.5 Q97.5 black hgc_9 exper ## 1 1.721590 0.010598941 1.701243 1.742709 0 0 0.0000000 ## 2 1.740120 0.010144588 1.720661 1.760299 0 0 0.3793103 ## 3 1.758651 0.009765267 1.739506 1.778078 0 0 0.7586207 ## 4 1.777181 0.009469999 1.758576 1.795902 0 0 1.1379310 ## 5 1.795712 0.009266821 1.777720 1.814151 0 0 1.5172414 ## 6 1.814242 0.009161863 1.796453 1.832464 0 0 1.8965517 Here it is, our two-panel version of Figure 5.2. f %&gt;% mutate(black = factor(black, labels = c(&quot;Latinos and Whites&quot;, &quot;Blacks&quot;)), hgc_9 = factor(hgc_9, labels = c(&quot;9th grade dropouts&quot;, &quot;12th grade dropouts&quot;))) %&gt;% ggplot(aes(x = exper, color = black, fill = black)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate)) + scale_fill_viridis_d(NULL, option = &quot;C&quot;, begin = .25, end = .75) + scale_color_viridis_d(NULL, option = &quot;C&quot;, begin = .25, end = .75) + ylab(&quot;lnw&quot;) + coord_cartesian(ylim = c(1.6, 2.4)) + theme(panel.grid = element_blank()) + facet_wrap(~ hgc_9) This leads in nicely to a brief discussion of posterior predictive checks (PPC). The basic idea is that good models should be able to retrodict the data used to produce them. Table 5.3 in the text introduced the data set by highlighting three participants and we went ahead and looked at their data in a plot. One way to do a PPC might be to plot their original data atop their model estimates. The fitted() function will help us with the preparatory work. nd &lt;- wages_pp %&gt;% filter(id %in% c(206, 332, 1028)) f &lt;- fitted(fit5.5, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) head(f) ## Estimate Est.Error Q2.5 Q97.5 id lnw exper ged postexp black hispanic hgc hgc_9 uerate ## 1 2.057977 0.1382457 1.788240 2.322068 206 2.028 1.874 0 0 0 0 10 1 9.200 ## 2 2.118612 0.1377214 1.850584 2.384066 206 2.297 2.814 0 0 0 0 10 1 11.000 ## 3 2.215370 0.1559436 1.910120 2.519224 206 2.482 4.314 0 0 0 0 10 1 6.295 ## 4 1.461034 0.1366113 1.184824 1.722247 332 1.630 0.125 0 0 0 1 8 -1 7.100 ## 5 1.644720 0.1109559 1.420882 1.858243 332 1.476 1.625 0 0 0 1 8 -1 9.600 ## 6 1.741217 0.1005240 1.538177 1.934535 332 1.804 2.413 0 0 0 1 8 -1 7.200 ## ue.7 ue.centert1 ue.mean ue.person.cen ue1 ## 1 2.200 0.000 8.831667 0.3683333 9.2 ## 2 4.000 1.800 8.831667 2.1683333 9.2 ## 3 -0.705 -2.905 8.831667 -2.5366667 9.2 ## 4 0.100 0.000 5.906500 1.1935000 7.1 ## 5 2.600 2.500 5.906500 3.6935000 7.1 ## 6 0.200 0.100 5.906500 1.2935000 7.1 Here’s the plot. f %&gt;% mutate(id = str_c(&quot;id = &quot;, id)) %&gt;% ggplot(aes(x = exper)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, color = id)) + geom_point(aes(y = lnw)) + scale_color_viridis_d(option = &quot;B&quot;, begin = .35, end = .8) + labs(subtitle = &quot;The black dots are the original data. The colored points and vertical lines are the participant-specific posterior\\nmeans and 95% intervals.&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ id) Although each participant got their own intercept and slope, the estimates all fall in straight lines. Since we’re only working with time-invariant covariates, that’s about the best we can do. Though our models can express gross trends over time, they’re unable to speak to variation from occasion to occasion. Just a little later on in this chapter and we’ll learn how to do better. 5.2.2 Practical problems that may arise when analyzing unbalanced data sets. With HMC, the issues with non-convergence aren’t quite the same as with maximum likelihood estimation. However, the basic issue still remains: Estimation of variance components requires that enough people have sufficient data to allow quantification of within-person residual variation–variation in the residuals over and above the fixed effects. If too many people have too little data, you will be unable to quantify [have difficulty quantifying] this residual variability. (p. 152) The big difference is that as Bayesians, our priors add additional information that will help us define the posterior distributions of our variance components. Thus our challenge will choosing sensible priors for our \\(\\sigma\\)’s. 5.2.2.1 Boundary constraints. Unlike with the frequentist multilevel software discussed in the text, brms will not yield negative values on the \\(\\sigma\\) parameters. This is because the brms default is to set a lower limit of zero on those parameters. For example, see what happens when we execute fit5.3$model. fit5.3$model ## // generated with brms 2.19.0 ## functions { ## /* compute correlated group-level effects ## * Args: ## * z: matrix of unscaled group-level effects ## * SD: vector of standard deviation parameters ## * L: cholesky factor correlation matrix ## * Returns: ## * matrix of scaled group-level effects ## */ ## matrix scale_r_cor(matrix z, vector SD, matrix L) { ## // r is stored in another dimension order than z ## return transpose(diag_pre_multiply(SD, L) * z); ## } ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## // data for group-level effects of ID 1 ## int&lt;lower=1&gt; N_1; // number of grouping levels ## int&lt;lower=1&gt; M_1; // number of coefficients per level ## int&lt;lower=1&gt; J_1[N]; // grouping indicator per observation ## // group-level predictor values ## vector[N] Z_1_1; ## vector[N] Z_1_2; ## int&lt;lower=1&gt; NC_1; // number of group-level correlations ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## vector[K] b; // population-level effects ## real&lt;lower=0&gt; sigma; // dispersion parameter ## vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations ## matrix[M_1, N_1] z_1; // standardized group-level effects ## cholesky_factor_corr[M_1] L_1; // cholesky factor of correlation matrix ## } ## transformed parameters { ## matrix[N_1, M_1] r_1; // actual group-level effects ## // using vectors speeds up indexing in loops ## vector[N_1] r_1_1; ## vector[N_1] r_1_2; ## real lprior = 0; // prior contributions to the log posterior ## // compute actual group-level effects ## r_1 = scale_r_cor(z_1, sd_1, L_1); ## r_1_1 = r_1[, 1]; ## r_1_2 = r_1[, 2]; ## lprior += normal_lpdf(b[1] | 1.335, 1); ## lprior += normal_lpdf(b[2] | 0, 0.5); ## lprior += student_t_lpdf(sigma | 3, 0, 1) ## - 1 * student_t_lccdf(0 | 3, 0, 1); ## lprior += student_t_lpdf(sd_1 | 3, 0, 1) ## - 2 * student_t_lccdf(0 | 3, 0, 1); ## lprior += lkj_corr_cholesky_lpdf(L_1 | 4); ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## // initialize linear predictor term ## vector[N] mu = rep_vector(0.0, N); ## for (n in 1:N) { ## // add more terms to the linear predictor ## mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n]; ## } ## target += normal_id_glm_lpdf(Y | X, mu, b, sigma); ## } ## // priors including constants ## target += lprior; ## target += std_normal_lpdf(to_vector(z_1)); ## } ## generated quantities { ## // compute group-level correlations ## corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1); ## vector&lt;lower=-1,upper=1&gt;[NC_1] cor_1; ## // extract upper diagonal of correlation matrix ## for (k in 1:M_1) { ## for (j in 1:(k - 1)) { ## cor_1[choose(k - 1, 2) + j] = Cor_1[j, k]; ## } ## } ## } That returned the Stan code corresponding to our brms::brm() code, above. Notice the second and third lines in the parameters block. Both contained &lt;lower=0&gt;, which indicated the lower bounds for those parameters was zero. See? Stan has you covered. Let’s load the wages_small_pp.csv data. wages_small_pp &lt;- read_csv(&quot;data/wages_small_pp.csv&quot;) %&gt;% rename(hgc_9 = hcg.9) glimpse(wages_small_pp) ## Rows: 257 ## Columns: 5 ## $ id &lt;dbl&gt; 206, 206, 206, 266, 304, 329, 329, 329, 336, 336, 336, 394, 394, 394, 518, 518, 541,… ## $ lnw &lt;dbl&gt; 2.028, 2.297, 2.482, 1.808, 1.842, 1.422, 1.308, 1.885, 1.892, 1.279, 2.224, 2.383, … ## $ exper &lt;dbl&gt; 1.874, 2.814, 4.314, 0.322, 0.580, 0.016, 0.716, 1.756, 1.910, 2.514, 3.706, 1.890, … ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, … ## $ hgc_9 &lt;dbl&gt; 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 2, -1, 0,… Here’s the distribution of the number of measurement occasions for our small data set. wages_small_pp %&gt;% count(id) %&gt;% ggplot(aes(y = n)) + geom_bar() + scale_y_continuous(&quot;# measurement occasions&quot;, breaks = 1:13, limits = c(.5, 13)) + xlab(&quot;count of cases&quot;) + theme(panel.grid = element_blank()) Our brm() code is the same as that for fit5.5, above, with just a slightly different data argument. If we wanted to, we could be hasty and just use update(), instead. But since we’re still practicing setting our priors and such, here we’ll be exhaustive. fit5.6 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.06&quot;) print(fit5.6) ## Warning: There were 28 divergent transitions after warmup. Increasing adapt_delta above 0.8 may ## help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.29 0.05 0.19 0.38 1.01 681 628 ## sd(exper) 0.04 0.03 0.00 0.11 1.02 338 213 ## cor(Intercept,exper) -0.05 0.31 -0.62 0.59 1.00 974 648 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.74 0.05 1.64 1.83 1.00 2166 2413 ## hgc_9 0.05 0.02 -0.00 0.10 1.00 2186 2537 ## exper 0.05 0.02 0.00 0.10 1.00 2283 1898 ## exper:black -0.05 0.04 -0.13 0.02 1.00 1686 2786 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.34 0.02 0.30 0.39 1.00 1118 2328 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s walk through this slow. You may have noticed that warning message about divergent transitions. We’ll get to that in a bit. First focus on the parameter estimates for sd(exper). Unlike in the text, our posterior mean is not 0.000. But do remember that our posterior is parameterized in the \\(\\sigma\\) metric. Let’s do a little converting and look at it in a plot. draws &lt;- as_draws_df(fit5.6) v &lt;- draws %&gt;% transmute(sigma_1 = sd_id__exper) %&gt;% mutate(sigma_2_1 = sigma_1^2) %&gt;% set_names(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;) %&gt;% pivot_longer(everything()) Plot. v %&gt;% ggplot(aes(x = value, y = name)) + stat_halfeye(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = parse(text = c(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;))) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) In the \\(\\sigma\\) metric, the posterior is bunched up a little on the boundary, but much of its mass is a gently right-skewed mound concentrated in the 0—0.1 range. When we convert the posterior to the \\(\\sigma^2\\) metric, the parameter appears much more bunched up against the boundary. Because we typically summarize our posteriors with means or medians, the point estimate still moves away from zero. v %&gt;% group_by(name) %&gt;% mean_qi() %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[1] 0.0412 0.0018 0.108 0.95 mean qi ## 2 sigma[1]^2 0.0025 0 0.0117 0.95 mean qi v %&gt;% group_by(name) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[1] 0.0368 0.0018 0.108 0.95 median qi ## 2 sigma[1]^2 0.0014 0 0.0117 0.95 median qi But it really does start to shoot to zero if we attempt to summarize the central tendency with the mode, as within the maximum likelihood paradigm. v %&gt;% group_by(name) %&gt;% mode_qi() %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[1] 0.0159 0.0018 0.108 0.95 mode qi ## 2 sigma[1]^2 0 0 0.0117 0.95 mode qi Backing up to that warning message, we were informed that “Increasing adapt_delta above 0.8 may help.” The adapt_delta parameter ranges from 0 to 1. The brm() default is .8. In my experience, increasing to .9 or .99 is often a good place to start. For this model, .9 wasn’t quite enough, but .99 worked. Here’s how to do it. fit5.7 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .99), file = &quot;fits/fit05.07&quot;) Now look at the summary. print(fit5.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.28 0.05 0.19 0.37 1.00 1076 1236 ## sd(exper) 0.04 0.03 0.00 0.10 1.02 321 791 ## cor(Intercept,exper) -0.05 0.32 -0.62 0.60 1.00 2443 2768 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.74 0.05 1.64 1.83 1.00 2364 2688 ## hgc_9 0.05 0.02 -0.00 0.10 1.00 1981 2913 ## exper 0.05 0.02 0.01 0.09 1.00 2410 2746 ## exper:black -0.05 0.04 -0.12 0.02 1.00 2841 3135 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.34 0.02 0.30 0.39 1.00 1398 2111 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our estimates were pretty much the same as before. Happily, this time we got our summary without any warning signs. It won’t always be that way, so make sure to take adapt_delta warnings seriously. Now do note that for both fit5.6 and fit5.7, our effective sample sizes for \\(\\sigma_0\\) and \\(\\sigma_1\\) aren’t terribly large relative to the total number of post-warmup draws, 4,000. If it was really important that you had high-quality summary statistics for these parameters, you might need to refit the model with something like iter = 20000, warmup = 2000. In Model B in Table 5.5, Singer and Willett gave the results of a model with the boundary constraints on the \\(\\sigma^2\\) parameters removed. I am not going to attempt something like that with brms. If you’re interested, you’re on your own. But we will fit a version of their Model C where we’ve removed the \\(\\sigma_1\\) parameter. Notice that this results in our removal of the LKJ prior for \\(\\rho_{01}\\), too. Without a \\(\\sigma_1\\), there’s no other parameter with which our lonely \\(\\sigma_0\\) might covary. fit5.8 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.08&quot;) Here is the basic model summary. print(fit5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 | id) ## Data: wages_small_pp (Number of observations: 257) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.30 0.04 0.22 0.37 1.00 975 2080 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.74 0.05 1.64 1.83 1.00 2494 3027 ## hgc_9 0.05 0.03 -0.01 0.10 1.00 2323 2477 ## exper 0.05 0.02 0.01 0.10 1.00 2504 2850 ## exper:black -0.06 0.04 -0.13 0.01 1.00 3029 2647 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.34 0.02 0.30 0.39 1.00 1616 2259 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). No warning messages and our effective samples for \\(\\sigma_0\\) improved a bit. Compute the WAIC for both models. fit5.7 &lt;- add_criterion(fit5.7, criterion = &quot;waic&quot;) fit5.8 &lt;- add_criterion(fit5.8, criterion = &quot;waic&quot;) Compare. loo_compare(fit5.7, fit5.8, criterion = &quot;waic&quot;) %&gt;% print(simplify = F, digits = 3) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.8 0.000 0.000 -132.245 20.411 70.235 11.916 264.490 40.821 ## fit5.7 -1.822 2.195 -134.067 21.880 71.175 13.388 268.135 43.760 Yep. Those WAIC estimates are quite similar and when you compare them with formal \\(\\text{elpd}\\) difference scores, the standard error is about the same size as the difference itself. Though we’re stepping away from the text a bit, we should explore more alternatives for this boundary issue. The Stan team has put together a Prior Choice Recommendations wiki at https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations. In the Boundary-avoiding priors for modal estimation (posterior mode, MAP, marginal posterior mode, marginal maximum likelihood, MML) section, we read: These are for parameters such as group-level scale parameters, group-level correlations, group-level covariance matrix What all these parameters have in common is that (a) they’re defined on a space with a boundary, and (b) the likelihood, or marginal likelihood, can have a mode on the boundary. Most famous example is the group-level scale parameter tau for the 8-schools hierarchical model. With full Bayes the boundary shouldn’t be a problem (as long as you have any proper prior). But with modal estimation, the estimate can be on the boundary, which can create problems in posterior predictions. For example, consider a varying-intercept varying-slope multilevel model which has an intercept and slope for each group. Suppose you fit marginal maximum likelihood and get a modal estimate of 1 for the group-level correlation. Then in your predictions the intercept and slope will be perfectly correlated, which in general will be unrealistic. For a one-dimensional parameter restricted to be positive (e.g., the scale parameter in a hierarchical model), we recommend Gamma(2,0) prior (that is, p(tau) proportional to tau) which will keep the mode away from 0 but still allows it to be arbitrarily close to the data if that is what the likelihood wants. For details see this paper by Chung et al.: http://www.stat.columbia.edu/~gelman/research/published/chung_etal_Pmetrika2013.pdf Gamma(2,0) biases the estimate upward. When number of groups is small, try Gamma(2,1/A), where A is a scale parameter representing how high tau can be. We should walk those Gamma priors out, a bit. The paper by Chung et al. (2013) is quite helpful. We’ll first let them give us a little more background in the topic: Zero group-level variance estimates can cause several problems. Zero variance can go against prior knowledge of researchers and results in underestimation of uncertainty in fixed coefficient estimates. Inferences for groups are often of interest to researchers, but when the group-level variance is estimated as zero, the resulting predictions of the group-level errors will all be zero, so one fails to find unexplained differences between groups. In addition, uncertainty in predictions for new and existing groups is also understated. (p. 686) They expounded further on page 687. When a variance parameter is estimated as zero, there is typically a large amount of uncertainty about this variance. One possibility is to declare in such situations that not enough information is available to estimate a multilevel model. However, the available alternatives can be unappealing since, as noted in the introduction, discarding a variance component or setting the variance to zero understates the uncertainty. In particular, standard errors for coefficients of covariates that vary between groups will be too low as we will see in Section 2.2. The other extreme is to fit a regression with indicators for groups (a fixed-effects model), but this will overcorrect for group effects (it is mathematically equivalent to a mixed-effects model with variance set to infinity), and also does not allow predictions for new groups. Degenerate variance estimates lead to complete shrinkage of predictions for new and existing groups and yield estimated prediction standard errors that understate uncertainty. This problem has been pointed out by Li and Lahiri (2010) and Morris and Tang (2011) in small area estimation…. If zero variance is not a null hypothesis of interest, a boundary estimate, and the corresponding zero likelihood ratio test statistic, should not necessarily lead us to accept the null hypothesis and to proceed as if the true variance is zero. In their paper, they covered both penalized maximum likelihood and full Bayesian estimation. We’re just going to focus on Bayes, but some of the quotes will contain ML talk. Further, we read: We recommend a class of log-gamma penalties (or gamma priors) that in our default setting (the log-gamma(2, \\(\\lambda\\)) penalty with \\(\\lambda \\rightarrow 0\\)) produce maximum penalized likelihood (MPL) estimates (or Bayes modal estimates) approximately one standard error away from zero when the maximum likelihood estimate is at zero. We consider these priors to be weakly informative in the sense that they supply some direction but still allow inference to be driven by the data. The penalty has little influence when the number of groups is large or when the data are informative about the variance, and the asymptotic mean squared error of the proposed estimator is the same as that of the maximum likelihood estimator. (p. 686) In the upper left panel of Figure 3, Chung and colleagues gave an example of what they mean by \\(\\lambda \\rightarrow 0\\): \\(\\lambda = 0.1\\). Here’s an example of what \\(\\operatorname{Gamma}(2, 0.1)\\) looks like across the parameter space of 0 to 100. library(ggdist) prior(gamma(2, 0.1)) %&gt;% parse_dist() %&gt;% ggplot(aes(xdist = .dist_obj, y = prior)) + stat_halfeye(.width = .95, p_limits = c(.0001, .9999)) + scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) + labs(title = &quot;Gamma(2, 0.1)&quot;, x = &quot;parameter space&quot;) + coord_cartesian(xlim = c(0, 100)) + theme(panel.grid = element_blank()) Given we’re working with data on the log scale, that’s a massively permissive prior. Let’s zoom in and see what it means for the parameter space of possible values for our data. prior(gamma(2, 0.1)) %&gt;% parse_dist() %&gt;% ggplot(aes(xdist = .dist_obj, y = prior)) + stat_halfeye(.width = .95, p_limits = c(.000001, .99)) + scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) + labs(title = &quot;Gamma(2, 0.1)&quot;, x = &quot;parameter space (zoomed in)&quot;) + coord_cartesian(xlim = c(0, 2)) + theme(panel.grid = element_blank()) Now keep that picture in mind as we read further along in the paper: In addition, with \\(\\lambda \\rightarrow 0\\), the gamma density function has a positive constant derivative at zero, which allows the likelihood to dominate if it is strongly curved near zero. The positive constant derivative implies that the prior is linear at zero so that there is no dead zone near zero. The top-left panel of Figure 3 shows that the gamma(2,0.1) density increases linearly from zero with a gentle slope. The shape will be even flatter with a smaller rate parameter. (p. 691) In case you’re not familiar with the gamma distribution, the rate parameter is what we’ve been calling \\(\\lambda\\). Let’s test this baby out with our model. Here’s how to specify it in brms. fit5.9 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(gamma(2, 0.1), class = sd, group = id, coef = exper), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .85), file = &quot;fits/fit05.09&quot;) Notice how we had to increase adapt_delta a bit. Here are the results. print(fit5.9, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.282 0.053 0.175 0.386 1.004 953 1685 ## sd(exper) 0.059 0.028 0.012 0.118 1.005 554 1239 ## cor(Intercept,exper) -0.091 0.312 -0.642 0.541 1.001 2070 2666 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.735 0.050 1.639 1.835 1.001 2811 3069 ## hgc_9 0.048 0.025 -0.001 0.097 1.002 2871 2588 ## exper 0.050 0.025 0.002 0.097 1.001 3205 3038 ## exper:black -0.051 0.038 -0.125 0.022 1.000 2624 2718 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.344 0.023 0.303 0.393 1.002 1171 2337 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our sd(exper) is still quite close to zero. But notice how not the lower level of the 95% interval is higher than zero. Here’s what it looks like in both \\(\\sigma\\) and \\(\\sigma^2\\) metrics. as_draws_df(fit5.9) %&gt;% transmute(sigma_1 = sd_id__exper) %&gt;% mutate(sigma_2_1 = sigma_1^2) %&gt;% set_names(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + stat_halfeye(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = parse(text = c(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;))) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) Let’s zoom in on the leftmost part of the plot. as_draws_df(fit5.9) %&gt;% transmute(sigma_1 = sd_id__exper) %&gt;% mutate(sigma_2_1 = sigma_1^2) %&gt;% set_names(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_halfeye(.width = .95, normalize = &quot;xy&quot;) + scale_y_discrete(NULL, labels = parse(text = c(&quot;sigma[1]&quot;, &quot;sigma[1]^2&quot;))) + coord_cartesian(xlim = c(0, 0.01)) + theme(panel.grid = element_blank()) Although we are still brushing up on the boundary with \\(\\sigma_1^2\\), the mode is no longer at zero. In the discussion, Chung and colleagues pointed out “sometimes weak prior information is available about a variance parameter. When \\(\\alpha = 2\\), the gamma density has its mode at \\(1 / \\lambda\\), and so one can use the \\(\\operatorname{gamma}(\\alpha, \\lambda)\\) prior with \\(1 / \\lambda\\) set to the prior estimate of \\(\\sigma_\\theta\\)” (p. 703). Let’s say we only had our wages_small_pp, but the results of something like the wages_pp data were published by some earlier group of researchers. In this case, we do have good prior data; we have the point estimate from the model of the wages_pp data! Here’s what that was in terms of the median. as_draws_df(fit5.3) %&gt;% median_qi(sd_id__exper) ## # A tibble: 1 × 6 ## sd_id__exper .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.0413 0.0366 0.0469 0.95 median qi And here’s what that value is when set as the divisor of 1. 1 / 0.04154273 ## [1] 24.0716 What does that distribution look like? prior(gamma(2, 24.0716)) %&gt;% parse_dist() %&gt;% ggplot(aes(xdist = .dist_obj, y = prior)) + stat_halfeye(.width = .95, p_limits = c(.0001, .9999)) + scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) + labs(title = &quot;Gamma(2, 24.0716)&quot;, x = &quot;parameter space&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(panel.grid = element_blank()) So this is much more informative than our gamma(2, 0.1) prior from before. But given the magnitude of the estimate from fit5.3, it’s still fairly liberal. Let’s practice using it. fit5.10 &lt;- brm(data = wages_small_pp, family = gaussian, lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(gamma(2, 24.0716), class = sd, group = id, coef = exper), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.10&quot;) Check out the results. print(fit5.10, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) ## Data: wages_small_pp (Number of observations: 257) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 124) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.286 0.047 0.191 0.376 1.002 985 1685 ## sd(exper) 0.042 0.024 0.007 0.095 1.008 524 825 ## cor(Intercept,exper) -0.039 0.309 -0.583 0.595 1.001 1684 2298 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.735 0.048 1.638 1.825 1.000 1688 2461 ## hgc_9 0.046 0.025 -0.004 0.096 1.005 1756 2425 ## exper 0.051 0.023 0.005 0.095 1.001 1801 2211 ## exper:black -0.054 0.037 -0.127 0.019 1.002 1810 2320 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.344 0.022 0.303 0.392 1.001 1297 2428 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we compare the three ways to specify the \\(\\sigma_1\\) prior with the posterior from the original model fit with the full data set. For simplicity, we’ll just look at the results in the brms-like \\(\\sigma\\) metric. Hopefully by now you’ll know how to do the conversions to get the values into the \\(\\sigma^2\\) metric. tibble(`full data, student_t(3, 0, 1) prior` = VarCorr(fit5.3, summary = F)[[1]][[1]][1:4000, 2], `small data, student_t(3, 0, 1) prior` = VarCorr(fit5.7, summary = F)[[1]][[1]][, 2], `small data, gamma(2, 0.1) prior` = VarCorr(fit5.9, summary = F)[[1]][[1]][, 2], `small data, gamma(2, 24.0716) prior` = VarCorr(fit5.10, summary = F)[[1]][[1]][, 2]) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;full data, student_t(3, 0, 1) prior&quot;, &quot;small data, student_t(3, 0, 1) prior&quot;, &quot;small data, gamma(2, 0.1) prior&quot;, &quot;small data, gamma(2, 24.0716) prior&quot;))) %&gt;% ggplot(aes(x = value, y = 0, fill = name == &quot;full data, student_t(3, 0, 1) prior&quot;)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + stat_halfeye(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_manual(values = c(&quot;grey75&quot;, &quot;darkgoldenrod2&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ name, ncol = 1) One thing to notice is that when you’re working with full Bayesian estimation with even a rather vague prior with a boundary on zero, the measure of central tendency in the posterior is away from zero. Things get more compact when you’re working in the \\(\\sigma^2\\) metric. But remember that when we’re fitting our models with brms, we’re in the \\(\\sigma\\) metric, anyway. And with either of these three options, you don’t have a compelling reason to set the \\(\\sigma_\\theta\\) parameter to zero the way you would with ML. Even a rather vague prior will add enough information to the model that we can feel confident about keeping our theoretically-derived \\(\\sigma_1\\) parameter. 5.2.2.2 Nonconvergence [i.e., it’s time to talk chains and such]. As discussed in Section 4.3, all multilevel modeling programs implement iterative numeric algorithms for model fitting. (p. 155). This is also true for our Stan-propelled brms software. However, what’s going on under the hood, here, is not what’s happening with the frequentist packages discussed by Singer and Willett. We’re using Hamiltonian Monte Carlo (HMC) to draw from the posterior. To my eye, Bürkner gave in a (2020) preprint probably the clearest and most direct introduction to why we need fancy algorithms like HMC to fit Bayesian models. First, Bürkner warmed up by contrasting Bayes with conventional frequentist inference: In frequentist statistics, parameter estimates are usually obtained by finding those parameter values that maximise the likelihood. In contrast, Bayesian statistics aims to estimate the full (joint) posterior distribution of the parameters. This is not only fully consistent with probability theory, but also much more informative than a single point estimate (and an approximate measure of uncertainty commonly known as ‘standard error’). (p. 9) Those iterative algorithms Singer and Willett discussed in this section, that’s what they’re doing. They are maximizing the likelihood. But with Bayes, we have the more challenging goal of describing the entire posterior distribution, which is the product of the likelihood and the prior. As such, Obtaining the posterior distribution analytically is only possible in certain cases of carefully chosen combinations of prior and likelihood, which may considerably limit modeling flexibilty but yield a computational advantage. However, with the increased power of today’s computers, Markov-Chain Monte-Carlo (MCMC) sampling methods constitute a powerful and feasible alternative to obtaining posterior distributions for complex models in which the majority of modeling decisions is made based on theoretical and not computational grounds. Despite all the computing power, these sampling algorithms are computationally very intensive and thus fitting models using full Bayesian inference is usually much slower than in point estimation techniques. However, advantages of Bayesian inference – such as greater modeling flexibility, prior distributions, and more informative results – are often worth the increased computational cost (Gelman, Carlin, Stern, and Rubin 2013). (pp. 9–10) The gritty details are well beyond the scope of this project. If you’d like a more thorough walk-through on why it’s analytically and computationally challenging to get the posterior, I recommend working through the first several chapters in Kruschke’s (2015) Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. But so anyways, our primary algorithm is HMC as implemented by Stan. You can find all kinds of technical details at https://mc-stan.org/users/documentation/. Because it’s rather difficult to describe our Bayesian multilevel models analytically, we use HMC to draw from the posterior instead. We then summarize the marginal and joint distributions of those parameters with things like measures of central tendency (i.e., means, medians, modes) and spread (i.e., standard deviations, percentile-based intervals). We make lots of plots. And somewhat like with the frequentist iterative algorithms, we need to make sure our sweet Stan-based HMC is working well, too. One way is with trace plots. 5.2.2.2.1 Trace plots. We can get the trace plots for a model by placing a brm() fit object into the plot() function. Here’s an example with the full model, fit5.4. plot(fit5.4) You’ll notice we get two plots for each of the major model parameters, the \\(\\gamma\\)’s, the \\(\\sigma\\)’s and the \\(\\rho\\)’s. The plots on the left are the density plots for each parameter. On the right, we have the actual trace plots. On the \\(x\\)-axis, we have an ordering of the posterior draws; on the \\(y\\), we have the parameter space. Since we requested three HMC chains to draw from the posterior (chains = 3), those three chains are depicted by different colored lines. We generally like it when the lines for our chains all overlap with each other in a stable zig-zag sort of way. Trace plots are sometimes called caterpillar plots because, when things are going well, they often resemble nice multicolored fuzzy caterpillars. If you’d like more control over your trace plot visuals, you might check out the bayesplot package (Gabry et al., 2019; Gabry &amp; Mahr, 2022). library(bayesplot) Our main function will be mcmc_trace(). Unlike with the brms::plot() method, bayesplog::mcmc_trace() takes the posterior draws themselves as input. So we’ll have to use as_draws_df() first. draws &lt;- as_draws_df(fit5.4) We can use the pars argument to focus on particular parameters. mcmc_trace(draws, pars = &quot;sigma&quot;) If we use the pars = vars(...) format, we can use function helpers from dplyr to select subsets of parameters. For example, here’s how we might single out the \\(\\gamma\\)’s. mcmc_trace(draws, pars = vars(starts_with(&quot;b_&quot;)), facet_args = list(ncol = 2)) Notice how we used the facet_args argument to adjust the number of columns in the output. We can also use familiar ggplot2 functions to customize the plots further. draws %&gt;% mutate(`sigma[0]` = sd_id__Intercept, `sigma[1]` = sd_id__exper, `sigma[epsilon]` = sigma) %&gt;% mcmc_trace(pars = vars(starts_with(&quot;sigma[&quot;)), facet_args = list(labeller = label_parsed)) + scale_color_viridis_d(option = &quot;A&quot;) + scale_x_continuous(NULL, breaks = NULL) + ggtitle(&quot;I can&#39;t wait to show these traceplots to my mom.&quot;) + theme_grey() + theme(legend.position = &quot;bottom&quot;, panel.grid = element_blank(), panel.grid.major.y = element_line(color = &quot;white&quot;, size = 1/4), strip.text = element_text(size = 12)) Trace plots are connected to other important concepts, like autocorrelation and effective sample size. 5.2.2.2.2 Autocorrelation. When using Markov chain Monte Carlo methods, of which HMC is a special case, the notions of autocorrelation and effective sample size are closely connected. Both have to do with the question, How many post-warmup draws from the posterior do I need to take? If you take too few, you won’t have a good sense of the shape of the posterior. If you take more than necessary, you’re just wasting time and computer memory. Here’s how McElreath introduced the topic in his (2015) text: So how many samples do we need for accurate inference about the posterior distribution? It depends. First, what really matters is the effective number of samples, not the raw number. The effective number of samples is an estimate of the number of independent samples from the posterior distribution. Markov chains are typically autocorrelated, so that sequential samples are not entirely independent. Stan chains tend to be less autocorrelated than those produced by other engines [e.g., the Gibbs sampler], but there is always some autocorrelation. (p. 255, emphasis in the original) I’m not aware of a way to query the autocorrelations from a brm() fit using brms convenience functions. However, we can get those diagnostics from the bayesplot::mcmc_acf() function. mcmc_acf(draws, pars = vars(starts_with(&quot;b_&quot;)), lags = 10) + theme_grey() + theme(panel.grid = element_blank()) The mcmc_acf() function gives a wealth of granular output. The columns among the plots are the specified parameters. The rows are the chains, one for each. In this particular case, the autocorrelations were quite low for all our \\(\\gamma\\) parameters by the second or third lag. That’s really quite good and not uncommon for HMC. Do note, however, that this won’t always be the case. For example, here are the plots for our variance parameters and \\(\\rho_{01}\\). mcmc_acf(draws, pars = vars(starts_with(&quot;sd_&quot;), &quot;sigma&quot;, starts_with(&quot;cor&quot;)), lags = 10) + theme_grey() + theme(panel.grid = element_blank(), strip.text = element_text(size = 7)) On the whole, all of them are pretty okay. But notice how the autocorrelations for \\(\\sigma_1\\) and \\(\\rho_{01}\\) remained relatively high up until the 10th lag. The plots from mcmc_act() are quite handy for focused diagnostics. But if you want a more global perspective, they’re too tedious. Fortunately for us, we have other diagnostic tools. 5.2.2.2.3 Effective sample size. Above we quoted McElreath as pointing out “what really matters is the effective number of samples, not the raw number.” With brms, you typically get the effective number of samples in the print() or summary() output. Here it is again for fit4. summary(fit5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.01 0.21 0.25 1.00 1467 2232 ## sd(exper) 0.04 0.00 0.04 0.05 1.01 501 1219 ## cor(Intercept,exper) -0.29 0.07 -0.42 -0.14 1.01 566 1296 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.72 0.01 1.69 1.74 1.00 2367 3009 ## hgc_9 0.03 0.01 0.02 0.05 1.00 2907 3462 ## black 0.02 0.02 -0.03 0.06 1.00 2135 2973 ## exper 0.05 0.00 0.04 0.05 1.00 2349 3573 ## hgc_9:exper 0.00 0.00 -0.00 0.00 1.00 2667 3164 ## black:exper -0.02 0.01 -0.03 -0.01 1.00 2152 2714 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.31 0.00 0.30 0.32 1.00 2849 3100 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See the last two columns on the right: Bulk_ESS and Tail_ESS? Following Vehtari et al. (2019), those describe our effective sample size in two ways. From the paper, we read: If you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original) For more technical details, see the paper. In short, Bulk_ESS indexes the number of effective samples in ‘the center of the’ posterior distribution (i.e., the posterior mean or median). But since we also care about uncertainty in our parameters, we care about stability in the 95% intervals and such. The Tail_ESS column allows us to gauge the effective sample size for those intervals. Like with the autocorrelations, each parameter gets its own estimate for both ESS measures. You might compare the numbers to the number of post-warmup iterations, 4,500 in this case. You may wonder, how many effective samples do I need? Back to McElreath: If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. (p. 255) At the moment, brms does not offer a convenience function that allows users to collect the Bulk_ESS and Tail_ESS values in a data frame. However you can do so with help from the posterior package (Bürkner et al., 2022). For our purposes, the function of interest is summarise_draws(), which will take the output from as_draws_df() as input. We’ll save the results as draws_sum. library(posterior) draws_sum &lt;- as_draws_df(fit5.4) %&gt;% summarise_draws() draws_sum %&gt;% head(n = 10) ## # A tibble: 10 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1 b_Intercept 1.72 1.72 0.0126 0.0126 1.70 1.74 1.00 2367. 3009. ## 2 b_hgc_9 0.0349 0.0349 0.00759 0.00765 0.0225 0.0474 1.00 2907. 3462. ## 3 b_black 0.0159 0.0157 0.0236 0.0237 -0.0220 0.0536 1.00 2135. 2973. ## 4 b_exper 0.0493 0.0493 0.00258 0.00258 0.0450 0.0534 1.00 2349. 3573. ## 5 b_hgc_9:exper 0.00126 0.00128 0.00171 0.00169 -0.00153 0.00405 1.00 2667. 3164. ## 6 b_black:exper -0.0183 -0.0182 0.00544 0.00539 -0.0273 -0.00939 1.00 2152. 2714. ## 7 sd_id__Intercept 0.227 0.227 0.0108 0.0107 0.210 0.245 1.00 1467. 2232. ## 8 sd_id__exper 0.0404 0.0404 0.00268 0.00268 0.0361 0.0449 1.01 501. 1219. ## 9 cor_id__Intercept__e… -0.293 -0.297 0.0706 0.0699 -0.401 -0.170 1.01 566. 1296. ## 10 sigma 0.309 0.309 0.00313 0.00313 0.304 0.314 1.00 2849. 3100. Note how the last two columns are the ess_bulk and the ess_tail. Here we summarize them with histograms. draws_sum %&gt;% pivot_longer(starts_with(&quot;ess&quot;)) %&gt;% ggplot(aes(x = value)) + geom_histogram(binwidth = 100) + xlim(0, NA) + theme(panel.grid = element_blank()) + facet_wrap(~ name) If you wanted a focused plot of the effective sample sizes for our primary summary parameters, you would just wrangle the output a little. Since it’s fun, we’ll switch to a lollipop plot. draws_sum %&gt;% slice(1:10) %&gt;% pivot_longer(contains(&quot;ess&quot;)) %&gt;% mutate(ess = str_remove(name, &quot;ess_&quot;)) %&gt;% ggplot(aes(y = reorder(variable, value))) + geom_linerange(aes(xmin = 0, xmax = value)) + geom_point(aes(x = value)) + labs(x = &quot;effective sample size&quot;, y = NULL) + xlim(0, 4000) + theme(panel.grid = element_blank(), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) + facet_wrap(~ ess) You may have noticed from the ESS histogram that many of the parameters seemed to have bulk ESS values well above the total number of posterior draws (4,500). What’s that about? As is turns out, the sampling in Stan is so good that sometimes the HMC chains for a parameter can be negatively autocorrelated. When this is the case, your effective sample size can be larger than your actual sample size. Madness, I know. This was the case for many of our random effects. Here’s a look at the parameters with ten largest values. draws_sum %&gt;% arrange(desc(ess_bulk)) %&gt;% select(variable, ess_bulk) %&gt;% slice(1:10) ## # A tibble: 10 × 2 ## variable ess_bulk ## &lt;chr&gt; &lt;num&gt; ## 1 r_id[1282,Intercept] 10330. ## 2 r_id[9601,Intercept] 9550. ## 3 r_id[7311,Intercept] 9376. ## 4 r_id[9294,exper] 9231. ## 5 r_id[6507,Intercept] 9191. ## 6 r_id[7567,Intercept] 9109. ## 7 r_id[7110,exper] 8996. ## 8 r_id[6792,exper] 8983. ## 9 r_id[7027,exper] 8977. ## 10 r_id[7257,exper] 8944. Let’s take the first 4 and check their autocorrelation plots. mcmc_acf(draws, pars = vars(`r_id[12335,exper]`, `r_id[5968,Intercept]`, `r_id[10476,Intercept]`, `r_id[7117,Intercept]`), lags = 5) + theme_grey() + theme(panel.grid = element_blank(), strip.text = element_text(size = 10)) See those dips below zero for the first lag in each? That’s what a negative autocorrelation looks like. Beautiful. For more on negative autocorrelations within chains and how it influences the number of effective samples, check out this thread on the Stan forums where many members of the Stan team chimed in. 5.2.2.2.4 \\(\\widehat R\\). Return again to the default print() output for a brms::brm() fit. print(fit5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.23 0.01 0.21 0.25 1.00 1467 2232 ## sd(exper) 0.04 0.00 0.04 0.05 1.01 501 1219 ## cor(Intercept,exper) -0.29 0.07 -0.42 -0.14 1.01 566 1296 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.72 0.01 1.69 1.74 1.00 2367 3009 ## hgc_9 0.03 0.01 0.02 0.05 1.00 2907 3462 ## black 0.02 0.02 -0.03 0.06 1.00 2135 2973 ## exper 0.05 0.00 0.04 0.05 1.00 2349 3573 ## hgc_9:exper 0.00 0.00 -0.00 0.00 1.00 2667 3164 ## black:exper -0.02 0.01 -0.03 -0.01 1.00 2152 2714 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.31 0.00 0.30 0.32 1.00 2849 3100 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The third last column for each parameter is Rhat. “Rhat is a complicated estimate of the convergence of the Markov chains to the target distribution. It should approach 1.00 from above, when all is well” (McElreath, 2015, p. 250). We can extract \\(\\widehat R\\) directly with the brms::rhat() function. brms::rhat(fit5.4) %&gt;% str() ## Named num [1:1788] 1 1 1 1 1 ... ## - attr(*, &quot;names&quot;)= chr [1:1788] &quot;b_Intercept&quot; &quot;b_hgc_9&quot; &quot;b_black&quot; &quot;b_exper&quot; ... For our fit5.4, the brms::rhat() function returned a named numeric vector, with one row for each of the 1787 parameters in the model. You can subset the rhat() output to focus on a few parameters. brms::rhat(fit5.4)[1:10] ## b_Intercept b_hgc_9 b_black b_exper ## 1.000080 1.001269 1.001654 1.000477 ## b_hgc_9:exper b_black:exper sd_id__Intercept sd_id__exper ## 1.000478 1.000449 1.001643 1.005972 ## cor_id__Intercept__exper sigma ## 1.006086 1.000958 Note also that our draws_sum object from above has an rhat column, too. draws_sum %&gt;% select(variable, rhat) %&gt;% slice(1:4) ## # A tibble: 4 × 2 ## variable rhat ## &lt;chr&gt; &lt;num&gt; ## 1 b_Intercept 1.00 ## 2 b_hgc_9 1.00 ## 3 b_black 1.00 ## 4 b_exper 1.00 For a more global perspective, just plot. draws_sum %&gt;% ggplot(aes(x = rhat)) + geom_vline(xintercept = 1, color = &quot;white&quot;) + geom_histogram(binwidth = .0001) + theme(panel.grid = element_blank()) The bayesplot package offers a convenience function for plotting brms::rhat() output. Here we’ll focus on the first 20 parameters. mcmc_rhat(brms::rhat(fit5.4)[1:20]) + yaxis_text(hjust = 0) By default, mcmc_rhat() does not return text on the \\(y\\)-axis. But you can retrieve that text with the yaxis_text() function. For more on the \\(\\widehat R\\), you might check out the Rhat: potential scale reduction statistic subsection of Gabry and Modrák’s (2020) vignette, Visual MCMC diagnostics using the bayesplot package. We should also point out that the Stan team has found some deficiencies with the \\(\\widehat R\\). They’ve made recommendations that will be implemented in the Stan ecosystem sometime soon. In the meantime, you can read all about it in their preprint (Vehtari et al., 2019) and in Dan Simpson’s blog post, Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it. 5.2.3 Distinguishing among different types of missingness. Missingness, in and of itself, is not necessarily problematic. It all depends upon what statisticians call the type of missingness. In seminal work on this topic, R. J. Little (1995), refining earlier work with Rubin (R. J. A. Little &amp; Rubin, 1987), distinguished among three types of missingness: (1) missing completely at random (MCAR); (2) covariate-dependent dropout (CDD); and (3) missing at random (MAR) (see also Schafer, 1997). When we say that data are MCAR, we argue that the observed values are a random sample of all the values that could have been observed (according ot plan), had there been no missing data. …Covariate dependent dropout (CDD) is a less restrictive assumption that permits associations between the probability of missingness and observed predictor values (“covariates”). Data can be CDD even if the probability of missingness is systematically related to either TIME or observed substantive predictors. …When data are MAR, the probability of missingness can depend upon any observed data, for either the predictors or any outcome values. It cannot, however, depend upon an unobserved value of either any predictor or the outcome. (pp. 157–158, emphasis in the original) For some more current introductions to missing data methods, I recommend Enders’ (2010) Applied missing data analysis, for which you can find a free sample chapter here, and Little and Rubin’s (2019) Statistical analysis with missing data, 3rd Edition. You might also check out van Burren’s great (2018) online text Flexible imputation of missing data. Second edition. If you’re a fan of the podcast medium, you might listen to episode 16 from the first season of the Quantitude podcast, IF EPISODE=16 THEN EPISODE=-999;, in which Patrick Curran and Greg Hancock do a fine job introducing the basics of missing data. And very happily, brms has several ways to handle missing data, about which you can learn more from Bürkner’s (2021b) vignette, Handle missing values with brms. 5.3 Time-varying predictors A time-varying predictor is a variable whose values may differ over time. Unlike their time-invariant cousins, which record an individual’s static status, time-varying predictors record an individual’s potentially differing status on each associated measurement occasion. Some time-varying predictors have values that change naturally; others have values that change by design. (pp. 159–160, emphasis in the original) 5.3.1 Including the main effect of a time-varying predictor. You can find Ginexi and colleagues’ (2000) unemployment study data in the reading_pp.csv file. unemployment_pp &lt;- read_csv(&quot;data/unemployment_pp.csv&quot;) head(unemployment_pp) ## # A tibble: 6 × 4 ## id months cesd unemp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 103 1.15 25 1 ## 2 103 5.95 16 1 ## 3 103 12.9 33 1 ## 4 641 0.789 27 1 ## 5 641 4.86 7 0 ## 6 641 11.8 25 0 We have 254 unique participants. unemployment_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 254 Here’s one way to compute the number of participants who were never employed during the study. unemployment_pp %&gt;% filter(unemp == 0) %&gt;% distinct(id) %&gt;% count() %&gt;% summarise(never_employed = 254 - n) ## # A tibble: 1 × 1 ## never_employed ## &lt;dbl&gt; ## 1 132 In case it wasn’t clear, participants had up to 3 interviews. By recruiting 254 participants from local unemployment offices, the researchers were able to interview individuals soon after job loss (within the first 2 months). Follow-up interviews were conducted between 3 and 8 months and 10 and 16 months after job loss. (p. 161) Those times were encoded in the months variable. Here’s what that looks like. unemployment_pp %&gt;% ggplot(aes(x = months)) + geom_vline(xintercept = c(3, 8), color = &quot;white&quot;) + geom_histogram(binwidth = .5) + theme(panel.grid = element_blank()) To make some of our data questions easier, we can use those 3- and 8-month thresholds to make an interview variable to indicate the periods during which the interviews were conducted. unemployment_pp &lt;- unemployment_pp %&gt;% mutate(interview = ifelse(months &lt; 3, 1, ifelse(months &gt; 8, 3, 2))) unemployment_pp %&gt;% ggplot(aes(x = interview)) + geom_bar() + theme(panel.grid = element_blank()) With a little wrangling, we can display all possible employment patterns along with counts on how many followed them. unemployment_pp %&gt;% select(-months, -cesd) %&gt;% mutate(interview = str_c(&quot;int_&quot;, interview)) %&gt;% spread(key = interview, value = unemp) %&gt;% group_by(int_1, int_2, int_3) %&gt;% count() %&gt;% arrange(desc(n)) %&gt;% flextable::flextable() .cl-877dd1c4{}.cl-874bf898{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8775a102{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8775d0fa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8775d104{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8775d10e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}int_1int_2int_3n111781005511041127112210119104114102101111 It takes a little work to see how Singer and Willett came to the conclusion “62 were always working after the first interview” (p. 161). Based on an analysis of those who had complete data, that corresponds to the pattern in the top row, [1, 0, 0], which we have counted as 55 (i.e., row 2). If you add to that the two rows with missingness on one of the critical values (i.e., [1, 0, NA], [1, NA, 0], and [NA, 1, 1]), that gets you \\(55 + 4 + 2 + 1 = 62\\). We can confirm that “41 were still unemployed at the second interview but working by the third” (p. 161). That’s our pattern [1, 1, 0], shown in row 3. We can also confirm “19 were working by the second interview but unemployed at the third” (p. 161). That’s shown in our pattern [1, 0, 1], shown in row 6. Before we configure our unconditional growth model, we might familiarize ourselves with our criterion variable, cesd. Singer and Willett informed us: Each time participants completed the Center for Epidemiologic Studies’ Depression (CES-D) scale (Radloff, 1977), which asks them to rate, on a four-point scale, the frequency with which they experience each of the 20 depressive symptoms. The CES-D scores can vary from a low or 0 for someone with no symptoms to a high of 80 for someone in serious distress. (p. 161) In addition to Radloff’s original article, you can get a copy of the CES-D here. To help us pick our priors, [Brown and Gary (1985) listed the means and standard deviations of the CES-D scores for unemployed African-American adults. They gave the summary statistics broken down by sex: Males: 14.05 (8.86), \\(n = 37\\) Females: 15.35 (9.39), \\(n = 72\\) Based the variables in the data set and the descriptions of it in the text, we don’t have a good sense of the demographic backgrounds of the participants. But with the information we have in hand, a reasonable empirically-based but nonetheless noncommittal prior for baseline CES-D might be something like normal(14.5, 20). A weakly-regularizing prior on change over 1 month might be normal(0, 10). It’d be fair if you wanted to argue about these priors. Try your own! But if you are willing to go along with me, we might write the statistical formula for the unconditional growth model as $$ \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\] $$ Those \\(\\operatorname{Student-t}(3, 0, 11.9)\\) priors for the \\(\\sigma\\)’s are the defaults, which you can confirm with get_prior(). The \\(\\operatorname{LKJ} (4)\\) for \\(\\rho_{01}\\) will weakly regularize the correlation towards zero. Here’s how we might fit that model. fit5.11 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + months + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 11.9), class = sd), prior(student_t(3, 0, 11.9), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .99), file = &quot;fits/fit05.11&quot;) Here are the results. print(fit5.11, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + months + (1 + months | id) ## Data: unemployment_pp (Number of observations: 674) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 8.679 0.832 7.078 10.306 1.007 410 1824 ## sd(months) 0.417 0.198 0.035 0.763 1.013 203 540 ## cor(Intercept,months) -0.368 0.238 -0.692 0.284 1.004 1139 927 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 17.657 0.763 16.205 19.170 1.002 1967 2849 ## months -0.420 0.082 -0.580 -0.262 1.001 4467 3467 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.577 0.407 7.794 9.354 1.005 361 1346 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are the posteriors for the CES-D at the first day of job loss (i.e., \\(\\gamma_{00}\\)) and the expected rate of change over one month (i.e., \\(\\gamma_{10}\\)). as_draws_df(fit5.11) %&gt;% transmute(`first day of job loss` = b_Intercept, `linear decline by month` = b_months) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;CES-D composite score&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) We might use conditional_effects() to get a quick view on what that might looks like. plot(conditional_effects(fit5.11), plot = FALSE)[[1]] + geom_hline(yintercept = 14.5, color = &quot;grey50&quot;, linetype = 2) + coord_cartesian(ylim = c(0, 20)) + theme(panel.grid = element_blank()) For reference, the dashed gray line is the value we centered our prior for initial status on. 5.3.1.1 Using a composite specification. We might specify Model B, our first model with a time-varying covariate, like this: $$ \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\gamma_{20} \\text{unemp}_{ij} \\big ] + \\big [ \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\big ]\\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10} \\text{ and } \\gamma_{20} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ}(4). \\end{align*}\\] $$ Note a few things about the priors. First, we haven’t changed any of the priors from the previous model. All we did was add \\(\\gamma_{20} \\sim \\text{Normal}(0, 10)\\) for our new parameter. Given how weakly-informative our other priors have been for these data, this isn’t an unreasonable approach. However, the meaning for our intercept, \\(\\gamma_{01}\\), has changed. Now it’s the initial status for someone who is employed at baseline. But remember that for Model A, we set that prior with unemployed people in mind. A careful researcher might want to dive back into the literature to see if some lower value than 14.5 would be more reasonable to set for the mean of that prior. However, since the standard deviations for our intercepts priors and the covariate priors are all rather wide and permissive, this just won’t be much of a problem, for us. Buy anyway, second, note that we’ve centered our prior for \\(\\gamma_{20}\\) on zero. This is a weakly-regularizing prior, slightly favoring smaller effects over larger ones. And like before, one could easily argue for different priors. Here’s how to fit the model in brms. fit5.12 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + months + unemp + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 11.9), class = sd), prior(student_t(3, 0, 11.9), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .9), file = &quot;fits/fit05.12&quot;) If you compare our results to those in Table 5.7, you’ll see they’re quite similar. print(fit5.12) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + months + unemp + (1 + months | id) ## Data: unemployment_pp (Number of observations: 674) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9.11 0.84 7.42 10.74 1.01 423 923 ## sd(months) 0.55 0.18 0.11 0.85 1.02 216 274 ## cor(Intercept,months) -0.47 0.17 -0.70 -0.01 1.00 1053 718 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 12.79 1.22 10.49 15.16 1.00 1890 2343 ## months -0.21 0.09 -0.38 -0.03 1.00 2787 3331 ## unemp 4.98 1.00 2.98 6.87 1.00 2496 3096 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.15 0.42 7.37 8.98 1.01 355 796 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we make our versions of Figure 5.3, let’s first compare \\(\\gamma_{01}\\) posteriors by model. On page 166 of the text, Singer and Willett reported the monthly rate of decline “had been cut in half (to 0.20 from 0.42 in Model A)”. fixef(fit5.11)[&quot;months&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## -0.4200590 0.0822367 -0.5800403 -0.2617752 fixef(fit5.12)[&quot;months&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## -0.20583470 0.08974567 -0.37996955 -0.03253020 You might be wondering why the quote from Singer and Willett used positive numbers while our parameter estimates have negative ones. No, there’s no mistake, there. Negative parameter estimates for monthly trajectories are then same thing as expressing a rate of decline with a positive number. But anyways, you see our estimates are on par with theirs. With our Bayesian paradigm, it’s also easy to get a formal difference distribution. tibble(fit5.11 = as_draws_df(fit5.11) %&gt;% pull(&quot;b_months&quot;), fit5.12 = as_draws_df(fit5.12) %&gt;% pull(&quot;b_months&quot;)) ## # A tibble: 4,000 × 2 ## fit5.11 fit5.12 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.394 -0.324 ## 2 -0.490 -0.179 ## 3 -0.368 -0.206 ## 4 -0.525 -0.287 ## 5 -0.264 -0.273 ## 6 -0.603 -0.237 ## 7 -0.380 -0.365 ## 8 -0.531 -0.180 ## 9 -0.460 -0.317 ## 10 -0.380 -0.161 ## # ℹ 3,990 more rows tibble(fit5.11 = as_draws_df(fit5.11) %&gt;% pull(&quot;b_months&quot;), fit5.12 = as_draws_df(fit5.12) %&gt;% pull(&quot;b_months&quot;)) %&gt;% mutate(dif = fit5.12 - fit5.11) %&gt;% ggplot(aes(x = dif, y = 0)) + stat_halfeye(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(&quot;Difference in &quot;, gamma[1][0]))) + theme(panel.grid = element_blank()) Here’s our posterior for \\(\\gamma_{20}\\), b_unemp. as_draws_df(fit5.12) %&gt;% ggplot(aes(x = b_unemp, y = 0)) + stat_halfeye(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Let’s compute our WAIC estimates for fit5.11 and fit5.12. fit5.11 &lt;- add_criterion(fit5.11, criterion = &quot;waic&quot;) fit5.12 &lt;- add_criterion(fit5.12, criterion = &quot;waic&quot;) Now we’ll compare the models by both their WAIC differences and their WAIC weights. loo_compare(fit5.11, fit5.12, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.12 0.0 0.0 -2491.5 21.4 196.7 10.7 4983.0 42.9 ## fit5.11 -22.3 4.7 -2513.8 21.6 179.9 10.1 5027.5 43.2 model_weights(fit5.11, fit5.12, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.11 fit5.12 ## 0 1 By both metrics, fit5.12 came out as the clear favorite. It’s finally time to make our version of the upper left panel of Figure 5.3. We’ll do so using fitted(). nd &lt;- tibble(unemp = 1, months = seq(from = 0, to = 14, by = .5)) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) f %&gt;% ggplot(aes(x = months)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Remain unemployed&quot;) + theme(panel.grid = element_blank()) The upper right panel will take more care. We’ll still use fitted(), but we’ll have to be tricky with how we define the two segments. When we defined the sequence of months values over which we wanted to plot the model trajectory, we just casually set length.out = 30 within the seq() function. But now we need to make sure two of those sequential points are at 5. One way to do so is to use the by = .5 argument within seq(), instead. Since we’ll be defining the end points in our range with integer values, dividing up the sequence by every .5th value will ensure we’ll both be able to stop at 5 and that we’ll have a reasonable amount of values in the sequence to ensure the bowtie-shaped 95% intervals don’t look chunky. But anyway, that also means we’ll need to do a good job determining how many values we’ll need to repeat our desired unemp values over. So here’s a quick way to do the math. Since we’re using every .5 in the sequence, you just subtract the integer at the beginning of the sequence from the integer at the end of the sequence, multiply that value by 2, and then add 1 to the product. Like this: 2 * (5 - 0) + 1 ## [1] 11 2 * (14 - 5) + 1 ## [1] 19 Those are the number of times we need to repeat unemp == 1 and unemp == 0, respectively. You’ll see. Now wrangle and plot. nd &lt;- tibble(unemp = rep(1:0, times = c(11, 19)), months = c(seq(from = 0, to = 5, by = .5), seq(from = 5, to = 14, by = .5))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_segment(x = 5, xend = 5, y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 5, yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 5 + fixef(fit5.12)[3, 1], size = 1/3, linetype = 2) + annotate(geom = &quot;text&quot;, x = 8, y = 14.5, label = &quot;gamma[2][0]&quot;, parse = T) + geom_segment(x = 7, xend = 5.5, y = 14.5, yend = 14.5, arrow = arrow(length = unit(0.05, &quot;inches&quot;))) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Reemployed at 5 months&quot;) + theme(panel.grid = element_blank()) Same deal for the lower left panel of Figure 5.3. 2 * (10 - 0) + 1 ## [1] 21 2 * (14 - 10) + 1 ## [1] 9 Now wrangle and plot. nd &lt;- tibble(unemp = rep(1:0, times = c(21, 9)), months = c(seq(from = 0, to = 10, by = .5), seq(from = 10, to = 14, by = .5))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_segment(x = 10, xend = 10, y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 10, yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 10 + fixef(fit5.12)[3, 1], size = 1/3, linetype = 2) + annotate(geom = &quot;text&quot;, x = 7, y = 13.5, label = &quot;gamma[2][0]&quot;, parse = T) + geom_segment(x = 8, xend = 9.5, y = 13.5, yend = 13.5, arrow = arrow(length = unit(0.05, &quot;inches&quot;))) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Reemployed at 10 months&quot;) + theme(panel.grid = element_blank()) It’s just a little bit trickier to get that lower right panel. Now we need to calculate three values. 2 * (5 - 0) + 1 ## [1] 11 2 * (10 - 5) + 1 ## [1] 11 2 * (14 - 10) + 1 ## [1] 9 Get that plot. nd &lt;- tibble(unemp = rep(c(1, 0, 1), times = c(11, 11, 9)), months = c(seq(from = 0, to = 5, by = .5), seq(from = 5, to = 10, by = .5), seq(from = 10, to = 14, by = .5)), group = rep(letters[1:3], times = c(11, 11, 9))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) lines &lt;- tibble(group = letters[1:2], x = c(5, 10)) %&gt;% mutate(xend = x, y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * x, yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * x + fixef(fit5.12)[3, 1]) arrow &lt;- tibble(x = c(6.75, 8.25), y = 14, xend = c(5.5, 9.5), yend = c(14.5, 13.5)) f %&gt;% ggplot(aes(x = months)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = group), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate, group = group)) + geom_segment(data = lines, aes(x = x, xend = xend, y = y, yend = yend, group = group), size = 1/3, linetype = 2) + annotate(geom = &quot;text&quot;, x = 7.5, y = 14, label = &quot;gamma[2][0]&quot;, parse = T) + geom_segment(data = arrow, aes(x = x, xend = xend, y = y, yend = yend), arrow = arrow(length = unit(0.05, &quot;inches&quot;))) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Reemployed at 5 months\\nunemployed again at 10&quot;) + theme(panel.grid = element_blank()) Now we’ve been on a plotting roll, let’s knock out the leftmost panel of Figure 5.4. It’s just a small extension of what we’ve been doing. 2 * (14 - 0) + 1 ## [1] 29 2 * (3.5 - 0) + 1 ## [1] 8 2 * (14 - 3.5) + 1 ## [1] 22 nd &lt;- tibble(unemp = rep(1:0, times = c(29, 22)), months = c(seq(from = 0, to = 14, by = .5), seq(from = 3.5, to = 14, by = .5))) f &lt;- fitted(fit5.12, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(label = str_c(&quot;unemp = &quot;, unemp)) f %&gt;% ggplot(aes(x = months, group = unemp)) + # new trick geom_abline(intercept = fixef(fit5.12)[1, 1], slope = fixef(fit5.12)[2, 1], color = &quot;grey80&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + # another new trick geom_text(data = f %&gt;% filter(months == 14), aes(label = label, y = Estimate), hjust = -.05) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Main effects of unemp and time&quot;) + # don&#39;t forget this part coord_cartesian(clip = &quot;off&quot;) + theme(panel.grid = element_blank(), plot.margin = margin(6, 55, 6, 6)) 5.3.1.2 Using a level-1/level-2 specification. If we wanted to reexpress our composite equation for fit5.12 using the level-1/level-2 form, the level-1 model would be \\[ \\text{cesd}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{months}_{ij} + \\pi_{2i} \\text{unemp}_{ij} + \\epsilon_{ij}. \\] Here’s the corresponding level-2 model: \\[ \\begin{align*} \\pi_{0i} &amp; = \\gamma_{00} + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\zeta_{1i} \\\\ \\pi_{2i} &amp; = \\gamma_{20}. \\end{align*} \\] If we wanted the effects of the time-varying covariate unemp to vary across individuals, then we’d expand the definition of \\(\\pi_{2i}\\) to be \\[ \\pi_{2i} = \\gamma_{20} + \\zeta_{2i}. \\] Although this doesn’t change the way we model \\(\\epsilon_{ij}\\), which remains \\[ \\epsilon_{ij} \\sim \\text{Normal} (0, \\sigma_\\epsilon), \\] it does change the model for the \\(\\zeta\\)s. Within our Stan/brms paradigm, that would now be $$ \\[\\begin{align*} \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{2i} \\end{bmatrix} &amp; \\sim \\text{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\end{pmatrix}, \\text{where} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_2 \\end{bmatrix} \\text{and} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} &amp; \\rho_{02} \\\\ \\rho_{01} &amp; 1 &amp; \\rho_{12} \\\\ \\rho_{02} &amp; \\rho_{12} &amp; 1 \\end{bmatrix}. \\end{align*}\\] $$ Reworded slightly from the text (p. 169), by adding one residual parameter, \\(\\zeta_{2i}\\), we got an additional corresponding standard deviation parameter, \\(\\sigma_2\\), and two more correlation parameters, \\(\\rho_{02}\\) and \\(\\rho_{12}\\). Staying with our weakly-regularizing prior approach, the priors for the updated model might look like \\[ \\begin{align*} \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10}, \\gamma_{20} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\dots, \\sigma_2 &amp; \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\ \\Omega &amp; \\sim \\operatorname{LKJ} (4). \\end{align*} \\] Singer and Willett then cautioned readers about hastily adding \\(\\zeta\\) parameters to their models, particularly in cases where you’re likely to run into estimation issues, such as boundary constraints. Within our Stan/brms paradigm, we still have to be aware of these difficulties. However, with skillfully-chosen priors, I think you’ll find we can fit more ambitious models than would typically be possible with frequentist estimators. But do beware that as you stretch your data further and further, your choices in likelihoods and priors more heavily influence the results. For more on the topic, check out Michael Frank’s blog post, Mixed effects models: Is it time to go Bayesian by default?, and make sure not to miss the action in the comments section. 5.3.1.3 Time-varying predictors and variance components. When you add a time-varying predictor, it’s not uncommon to see a reduction in \\(\\sigma_\\epsilon^2\\). Here we compare fit5.11 and fit5.12. v &lt;- cbind(VarCorr(fit5.11, summary = F)[[2]][[1]], VarCorr(fit5.12, summary = F)[[2]][[1]]) %&gt;% data.frame() %&gt;% set_names(str_c(&quot;fit5.&quot;, 11:12)) %&gt;% transmute(fit5.11 = fit5.11^2, fit5.12 = fit5.12^2) %&gt;% mutate(`fit5.11 - fit5.12` = fit5.11 - fit5.12) v %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;fit5.11&quot;, &quot;fit5.12&quot;, &quot;fit5.11 - fit5.12&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma[epsilon]^2)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Using the full posterior for both models, here is the percent variance in CES-D explained by unemp. v %&gt;% transmute(percent = (fit5.11 - fit5.12) / fit5.11) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 1 × 6 ## percent .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.099 -0.182 0.305 0.95 median qi When you go beyond point estimates and factor in full posterior uncertainty, it becomes clear how fragile ad hoc statistics like this can be. Interpret them with caution. 5.3.2 Allowing the effect of a time-varying predictor to vary over time. “Might unemployment status also affect the trajectory’s slope” (p. 171)? Here’s the statistical model: $$ \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{months}_{ij} \\times \\text{unemp}_{ij} \\big ] \\\\ &amp; \\;\\;\\; + \\big [ \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf \\Omega \\mathbf D&#39; \\end{pmatrix}\\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10}, \\gamma_{20}, \\gamma_{30} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\sigma_1 &amp; \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\ \\rho_{01} &amp; \\sim \\operatorname{LKJ}(4). \\end{align*}\\] $$ Since \\(\\gamma_{30}\\) is an interaction term, it might make sense to give it an ever tighter prior, something like \\(\\text{Normal}(0, 5)\\). Here we’ll just stay wide and loose. fit5.13 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + months + unemp + months:unemp + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 11.9), class = sd), prior(student_t(3, 0, 11.9), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .9), file = &quot;fits/fit05.13&quot;) Here are the results. print(fit5.13) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + months + unemp + months:unemp + (1 + months | id) ## Data: unemployment_pp (Number of observations: 674) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 9.19 0.84 7.51 10.84 1.01 360 663 ## sd(months) 0.56 0.19 0.08 0.86 1.03 203 338 ## cor(Intercept,months) -0.48 0.17 -0.70 -0.04 1.01 1140 640 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.96 1.90 6.24 13.68 1.00 1983 2570 ## months 0.13 0.19 -0.25 0.51 1.00 1936 2523 ## unemp 8.15 1.90 4.42 11.87 1.00 2094 2417 ## months:unemp -0.43 0.22 -0.86 -0.02 1.00 2141 2534 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.10 0.43 7.32 8.94 1.02 285 698 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s how we might make our version of the middle panel of Figure 5.4. nd &lt;- tibble(unemp = rep(1:0, times = c(29, 22)), months = c(seq(from = 0, to = 14, by = .5), seq(from = 3.5, to = 14, by = .5))) f &lt;- fitted(fit5.13, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(label = str_c(&quot;unemp = &quot;, unemp)) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_abline(intercept = fixef(fit5.13)[1, 1], slope = fixef(fit5.13)[2, 1], color = &quot;grey80&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_text(data = f %&gt;% filter(months == 14), aes(label = label, y = Estimate), hjust = -.05) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Main effects of unemp and time&quot;) + coord_cartesian(clip = &quot;off&quot;) + theme(panel.grid = element_blank(), plot.margin = margin(6, 55, 6, 6)) Here’s the posterior for \\(\\gamma_{10}\\). as_draws_df(fit5.13) %&gt;% ggplot(aes(x = b_months, y = 0)) + stat_halfeye(.width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(gamma[1][0], &quot;, the main effect for time&quot;))) + theme(panel.grid = element_blank()) It’s quite uncertain and almost symmetrically straddles the parameter space between -0.5 and 0.5. The next model follows the form $$ \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{unemp}_{ij} \\times \\text{months}_{ij} \\big ] \\\\ &amp; \\;\\;\\; + \\big [ \\zeta_{0i} + \\zeta_{3i} \\text{unemp}_{ij} \\times \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{3i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_3 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{03} \\\\ \\rho_{03} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{20}, \\gamma_{30} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\sigma_3 &amp; \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\ \\rho_{03} &amp; \\sim \\operatorname{LKJ}(4). \\end{align*}\\] $$ Here’s how to fit it with brms::brm(). fit5.14 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + unemp + months:unemp + (1 + months:unemp | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 11.9), class = sd), prior(student_t(3, 0, 11.9), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, control = list(adapt_delta = .95), file = &quot;fits/fit05.14&quot;) It’s easy to miss this if you’re not following along quite carefully with the text, but this model, which corresponds to Equation 5.9 in the text, is NOT Model D. Rather, it’s an intermediary model between Model C and Model D. All this means we can’t compare our results with those in Table 5.7. But here they are, anyway. print(fit5.14) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + unemp + months:unemp + (1 + months:unemp | id) ## Data: unemployment_pp (Number of observations: 674) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 7.99 0.64 6.76 9.32 1.00 1035 1723 ## sd(months:unemp) 0.38 0.21 0.02 0.78 1.01 266 436 ## cor(Intercept,months:unemp) -0.10 0.26 -0.53 0.50 1.00 1225 1737 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 11.17 0.91 9.34 12.86 1.00 2091 2560 ## unemp 6.95 0.93 5.16 8.75 1.00 3167 2620 ## unemp:months -0.30 0.10 -0.50 -0.10 1.00 3810 2771 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.55 0.35 7.87 9.23 1.01 854 1483 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ve already computed the WAIC for fit5.11 and fit5.12. Here we do so for fit5.13 and fit5.14. fit5.13 &lt;- add_criterion(fit5.13, criterion = &quot;waic&quot;) fit5.14 &lt;- add_criterion(fit5.14, criterion = &quot;waic&quot;) loo_compare(fit5.11, fit5.12, fit5.13, fit5.14, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.13 0.0 0.0 -2488.0 21.4 197.9 10.7 4975.9 42.8 ## fit5.12 -3.5 2.0 -2491.5 21.4 196.7 10.7 4983.0 42.9 ## fit5.14 -16.8 3.4 -2504.8 21.8 169.5 9.8 5009.6 43.6 ## fit5.11 -25.8 5.0 -2513.8 21.6 179.9 10.1 5027.5 43.2 model_weights(fit5.11, fit5.12, fit5.13, fit5.14, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.11 fit5.12 fit5.13 fit5.14 ## 0.000 0.028 0.972 0.000 Yep, it appears fit5.14 is not an improvement on fit5.13 (i.e., our analogue to Model C in the text). Here’s our version of Model D: $$ \\[\\begin{align*} \\text{cesd}_{ij} &amp; = \\big [ \\gamma_{00} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{unemp}_{ij} \\times \\text{months}_{ij} \\big ] \\\\ &amp; \\;\\;\\; + \\big [ \\zeta_{0i} + \\zeta_{3i} \\text{unemp}_{ij} \\times \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{2i} \\\\ \\zeta_{3i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{02} &amp; \\rho_{03} \\\\ \\rho_{02} &amp; 1 &amp; \\rho_{23} \\\\ \\rho_{03} &amp; \\rho_{23} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{20}, \\gamma_{30} &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\dots , \\sigma_3 &amp; \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\ \\mathbf\\Omega &amp; \\sim \\operatorname{LKJ}(4). \\end{align*}\\] $$ We’ll call it fit5.15. Here’s the brm() code. fit5.15 &lt;- brm(data = unemployment_pp, family = gaussian, cesd ~ 0 + Intercept + unemp + months:unemp + (1 + unemp + months:unemp | id), prior = c(prior(normal(14.5, 20), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 11.9), class = sd), prior(student_t(3, 0, 11.9), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.15&quot;) print(fit5.15) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: cesd ~ 0 + Intercept + unemp + months:unemp + (1 + unemp + months:unemp | id) ## Data: unemployment_pp (Number of observations: 674) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 254) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 6.85 0.86 5.16 8.61 1.00 1300 1778 ## sd(unemp) 5.06 1.65 1.36 8.09 1.02 227 324 ## sd(unemp:months) 0.67 0.22 0.14 1.03 1.02 309 253 ## cor(Intercept,unemp) 0.22 0.22 -0.20 0.66 1.00 748 1653 ## cor(Intercept,unemp:months) -0.17 0.22 -0.55 0.28 1.00 967 1515 ## cor(unemp,unemp:months) -0.42 0.26 -0.82 0.20 1.01 451 928 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 11.17 0.83 9.49 12.77 1.00 1647 2301 ## unemp 6.91 0.95 5.02 8.78 1.00 2679 2684 ## unemp:months -0.29 0.11 -0.50 -0.07 1.00 3943 3391 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.03 0.42 7.24 8.89 1.02 397 1160 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we can finally make our version of the right panel of Figure 5.4. f &lt;- fitted(fit5.15, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(label = str_c(&quot;unemp = &quot;, unemp)) f %&gt;% ggplot(aes(x = months, group = unemp)) + geom_abline(intercept = fixef(fit5.15)[1, 1], slope = 0, color = &quot;grey80&quot;, linetype = 2) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey67&quot;, alpha = 1/2) + geom_line(aes(y = Estimate)) + geom_text(data = f %&gt;% filter(months == 14), aes(label = label, y = Estimate), hjust = -.05) + scale_x_continuous(&quot;Months since job loss&quot;, breaks = seq(from = 0, to = 14, by = 2)) + scale_y_continuous(&quot;CES-D&quot;, limits = c(5, 20)) + labs(subtitle = &quot;Constraining the effects time\\namong the re-employed&quot;) + coord_cartesian(clip = &quot;off&quot;) + theme(panel.grid = element_blank(), plot.margin = margin(6, 55, 6, 6)) By carefully using filter(), we can extract the posterior for summary the expected CES-D value for “the average unemployed person in the population”, “immediately upon layoff” (p. 173). f %&gt;% filter(unemp == 1 &amp; months == 0) ## Estimate Est.Error Q2.5 Q97.5 unemp months label ## 1 18.07945 0.7779 16.53265 19.59696 1 0 unemp = 1 To get the decline rate per month, just use fixef() and subset. fixef(fit5.15)[&quot;unemp:months&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## -0.28751601 0.10972618 -0.50487306 -0.07413347 How much lower, on average, are “CES-D scores among those who find a job” right after layoff (p. 173)? Again, just use fixef(). fixef(fit5.15)[&quot;unemp&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 6.9109977 0.9536524 5.0201413 8.7795774 But if we’d like to get the posterior for the difference at 12 months later, we’ll need to go back to fitted(). nd &lt;- tibble(unemp = 1:0, months = 12) fitted(fit5.15, newdata = nd, re_formula = NA, summary = F) %&gt;% data.frame() %&gt;% set_names(str_c(c(&quot;un&quot;, &quot;&quot;), &quot;employed_cesd_at_12&quot;)) %&gt;% transmute(difference = unemployed_cesd_at_12 - employed_cesd_at_12) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 1 × 6 ## difference .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3.48 0.9 6.05 0.95 median qi There’s more posterior uncertainty, there, that you might expect from simply using the point estimates. Always beware the posterior uncertainty. We may as well finish off with a little WAIC. fit5.15 &lt;- add_criterion(fit5.15, criterion = &quot;waic&quot;) loo_compare(fit5.11, fit5.12, fit5.13, fit5.14, fit5.15, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.15 0.0 0.0 -2486.2 21.6 203.1 11.0 4972.5 43.2 ## fit5.13 -1.7 3.7 -2488.0 21.4 197.9 10.7 4975.9 42.8 ## fit5.12 -5.3 3.6 -2491.5 21.4 196.7 10.7 4983.0 42.9 ## fit5.14 -18.5 4.0 -2504.8 21.8 169.5 9.8 5009.6 43.6 ## fit5.11 -27.5 6.8 -2513.8 21.6 179.9 10.1 5027.5 43.2 model_weights(fit5.11, fit5.12, fit5.13, fit5.14, fit5.15, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.11 fit5.12 fit5.13 fit5.14 fit5.15 ## 0.000 0.004 0.151 0.000 0.845 fit5.15, fit5.13, and fit5.12 are all very close, with a modest edge for fit5.15. 5.3.3 Recentering time-varying predictors. Back to the wages_pp data! Here’s the generic statistical model we’ll be fooling with: \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{01} (\\text{hgc}_i - 9) + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij}. \\end{align*} \\] We will fit the model with three versions of uerate. If you execute head(wages_pp), you’ll discover they’re already in the data. But it might be worth walking out how to compute those variables. First, centering uerate at 7 is easy enough. Just subtract. wages_pp &lt;- wages_pp %&gt;% mutate(uerate_7 = uerate - 7) Continuing on with the same priors from before, here’s how to fit the new model, our version of Model A on page 175. fit5.16 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.16&quot;) print(fit5.16, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.224 0.010 0.204 0.245 1.002 1704 2884 ## sd(exper) 0.040 0.003 0.035 0.046 1.004 605 917 ## cor(Intercept,exper) -0.306 0.068 -0.432 -0.165 1.004 713 1634 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.749 0.011 1.727 1.772 1.000 2856 3207 ## exper 0.044 0.003 0.039 0.049 1.000 2694 3522 ## hgc_9 0.040 0.006 0.027 0.052 1.002 2100 3469 ## uerate_7 -0.012 0.002 -0.016 -0.008 1.001 5116 4125 ## exper:black -0.018 0.005 -0.027 -0.009 1.001 2338 2587 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.314 1.001 3798 3478 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). For the next model, we add a couple variables to the wages_pp data. wages_pp &lt;- wages_pp %&gt;% group_by(id) %&gt;% mutate(uerate_id_mu = mean(uerate)) %&gt;% ungroup() %&gt;% mutate(uerate_id_dev = uerate - uerate_id_mu) In the original data set, these were the ue.mean and ue.person.cen variables, respectively. Here’s how to fit the model. fit5.17 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_mu + uerate_id_dev + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.17&quot;) print(fit5.17, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_mu + uerate_id_dev + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.225 0.011 0.204 0.247 1.001 1905 3162 ## sd(exper) 0.040 0.003 0.035 0.045 1.001 824 1694 ## cor(Intercept,exper) -0.313 0.067 -0.438 -0.175 1.001 946 2238 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.873 0.030 1.816 1.931 1.000 2772 3291 ## exper 0.045 0.003 0.040 0.050 1.000 3163 2937 ## hgc_9 0.040 0.006 0.028 0.053 1.000 3320 3476 ## uerate_id_mu -0.018 0.004 -0.025 -0.011 1.000 2769 3374 ## uerate_id_dev -0.010 0.002 -0.014 -0.006 1.000 5223 3588 ## exper:black -0.019 0.005 -0.028 -0.010 1.000 3622 3579 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.314 1.000 3930 3647 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Within the tidyverse, probably the easiest way to center on the first value for each id is to first group_by(id) and then make use of the dplyr::first() function, which you can learn more about here. wages_pp &lt;- wages_pp %&gt;% group_by(id) %&gt;% mutate(uerate_id_1 = first(uerate)) %&gt;% ungroup() %&gt;% mutate(uerate_id_1_dev = uerate - uerate_id_1) In the original data set, these were the ue1 and ue.centert1 variables, respectively. Here’s how to fit the updated model. fit5.18 &lt;- update(fit5.16, newdata = wages_pp, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_1 + uerate_id_1_dev + (1 + exper | id), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.18&quot;) print(fit5.18, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ Intercept + exper + hgc_9 + uerate_id_1 + uerate_id_1_dev + (1 + exper | id) + exper:black - 1 ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.224 0.010 0.203 0.244 1.000 2164 3163 ## sd(exper) 0.040 0.003 0.035 0.046 1.005 725 1480 ## cor(Intercept,exper) -0.301 0.069 -0.427 -0.157 1.003 852 1844 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.869 0.026 1.819 1.920 1.000 3281 3378 ## exper 0.045 0.003 0.040 0.050 1.000 3305 3497 ## hgc_9 0.040 0.006 0.028 0.052 1.000 3620 3471 ## uerate_id_1 -0.016 0.003 -0.021 -0.011 1.000 3166 3483 ## uerate_id_1_dev -0.010 0.002 -0.014 -0.006 1.000 6928 3958 ## exper:black -0.018 0.004 -0.027 -0.010 1.000 3356 3681 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.314 1.001 4657 3655 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are the WAIC comparisons. fit5.16 &lt;- add_criterion(fit5.16, criterion = &quot;waic&quot;) fit5.17 &lt;- add_criterion(fit5.17, criterion = &quot;waic&quot;) fit5.18 &lt;- add_criterion(fit5.18, criterion = &quot;waic&quot;) loo_compare(fit5.16, fit5.17, fit5.18, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.18 0.0 0.0 -2034.9 104.2 862.7 26.9 4069.8 208.4 ## fit5.16 -2.7 1.9 -2037.6 104.2 864.8 27.0 4075.1 208.4 ## fit5.17 -3.2 1.6 -2038.1 104.3 865.5 27.2 4076.2 208.5 model_weights(fit5.16, fit5.17, fit5.18, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.16 fit5.17 fit5.18 ## 0.063 0.036 0.901 Like in the text, these three models are all close with respect to the WAIC. Based on their WAIC weights, fit5.18 (Model C) and fit5.16 (Model A) seen the be the best depictions of the data. 5.3.4 An important caveat: The problem of reciprocal causation. In this section, Singer and Willett gave a typology for time-varying covariates. A variable is defined if, “in advance to data collection, its values are predetermined for everyone under study” (p. 177). Examples are time and the seasons. A variable is ancillary if “its values cannot be influenced by study participants because they are determined by a stochastic process totally external to them” (p. 178). Within the context of a study on people within monogamous relationships, the availability of potential mates within the region would be an example. A variable is contextual if “it describes an ‘external’ stochastic process, but the connection between units is closer–between husbands and wives, parents and children, teachers and students, employers and employees. Because of this proximity, contextual predictors can be influenced by an individual’s contemporaneous outcome values; if so, they are susceptible to issues of reciprocal causation” (p. 179). A variable is internal if it describes an “individual’s potentially changeable status over time” (p. 179). Examples include mood, psychiatric syndromes, and employment status. 5.4 Recentering the effect of TIME Our version of the data from Tomarken, Shelton, Elkins, and Anderson (1997) is saved as medication_pp.csv. medication_pp &lt;- read_csv(&quot;data/medication_pp.csv&quot;) glimpse(medication_pp) ## Rows: 1,242 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10… ## $ treat &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ wave &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 2, 3… ## $ day &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 0, 0, 0, 1, 1, 1, … ## $ time.of.day &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0… ## $ time &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 1.0000000, 1.3333333, 1.6666667, 2.0000000, 2… ## $ time333 &lt;dbl&gt; -3.3333333, -3.0000000, -2.6666667, -2.3333333, -2.0000000, -1.6666667, -1.333… ## $ time667 &lt;dbl&gt; -6.6666667, -6.3333333, -6.0000000, -5.6666667, -5.3333333, -5.0000000, -4.666… ## $ initial &lt;dbl&gt; 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, … ## $ final &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, … ## $ pos &lt;dbl&gt; 106.6667, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000… The medication_pp data do not come with a reading variable, as in Table 5.9. Here we’ll make one and display the time variables highlighted in Table 5.9. medication_pp &lt;- medication_pp %&gt;% mutate(reading = ifelse(time.of.day == 0, &quot;8 am&quot;, ifelse(time.of.day &lt; .6, &quot;3 pm&quot;, &quot;10 pm&quot;))) medication_pp %&gt;% select(wave, day, reading, time.of.day:time667) %&gt;% head(n = 10) ## # A tibble: 10 × 7 ## wave day reading time.of.day time time333 time667 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 8 am 0 0 -3.33 -6.67 ## 2 2 0 3 pm 0.333 0.333 -3 -6.33 ## 3 3 0 10 pm 0.667 0.667 -2.67 -6 ## 4 4 1 8 am 0 1 -2.33 -5.67 ## 5 5 1 3 pm 0.333 1.33 -2 -5.33 ## 6 6 1 10 pm 0.667 1.67 -1.67 -5 ## 7 7 2 8 am 0 2 -1.33 -4.67 ## 8 8 2 3 pm 0.333 2.33 -1 -4.33 ## 9 9 2 10 pm 0.667 2.67 -0.667 -4 ## 10 10 3 8 am 0 3 -0.333 -3.67 To help get a sense of the balance in the data, here is a bar plot of the distribution of the numbers of measurement occasions within participants. It’s color coded by treatment status. medication_pp %&gt;% mutate(treat = str_c(&quot;treat = &quot;, treat)) %&gt;% group_by(treat, id) %&gt;% count() %&gt;% ggplot(aes(y = n, fill = treat)) + geom_bar() + scale_y_continuous(breaks = c(2,12, 15:21)) + scale_fill_viridis_d(NULL, end = .8) + labs(x = &quot;count of cases&quot;, y = &quot;# measurement occasions&quot;) + theme(panel.grid = element_blank()) For this section, our basic model will be \\[ \\begin{align*} \\text{pos}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} (\\text{time}_{ij} - c) + \\epsilon_{ij} \\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{treat}_i + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{treat}_i + \\zeta_{1i}, \\end{align*} \\] where \\(c\\) is a generic constant. For our three variants of \\(\\text{time}_{ij}\\), \\(c\\) will be 0 for time, 3.3333333 for time333, and 6.666667 for time667. Our criterion variable is pos, positive mood rating. The text told us these were from “a package of mood diaries (which use a five-point scale to assess positive and negative moods)” (p. 182). However, we don’t know what numerals were assigned to the points on the scale, we don’t know how many items were used, and we don’t even know whether the items were taken from an existing questionnaire. And unfortunately, the citation Singer and Willett gave for the study is from a conference presentation, making it a pain to track down background information on the internet. In such a situation, it’s difficult to figure out how to set our priors. Though suboptimal, we might first get a sense of the pos data with a histogram. medication_pp %&gt;% ggplot(aes(x = pos)) + geom_histogram(binwidth = 10) + theme(panel.grid = element_blank()) Here’s the range(). range(medication_pp$pos) ## [1] 100.0000 466.6667 Starting with the prior for our intercept, I think there’s a lot of room for argument, here. To keep with our weakly-regularizing approach to priors, it might make sense to use something like normal(200, 100). But then again, we know something about the study design. At the beginning, participants were in treatment for depression, so we’d expect the starting point to be closer to the lower end of the scale. In that case, we might update our approach to something like normal(150, 50). Feel free to play with alternatives. What I hope this illustrates is that our task would be much easier with more domain knowledge. Anyway, given the scale of the data, weakly-regularizing priors for the predictor variables might take the form of something like normal(0, 25). We’ll use student_t(0, 50) on the \\(\\sigma\\)s and stay steady with lkj(4) on \\(\\rho_{01}\\). Here we fit the model with all three versions of time. fit5.19 &lt;- brm(data = medication_pp, family = gaussian, pos ~ 0 + Intercept + time + treat + time:treat + (1 + time | id), prior = c(prior(normal(150, 50), class = b, coef = Intercept), prior(normal(0, 25), class = b), prior(student_t(3, 0, 50), class = sd), prior(student_t(3, 0, 50), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.19&quot;) fit5.20 &lt;- update(fit5.19, newdata = medication_pp, pos ~ 0 + Intercept + time333 + treat + time333:treat + (1 + time333 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.20&quot;) fit5.21 &lt;- update(fit5.19, newdata = medication_pp, pos ~ 0 + Intercept + time667 + treat + time667:treat + (1 + time667 | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.21&quot;) Given the size of our posterior standard deviations, our \\(\\gamma\\) posteriors are quite comparable to the point estimates (and their standard errors) in the text. print(fit5.19) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ 0 + Intercept + time + treat + time:treat + (1 + time | id) ## Data: medication_pp (Number of observations: 1242) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 46.69 4.64 38.50 56.88 1.00 1194 2131 ## sd(time) 8.25 0.92 6.56 10.24 1.00 1491 2708 ## cor(Intercept,time) -0.28 0.13 -0.52 -0.02 1.00 1366 2256 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 166.38 8.99 148.76 183.75 1.00 869 1394 ## time -2.33 1.78 -5.91 1.06 1.00 1152 2001 ## treat -2.11 11.35 -24.39 20.13 1.01 954 1540 ## time:treat 5.50 2.35 0.72 10.10 1.00 1098 1762 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.13 0.75 33.74 36.65 1.00 6220 2927 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Given the scales we’re working with, it’s difficult to compare our \\(\\sigma\\) summaries with the \\(\\sigma^2\\) summaries in Table 5.10 of the text. Here we convert and re-summarize. v &lt;- as_draws_df(fit5.19) %&gt;% transmute(sigma_2_0 = sd_id__Intercept^2, sigma_2_1 = sd_id__time^2, sigma_2_epsilon = sigma^2) v %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi() %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 3 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma_2_0 2150. 1482. 3236. 0.95 median qi ## 2 sigma_2_1 67.3 43.0 105. 0.95 median qi ## 3 sigma_2_epsilon 1234. 1138. 1343. 0.95 median qi Turns out the posterior medians are quite similar to the ML estimates in the text. Here’s what the entire distributions looks like. v %&gt;% set_names(&quot;sigma[0]^2&quot;, &quot;sigma[1]^2&quot;, &quot;sigma[epsilon]^2&quot;) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.text = element_text(size = 11)) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) Here’s our version of Figure 5.5. nd &lt;- crossing(treat = 0:1, time = seq(from = 0, to = 7, length.out = 30)) text &lt;- tibble(treat = 0:1, time = 4, y = c(135, 197), label = c(&quot;control&quot;, &quot;treatment&quot;), angle = c(350, 15)) fitted(fit5.19, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = time, fill = treat, color = treat, group = treat)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/3, size = 0) + geom_line(aes(y = Estimate)) + geom_text(data = text, aes(y = y, label = label, angle = angle)) + scale_fill_viridis_c(option = &quot;A&quot;, begin = .3, end = .6) + scale_color_viridis_c(option = &quot;A&quot;, begin = .3, end = .6) + labs(x = &quot;days&quot;, y = &quot;pos&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Although we still have clear evidence of an interaction, adding those 95% intervals makes it look less impressive, doesn’t it? Here are the summaries for the models with the alternative versions of \\((\\text{time}_{ij} - c)\\). print(fit5.20) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ Intercept + time333 + treat + (1 + time333 | id) + time333:treat - 1 ## Data: medication_pp (Number of observations: 1242) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 46.05 4.09 38.82 55.01 1.00 463 995 ## sd(time333) 8.34 0.96 6.66 10.47 1.00 881 1295 ## cor(Intercept,time333) 0.21 0.13 -0.05 0.45 1.00 960 1723 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 160.93 8.17 144.87 176.89 1.02 167 453 ## time333 -2.34 1.80 -5.87 1.22 1.01 662 998 ## treat 12.39 10.52 -9.31 32.20 1.01 287 513 ## time333:treat 5.43 2.34 0.82 9.95 1.00 677 1351 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.10 0.74 33.67 36.59 1.00 3052 2702 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit5.21) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ Intercept + time667 + treat + (1 + time667 | id) + time667:treat - 1 ## Data: medication_pp (Number of observations: 1242) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 57.79 5.52 48.11 69.72 1.00 1148 1877 ## sd(time667) 8.07 0.93 6.46 10.07 1.00 1381 2117 ## cor(Intercept,time667) 0.59 0.09 0.38 0.75 1.00 1826 2395 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 156.86 10.65 136.54 178.07 1.00 798 1505 ## time667 -1.94 1.71 -5.18 1.48 1.00 1261 2008 ## treat 24.16 13.37 -1.95 50.42 1.00 904 1478 ## time667:treat 4.67 2.23 0.29 9.03 1.00 1123 2168 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.12 0.72 33.70 36.56 1.00 6251 3114 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It might be easier to compare the \\(\\gamma\\) summaries across models with a well-designed coefficient plot. Here’s an attempt. tibble(name = str_c(&quot;fit5.&quot;, 19:21)) %&gt;% mutate(fixef = map(name, ~ get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(fixef) %&gt;% mutate(gamma = rep(c(&quot;gamma[0][0]&quot;, &quot;gamma[1][0]&quot;, &quot;gamma[0][1]&quot;, &quot;gamma[1][1]&quot;), times = 3)) %&gt;% ggplot(aes(x = name, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_pointrange() + xlab(NULL) + coord_flip() + theme(axis.ticks.y = element_blank(), panel.grid = element_blank(), strip.text = element_text(size = 11)) + facet_wrap(~ gamma, scales = &quot;free_x&quot;, labeller = label_parsed, ncol = 4) Since we’re juggling less information, we might compare the posteriors for \\(\\sigma_1^2\\) across the three models with good old tidybayes::geom_halfeyeh(). tibble(name = str_c(&quot;fit5.&quot;, 19:21)) %&gt;% mutate(fit = map(name, get)) %&gt;% mutate(sigma_2_1 = map(fit, ~ VarCorr(., summary = F)[[1]][[1]][, 1]^2 %&gt;% data.frame() %&gt;% set_names(&quot;sigma_2_1&quot;))) %&gt;% unnest(sigma_2_1) %&gt;% ggplot(aes(x = sigma_2_1, y = name)) + stat_halfeye(.width = c(.5, .95)) + scale_x_continuous(expression(sigma[1]^2), limits = c(0, NA)) + ylab(NULL) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) For kicks and giggles, we marked off both 50% and 95% intervals for each. Here’s the same for \\(\\rho_{01}\\). tibble(name = str_c(&quot;fit5.&quot;, 19:21)) %&gt;% mutate(fit = map(name, get)) %&gt;% mutate(rho = map(fit, ~ VarCorr(., summary = F)[[1]][[2]][, 2, &quot;Intercept&quot;] %&gt;% data.frame() %&gt;% set_names(&quot;rho&quot;))) %&gt;% unnest(rho) %&gt;% ggplot(aes(x = rho, y = name)) + stat_halfeye(.width = c(.5, .95)) + labs(x = expression(rho[0][1]), y = NULL) + coord_cartesian(xlim = c(-1, 1)) + theme(axis.ticks.y = element_blank(), panel.grid = element_blank()) “As Rogosa and Willett (1985) demonstrate, you can always alter the correlation between the level-1 growth parameters simply by changing the centering constant” (p. 186). Here are the WAIC comparisons. fit5.19 &lt;- add_criterion(fit5.19, criterion = &quot;waic&quot;) fit5.20 &lt;- add_criterion(fit5.20, criterion = &quot;waic&quot;) fit5.21 &lt;- add_criterion(fit5.21, criterion = &quot;waic&quot;) loo_compare(fit5.19, fit5.20, fit5.21, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.21 0.0 0.0 -6242.1 41.4 110.2 7.5 12484.2 82.8 ## fit5.19 -0.4 0.7 -6242.5 41.5 110.2 7.6 12484.9 83.0 ## fit5.20 -0.5 0.5 -6242.6 41.5 111.4 7.7 12485.2 83.0 model_weights(fit5.19, fit5.20, fit5.21, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.19 fit5.20 fit5.21 ## 0.297 0.265 0.437 As one might hope, not much going on. As Singer and Willett alluded to in the text (p. 187), the final model in this chapter is an odd one. Do note the initial and final columns in the data. glimpse(medication_pp) ## Rows: 1,242 ## Columns: 12 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10… ## $ treat &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ wave &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 2, 3… ## $ day &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 0, 0, 0, 1, 1, 1, … ## $ time.of.day &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0… ## $ time &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 1.0000000, 1.3333333, 1.6666667, 2.0000000, 2… ## $ time333 &lt;dbl&gt; -3.3333333, -3.0000000, -2.6666667, -2.3333333, -2.0000000, -1.6666667, -1.333… ## $ time667 &lt;dbl&gt; -6.6666667, -6.3333333, -6.0000000, -5.6666667, -5.3333333, -5.0000000, -4.666… ## $ initial &lt;dbl&gt; 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, … ## $ final &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, … ## $ pos &lt;dbl&gt; 106.6667, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000… ## $ reading &lt;chr&gt; &quot;8 am&quot;, &quot;3 pm&quot;, &quot;10 pm&quot;, &quot;8 am&quot;, &quot;3 pm&quot;, &quot;10 pm&quot;, &quot;8 am&quot;, &quot;3 pm&quot;, &quot;10 pm&quot;, &quot;8 … With this model \\[ \\begin{align*} \\text{pos}_{ij} &amp; = \\pi_{0i} \\bigg (\\frac{6.67 - \\text{time}_{ij}}{6.67} \\bigg ) + \\pi_{1i} \\bigg (\\frac {\\text{time}_{ij}}{6.67} \\bigg ) + \\epsilon_{ij} \\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\text{treat}_i + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\text{treat}_i + \\zeta_{1i}. \\end{align*} \\] Because of the parameterization, we’ll use both variables simultaneously to indicate time. In the code, below, you’ll notice we no longer have an intercept parameter. Rather, we just have initial and final. As such, both of those parameters get the same prior we’ve been using for the intercept in the previous models. fit5.22 &lt;- brm(data = medication_pp, family = gaussian, pos ~ 0 + initial + final + initial:treat + final:treat + (0 + initial + final | id), prior = c(prior(normal(150, 50), class = b, coef = initial), prior(normal(150, 50), class = b, coef = final), prior(normal(0, 25), class = b), prior(student_t(3, 0, 50), class = sd), prior(student_t(3, 0, 50), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/fit05.22&quot;) Our results are quite similar to those in the text. print(fit5.22) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pos ~ 0 + initial + final + initial:treat + final:treat + (0 + initial + final | id) ## Data: medication_pp (Number of observations: 1242) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 64) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(initial) 47.20 4.73 38.80 57.12 1.00 1774 2418 ## sd(final) 58.82 5.57 48.81 70.63 1.00 1853 2775 ## cor(initial,final) 0.42 0.11 0.19 0.62 1.00 2054 2506 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## initial 167.72 8.87 149.84 185.14 1.00 1456 2191 ## final 156.04 10.59 135.71 177.00 1.00 2510 2772 ## initial:treat -3.84 11.04 -25.04 17.96 1.00 1937 2171 ## final:treat 25.33 12.90 -0.15 50.89 1.00 2915 2966 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.11 0.76 33.62 36.60 1.00 4710 2619 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s finish up with the WAIC. fit5.22 &lt;- add_criterion(fit5.22, criterion = &quot;waic&quot;) loo_compare(fit5.19, fit5.20, fit5.21, fit5.22, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit5.22 0.0 0.0 -6242.1 41.5 110.8 7.6 12484.1 83.0 ## fit5.21 0.0 0.7 -6242.1 41.4 110.2 7.5 12484.2 82.8 ## fit5.19 -0.4 0.6 -6242.5 41.5 110.2 7.6 12484.9 83.0 ## fit5.20 -0.5 0.4 -6242.6 41.5 111.4 7.7 12485.2 83.0 model_weights(fit5.19, fit5.20, fit5.21, fit5.22, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit5.19 fit5.20 fit5.21 fit5.22 ## 0.206 0.183 0.302 0.309 All about the same. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] posterior_1.4.1 bayesplot_1.10.0 ggdist_3.3.0 tidybayes_3.0.4 ## [5] brms_2.19.0 Rcpp_1.0.10 cmdstanr_0.5.3 rstan_2.21.8 ## [9] StanHeaders_2.26.25 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 ## [13] dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [17] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] svUnit_1.0.6 shinythemes_1.2.0 splines_4.3.0 ## [4] later_1.3.1 gamm4_0.2-6 xts_0.13.1 ## [7] lifecycle_1.0.3 processx_3.8.1 lattice_0.21-8 ## [10] vroom_1.6.3 MASS_7.3-58.4 crosstalk_1.2.0 ## [13] backports_1.4.1 magrittr_2.0.3 sass_0.4.6 ## [16] rmarkdown_2.21 jquerylib_0.1.4 httpuv_1.6.11 ## [19] zip_2.3.0 askpass_1.1 pkgbuild_1.4.0 ## [22] minqa_1.2.5 multcomp_1.4-23 abind_1.4-5 ## [25] TH.data_1.1-2 tensorA_0.36.2 sandwich_3.0-2 ## [28] gdtools_0.3.3 inline_0.3.19 crul_1.4.0 ## [31] xslt_1.4.4 bridgesampling_1.1-2 codetools_0.2-19 ## [34] DT_0.27 xml2_1.3.4 tidyselect_1.2.0 ## [37] shape_1.4.6 httpcode_0.3.0 farver_2.1.1 ## [40] lme4_1.1-33 matrixStats_0.63.0 stats4_4.3.0 ## [43] base64enc_0.1-3 jsonlite_1.8.4 ellipsis_0.3.2 ## [46] survival_3.5-5 emmeans_1.8.6 systemfonts_1.0.4 ## [49] projpred_2.5.0 tools_4.3.0 ragg_1.2.5 ## [52] glue_1.6.2 gridExtra_2.3 xfun_0.39 ## [55] mgcv_1.8-42 distributional_0.3.2 loo_2.6.0 ## [58] withr_2.5.0 fastmap_1.1.1 boot_1.3-28.1 ## [61] fansi_1.0.4 shinyjs_2.1.0 openssl_2.0.6 ## [64] callr_3.7.3 digest_0.6.31 timechange_0.2.0 ## [67] R6_2.5.1 mime_0.12 estimability_1.4.1 ## [70] textshaping_0.3.6 colorspace_2.1-0 gtools_3.9.4 ## [73] markdown_1.7 threejs_0.3.3 utf8_1.2.3 ## [76] generics_0.1.3 fontLiberation_0.1.0 data.table_1.14.8 ## [79] prettyunits_1.1.1 htmlwidgets_1.6.2 pkgconfig_2.0.3 ## [82] dygraphs_1.1.1.6 gtable_0.3.3 htmltools_0.5.5 ## [85] fontBitstreamVera_0.1.1 bookdown_0.34 scales_1.2.1 ## [88] knitr_1.42 rstudioapi_0.14 tzdb_0.4.0 ## [91] reshape2_1.4.4 uuid_1.1-0 coda_0.19-4 ## [94] checkmate_2.2.0 nlme_3.1-162 curl_5.0.0 ## [97] nloptr_2.0.3 cachem_1.0.8 zoo_1.8-12 ## [100] flextable_0.9.1 miniUI_0.1.1.1 pillar_1.9.0 ## [103] grid_4.3.0 vctrs_0.6.2 shinystan_2.6.0 ## [106] promises_1.2.0.1 arrayhelpers_1.1-0 xtable_1.8-4 ## [109] evaluate_0.21 mvtnorm_1.1-3 cli_3.6.1 ## [112] compiler_4.3.0 rlang_1.1.1 crayon_1.5.2 ## [115] rstantools_2.3.1 labeling_0.4.2 ps_1.7.5 ## [118] plyr_1.8.8 stringi_1.7.12 viridisLite_0.4.2 ## [121] munsell_0.5.0 colourpicker_1.2.0 V8_4.3.0 ## [124] Brobdingnag_1.2-9 fontquiver_0.2.1 Matrix_1.5-4 ## [127] hms_1.1.3 bit64_4.0.5 gfonts_0.2.0 ## [130] shiny_1.7.4 highr_0.10 igraph_1.4.2 ## [133] RcppParallel_5.1.7 bslib_0.4.2 bit_4.0.5 ## [136] katex_1.4.1 officer_0.6.2 equatags_0.2.0 References Brown, D. R., &amp; Gary, L. E. (1985). Predictors of depressive symptoms among unemployed Black adults. Journal of Sociology and Social Welfare, 12, 736. https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=1721&amp;amp=&amp;context=jssw&amp;amp=&amp;sei-redir=1&amp;referer=https%253A%252F%252Fscholar.google.com%252Fscholar%253Fq%253D%252522CES-D%252522%252Bunemployment%2526hl%253Den%2526as_sdt%253D0%25252C44%2526as_ylo%253D1977%2526as_yhi%253D2000#search=%22CES-D%20unemployment%22 Bürkner, P.-C. (2020). Bayesian item response modeling in R with brms and Stan. arXiv:1905.09501 [Stat]. http://arxiv.org/abs/1905.09501 Bürkner, P.-C. (2021b). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html Bürkner, P.-C., Gabry, J., Kay, M., &amp; Vehtari, A. (2022). posterior: Tools for working with posterior distributions. https://CRAN.R-project.org/package=posterior Chung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., &amp; Liu, J. (2013). A nondegenerate penalized likelihood estimator for variance parameters in multilevel models. Psychometrika, 78(4), 685–709. https://doi.org/10.1007/s11336-013-9328-2 Enders, C. K. (2010). Applied missing data analysis. Guilford Press. http://www.appliedmissingdata.com/ Gabry, J., &amp; Mahr, T. (2022). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot Gabry, J., &amp; Modrák, M. (2020). Visual MCMC diagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Ginexi, E. M., Howe, G. W., &amp; Caplan, R. D. (2000). Depression and control beliefs in relation to reemployment: What are the directions of effect? Journal of Occupational Health Psychology, 5(3), 323–336. https://doi.org/10.1037/1076-8998.5.3.323 Kay, M. (2023). tidybayes: Tidy data and ’geoms’ for Bayesian models. https://CRAN.R-project.org/package=tidybayes Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Li, H., &amp; Lahiri, P. (2010). An adjusted maximum likelihood method for solving small area estimation problems. Journal of Multivariate Analysis, 101(4), 882–892. https://doi.org/10.1016/j.jmva.2009.10.009 Little, R. J. (1995). Modeling the drop-out mechanism in repeated-measures studies. Journal of the American Statistical Association, 90(431), 1112–1121. https://doi.org/10.1080/01621459.1995.10476615 Little, R. J. A., &amp; Rubin, D., B. (1987). Statistical analysis with missing data. Wiley. Little, R. J., &amp; Rubin, D. B. (2019). Statistical analysis with missing data (third, Vol. 793). John Wiley &amp; Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798 McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2020b). rethinking R package. https://xcelab.net/rm/software/ Morris, C., &amp; Tang, R. (2011). Estimating random effects via adjustment for density maximization. Statistical Science, 26(2), 271–287. https://doi.org/10.1214/10-STS349 Radloff, L. S. (1977). The CES-D Scale: A self-report depression scale for research in the general population. Applied Psychological Measurement, 1(3), 385–401. https://doi.org/10.1177/014662167700100306 Rogosa, D. R., &amp; Willett, J. B. (1985). Understanding correlates of change by modeling individual differences in growth. Psychometrika, 50(2), 203–228. https://doi.org/10.1007/BF02294247 Schafer, J. L. (1997). Analysis of incomplete multivariate data. CRC press. https://www.routledge.com/Analysis-of-Incomplete-Multivariate-Data/Schafer/p/book/9780412040610 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Tomarken, A., Shelton, R., Elkins, L., &amp; Anderson, T. (1997). Sleep deprivation and anti-depressant medication: Unique effects on positive and negative affect. American Psychological Society Meeting, Washington, DC. van Buuren, S. (2018). Flexible imputation of missing data (Second Edition). CRC Press. https://stefvanbuuren.name/fimd/ Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved \\(\\widehat{R}\\) for assessing convergence of MCMC. arXiv Preprint arXiv:1903.08008. https://arxiv.org/abs/1903.08008? "],["modeling-discontinuous-and-nonlinear-change.html", "6 Modeling Discontinuous and Nonlinear Change 6.1 Discontinuous individual change 6.2 Using transformations to model nonlinear individual change 6.3 Representing individual change using a polynomial function of TIME 6.4 Truly nonlinear trajectories 6.5 Bonus: The logistic growth model Session info Footnote", " 6 Modeling Discontinuous and Nonlinear Change All the multilevel models for change presented so far assume that individual growth is smooth and linear. Yet individual change can also be discontinuous or nonlinear… In this chapter, we introduce strategies for fitting models in which individual change is explicitly discontinuous or nonlinear. Rather than view these patterns as inconveniences, we treat them as substantively compelling opportunities. In doing so, we broaden our questions about the nature of change beyond the basic concepts of initial status and rate of change to a consideration of acceleration, deceleration, turning points, shifts, and asymptotes. The strategies that we use fall into two broad classes. Empirical strategies that let the “data speak for themselves.” Under this approach, you inspect observed growth records systematically and identify a transformation of the outcome, or of TIME, that linearizes the individual change trajectory. Unfortunately, this approach can lead to interpretive difficulties, especially if it involves esoteric transformations or higher order polynomials. Under rational strategies, on the other hand, you use theory to hypothesize a substantively meaningful functional form for the individual change trajectory. Although rational strategies generally yield clearer interpretations, their dependence on good theory makes them somewhat more difficult to develop and apply. (Singer &amp; Willett, 2003, pp. 189–190, emphasis in the original) 6.1 Discontinuous individual change Not all individual change trajectories are continuous functions of time… If you have reason to believe that individual change trajectories might shift in elevation and/or slope, your level-1 model should reflect this hypothesis. Doing so allows you to test ideas about how the trajectory’s shape might change over time… To postulate a discontinuous individual change trajectory, you need to know not just why the shift might occur but also when. This is because your level-1 individual growth model must include one (or more) time-varying predictor(s) that specify whether and if so, when each person experiences the hypothesized shift. (pp. 190–191, emphasis in the original) 6.1.1 Alternative discontinuous level-1 models for change. To postulate a discontinuous level-1 individual growth model, you must first decide on its functional form. Although you can begin empirically, we prefer to focus on substance and the longitudinal process that gave rise to the data. What kind of discontinuity might the precipitating event create? What would a plausible level-1 trajectory look like? Before parameterizing models and constructing variables, we suggest that you: (1) take a pen and paper and sketch some options; and (2) articulate–in words, not equations–the rationale for each. We recommend these steps because, as we demonstrate, the easiest models to specify may not display the type of discontinuity you expect to find. (pp. 191–192) I’ll leave the pen and paper scribbling to you. Here we load the wages_pp.csv data. library(tidyverse) wages_pp &lt;- read_csv(&quot;data/wages_pp.csv&quot;) glimpse(wages_pp) ## Rows: 6,402 ## Columns: 15 ## $ id &lt;dbl&gt; 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 53, 53, 53, 53… ## $ lnw &lt;dbl&gt; 1.491, 1.433, 1.469, 1.749, 1.931, 1.709, 2.086, 2.129, 1.982, 1.798, 2.256, 2.573, 1.… ## $ exper &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040, 3.021, 4.… ## $ ged &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,… ## $ postexp &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040, 3.021, 4.… ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ hispanic &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,… ## $ hgc &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, … ## $ hgc.9 &lt;dbl&gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -2, -2, -2, -2, -2, … ## $ uerate &lt;dbl&gt; 3.215, 3.215, 3.215, 3.295, 2.895, 2.495, 2.595, 4.795, 4.895, 7.400, 7.400, 5.295, 4.… ## $ ue.7 &lt;dbl&gt; -3.785, -3.785, -3.785, -3.705, -4.105, -4.505, -4.405, -2.205, -2.105, 0.400, 0.400, … ## $ ue.centert1 &lt;dbl&gt; 0.000, 0.000, 0.000, 0.080, -0.320, -0.720, -0.620, 1.580, 0.000, 2.505, 2.505, 0.400,… ## $ ue.mean &lt;dbl&gt; 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 3.2150, 5.0965, 5.0965, 5.0965… ## $ ue.person.cen &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0800, -0.3200, -0.7200, -0.6200, 1.5800, -0.2015, 2.3035, 2.… ## $ ue1 &lt;dbl&gt; 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 4.895, 4.895, 4.895, 4.895, 4.… Here’s a more focused look along the lines of Table 6.1. wages_pp %&gt;% select(id, lnw, exper, ged, postexp) %&gt;% mutate(`ged by exper` = ged * exper) %&gt;% filter(id %in% c(206, 2365, 4384)) ## # A tibble: 22 × 6 ## id lnw exper ged postexp `ged by exper` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 206 2.03 1.87 0 0 0 ## 2 206 2.30 2.81 0 0 0 ## 3 206 2.48 4.31 0 0 0 ## 4 2365 1.78 0.66 0 0 0 ## 5 2365 1.76 1.68 0 0 0 ## 6 2365 1.71 2.74 0 0 0 ## 7 2365 1.74 3.68 0 0 0 ## 8 2365 2.19 4.68 1 0 4.68 ## 9 2365 2.04 5.72 1 1.04 5.72 ## 10 2365 2.32 6.72 1 2.04 6.72 ## # ℹ 12 more rows Similar to what we did in Section 5.2.1, here is a visualization of the two primary variables, exper and lnw, for those three participants. wages_pp %&gt;% filter(id %in% c(206, 2365, 4384)) %&gt;% mutate(id = factor(id)) %&gt;% ggplot(aes(x = exper, y = lnw)) + geom_point(aes(color = id), size = 4) + geom_line(aes(color = id)) + geom_text(aes(label = ged), size = 3) + scale_x_continuous(breaks = 1:13) + scale_color_viridis_d(option = &quot;B&quot;, begin = .6, end = .9) + labs(caption = expression(italic(&quot;Note&quot;)*&#39;. GED status is coded 0 = &quot;not yet&quot;, 1 = &quot;yes.&quot;&#39;)) + theme(panel.grid = element_blank(), plot.caption = element_text(hjust = 0)) Note how the time-varying predictor ged is depicted as either an 0 or a 1 in the center of the dots. Maybe we might describe that change as linear with the simple model \\(\\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}\\). Maybe a different model that included ged would be helpful. In the data, \\(n = 581\\) never got their GED’s, while \\(n = 307\\) did. wages_pp %&gt;% group_by(id) %&gt;% summarise(got_ged = sum(ged) &gt; 0) %&gt;% count(got_ged) ## # A tibble: 2 × 2 ## got_ged n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 581 ## 2 TRUE 307 However, those who did get their GED’s did so at different times. Here’s their distribution. wages_pp %&gt;% filter(ged == 1) %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(x = exper)) + geom_histogram(binwidth = 0.5) + labs(subtitle = expression(The~italic(timing)~of~the~GED~attainment~varies)) + theme(panel.grid = element_blank()) Here’s another, more focused, look at the GED status for our three focal participants, over exper. wages_pp %&gt;% select(id, lnw, exper, ged, postexp) %&gt;% filter(id %in% c(206, 2365, 4384)) %&gt;% mutate(id = factor(id)) %&gt;% ggplot(aes(x = exper, y = ged)) + geom_point(aes(color = id), size = 4) + scale_x_continuous(breaks = 1:13) + scale_y_continuous(breaks = 0:1, limits = c(-0.2, 1.2)) + scale_color_viridis_d(option = &quot;B&quot;, begin = .6, end = .9, breaks = NULL) + theme(panel.grid = element_blank(), plot.caption = element_text(hjust = 0)) + facet_wrap(~ id) One might wonder: “How might GED receipt affect individual \\(i\\)’s wage trajectory?” (p. 193). Here we reproduce Figure 6.1, which entertains four possibilities. tibble(exper = c(0, 3, 3, 10), ged = rep(0:1, each = 2)) %&gt;% expand(model = letters[1:4], nesting(exper, ged)) %&gt;% mutate(exper2 = if_else(ged == 0, 0, exper - 3)) %&gt;% mutate(lnw = case_when( model == &quot;a&quot; ~ 1.60 + 0.04 * exper, model == &quot;b&quot; ~ 1.65 + 0.04 * exper + 0.05 * ged, model == &quot;c&quot; ~ 1.75 + 0.04 * exper + 0.02 * exper2 * ged, model == &quot;d&quot; ~ 1.85 + 0.04 * exper + 0.01 * ged + 0.02 * exper * ged ), model = fct_rev(model)) %&gt;% ggplot(aes(x = exper, y = lnw)) + geom_line(aes(color = model), linewidth = 1) + scale_color_viridis_d(option = &quot;D&quot;, begin = 1/4, end = 3/4) + ylim(1.5, 2.5) + theme(panel.grid.minor = element_blank()) 6.1.1.1 Including a discontinuity in elevation, not slope. We can write the level-1 formula for when there is a change in elevation, but not slope, as \\[ \\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\epsilon_{ij}. \\] Because we are equating ged values as relating to the intercept, but not the slope, it might be helpful to rewrite that formula as \\[ \\text{lnw}_{ij} = (\\pi_{0i} + \\pi_{2i} \\text{ged}_{ij}) + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}, \\] where the portion inside of the parentheses concerns initial status and discontinuity in elevation, but not slope. Because the ged values only come in 0’s and 1’s, we can express the two versions of this equation as \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = [\\pi_{0i} + \\pi_{2i} (0)] + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\;\\;\\; \\text{and} \\\\ &amp; = [\\pi_{0i} + \\pi_{2i} (1)] + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}. \\end{align*} \\] In other words, whereas the pre-GED intercept is \\(\\pi_{0i}\\), the post-GED intercept is \\(\\pi_{0i} + \\pi_{2i}\\). To get a better sense of this, we might make a version of the upper left panel of Figure 6.2. Since we’ll be making four of these over the next few sections, we might reduce the redundancies in the code by making a custom plotting function. We’ll call it plot_figure_6.2(). plot_figure_6.2 &lt;- function(data, mapping, sizes = c(1, 1/4), linetypes = c(1, 2), ...) { ggplot(data, mapping) + geom_line(aes(size = model, linetype = model)) + geom_text(data = text, aes(label = label, hjust = hjust), size = 3, parse = T) + geom_segment(data = arrow, aes(xend = xend, yend = yend), arrow = arrow(length = unit(0.075, &quot;inches&quot;), type = &quot;closed&quot;), size = 1/4) + scale_size_manual(values = sizes) + scale_linetype_manual(values = linetypes) + scale_x_continuous(expand = expansion(mult = c(0, 0.05))) + scale_y_continuous(breaks = 0:4 * 0.2 + 1.6, expand = c(0, 0)) + coord_cartesian(ylim = c(1.6, 2.4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) } Now we have our custom plotting function, let’s take it for a spin. text &lt;- tibble(exper = c(4.5, 4.5, 7.5, 7.5, 1), lnw = c(2.24, 2.2, 1.82, 1.78, 1.62), label = c(&quot;Common~rate~of~change&quot;, &quot;Pre-Post~GED~(pi[1][italic(i)])&quot;, &quot;Elevation~differential&quot;, &quot;on~GED~receipt~(pi[2][italic(i)])&quot;, &quot;italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])&quot;), hjust = c(.5, .5, .5, .5, 0)) arrow &lt;- tibble(exper = c(2.85, 5.2, 5.5, 1.7), xend = c(2, 6.8, 3.1, 0.05), lnw = c(2.18, 2.18, 1.8, 1.64), yend = c(1.84, 2.08, 1.9, 1.74)) p1 &lt;- tibble(exper = c(0, 3, 3, 10), ged = rep(0:1, each = 2)) %&gt;% expand(model = letters[1:2], nesting(exper, ged)) %&gt;% mutate(exper2 = if_else(ged == 0, 0, exper - 3)) %&gt;% mutate(lnw = case_when( model == &quot;a&quot; ~ 1.75 + 0.04 * exper, model == &quot;b&quot; ~ 1.75 + 0.04 * exper + 0.05 * ged), model = fct_rev(model)) %&gt;% plot_figure_6.2(aes(x = exper, y = lnw)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. p1 Worked like a dream! 6.1.1.2 Including a discontinuity in slope, not elevation. To specify a level-1 individual growth model that includes a discontinuity in slope, not elevation, you need a different time-varying predictor. Unlike GED, this predictor must clock the passage of time (like EXPER). But unlike EXPER, it must do so within only one of the two epochs (pre- or post-GED receipt). Adding a second temporal predictor allows each individual change trajectory to have two distinct slopes: one before the hypothesized discontinuity an another after. (p. 195, emphasis in the original) In the wages_pp data, postexp is this second variable. Here is how it compares to the other relevant variables. wages_pp %&gt;% select(id, ged, exper, postexp) %&gt;% filter(id &gt;= 53) ## # A tibble: 6,384 × 4 ## id ged exper postexp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 53 0 0.781 0 ## 2 53 0 0.943 0 ## 3 53 1 0.957 0 ## 4 53 1 1.04 0.08 ## 5 53 1 1.06 0.1 ## 6 53 1 1.11 0.152 ## 7 53 1 1.18 0.227 ## 8 53 1 1.78 0.82 ## 9 122 0 2.04 0 ## 10 122 0 2.64 0 ## # ℹ 6,374 more rows Singer and Willett then went on to report “construction of a suitable time-varying predictor to register the desired discontinuity is often the hardest part of the model specification” (p. 195). They weren’t kidding. This concept caused me a good bit of frustration when learning about these models. Let’s walk through this slowly. In the last code block, we looked at four relevant variables. You may wonder why we executed filter(id &gt;= 53). This is because the first two participants always had ged == 1. They’re valid cases and all, but those data won’t be immediately helpful for understanding what’s going on with postexp. Happily, the next case, id == 53, is perfect for our goal. First, notice how that person’s postexp values are always 0 when ged == 0. Second, notice how the first time where ged == 1, postexp is still a 0. Third, notice that after that first initial row, postexp increases. If you caught all that, go you! To make the next point, it’ll come in handy to subset the data. Because we’re trying to understand the relationship between exper and postesp conditional on ged, cases for which ged is always the same will be of little use. Let’s drop them. wages_pp_subset &lt;- wages_pp %&gt;% group_by(id) %&gt;% filter(mean(ged) &gt; 0) %&gt;% filter(mean(ged) &lt; 1) %&gt;% ungroup() %&gt;% select(id, ged, exper, postexp) wages_pp_subset ## # A tibble: 819 × 4 ## id ged exper postexp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 53 0 0.781 0 ## 2 53 0 0.943 0 ## 3 53 1 0.957 0 ## 4 53 1 1.04 0.08 ## 5 53 1 1.06 0.1 ## 6 53 1 1.11 0.152 ## 7 53 1 1.18 0.227 ## 8 53 1 1.78 0.82 ## 9 134 0 0.192 0 ## 10 134 0 0.972 0 ## # ℹ 809 more rows What might not be obvious yet is exper and postexp scale together. To show how this works, we’ll make two new columns. First, we’ll mark the minimum exper value for each level of id. Then we’ll make a exper - postexp which is exactly what the name implies. Here’s what that looks like. wages_pp_subset %&gt;% filter(ged == 1) %&gt;% group_by(id) %&gt;% mutate(min_exper = min(exper), `exper - postexp` = exper - postexp) ## # A tibble: 525 × 6 ## # Groups: id [107] ## id ged exper postexp min_exper `exper - postexp` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 53 1 0.957 0 0.957 0.957 ## 2 53 1 1.04 0.08 0.957 0.957 ## 3 53 1 1.06 0.1 0.957 0.957 ## 4 53 1 1.11 0.152 0.957 0.958 ## 5 53 1 1.18 0.227 0.957 0.958 ## 6 53 1 1.78 0.82 0.957 0.957 ## 7 134 1 3.92 0 3.92 3.92 ## 8 134 1 4.64 0.72 3.92 3.92 ## 9 134 1 5.64 1.72 3.92 3.92 ## 10 134 1 6.75 2.84 3.92 3.92 ## # ℹ 515 more rows Huh. For each case, the min_exper value is (near)identical with exper - postexp. The reason they’re not always identical is simply rounding error. Had we computed them by hand without rounding, they would always be the same. This relationship is the consequence of our having coded postexp == 0 the very first time ged == 1, but allowed it to linearly increase afterward. Within each level of id–and conditional on ged == 1–, the way it increases is simply exper – min(exper). Here’s that value. wages_pp_subset %&gt;% filter(ged == 1) %&gt;% group_by(id) %&gt;% mutate(min_exper = min(exper), `exper - postexp` = exper - postexp) %&gt;% mutate(`exper - min_exper` = exper - min_exper) ## # A tibble: 525 × 7 ## # Groups: id [107] ## id ged exper postexp min_exper `exper - postexp` `exper - min_exper` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 53 1 0.957 0 0.957 0.957 0 ## 2 53 1 1.04 0.08 0.957 0.957 0.0800 ## 3 53 1 1.06 0.1 0.957 0.957 0.1 ## 4 53 1 1.11 0.152 0.957 0.958 0.153 ## 5 53 1 1.18 0.227 0.957 0.958 0.228 ## 6 53 1 1.78 0.82 0.957 0.957 0.82 ## 7 134 1 3.92 0 3.92 3.92 0 ## 8 134 1 4.64 0.72 3.92 3.92 0.72 ## 9 134 1 5.64 1.72 3.92 3.92 1.72 ## 10 134 1 6.75 2.84 3.92 3.92 2.84 ## # ℹ 515 more rows See? Our new exper - min_exper column is the same, within rounding error, as postexp. A fundamental feature of POSTEXP–indeed, any temporal predictor designed to register a shift in slope–is that the difference between each non-zero pair of consecutive values must be numerically identical to the difference between the corresponding pair of values for the basic predictor (here, EXPER). (p. 197, emphasis in the original) wages_pp %&gt;% group_by(id) %&gt;% filter(mean(ged) &gt; 0 &amp; mean(ged) &lt; 1) %&gt;% ungroup() %&gt;% pivot_longer(c(exper, postexp), names_to = &quot;temporal predictor&quot;) %&gt;% filter(id &lt; 250) %&gt;% ggplot(aes(x = value, y = lnw)) + geom_line(aes(linetype = `temporal predictor`, color = `temporal predictor`)) + scale_color_viridis_d(option = &quot;A&quot;, begin = 1/3, end = 2/3, direction = -1) + theme(legend.position = c(5/6, .25), panel.grid = element_blank()) + facet_wrap(~id, scales = &quot;free_x&quot;) See how those two scale together within each level of id? All of this work is a setup for the level-1 equation \\[ \\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{3i} \\text{postexp}_{ij} + \\epsilon_{ij}, \\] where \\(\\pi_{0i}\\) is the only intercept parameter and \\(\\pi_{1i}\\) and \\(\\pi_{3i}\\) are two slope parameters. Singer and Willett explained each slope assesses the effect of work experience, but it does so from a different origin: (1) \\(\\pi_{1i}\\) captures the effects of total work experience (measured from labor force entry); and (2) \\(\\pi_{3i}\\) captures the added effect of post-GED work experience (measured from GED receipt). (p. 197, emphasis in the original) To get a sense of what this looks like, here’s our version of the upper right panel of Figure 6.2. text &lt;- tibble(exper = c(5, 5, 0.5, 0.5, 1), lnw = c(2.24, 2.2, 2, 1.96, 1.62), label = c(&quot;Slope~differential&quot;, &quot;Pre-Post~GED~(pi[3][italic(i)])&quot;, &quot;Rate~of~change&quot;, &quot;Pre~GED~(pi[1][italic(i)])&quot;, &quot;italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])&quot;), hjust = c(.5, .5, 0, 0, 0)) arrow &lt;- tibble(exper = c(5.2, 1.7, 1.7), xend = c(9.1, 1.7, 0.05), lnw = c(2.18, 1.93, 1.64), yend = c(2.15, 1.84, 1.74)) p2 &lt;- tibble(exper = c(0, 3, 3, 10), ged = rep(0:1, each = 2)) %&gt;% expand(model = letters[1:2], nesting(exper, ged)) %&gt;% mutate(postexp = ifelse(exper == 10, 1, 0)) %&gt;% mutate(lnw = case_when( model == &quot;a&quot; ~ 1.75 + 0.04 * exper, model == &quot;b&quot; ~ 1.75 + 0.04 * exper + 0.15 * postexp), model = fct_rev(model)) %&gt;% plot_figure_6.2(aes(x = exper, y = lnw)) + annotate(geom = &quot;curve&quot;, x = 8.5, xend = 8.8, y = 2.195, yend = 2.109, arrow = arrow(length = unit(0.05, &quot;inches&quot;), type = &quot;closed&quot;, ends = &quot;both&quot;), size = 1/4, linetype = 2, curvature = -0.85) p2 6.1.1.3 Including discontinuities in both elevation and slope. There are (at least) two ways to do this. They are similar, but not identical. The first is an extension of the model from the last subsection where we retain postexp from our second slope parameter. We can express this as the equation \\[ \\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} \\text{postexp}_{ij} + \\epsilon_{ij}. \\] For those without a GED, the equation reduces to \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (0) + \\pi_{3i} (0) + \\epsilon_{ij} \\\\ &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}. \\end{align*} \\] Once people secure their GED, \\(\\pi_{2i}\\) is always multiplied by 1 (i.e., \\(\\pi_{2i} (1)\\)) and the values by which we multiply \\(\\pi_{3i}\\) scale linearly with exper, but with the offset the way we discussed in the previous subsection. To emphasize that, we might rewrite the equation as \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (1) + \\pi_{3i} \\text{postexp} + \\epsilon_{ij} \\\\ &amp; = (\\pi_{0i} + \\pi_{2i}) + \\pi_{1i} \\text{exper}_{ij} + \\pi_{3i} \\text{postexp} + \\epsilon_{ij}. \\end{align*} \\] To get a sense of what this looks like, here’s our version of the lower left panel of Figure 6.2. text &lt;- tibble(exper = c(5, 5, 0.5, 0.5, 1, 7, 7), lnw = c(2.24, 2.2, 2, 1.96, 1.62, 1.78, 1.74), label = c(&quot;Slope~differential&quot;, &quot;Pre-Post~GED~(pi[3][italic(i)])&quot;, &quot;Rate~of~change&quot;, &quot;Pre~GED~(pi[1][italic(i)])&quot;, &quot;italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])&quot;, &quot;Constant~elevation~differential&quot;, &quot;on~GED~receipt~(pi[2][italic(i)])&quot;), hjust = c(.5, .5, 0, 0, 0, .5, .5)) arrow &lt;- tibble(exper = c(5.2, 1.7, 1.7, 6), xend = c(9.1, 1.7, 0.05, 3.1), lnw = c(2.18, 1.93, 1.64, 1.8), yend = c(2.15, 1.84, 1.74, 1.885)) p3 &lt;- tibble(exper = c(0, 3, 3, 10), ged = rep(0:1, each = 2)) %&gt;% expand(model = letters[1:3], nesting(exper, ged)) %&gt;% mutate(postexp = ifelse(exper == 10, 1, 0)) %&gt;% mutate(lnw = case_when( model == &quot;a&quot; ~ 1.75 + 0.04 * exper, model == &quot;b&quot; ~ 1.75 + 0.04 * exper + 0.02 * ged, model == &quot;c&quot; ~ 1.75 + 0.04 * exper + 0.02 * ged + 0.1 * postexp), model = fct_rev(model)) %&gt;% plot_figure_6.2(aes(x = exper, y = lnw), sizes = c(1, 1/4, 1/4), linetypes = c(1, 2, 2)) + annotate(geom = &quot;curve&quot;, x = 8.5, xend = 8.8, y = 2.185, yend = 2.125, arrow = arrow(length = unit(0.05, &quot;inches&quot;), type = &quot;closed&quot;, ends = &quot;both&quot;), size = 1/4, linetype = 2, curvature = -0.85) p3 The second way to include discontinuities in both elevation and slope replaces the postexp variable with an interaction between exper and ged. Here’s the equation: \\[ \\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} (\\text{exper}_{ij} \\times \\text{ged}_{ij}) + \\epsilon_{ij}. \\] For those without a GED, the equation simplifies to \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (0) + \\pi_{3i} (\\text{exper}_{ij} \\times 0) + \\epsilon_{ij} \\\\ &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}. \\end{align*} \\] Once a participant secures their GED, the equation changes to \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (1) + \\pi_{3i} (\\text{exper}_{ij} \\times 1) + \\epsilon_{ij} \\\\ &amp; = (\\pi_{0i} + \\pi_{2i}) + (\\pi_{1i} + \\pi_{3i}) \\text{exper}_{ij} + \\epsilon_{ij}. \\end{align*} \\] So again, the two ways we might include discontinuities in both elevation and slope are \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} \\text{postexp}_{ij} + \\epsilon_{ij} &amp; \\text{and} \\\\ \\text{lnw}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} (\\text{exper}_{ij} \\times \\text{ged}_{ij}) + \\epsilon_{ij}. \\end{align*} \\] The \\(\\pi_{0i}\\) and \\(\\pi_{1i}\\) terms have the same meaning in both. Even though \\(\\pi_{3i}\\) is multiplied by different values in the two equations, it has the same interpretation: “it represents the increment (or decrement) of the slope in the post-GED epoch” (p. 200). However, the big difference is the behavior and interpretation for \\(\\pi_{2i}\\). In the equation for the first approach, it “assesses the magnitude of the instantaneous increment (or decrement) associated with GED attainment” (p. 200). But in the equation for the second approach, “\\(\\pi_{2i}\\) assesses the magnitude of the increment (or decrement) associated with GED attainment at a particular–and not particularly meaningful–moment: the day of labor force entry” (p. 220, emphasis added). That is, whereas \\(\\pi_{2i}\\) has a fixed value for the first approach, its magnitude changes with time in the second. To get a sense of what this looks like, here’s our version of the lower right panel of Figure 6.2. text &lt;- tibble(exper = c(5, 5, 0.5, 0.5, 1, 7, 7, 8, 8, 8), lnw = c(2.28, 2.24, 2.1, 2.06, 1.62, 1.76, 1.72, 1.94, 1.9, 1.86), label = c(&quot;Slope~differential&quot;, &quot;Pre-Post~GED~(pi[3][italic(i)])&quot;, &quot;Rate~of~change&quot;, &quot;Pre~GED~(pi[1][italic(i)])&quot;, &quot;italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])&quot;, &quot;GED~differential~at&quot;, &quot;labor~force~entry~(pi[2][italic(i)])&quot;, &quot;Elevation~differential&quot;, &quot;on~GED~receipt&quot;, &quot;(pi[2][italic(i)]+pi[3][italic(i)]*italic(EXPER))&quot;), hjust = c(.5, .5, 0, 0, 0, .5, .5, .5, .5, .5)) arrow &lt;- tibble(exper = c(5.2, 1.7, 1.7, 4.9, 6.2), xend = c(8.8, 1.7, 0.05, 0.05, 3.1), lnw = c(2.22, 2.03, 1.64, 1.745, 1.9), yend = c(2.18, 1.825, 1.74, 1.775, 1.9)) p4 &lt;- crossing(model = letters[1:4], point = 1:4) %&gt;% mutate(exper = ifelse(point == 1, 0, ifelse(point == 4, 10, 3)), ged = c(0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1)) %&gt;% mutate(lnw = case_when( model %in% letters[1:3] ~ 1.75 + 0.04 * exper + 0.04 * ged + 0.01 * exper * ged, model == &quot;d&quot; ~ 1.75 + 0.04 * exper + 0.04 * ged)) %&gt;% plot_figure_6.2(aes(x = exper, y = lnw), sizes = c(1, 1/4, 1/4, 1/4), linetypes = c(1, 2, 2, 2)) + annotate(geom = &quot;curve&quot;, x = 8.5, xend = 8.8, y = 2.205, yend = 2.145, arrow = arrow(length = unit(0.05, &quot;inches&quot;), type = &quot;closed&quot;, ends = &quot;both&quot;), size = 1/4, linetype = 2, curvature = -0.85) p4 You may have noticed we’ve been saving the various subplot panels as objects. Here we combine them to make the full version of Figure 6.2. library(patchwork) (p1 + p2) / (p3 + p4) Glorious. 6.1.2 Selecting among the alternative discontinuous models. Our first model in this section will be a call back from the last chapter, fit5.16. library(brms) # model a fit5.16 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 5, file = &quot;fits/fit05.16&quot;) Review the summary. print(fit5.16, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.224 0.010 0.204 0.245 1.002 1704 2884 ## sd(exper) 0.040 0.003 0.035 0.046 1.004 605 917 ## cor(Intercept,exper) -0.306 0.068 -0.432 -0.165 1.004 713 1634 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.749 0.011 1.727 1.772 1.000 2856 3207 ## exper 0.044 0.003 0.039 0.049 1.000 2694 3522 ## hgc_9 0.040 0.006 0.027 0.052 1.002 2100 3469 ## uerate_7 -0.012 0.002 -0.016 -0.008 1.001 5116 4125 ## exper:black -0.018 0.005 -0.027 -0.009 1.001 2338 2587 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.308 0.003 0.302 0.314 1.001 3798 3478 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we move forward with the next models, we’ll need to wrangle the data a bit. First, we rename hgc.9 to the more tidyverse-centric hgc_9. Then we compute uerate_7, which is uerate centered on 7. wages_pp &lt;- wages_pp %&gt;% rename(hgc_9 = hgc.9) %&gt;% mutate(uerate_7 = uerate - 7) Our model A (fit5.16) and the rest of the models B through J (fit6.1 through fit6.9) can all be thought of as variants of two parent models. The first parent model is model F (fit6.5), which follows the form \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_i - 9) \\\\ &amp; \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\ &amp; \\;\\;\\; + \\gamma_{20} (\\text{uerate}_{ij} - 7) \\\\ &amp; \\;\\;\\; + \\gamma_{30} \\text{ged}_{ij} \\\\ &amp; \\;\\;\\; + \\gamma_{40} \\text{postexp}_{ij}\\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\zeta_{3i} \\text{ged}_{ij} + \\zeta_{4i} \\text{postexp}_{ij} + \\epsilon_{ij}, \\;\\;\\; \\text{where} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{3i} \\\\ \\zeta_{4i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf \\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_4 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} &amp; \\rho_{03} &amp; \\rho_{04} \\\\ \\rho_{10} &amp; 1 &amp; \\rho_{13} &amp; \\rho_{14} \\\\ \\rho_{30} &amp; \\rho_{31} &amp; 1 &amp; \\rho_{34} \\\\ \\rho_{40} &amp; \\rho_{41} &amp; \\rho_{43} &amp;1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(1.335, 1) \\\\ \\gamma_{01}, \\dots, \\gamma_{40} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_0, \\dots, \\sigma_4 &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\mathbf\\Omega &amp; \\sim \\operatorname{LKJ}(4), \\end{align*} \\] which uses the postexp-based approach for discontinuity in slopes. Notice how we’re using the same basic prior specification as with fit5.16. The second parent model is model I (fit6.8), which follows the form \\[ \\begin{align*} \\text{lnw}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_i - 9) \\\\ &amp; \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\ &amp; \\;\\;\\; + \\gamma_{20} (\\text{uerate}_{ij} - 7) \\\\ &amp; \\;\\;\\; + \\gamma_{30} \\text{ged}_{ij} \\\\ &amp; \\;\\;\\; + \\gamma_{50} \\text{ged}_{ij} \\times \\text{exper}_{ij} \\\\ &amp; \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\zeta_{3i} \\text{ged}_{ij} + \\zeta_{5i} \\text{ged}_{ij} \\times \\text{exper}_{ij} + \\epsilon_{ij}, \\;\\;\\; \\text{where} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{3i} \\\\ \\zeta_{5i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf \\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_5 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} &amp; \\rho_{03} &amp; \\rho_{05} \\\\ \\rho_{10} &amp; 1 &amp; \\rho_{13} &amp; \\rho_{15} \\\\ \\rho_{30} &amp; \\rho_{31} &amp; 1 &amp; \\rho_{35} \\\\ \\rho_{50} &amp; \\rho_{51} &amp; \\rho_{53} &amp;1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(1.335, 1) \\\\ \\gamma_{01}, \\dots, \\gamma_{50} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_0, \\dots, \\sigma_5 &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Student-t}(3, 0, 1) \\\\ \\mathbf\\Omega &amp; \\sim \\operatorname{LKJ}(4). \\end{align*} \\] which uses the ged:exper-based approach for discontinuity in slopes. Here we fit the models in bulk. # model b fit6.1 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + (1 + exper + ged | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.01&quot;) # model c fit6.2 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.02&quot;) # model d fit6.3 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + postexp + (1 + exper + postexp | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, control = list(adapt_delta = .99), file = &quot;fits/fit06.03&quot;) # model e fit6.4 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + postexp + (1 + exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.04&quot;) # model f fit6.5 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + ged + postexp | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.05&quot;) # model g fit6.6 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + ged | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.06&quot;) # model h fit6.7 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + postexp | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, control = list(adapt_delta = .99), file = &quot;fits/fit06.07&quot;) # model i fit6.8 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + ged:exper + (1 + exper + ged + ged:exper | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, control = list(adapt_delta = .99), file = &quot;fits/fit06.08&quot;) # model j fit6.9 &lt;- brm(data = wages_pp, family = gaussian, lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + ged:exper + (1 + exper + ged | id), prior = c(prior(normal(1.335, 1), class = b, coef = Intercept), prior(normal(0, 0.5), class = b), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.09&quot;) To keep from cluttering up this ebook, I’m not going to show the summary print() output for fit6.1 through fit6.7. If you go through that output yourself, you’ll see that several of them had low effective sample size estimates for one or a few of the \\(\\sigma\\) parameters. For our pedagogical purposes, I’m okay with moving forward with these. But do note that when fitting a model for your scientific or other real-world projects, make sure you attend to your effective sample size issues (i.e., extract more posterior draws, as needed, by adjusting the warmup, iter, and chains arguments). Though we’re avoiding print() output, we might take a birds-eye perspective and summarize the parameters for our competing models with a faceted coefficient plot. First we extract and save the relevant information as an object, post, and then we wrangle and plot. # compute post &lt;- tibble(model = letters[1:10], fit = c(&quot;fit5.16&quot;, str_c(&quot;fit6.&quot;, 1:9))) %&gt;% mutate(p = map(fit, ~ get(.) %&gt;% posterior_summary() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;) %&gt;% filter(!str_detect(parameter, &quot;r_id\\\\[&quot;) &amp; parameter != &quot;lp__&quot; &amp; parameter != &quot;lprior&quot;))) %&gt;% unnest(p) # wrangle post %&gt;% mutate(greek = case_when( parameter == &quot;b_Intercept&quot; ~ &quot;gamma[0][0]&quot;, parameter == &quot;b_hgc_9&quot; ~ &quot;gamma[0][1]&quot;, parameter == &quot;b_exper&quot; ~ &quot;gamma[1][0]&quot;, parameter == &quot;b_exper:black&quot; ~ &quot;gamma[1][2]&quot;, parameter == &quot;b_uerate_7&quot; ~ &quot;gamma[2][0]&quot;, parameter == &quot;b_ged&quot; ~ &quot;gamma[3][0]&quot;, parameter == &quot;b_postexp&quot; ~ &quot;gamma[4][0]&quot;, parameter == &quot;b_exper:ged&quot; ~ &quot;gamma[5][0]&quot;, parameter == &quot;sd_id__Intercept&quot; ~ &quot;sigma[0]&quot;, parameter == &quot;sd_id__exper&quot; ~ &quot;sigma[1]&quot;, parameter == &quot;sd_id__ged&quot; ~ &quot;sigma[3]&quot;, parameter == &quot;sd_id__postexp&quot; ~ &quot;sigma[4]&quot;, parameter == &quot;sd_id__exper:ged&quot; ~ &quot;sigma[5]&quot;, parameter == &quot;sigma&quot; ~ &quot;sigma[epsilon]&quot;, parameter == &quot;cor_id__Intercept__exper&quot; ~ &quot;rho[0][1]&quot;, parameter == &quot;cor_id__Intercept__ged&quot; ~ &quot;rho[0][3]&quot;, parameter == &quot;cor_id__exper__ged&quot; ~ &quot;rho[1][3]&quot;, parameter == &quot;cor_id__Intercept__postexp&quot; ~ &quot;rho[0][4]&quot;, parameter == &quot;cor_id__Intercept__exper:ged&quot; ~ &quot;rho[0][5]&quot;, parameter == &quot;cor_id__exper__postexp&quot; ~ &quot;rho[1][4]&quot;, parameter == &quot;cor_id__exper__exper:ged&quot; ~ &quot;rho[1][5]&quot;, parameter == &quot;cor_id__ged__postexp&quot; ~ &quot;rho[3][4]&quot;, parameter == &quot;cor_id__ged__exper:ged&quot; ~ &quot;rho[3][5]&quot; )) %&gt;% mutate(model = fct_rev(model)) %&gt;% # plot ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = model)) + geom_pointrange(size = 1/4, fatten = 1) + labs(title = &quot;Marginal posteriors for models a thorugh j&quot;, x = NULL, y = NULL) + theme(axis.text = element_text(size = 6), axis.text.y = element_text(hjust = 0), panel.grid = element_blank()) + facet_wrap(~ greek, labeller = label_parsed, scales = &quot;free_x&quot;) For our model comparisons, let’s compute the WAIC estimates for each. fit5.16 &lt;- add_criterion(fit5.16, criterion = &quot;waic&quot;) fit6.1 &lt;- add_criterion(fit6.1, criterion = &quot;waic&quot;) fit6.2 &lt;- add_criterion(fit6.2, criterion = &quot;waic&quot;) fit6.3 &lt;- add_criterion(fit6.3, criterion = &quot;waic&quot;) fit6.4 &lt;- add_criterion(fit6.4, criterion = &quot;waic&quot;) fit6.5 &lt;- add_criterion(fit6.5, criterion = &quot;waic&quot;) fit6.6 &lt;- add_criterion(fit6.6, criterion = &quot;waic&quot;) fit6.7 &lt;- add_criterion(fit6.7, criterion = &quot;waic&quot;) fit6.8 &lt;- add_criterion(fit6.8, criterion = &quot;waic&quot;) fit6.9 &lt;- add_criterion(fit6.9, criterion = &quot;waic&quot;) On pages 202 through 204, Singer and Willett performed a number of model comparisons with deviance tests and the frequentist AIC and BIC. Here we do the analogous comparisons with WAIC difference estimates. # a vs b loo_compare(fit5.16, fit6.1, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.1 0.0 0.0 -2028.3 103.7 878.4 27.4 4056.6 207.5 ## fit5.16 -9.2 6.0 -2037.6 104.2 864.8 27.0 4075.1 208.4 # b vs c loo_compare(fit6.1, fit6.2, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.1 0.0 0.0 -2028.3 103.7 878.4 27.4 4056.6 207.5 ## fit6.2 -7.0 4.9 -2035.3 103.9 863.8 27.0 4070.6 207.8 # a vs d loo_compare(fit5.16, fit6.3, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.3 0.0 0.0 -2034.5 104.1 867.0 27.2 4069.0 208.3 ## fit5.16 -3.0 2.7 -2037.6 104.2 864.8 27.0 4075.1 208.4 # d vs e loo_compare(fit6.3, fit6.4, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.4 0.0 0.0 -2034.5 103.9 861.7 26.7 4069.0 207.8 ## fit6.3 0.0 1.9 -2034.5 104.1 867.0 27.2 4069.0 208.3 # f vs b loo_compare(fit6.5, fit6.1, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.5 0.0 0.0 -2022.8 103.9 887.6 27.9 4045.6 207.9 ## fit6.1 -5.5 3.6 -2028.3 103.7 878.4 27.4 4056.6 207.5 # f vs d loo_compare(fit6.5, fit6.3, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.5 0.0 0.0 -2022.8 103.9 887.6 27.9 4045.6 207.9 ## fit6.3 -11.7 7.6 -2034.5 104.1 867.0 27.2 4069.0 208.3 # f vs g loo_compare(fit6.5, fit6.6, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.5 0.0 0.0 -2022.8 103.9 887.6 27.9 4045.6 207.9 ## fit6.6 -8.9 3.7 -2031.7 104.0 881.2 27.8 4063.5 207.9 # f vs h loo_compare(fit6.5, fit6.7, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.5 0.0 0.0 -2022.8 103.9 887.6 27.9 4045.6 207.9 ## fit6.7 -10.3 6.2 -2033.1 103.8 865.6 26.8 4066.3 207.7 # i vs b loo_compare(fit6.8, fit6.1, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.8 0.0 0.0 -2026.9 103.7 886.7 27.8 4053.8 207.5 ## fit6.1 -1.4 2.6 -2028.3 103.7 878.4 27.4 4056.6 207.5 # j vs i loo_compare(fit6.9, fit6.8, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.8 0.0 0.0 -2026.9 103.7 886.7 27.8 4053.8 207.5 ## fit6.9 -2.8 2.5 -2029.7 103.8 881.5 27.6 4059.4 207.7 # i vs f loo_compare(fit6.8, fit6.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.5 0.0 0.0 -2022.8 103.9 887.6 27.9 4045.6 207.9 ## fit6.8 -4.1 2.4 -2026.9 103.7 886.7 27.8 4053.8 207.5 We might also just look at all of the WAIC estimates, plus and minus their standard errors, in a coefficient plot. loo_compare(fit5.16, fit6.1, fit6.2, fit6.3, fit6.4, fit6.5, fit6.6, fit6.7, fit6.8, fit6.9, criterion = &quot;waic&quot;) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;fit&quot;) %&gt;% arrange(fit) %&gt;% mutate(model = letters[1:n()]) %&gt;% ggplot(aes(x = waic, xmin = waic - se_waic, xmax = waic + se_waic, y = reorder(model, waic))) + geom_pointrange(fatten = 1) + labs(x = expression(WAIC%+-%s.e.), y = &quot;model&quot;) + theme(axis.text.y = element_text(hjust = 0), panel.grid = element_blank()) In the text, model I had the lowest (best) deviance and information criteria values, with model F coming in a close second. Our WAIC results are the reverse. However, look at the widths of the standard error intervals, for each, relative to their point estimates. I wouldn’t get too upset about differences in our results versus those in the text. Our Bayesian models reveal there’s massive uncertainty in each estimate, an insight missing from the frequent analyses reported in the text. Here’s a focused look at the parameter summary for model F. print(fit6.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + ged + postexp | id) ## Data: wages_pp (Number of observations: 6402) ## Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1; ## total post-warmup draws = 4500 ## ## Group-Level Effects: ## ~id (Number of levels: 888) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.205 0.011 0.183 0.228 1.002 1180 2248 ## sd(exper) 0.037 0.003 0.032 0.043 1.004 575 1674 ## sd(ged) 0.164 0.042 0.083 0.248 1.016 238 373 ## sd(postexp) 0.039 0.013 0.009 0.064 1.032 159 206 ## cor(Intercept,exper) -0.226 0.087 -0.385 -0.051 1.005 680 1163 ## cor(Intercept,ged) 0.183 0.212 -0.212 0.614 1.008 446 847 ## cor(exper,ged) -0.057 0.249 -0.495 0.465 1.008 403 730 ## cor(Intercept,postexp) -0.406 0.195 -0.750 0.016 1.006 644 1024 ## cor(exper,postexp) -0.079 0.245 -0.523 0.437 1.017 276 604 ## cor(ged,postexp) -0.301 0.232 -0.705 0.191 1.008 448 941 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.739 0.012 1.716 1.762 1.000 3337 3456 ## exper 0.041 0.003 0.036 0.047 1.000 3185 3550 ## hgc_9 0.039 0.006 0.027 0.051 1.001 3557 3700 ## uerate_7 -0.012 0.002 -0.015 -0.008 1.001 6343 3864 ## ged 0.041 0.022 -0.003 0.085 1.000 3486 3123 ## postexp 0.009 0.005 -0.002 0.020 1.000 4099 3249 ## exper:black -0.019 0.005 -0.028 -0.011 1.001 3341 3311 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.307 0.003 0.301 0.313 1.003 2671 2878 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we dive into the \\(\\gamma\\)-based counterfactual trajectories of Figure 6.3, we might use a plot to get at sense of the \\(\\zeta\\)’s, as summarized by the \\(\\sigma\\) and \\(\\rho\\) parameters. Here we extract the \\(\\zeta\\)’s. Since there’s such a large number, we’ll focus on their posterior means. r &lt;- tibble(`zeta[0]` = ranef(fit6.5)$id[, 1, &quot;Intercept&quot;], `zeta[1]` = ranef(fit6.5)$id[, 1, &quot;exper&quot;], `zeta[3]` = ranef(fit6.5)$id[, 1, &quot;ged&quot;], `zeta[4]` = ranef(fit6.5)$id[, 1, &quot;postexp&quot;]) glimpse(r) ## Rows: 888 ## Columns: 4 ## $ `zeta[0]` &lt;dbl&gt; -0.151683338, 0.101952101, 0.058855105, 0.032556479, 0.162725921, -0.115933285, 0.01842935… ## $ `zeta[1]` &lt;dbl&gt; 0.0077954424, 0.0092990259, -0.0055013067, -0.0018657393, 0.0295247250, -0.0084006310, 0.0… ## $ `zeta[3]` &lt;dbl&gt; -0.1044292573, 0.0873875749, 0.0881116827, 0.0045926705, 0.0276905819, -0.0756621507, -0.0… ## $ `zeta[4]` &lt;dbl&gt; 0.0179942545, 0.0012571856, -0.0121561226, -0.0022347957, -0.0045111664, 0.0005690249, -0.… We’ll be plotting with help from the GGally package (Schloerke et al., 2021), which does a nice job displaying a grid of bivariate plots via the ggpairs(). We’re going to get fancy with our ggpairs() plot by using a handful of custom settings. Here we save them as two functions. my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;black&quot;, linewidth = 0) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(size = 1/10, alpha = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } Now visualize the model F \\(\\zeta\\)’s with GGally::ggpairs(). library(GGally) r %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = NULL, labeller = label_parsed) Now we’re ready to make our version of Figure 6.3. Since we will be expressing the uncertainty of our counterfactual trajectories with 95% interval bands, we’ll be faceting the plot by the two levels of hgc_9. Otherwise, the overplotting would become too much. # define the new data nd &lt;- crossing(black = 0:1, hgc_9 = c(0, 3)) %&gt;% expand_grid(exper = seq(from = 0, to = 11, by = 0.02)) %&gt;% mutate(ged = ifelse(exper &lt; 3, 0, 1), postexp = ifelse(ged == 0, 0, exper - 3), uerate_7 = 0) # compute the fitted draws fitted(fit6.5, re_formula = NA, newdata = nd) %&gt;% # wrangle data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(race = ifelse(black == 0, &quot;White/Latino&quot;, &quot;Black&quot;), hgc_9 = ifelse(hgc_9 == 0, &quot;9th grade dropouts&quot;, &quot;12th grade dropouts&quot;)) %&gt;% mutate(race = fct_rev(race), hgc_9 = fct_rev(hgc_9)) %&gt;% # plot! ggplot(aes(x = exper, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = race, color = race)) + geom_ribbon(size = 0, alpha = 1/4) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;C&quot;, begin = .25, end = .75) + scale_color_viridis_d(NULL, option = &quot;C&quot;, begin = .25, end = .75) + scale_x_continuous(breaks = 0:5 * 2, expand = c(0, 0)) + scale_y_continuous(&quot;lnw&quot;, breaks = 1.6 + 0:4 * 0.2, expand = expansion(mult = c(0, 0.05))) + coord_cartesian(ylim = c(1.6, 2.4)) + theme(panel.grid = element_blank()) + facet_wrap(~ hgc_9) 6.1.3 Further extensions of the discontinuous growth model. “It is easy to generalize these strategies to models with other discontinuities” (p. 206). 6.1.3.1 Dividing TIME into multiple phases. “You can divide TIME into multiple epochs, allowing the trajectories to differ in elevation (and perhaps slope) during each” (p. 206, emphasis in the original). With our examples, above, we divided time into two epochs: before and (possibly) after receipt of one’s GED. More possible epochs might after completing college or graduate school. All such epochs might influence intercepts and/or slopes (by either of the slope methods, above). 6.1.3.2 Discontinuities at common points in time. In some data sets, the timing of the discontinuity will not be person-specific; instead, everyone will experience the hypothesized transition at a common point in time. You can hypothesize a similar discontinuous change trajectory for such data sets by applying the strategies outlined above. (p. 207) Examples might include months, seasons, and academic quarters or semesters. 6.2 Using transformations to model nonlinear individual change When confronted by obviously nonlinear trajectories, we usually begin with the transformation approach for two reasons. First, a straight line–even on a transformed scale–is a simple mathematical form whose two parameters have clear interpretations. Second, because the metrics of many variables are ad hoc to begin with, transformation to another ad hoc scale may sacrifice little. (p. 208) As an example, consider fit4.6 from back in Chapter 4. fit4.6 &lt;- brm(data = alcohol1_pp, family = gaussian, alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id), prior = c(prior(student_t(3, 0, 2.5), class = sd), prior(student_t(3, 0, 2.5), class = sigma), prior(lkj(1), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/fit04.06&quot;) In the alcohol1_pp data, the criterion alcuse was transformed by taking its square root. Since we fit a linear model on that transformed variable, our model is actually non-linear on the original metric of alcuse. To see, we’ll square the fitted()-based counterfactual trajectories and their intervals to make our version of Figure 6.4. # define the new data nd &lt;- crossing(coa = 0:1, peer = c(.655, 1.381)) %&gt;% expand_grid(age_14 = seq(from = 0, to = 2, length.out = 30)) # compute the counterfactual trajectories fitted(fit4.6, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # transform the predictions by squaring them mutate(Estimate = Estimate^2, Q2.5 = Q2.5^2, Q97.5 = Q97.5^2) %&gt;% # a little wrangling will make plotting much easier mutate(age = age_14 + 14, coa = ifelse(coa == 0, &quot;coa = 0&quot;, &quot;coa = 1&quot;), peer = factor(peer)) %&gt;% # plot! ggplot(aes(x = age, color = peer, fill = peer)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), size = 0, alpha = 1/4) + geom_line(aes(y = Estimate, size = peer)) + scale_size_manual(values = c(1/2, 1)) + scale_fill_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + scale_color_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + scale_y_continuous(&quot;alcuse&quot;, breaks = 0:3, expand = c(0, 0)) + labs(subtitle = &quot;High peer values are in red; low ones are in blue.&quot;) + coord_cartesian(xlim = c(13, 17), ylim = c(0, 3)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~coa) Compare these to the linear trajectories depicted back in Figure 4.3c. 6.2.1 The ladder of transformations and the rule of the bulge. I just not a fan of the “ladder of powers” idea and I’m not interested in reproducing Figure 6.5. However, we will make the next one, real quick. Load the berkeley_pp data. berkeley_pp &lt;- read_csv(&quot;data/berkeley_pp.csv&quot;) %&gt;% mutate(time = age) glimpse(berkeley_pp) ## Rows: 18 ## Columns: 3 ## $ age &lt;dbl&gt; 5, 7, 9, 10, 11, 12, 13, 14, 15, 18, 21, 24, 27, 36, 42, 48, 54, 60 ## $ iq &lt;dbl&gt; 37, 65, 85, 88, 95, 101, 103, 107, 113, 121, 148, 161, 165, 187, 205, 218, 218, 228 ## $ time &lt;dbl&gt; 5, 7, 9, 10, 11, 12, 13, 14, 15, 18, 21, 24, 27, 36, 42, 48, 54, 60 Here’s how we might make Figure 6.6. # left p1 &lt;- berkeley_pp %&gt;% ggplot(aes(x = time, y = iq)) + geom_point() + scale_y_continuous(expand = c(0, 0), limits = c(0, 250)) # middle p2 &lt;- berkeley_pp %&gt;% ggplot(aes(x = time, y = iq^2.3)) + geom_point() + scale_y_continuous(expression(iq^(2.3)), breaks = 0:6 * 5e4, limits = c(0, 3e5), expand = c(0, 0)) # right p3 &lt;- berkeley_pp %&gt;% ggplot(aes(x = time^(1/2.3), y = iq)) + geom_point() + scale_y_continuous(expand = c(0, 0), limits = c(0, 250)) + xlab(expression(time^(1/2.3))) # combine (p1 + p2 + p3) &amp; scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) &amp; theme(panel.grid = element_blank()) 6.3 Representing individual change using a polynomial function of TIME We can also model curvilinear change by including several level-1 predictors that collectively represent a polynomial function of time. Although the resulting polynomial growth model can be cumbersome, it can capture an even wider array of complex patterns of change over time. (p. 213, emphasis in the original) To my eye, it will be easiest to make Table by dividing it up into columns, making each column individually, and then combining them on the back end. For our first step, here’s our first column, the “Shape” column. p1 &lt;- tibble(x = 1, y = 9.5, label = c(&quot;No\\nchange&quot;, &quot;Linear\\nchange&quot;, &quot;Quadratic\\nchange&quot;, &quot;Cubic\\nchange&quot;), row = 1:4, col = &quot;Shape&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 0, vjust = 1, size = 3.25) + scale_x_continuous(expand = c(0, 0), limits = 1:2) + scale_y_continuous(expand = c(0, 0), limits = c(1, 10)) + theme_void() + theme(strip.background.y = element_blank(), strip.text.x = element_text(hjust = 0, size = 12), strip.text.y = element_blank()) + facet_grid(row ~ col) For our second step, here’s the “Level-1 model” column. p2 &lt;- tibble(x = c(1, 1, 1, 2, 1, 2, 2), y = c(9.5, 9.5, 9.5, 8.5, 9.5, 8.5, 7.25), label = c(&quot;italic(Y[i][j])==pi[0][italic(i)]+epsilon[italic(ij)]&quot;, &quot;italic(Y[i][j])==pi[0][italic(i)]+pi[1][italic(i)]*italic(TIME[ij])+epsilon[italic(ij)]&quot;, &quot;italic(Y[i][j])==pi[0][italic(i)]+pi[1][italic(i)]*italic(TIME[ij])&quot;, &quot;+pi[2][italic(i)]*italic(TIME[ij])^2+epsilon[italic(ij)]&quot;, &quot;italic(Y[i][j])==pi[0][italic(i)]+pi[1][italic(i)]*italic(TIME[ij])&quot;, &quot;+pi[2][italic(i)]*italic(TIME[ij])^2+pi[3][italic(i)]*italic(TIME[ij])^3&quot;, &quot;+epsilon[italic(ij)]&quot;), row = c(1:3, 3:4, 4, 4), col = &quot;Level-1 model&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 0, vjust = 1, size = 3.25, parse = T) + scale_x_continuous(expand = c(0, 0), limits = c(1, 10)) + scale_y_continuous(expand = c(0, 0), limits = c(1, 10)) + theme_void() + theme(strip.background.y = element_blank(), strip.text.x = element_text(hjust = 0, size = 12), strip.text.y = element_blank()) + facet_grid(row ~ col) For our third step, here’s the “Parameter values” column. p3 &lt;- tibble(x = 1, y = c(9.5, 9.5, 8.5, 9.5, 8.5, 7.5, 9.5, 8.5, 7.5, 6.5), label = c(&quot;pi[0][italic(i)]==71&quot;, &quot;pi[0][italic(i)]==71&quot;, &quot;pi[1][italic(i)]==1.2&quot;, &quot;pi[0][italic(i)]==50&quot;, &quot;pi[1][italic(i)]==3.8&quot;, &quot;pi[2][italic(i)]==-0.03&quot;, &quot;pi[0][italic(i)]==30&quot;, &quot;pi[1][italic(i)]==10&quot;, &quot;pi[2][italic(i)]==-0.2&quot;, &quot;pi[3][italic(i)]==0.0012&quot;), row = rep(1:4, times = 1:4), col = &quot;Parameter\\nvalues&quot;) %&gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(hjust = 0, vjust = 1, size = 3.25, parse = T) + scale_x_continuous(expand = c(0, 0), limits = c(1, 10)) + scale_y_continuous(expand = c(0, 0), limits = c(1, 10)) + theme_void() + theme(strip.background.y = element_blank(), strip.text.x = element_text(hjust = 0, size = 12), strip.text.y = element_blank()) + facet_grid(row ~ col) We’ll do our fourth step in three stages. First, we’ll make and save four data sets, one for each of the plot panels. Second, we’ll combine those into a single data set, which we’ll wrangle a bit. Third, we’ll make the plots in the final column. # make the four small data sets pi0 &lt;- 71 d1 &lt;- tibble(time = 0:100) %&gt;% mutate(y = pi0) pi0 &lt;- 71 pi1 &lt;- 1.2 d2 &lt;- tibble(time = 0:100) %&gt;% mutate(y = pi0 + pi1 * time) pi0 &lt;- 50 pi1 &lt;- 3.8 pi2 &lt;- -0.03 d3 &lt;- tibble(time = 0:100) %&gt;% mutate(y = pi0 + pi1 * time + pi2 * time^2) pi0 &lt;- 30 pi1 &lt;- 10 pi2 &lt;- -0.2 pi3 &lt;- 0.0012 d4 &lt;- tibble(time = 0:100) %&gt;% mutate(y = pi0 + pi1 * time + pi2 * time^2 + pi3 * time^3) # combine the data sets p4 &lt;- bind_rows(d1, d2, d3, d4) %&gt;% # wrangle mutate(row = rep(1:4, each = n() / 4), col = &quot;Plot of the true change trajectory&quot;) %&gt;% # plot! ggplot(aes(x = time, y = y)) + geom_line() + scale_x_continuous(expression(italic(TIME)), expand = c(0, 0), breaks = 0:2 * 50, limits = c(0, 100)) + scale_y_continuous(expression(italic(Y)), expand = c(0, 0), breaks = 0:2 * 100, limits = c(0, 205)) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text.x = element_text(hjust = 0, size = 12), strip.text.y = element_blank()) + facet_grid(row ~ col) Now we’re finally ready to combine all the elements to make Table 6.4. (p1 | p2 | p3 | p4) + plot_annotation(title = &quot;A taxonomy of polynomial individual change trajectories&quot;) + plot_layout(widths = c(1, 3, 2, 4)) 6.3.1 The shapes of polynomial individual change trajectories. “The ‘no change’ and ‘linear change’ models are familiar; the remaining models, which contain quadratic and cubic functions of TIME, are new” (p. 213, emphasis in the original). 6.3.1.1 “No change” trajectory. The “no change” trajectory is known as a polynomial function of “zero order” because TIME raised to the 0th power is 1 (i.e., \\(TIME^0 = 1\\)). This model is tantamount to including a constant predictor, 1, in the level-1 model, as a multiplier of the sole individual growth parameter, the intercept, \\(\\pi_{0i}\\)… Even though each trajectory is flat, different individuals can have different intercepts and so a collection of true “no change” trajectories is a set of vertically scattered horizontal lines. (p. 215, emphasis in the original) 6.3.1.2 “Linear change” trajectory. The “linear change” trajectory is known as a “first order” polynomial in time because TIME raised to the 1st power equals TIME itself (i.e., \\(TIME^1 = TIME\\)). Linear TIME is the sole predictor and the two individual growth parameters have the usual interpretations. (p. 215, emphasis in the original) 6.3.1.3 “Quadratic change” trajectory. Adding \\(TIME^2\\) to a level-1 individual growth model that already includes linear TIME yields a second order polynomial for quadratic change. Unlike a level-1 model that includes only \\(TIME^2\\), a second order polynomial change trajectory includes two TIME predictors and three growth parameters (\\(\\pi_{0i}\\), \\(\\pi_{1i}\\) and \\(\\pi_{2i}\\)). The first two parameters have interpretations that are similar, but not identical, to those in the linear change trajectory; the third is new. (p. 215, emphasis in the original) In this model, \\(\\pi_{0i}\\) is still the intercept. The \\(\\pi_{1i}\\) parameter is now the instantaneous rate of change when \\(TIME = 0\\). The new \\(\\pi_{2i}\\) parameter, sometimes called the curvature parameter, describes the change in the rate of change. 6.3.1.4 Higher order change trajectories. “Adding higher powers of TIME increases the complexity of the polynomial trajectory” (p. 216). 6.3.2 Selecting a suitable level-1 polynomial trajectory for change. It appears that both the external_pp.csv and external_pp.txt files within the data folder (here) are missing a few occasions. Happily, you can download a more complete version of the data from the good people at stats.idre.ucla.edu. external_pp &lt;- read.table(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2020/01/external_pp.txt&quot;, header = T, sep = &quot;,&quot;) glimpse(external_pp) ## Rows: 270 ## Columns: 5 ## $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6… ## $ external &lt;int&gt; 50, 57, 51, 48, 43, 19, 4, 6, 3, 3, 5, 12, 0, 1, 9, 26, 10, 24, 14, 5, 18, 31, 31, 23, 26, … ## $ female &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0… ## $ time &lt;int&gt; 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0… ## $ grade &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1… There are data from 45 children. external_pp %&gt;% distinct(id) %&gt;% nrow() ## [1] 45 The 45 kids were composed of 28 boys and 17 girls. external_pp %&gt;% distinct(id, female) %&gt;% count(female) %&gt;% mutate(percent = 100 * n / sum(n)) ## female n percent ## 1 0 28 62.22222 ## 2 1 17 37.77778 The data were collected over the children’s first through sixth grades. external_pp %&gt;% distinct(grade) ## grade ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 Our criterion, external, is the sum of the 34 items in the Externalizing subscale of the Child Behavior Checklist. Each item is rated on a 3-point Likert-type scale, ranging from 0 (rarely/never) to 2 (often). The possible range for the sum score of the Externalizing subscale is 0 to 68. Here’s the overall distribution. external_pp %&gt;% ggplot(aes(x = external)) + geom_histogram(binwidth = 1, boundary = 0) + xlim(0, 68) + theme(panel.grid = element_blank()) The data are strongly bound to the left, which is a good thing in this case. We generally like it when our children exhibit fewer externalizing behaviors. Figure 6.7 is based on a subset of the cases in the data. It might make our job easier if we just make a subset of the data, called external_pp_subset. subset &lt;- c(1, 6, 11, 25, 34, 36, 40, 26) external_pp_subset &lt;- external_pp %&gt;% filter(id %in% subset) %&gt;% # this is for the facets in the plot mutate(case = factor(id, levels = subset, labels = LETTERS[1:8])) glimpse(external_pp_subset) ## Rows: 48 ## Columns: 6 ## $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 11, 11, 11, 11, 11, 11, 25, 25, 25, 25, 25, 25, 26, 26,… ## $ external &lt;int&gt; 50, 57, 51, 48, 43, 19, 9, 10, 9, 29, 21, 50, 11, 8, 4, 1, 0, 0, 11, 9, 9, 13, 10, 10, 19, … ## $ female &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ time &lt;int&gt; 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0… ## $ grade &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1… ## $ case &lt;fct&gt; A, A, A, A, A, A, B, B, B, B, B, B, C, C, C, C, C, C, D, D, D, D, D, D, H, H, H, H, H, H, E… Since the order of the polynomials in Figure 6.7 is tailored to each case, we’ll have to first build the subplots in pieces, and then combine then at the end. Make the pieces. # a and e p1 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;A&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/4, linetype = 2) + # quartic stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), # quadratic se = F, size = 1/2) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # e p2 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;E&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/4, linetype = 2) + # quartic stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3), # cubic se = F, size = 1/2) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # b p3 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;B&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/4, linetype = 2) + # quartic stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), # quadratic se = F, size = 1/2) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # f p4 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;F&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/2) + # quartic scale_x_continuous(breaks = 0:7, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # c p5 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;C&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/4, linetype = 2) + # quartic stat_smooth(method = &quot;lm&quot;, formula = y ~ x, # linear se = F, size = 1/2) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # g p6 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;G&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/4, linetype = 2) + # quartic stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), # quadratic se = F, size = 1/2) + scale_x_continuous(breaks = 0:7, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # d p7 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;D&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/4, linetype = 2) + # quartic stat_smooth(method = &quot;lm&quot;, formula = y ~ x, # linear se = F, size = 1/2) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) # h p8 &lt;- external_pp_subset %&gt;% filter(case %in% c(&quot;H&quot;)) %&gt;% ggplot(aes(x = grade, y = external)) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3) + I(x^4), se = F, size = 1/2) + # quartic scale_x_continuous(breaks = 0:7, limits = c(0, 7), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ case) Now combine the subplots to make the full Figure 6.7. ((p1 / p2) | (p3 / p4) | (p5 / p6) | (p7 / p8)) &amp; coord_cartesian(ylim = c(0, 60)) &amp; theme(panel.grid = element_blank()) 6.3.3 Testing higher order terms in a polynomial level-1 model. Let’s talk about priors, first focusing on the overall intercept \\(\\pi_{01}\\). At the time of the original article by Margaret Kraatz Keiley et al. (2000), it was known that boys tended to show more externalizing behaviors than girls, and that boys tend to either increase or remain fairly stable during primary school. Less was known about typical trajectories for girls. However, Sandberg et al. (1991) can give us a sense of what values are reasonable to center on. I their paper, they compared externalizing in two groups if children, broken down between boys and girls. From their second and third tables, we get the following descriptive statistics: sample_statistics &lt;- crossing(gender = c(&quot;boys&quot;, &quot;girls&quot;), sample = c(&quot;School&quot;, &quot;CBCL Nonclinical&quot;)) %&gt;% mutate(n = c(300, 261, 300, 267), mean = c(10.8, 14.5, 10.7, 16.6), sd = c(8.4, 10.4, 8.6, 12.1)) sample_statistics %&gt;% flextable::flextable() .cl-c95e553c{}.cl-c9326c4c{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c958246e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c9582482{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c95845e8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c95845f2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c95845fc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c9584606{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c9584610{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c9584611{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}gendersamplenmeansdboysCBCL Nonclinical30010.88.4boysSchool26114.510.4girlsCBCL Nonclinical30010.78.6girlsSchool26716.612.1 Here’s the weighted mean of the externalizing scores. sample_statistics %&gt;% summarise(weighted_average = sum(n * mean) / sum(n)) ## # A tibble: 1 × 1 ## weighted_average ## &lt;dbl&gt; ## 1 13.0 Here’s the pooled standard deviation. sample_statistics %&gt;% summarise(pooled_sd = sqrt(sum((n - 1) * sd^2) / (sum(n) - 4))) ## # A tibble: 1 × 1 ## pooled_sd ## &lt;dbl&gt; ## 1 9.91 Thus, I propose a good place to start with is with a \\(\\operatorname{Normal}(13, 9.9)\\) prior on the overall intercept, \\(\\gamma_{00}\\). So far, we’ve been using the Student-\\(t\\) distribution for the priors on our variance parameters. Another option favored by McElreath in the second edition of his text is the exponential distribution. The exponential distribution has a single parameter, \\(\\lambda\\), which is often called the rate. The mean of the exponential distribution is the inverse of the rate, \\(1 / \\lambda\\). When you’re working with standardized data, a nice weakly-regularizing priors on the variance parameters is \\(\\operatorname{Exponential}(1)\\), which has a mean of 1. Since our data are not standardized, we might use the prior \\(\\operatorname{Exponential}(1 / s_p)\\), where \\(s_p\\) is the pooled standard deviation. In our case, that would be \\(\\operatorname{Exponential}(1 / 9.9)\\). Here’s what those priors would look like. # left p1 &lt;- tibble(x = seq(from = -30, to = 50, length.out = 200)) %&gt;% mutate(d = dnorm(x, mean = 13, sd = 9.9)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;prior for &quot;*gamma[0][0]), x = (expression(Normal(13*&#39;, &#39;*9.9)))) + theme(panel.grid = element_blank()) # right p2 &lt;- tibble(x = seq(from = 0, to = 60, length.out = 200)) %&gt;% mutate(d = dexp(x, rate = 1.0 / 9.9)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;priors for &quot;*sigma[0]~and~sigma[epsilon]), x = (expression(Exponential(1/9.9)))) + theme(panel.grid = element_blank()) # combine p1 | p2 Now using those priors, here’s how to fit the model. fit6.10 &lt;- brm(data = external_pp, family = gaussian, external ~ 1 + (1 | id), prior = c(prior(normal(13, 9.9), class = Intercept), prior(exponential(1.0 / 9.9), class = sd), prior(exponential(1.0 / 9.9), class = sigma)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.10&quot;) For the next three models, we’ll keep those priors for \\(\\gamma_{00}\\) and the \\(\\sigma\\) parameters. Yet now we have to consider the new \\(\\gamma_{00}\\) parameters which will account for the population-level effects if time, from a linear, quadratic, and cubic perspective. Given that time takes on integer values 0 through 5, the simple linear slope \\(\\gamma_{10}\\) is the expected change from one grade to the next. Since we know from the previous literature that boys tend to have either stable or slightly-increasing trajectories for their externalizing behaviors (recall we’re uncertain about girls), a mildly conservative prior might be centered on zero with, say, half of the pooled standard deviation on the scale, \\(\\operatorname{Normal}(0, 4.95)\\). This is the rough analogue to a \\(\\operatorname{Normal}(0, 0.5)\\) prior on standardized data. Without better information, we’ll extend that same \\(\\operatorname{Normal}(0, 4.95)\\) prior on \\(\\gamma_{20}\\) and \\(\\gamma_{30}\\). As to the new \\(\\rho\\) parameters, we’ll use our typical weakly-regularizing \\(\\operatorname{LKJ}(4)\\) prior on the level-2 correlation matrices. # linear fit6.11 &lt;- brm(data = external_pp, family = gaussian, external ~ 0 + Intercept + time + (1 + time | id), prior = c(prior(normal(0, 4.95), class = b), prior(normal(13, 9.9), class = b, coef = Intercept), prior(exponential(1.0 / 9.9), class = sd), prior(exponential(1.0 / 9.9), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, file = &quot;fits/fit06.11&quot;) # quadratic fit6.12 &lt;- brm(data = external_pp, family = gaussian, external ~ 0 + Intercept + time + I(time^2) + (1 + time + I(time^2) | id), prior = c(prior(normal(0, 4.95), class = b), prior(normal(13, 9.9), class = b, coef = Intercept), prior(exponential(1.0 / 9.9), class = sd), prior(exponential(1.0 / 9.9), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, control = list(adapt_delta = .995), file = &quot;fits/fit06.12&quot;) # cubic fit6.13 &lt;- brm(data = external_pp, family = gaussian, external ~ 0 + Intercept + time + I(time^2) + I(time^3) + (1 + time + I(time^2) + I(time^3) | id), prior = c(prior(normal(0, 4.95), class = b), prior(normal(13, 9.9), class = b, coef = Intercept), prior(exponential(1.0 / 9.9), class = sd), prior(exponential(1.0 / 9.9), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, control = list(adapt_delta = .85), file = &quot;fits/fit06.13&quot;) Compute and save the WAIC estimates. fit6.10 &lt;- add_criterion(fit6.10, criterion = &quot;waic&quot;) fit6.11 &lt;- add_criterion(fit6.11, criterion = &quot;waic&quot;) fit6.12 &lt;- add_criterion(fit6.12, criterion = &quot;waic&quot;) fit6.13 &lt;- add_criterion(fit6.13, criterion = &quot;waic&quot;) Here we’ll make a simplified version of the WAIC output, saving the results as waic_summary. # order the parameters, which will come in handy in the next block order &lt;- c(&quot;gamma[0][0]&quot;, &quot;gamma[1][0]&quot;, &quot;gamma[2][0]&quot;, &quot;gamma[3][0]&quot;, &quot;sigma[epsilon]&quot;, &quot;sigma[0]&quot;, &quot;sigma[1]&quot;, &quot;sigma[2]&quot;, &quot;sigma[3]&quot;, &quot;rho[0][1]&quot;, &quot;rho[0][2]&quot;, &quot;rho[0][3]&quot;, &quot;rho[1][2]&quot;, &quot;rho[1][3]&quot;, &quot;rho[2][3]&quot;, &quot;WAIC&quot;) waic_summary &lt;- loo_compare(fit6.10, fit6.11, fit6.12, fit6.13, criterion = &quot;waic&quot;) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;fit&quot;) %&gt;% arrange(fit) %&gt;% transmute(model = str_c(&quot;model &quot;, letters[1:n()]), summary = str_c(formatC(waic, digits = 2, format = &quot;f&quot;), &quot; (&quot;, formatC(se_waic, digits = 2, format = &quot;f&quot;), &quot;)&quot;), parameter = factor(&quot;WAIC&quot;, levels = order)) waic_summary ## model summary parameter ## 1 model a 1959.32 (29.68) WAIC ## 2 model b 1923.30 (28.15) WAIC ## 3 model c 1906.43 (26.52) WAIC ## 4 model d 1902.32 (26.24) WAIC Here we’ll use a little summarizing, wrangling, and creative plotting, we’ll make a streamlined version of Table 6.5 with ggplot2. # extract and wrangle the posterior summaries tibble(model = str_c(&quot;model &quot;, letters[1:4]), fit = str_c(&quot;fit6.1&quot;, 0:3)) %&gt;% mutate(p = map(fit, ~ get(.) %&gt;% posterior_summary() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;) %&gt;% filter(!str_detect(parameter, &quot;r_id\\\\[&quot;) &amp; parameter != &quot;lp__&quot; &amp; parameter != &quot;lprior&quot;))) %&gt;% unnest(p) %&gt;% mutate(greek = case_when( parameter == &quot;b_Intercept&quot; ~ &quot;gamma[0][0]&quot;, parameter == &quot;b_time&quot; ~ &quot;gamma[1][0]&quot;, parameter == &quot;b_ItimeE2&quot; ~ &quot;gamma[2][0]&quot;, parameter == &quot;b_ItimeE3&quot; ~ &quot;gamma[3][0]&quot;, parameter == &quot;sd_id__Intercept&quot; ~ &quot;sigma[0]&quot;, parameter == &quot;sd_id__time&quot; ~ &quot;sigma[1]&quot;, parameter == &quot;sd_id__ItimeE2&quot; ~ &quot;sigma[2]&quot;, parameter == &quot;sd_id__ItimeE3&quot; ~ &quot;sigma[3]&quot;, parameter == &quot;sigma&quot; ~ &quot;sigma[epsilon]&quot;, parameter == &quot;cor_id__Intercept__time&quot; ~ &quot;rho[0][1]&quot;, parameter == &quot;cor_id__Intercept__ItimeE2&quot; ~ &quot;rho[0][2]&quot;, parameter == &quot;cor_id__Intercept__ItimeE3&quot; ~ &quot;rho[0][3]&quot;, parameter == &quot;cor_id__time__ItimeE2&quot; ~ &quot;rho[1][2]&quot;, parameter == &quot;cor_id__time__ItimeE3&quot; ~ &quot;rho[1][3]&quot;, parameter == &quot;cor_id__ItimeE2__ItimeE3&quot; ~ &quot;rho[2][3]&quot; )) %&gt;% mutate(summary = str_c(formatC(Estimate, digits = 2, format = &quot;f&quot;), &quot; (&quot;, formatC(Est.Error, digits = 2, format = &quot;f&quot;), &quot;)&quot;), parameter = factor(greek, levels = order)) %&gt;% select(model, summary, parameter) %&gt;% # add in the WAIC summary information bind_rows(waic_summary) %&gt;% mutate(parameter = fct_rev(parameter)) %&gt;% # plot! ggplot(aes(x = model, y = parameter, label = summary)) + geom_text(hjust = 1, size = 3) + scale_x_discrete(NULL, position = &quot;top&quot;) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + coord_cartesian(xlim = c(NA, 3.5)) + theme_minimal() + theme(axis.text.x = element_text(hjust = 1, color = &quot;black&quot;), axis.text.y = element_text(hjust = 0, color = &quot;black&quot;), axis.line.x = element_line(linewidth = 1/4), panel.grid = element_blank()) Here are the formal WAIC difference estimates. loo_compare(fit6.10, fit6.11, fit6.12, fit6.13, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.13 0.0 0.0 -951.2 13.1 68.1 5.8 1902.3 26.2 ## fit6.12 -2.1 1.4 -953.2 13.3 66.6 5.9 1906.4 26.5 ## fit6.11 -10.5 4.9 -961.6 14.1 61.7 6.1 1923.3 28.1 ## fit6.10 -28.5 9.1 -979.7 14.8 38.6 4.1 1959.3 29.7 The WAIC difference estimates suggest the cubic and quadratic models are about the same, but both are notably better than the intercepts-only and linear models. In the text, Singer and Willett preferred the quadratic model (fit6.12). Now we’ve fit our Bayesian version of the model, why not use the posterior distribution to make a high-quality model-based version of Figure 6.7? # define the new data nd &lt;- external_pp_subset %&gt;% distinct(id, case) %&gt;% expand_grid(time = seq(from = 0, to = 5, length.out = 50)) %&gt;% mutate(grade = time + 1) # extract the fitted trajectories and wrangle fitted(fit6.12, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = grade, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/4) + geom_line(aes(y = Estimate)) + geom_point(data = external_pp_subset, aes(y = external)) + scale_x_continuous(breaks = 0:7, labels = 0:7, expand = c(0, 0), limits = c(0, 7)) + ylab(&quot;external&quot;) + coord_cartesian(ylim = c(0, 60)) + theme(panel.grid = element_blank()) + facet_wrap(~ case, ncol = 4) At the end of this section in the text, Singer and Willett briefly discussed fitting an expansion of the quadratic model which included the variable female as time-invariant predictor for the individual differences in initial status (\\(\\pi_{0i}\\)), the instantaneous rate of change (\\(\\pi_{1i}\\)), and curvature (\\(\\pi_{2i}\\)). We might express the model in formal notation as \\[ \\begin{align*} \\text{external}_{ij} &amp; = \\gamma_{00} + \\gamma_{01} \\text{female}_i \\\\ &amp; \\;\\;\\; + \\gamma_{10} \\text{time}_{ij} + \\gamma_{11} ( \\text{female}_i \\times \\text{time}_{ij} ) \\\\ &amp; \\;\\;\\; + \\gamma_{20} \\text{time}_{ij}^2 + \\gamma_{21} ( \\text{female}_i \\times \\text{time}_{ij}^2 ) \\\\ &amp; \\;\\;\\; + [\\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij} + \\zeta_{2i} \\text{time}_{ij}^2 + \\epsilon_{ij}] \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{2i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal}(\\mathbf 0, \\mathbf D \\mathbf \\Omega \\mathbf D&#39;) \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_2 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} &amp; \\rho_{02} \\\\ \\rho_{10} &amp; 1 &amp; \\rho_{12} \\\\ \\rho_{20} &amp; \\rho_{21} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(13, 9.9) \\\\ \\gamma_{01}, \\dots, \\gamma_{21} &amp; \\sim \\operatorname{Normal}(0, 4.95) \\\\ \\sigma_0, \\dots, \\sigma_2 &amp; \\sim \\operatorname{Exponential}(1 / 9.9) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Exponential}(1 / 9.9) \\\\ \\mathbf\\Omega &amp; \\sim \\operatorname{LKJ}(4). \\end{align*} \\] Now fit the model. fit6.14 &lt;- brm(data = external_pp, family = gaussian, external ~ 0 + Intercept + time + I(time^2) + female + time:female + I(time^2):female + (1 + time + I(time^2) | id), prior = c(prior(normal(0, 4.95), class = b), prior(normal(13, 9.9), class = b, coef = Intercept), prior(exponential(1.0 / 9.9), class = sd), prior(exponential(1.0 / 9.9), class = sigma), prior(lkj(4), class = cor)), iter = 2500, warmup = 1000, chains = 3, cores = 3, seed = 6, control = list(adapt_delta = .99), file = &quot;fits/fit06.14&quot;) Singer and Willett reported the new parameters \\(\\gamma_{01}\\), \\(\\gamma_{02}\\) and \\(\\gamma_{03}\\), were unimpressive. Let’s check their posteriors with a coefficient plot. posterior_summary(fit6.14)[4:6, ] %&gt;% data.frame() %&gt;% mutate(parameter = str_c(&quot;gamma[&quot;, 0:2, &quot;][1]&quot;)) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = parameter)) + geom_vline(xintercept = 0, color = &quot;white&quot;) + geom_pointrange(fatten = 1, size = 1/4) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + labs(subtitle = &quot;How well did &#39;female&#39; do?&quot;, x = &quot;marginal posterior&quot;) + theme(panel.grid = element_blank()) Yeah, they were either small or highly uncertain. Let’s go all the way with a WAIC comparison with fit6.12. fit6.14 &lt;- add_criterion(fit6.14, criterion = &quot;waic&quot;) loo_compare(fit6.12, fit6.14, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.12 0.0 0.0 -953.2 13.3 66.6 5.9 1906.4 26.5 ## fit6.14 -0.4 1.0 -953.6 13.3 67.8 6.1 1907.2 26.6 Yep, there’s no compelling reason to prefer fit6.14 over fit6.12. The predictor female looks like a dud, which suggests the externalizing behavior trajectories are about the same for the boys and the girls. 6.4 Truly nonlinear trajectories All the individual growth models described so far—including the curvilinear ones presented in this chapter–share an important mathematical property: they are linear in the individual growth parameters. Why do we use the label “linear” to describe trajectories that are blatantly nonlinear? The explanation for this apparent paradox is that this mathematical property depends not on the shape of the underlying growth trajectory but rather where–in which portion of the model–the nonlinearity arises. In all previous model, nonlinearity (or discontinuity) stems from the representation of the predictors. To allow the hypothesized trajectory to deviate from a straight line, TIME is either transformed or expressed using higher order polynomial terms. In the truly nonlinear models we now discuss, nonlinearity arises in a different way–through the parameters. (pp. 223–224, emphasis in the original) 6.4.1 What do we mean by truly nonlinear models? Linear models are linear in the sense that the expected value for the criterion (\\(\\operatorname E(y)\\)) is the sum of the \\(\\gamma\\)’s multiplies by either a constant (in the case of \\(\\gamma_{00}\\)) or b the regression weight (the point estimate for frequentists or the measure of central tendency in the posterior, often the mean or median, for Bayesians). In other words, \\(\\operatorname E(y)\\) is a weighted linear composite of the \\(\\gamma\\)’s multiplied by a given set of predictor values. Truly nonlinear models do not have this property. 6.4.2 The logistic individual growth curve. Load the data from Tivan’s (1980) unpublished dissertation. library(tidyverse) foxngeese_pp &lt;- read_csv(&quot;data/foxngeese_pp.csv&quot;) glimpse(foxngeese_pp) ## Rows: 445 ## Columns: 4 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, … ## $ game &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26… ## $ nmoves &lt;dbl&gt; 4, 7, 8, 3, 3, 3, 7, 6, 3, 7, 5, 3, 8, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 5, 3, 3, 6, 3, 3, … ## $ read &lt;dbl&gt; 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4… There are responses from 17 participants in these data. foxngeese_pp %&gt;% distinct(id) ## # A tibble: 17 × 1 ## id ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## 11 11 ## 12 12 ## 13 13 ## 14 14 ## 15 15 ## 16 16 ## 17 17 In this experiment, each child played up to 27 games, with some variation among the children. foxngeese_pp %&gt;% count(id, name = &quot;# games, per kid&quot;) %&gt;% count(`# games, per kid`) ## # A tibble: 3 × 2 ## `# games, per kid` n ## &lt;int&gt; &lt;int&gt; ## 1 14 1 ## 2 26 1 ## 3 27 15 Our criterion is nmoves, the number of a child made within a game “before making a catastrophic error” (p. 226). Here’s the overall distribution of nmoves. foxngeese_pp %&gt;% ggplot(aes(x = nmoves)) + geom_bar() + theme(panel.grid = element_blank()) We can get a sense of the child-level data with our version of Figure 6.8. subset &lt;- c(&quot;1&quot;, &quot;4&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;11&quot;, &quot;12&quot;, &quot;15&quot;) foxngeese_pp %&gt;% filter(id %in% subset) %&gt;% mutate(id = if_else(id &lt; 10, str_c(&quot;0&quot;, id), as.character(id))) %&gt;% ggplot(aes(x = game, y = nmoves)) + geom_point(size = 2/3) + scale_y_continuous(breaks = 0:5 * 5, limits = c(0, 25)) + theme(panel.grid = element_blank()) + facet_wrap(~ id, ncol = 4, labeller = label_both) In these data, nmoves always lies between 1 and 20. Based on what we know about the structure of these data and the experiment which produced them, Singer and Willett proposed our statistical model should include: a lower asymptote, which captures how each child had to make at least one move; an upper asymptote, which captures the maximum number of moves allowed, which seems to be 20; and a smooth curve showing growth from the lower asymptote to the upper, which involves a period of accelerated learning somewhere in the middle. They then point out these features are captured in a logistic trajectory. Their proposed logistic model of change follows the form \\[ \\begin{align*} \\text{nmoves}_{ij} &amp; = 1 + \\frac{19}{1 + \\pi_{0i} e^{-(\\pi_{1i} \\text{game}_{ij})}} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon^2). \\end{align*} \\] This equation is set up so that as the value for \\(\\text{game}_{ij} \\rightarrow \\infty\\), the denominator in the equation shrinks to 1, which leave \\(1 + 19/1 = 20\\), the upper asymptote. Yet as \\(\\text{game}_{ij} \\rightarrow -\\infty\\), the denominator inflates to \\(\\infty\\), leaving \\(1 + 0 = 1\\), the lower asymptote. The \\(\\pi_{0i}\\) and \\(\\pi_{1i}\\) terms are no longer simply the intercepts and time slopes, as in earlier models. To get a sense, we’ll make Figure 6.9. To help, we might make a custom function that follows the equation, above. sw_logistic &lt;- function(pi0 = 15, pi1 = 0.3, game = 10) { 1 + (19 / (1 + pi0 * exp(-pi1 * game))) } Now use our sw_logistic() function1 to make Figure 6.9. crossing(pi0 = c(1.5, 15, 150), pi1 = c(0.1, 0.3, 0.5)) %&gt;% expand(nesting(pi0, pi1), game = 0:30) %&gt;% mutate(y = sw_logistic(pi0, pi1, game), pi0_f = factor(str_c(&quot;pi[0]==&quot;, pi0), levels = c(&quot;pi[0]==150&quot;, &quot;pi[0]==15&quot;, &quot;pi[0]==1.5&quot;))) %&gt;% ggplot(aes(x = game, y = y, group = pi1)) + geom_line(aes(size = pi1)) + scale_size_continuous(expression(pi[1]), range = c(1/3, 1), breaks = c(0.1, 0.3, 0.5)) + scale_y_continuous(&quot;nmoves&quot;, limits = c(0, 25)) + theme(panel.grid = element_blank()) + facet_wrap(~pi0_f, labeller = label_parsed) Though \\(\\pi_{0i}\\) has clear implications for the intercept, it’s not quite the same thing as the intercept. Similarly, though \\(\\pi_{1i}\\) has an influence on how rapidly the trajectories approach the upper asymptote, it’s not the same thing as the slope. Once you go nonlinear, it can become tricky to interpret the parameters directly. Since we’ll be fitting this model as Bayesians, let’s talk about priors. Based on the plots in Figure 6.9, it seems like we should center the \\(\\gamma_{00}\\) prior on a relatively large value. If you compare the three panels of the plot, it seems like there’s a diminishing returns effect as the value for \\(\\pi_0\\) went from 1.5 to 15 to 150. I propose something like \\(\\operatorname N(15, 3)\\). Now let’s talk about the prior for \\(\\gamma_{10}\\). In each of the three panels of Figure 6.9, it looks like the values of 0.1 to 0.5 cover a good range of plausible values. If we were conservative about how quickly the children might learn the game, perhaps we’d settle for a prior like \\(\\operatorname N(0.2, 0.1)\\). A nonlinear model like this can be hard to understand even with the benefit of Figure 6.9. To get a clear sense of our priors, we might do a graphical prior predictive check. Here we simulate 100 draws from the prior predictive distribution of \\[ \\begin{align*} y_j &amp; = 1 + \\frac{19}{1 + \\gamma_{00} e^{-(\\gamma_{10} \\text{game}_j)}} \\\\ \\gamma_{00} &amp; \\sim \\operatorname N(15, 3) \\\\ \\gamma_{10} &amp; \\sim \\operatorname N(0.2, 0.1). \\end{align*} \\] # how many do you want? n &lt;- 100 # simulate set.seed(6) tibble(n = 1:n, gamma00 = rnorm(n, mean = 15, sd = 3), gamma10 = rnorm(n, mean = 0.2, sd = 0.1)) %&gt;% expand(nesting(n, gamma00, gamma10), game = 0:30) %&gt;% mutate(y = sw_logistic(gamma00, gamma10, game)) %&gt;% # plot! ggplot(aes(x = game, y = y, group = n)) + geom_hline(yintercept = c(1, 20), color = &quot;white&quot;) + geom_line(size = 1/4, alpha = 1/2) + scale_y_continuous(limits = c(1, 20)) + theme(panel.grid = element_blank()) To my eye, this looks like a respectable starting point. As for our variance parameters, I’d be comfortable just setting \\(\\sigma_0 \\sim \\operatorname{Exponential}(1)\\) and \\(\\sigma_\\epsilon \\sim \\operatorname{Exponential}(1)\\). Since the range for our \\(\\gamma_{10}\\) prior is about an order of magnitude smaller, I’d argue \\(\\operatorname{Exponential}(10)\\) would be a decent starting point for our \\(\\sigma_1\\) prior. As is typical, I recommend the weakly regularizing \\(\\operatorname{LKJ}(4)\\) for the level-2 correlation matrix. Here’s how to fit the model with brms. fit6.15 &lt;- brm(data = foxngeese_pp, family = gaussian, bf(nmoves ~ 1 + (19.0 / (1.0 + g0 * exp(-g1 * game))), g0 + g1 ~ 1 + (1 |i| id), nl = TRUE), prior = c(prior(normal(15, 3), nlpar = g0), prior(normal(0.2, 0.1), nlpar = g1), prior(exponential(1), class = sd, nlpar = g0), prior(exponential(10), class = sd, nlpar = g1), prior(exponential(1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, cores = 3, chains = 3, init = 0, control = list(adapt_delta = .995), file = &quot;fits/fit06.15&quot;) Before we explore the results, you might have noticed a few odd things about our syntax. For example, notice how we wrapped our model formula within a bf() statement, that the actual formula is composed of both variable names (e.g., game) AND parameter names (e.g., g0, which is an abbreviation for \\(\\gamma_{00}\\)), and that we set nl = TRUE. Further, did you notice how we used the nlpar argument in several of our prior() lines? Taken as a whole, these indicate we used the brms non-linear syntax. Since so few of the models in this book require the brms non-linear syntax, I’m not going to explain it in detail, here. However, you can learn all about it in Bürkner’s (2021a) vignette, Estimating non-linear models with brms. I also use the non-linear syntax quite a bit in my (2021) translation of the second edition of McElreath’s text. But as to the task at hand, let’s look at the summary for our model. print(fit6.15, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: nmoves ~ 1 + (19/(1 + g0 * exp(-g1 * game))) ## g0 ~ 1 + (1 | i | id) ## g1 ~ 1 + (1 | i | id) ## Data: foxngeese_pp (Number of observations: 445) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Group-Level Effects: ## ~id (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(g0_Intercept) 1.331 1.300 0.034 4.830 1.002 1709 1661 ## sd(g1_Intercept) 0.059 0.013 0.041 0.089 1.007 1012 1824 ## cor(g0_Intercept,g1_Intercept) 0.101 0.329 -0.581 0.687 1.014 219 394 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## g0_Intercept 13.578 1.939 10.085 17.596 1.000 3862 2577 ## g1_Intercept 0.124 0.016 0.093 0.156 1.004 1134 1767 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 3.748 0.134 3.496 4.021 1.000 4250 1919 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our posterior summaries are similar to the results Singer and Willett reported in their Table 6.6. To further explore the model, let’s use the fitted() approach to make our version of Figure 6.10a. nd &lt;- tibble(game = seq(from = 0, to = 30, by = 0.1)) p1 &lt;- fitted(fit6.15, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = game, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = c(1, 20), color = &quot;white&quot;) + geom_ribbon(alpha = 1/4) + geom_line() + scale_y_continuous(&quot;nmoves&quot;, limits = c(0, 25), expand = c(0, 0)) + labs(subtitle = &quot;Model A&quot;) + theme(panel.grid = element_blank()) p1 For the next model, Singer and Willett proposed we use a mean-centered version of read as predictor of the level-2 intercepts and slopes. Here we make a version of that variable, which we’ll call read_c. foxngeese_pp &lt;- foxngeese_pp %&gt;% mutate(read_c = read - mean(read)) For our new coefficients, I propose we continue to take a weakly-regularizing approach. Since the level-2 variables they’re predicting are on different scales, it seems like these priors should be on different scales, too. I suggest we specify \\(\\gamma_{01} \\sim \\operatorname{Normal}(0, 1)\\) and \\(\\gamma_{11} \\sim \\operatorname{Normal}(0, 0.1)\\). If you follow along, we might express our statistical model in formal notation as \\[ \\begin{align*} \\text{nmoves}_{ij} &amp; = 1 + \\frac{19}{1 + \\pi_{0i} e^{-(\\pi_{1i} \\text{game}_{ij})}} + \\epsilon_{ij} \\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} \\left (\\text{read}_i - \\overline{\\text{read}} \\right ) + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} \\left (\\text{read}_i - \\overline{\\text{read}} \\right ) + \\zeta_{1i} \\\\ \\epsilon_{ij} &amp; \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf \\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1\\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{10} &amp; 1 \\end{bmatrix}, \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(15, 3) \\\\ \\gamma_{10} &amp; \\sim \\operatorname{Normal}(0.2, 0.1) \\\\ \\gamma_{01} &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\gamma_{11} &amp; \\sim \\operatorname{Normal}(0, 0.1) \\\\ \\sigma_0 &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_1 &amp; \\sim \\operatorname{Exponential}(10) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf \\Omega &amp; \\sim \\operatorname{LKJ}(4). \\end{align*} \\] Fit the model. fit6.16 &lt;- brm(data = foxngeese_pp, family = gaussian, bf(nmoves ~ 1 + (19.0 / (1.0 + g0 * exp(-g1 * game))), g0 + g1 ~ 1 + read_c + (1 |i| id), nl = TRUE), prior = c(prior(normal(15, 3), nlpar = g0, coef = Intercept), prior(normal(0, 1), nlpar = g0, coef = read_c), prior(normal(0.2, 0.1), nlpar = g1, coef = Intercept), prior(normal(0, 0.1), nlpar = g1, coef = read_c), prior(exponential(1), class = sd, nlpar = g0), prior(exponential(10), class = sd, nlpar = g1), prior(exponential(1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, cores = 3, chains = 3, init = 0, control = list(adapt_delta = .999), file = &quot;fits/fit06.16&quot;) Check the results. print(fit6.16) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: nmoves ~ 1 + (19/(1 + g0 * exp(-g1 * game))) ## g0 ~ 1 + read_c + (1 | i | id) ## g1 ~ 1 + read_c + (1 | i | id) ## Data: foxngeese_pp (Number of observations: 445) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Group-Level Effects: ## ~id (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(g0_Intercept) 1.36 1.31 0.03 4.77 1.00 1729 2055 ## sd(g1_Intercept) 0.06 0.01 0.04 0.09 1.00 1282 1849 ## cor(g0_Intercept,g1_Intercept) 0.10 0.34 -0.60 0.68 1.01 413 849 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## g0_Intercept 13.60 1.90 10.14 17.59 1.00 4409 2172 ## g0_read_c 0.47 0.97 -1.44 2.35 1.00 4733 2143 ## g1_Intercept 0.12 0.02 0.09 0.16 1.00 1645 1939 ## g1_read_c 0.02 0.02 -0.02 0.06 1.00 1643 1644 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 3.75 0.13 3.51 4.00 1.00 4370 2248 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before you get worried about how our posterior mean for \\(\\gamma_{01}\\) has the opposite sign as the point estimate Singer and Willett reported in the text, take a look at the whole posterior. library(tidybayes) as_draws_df(fit6.16) %&gt;% ggplot(aes(x = b_g0_read_c, y = 0)) + stat_halfeye(.width = c(.8, .95)) + geom_vline(xintercept = -0.3745, linetype = 2) + annotate(geom = &quot;text&quot;, x = -0.3745, y = 0.03, label = &quot;Singer and Willett&#39;s\\npoint estimate&quot;, angle = 90, hjust = 0, size = 3) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(gamma[0][1])) + theme(panel.grid = element_blank()) The posterior is wide and the point estimate in the text fits comfortably within our inner 80% interval. Okay, let’s explore this model further by making the rest of Figure 6.10. nd &lt;- crossing(game = seq(from = 0, to = 30, by = 0.1), read_c = c(-1.58, 1.58)) p2 &lt;- fitted(fit6.16, newdata = nd, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(read_c = ifelse(read_c &lt; 0, &quot;low (-1.58)&quot;, &quot;high (1.58)&quot;)) %&gt;% ggplot(aes(x = game, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = read_c, color = read_c)) + geom_hline(yintercept = c(1, 20), color = &quot;white&quot;) + geom_ribbon(alpha = 1/4, linewidth = 0) + geom_line() + scale_fill_viridis_d(option = &quot;A&quot;, begin = .25, end = .75) + scale_color_viridis_d(option = &quot;A&quot;, begin = .25, end = .75) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 25), expand = c(0, 0)) + labs(subtitle = &quot;Model B&quot;) + theme(panel.grid = element_blank()) # combine p1 + p2 Notice how unimpressive the expected trajectories between the two levels of read_c are when you include the 95% interval bands. Beware plots of fitted lines that do not include the 95% intervals! We might compare our two nonlinear models with their WAIC estimates. fit6.15 &lt;- add_criterion(fit6.15, criterion = &quot;waic&quot;) fit6.16 &lt;- add_criterion(fit6.16, criterion = &quot;waic&quot;) loo_compare(fit6.15, fit6.16, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.16 0.0 0.0 -1236.0 18.3 28.9 3.3 2472.0 36.7 ## fit6.15 -0.1 0.6 -1236.1 18.4 28.8 3.3 2472.2 36.7 They’re nearly the same, suggesting fit6.16 was overfit. However, now that we have it, we might use our overfit model fit6.16 to an updated version of Figure 6.8. # define the new data nd &lt;- foxngeese_pp %&gt;% distinct(id, read_c) %&gt;% filter(id %in% subset) %&gt;% expand_grid(game = seq(from = 0, to = 30, by = 0.1)) # extract the fitted trajectories fitted(fit6.16, newdata = nd) %&gt;% # wrangle data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = game)) + geom_hline(yintercept = c(1, 20), color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = read_c), alpha = 1/4, linewidth = 0) + geom_line(aes(y = Estimate, color = read_c)) + geom_point(data = foxngeese_pp %&gt;% filter(id %in% subset), aes(y = nmoves), size = 2/3) + scale_fill_viridis_c(option = &quot;A&quot;, begin = .15, end = .85, limits = range(foxngeese_pp$read_c)) + scale_color_viridis_c(option = &quot;A&quot;, begin = .15, end = .85, limits = range(foxngeese_pp$read_c)) + scale_y_continuous(breaks = 0:5 * 5, limits = c(0, 25), expand = c(0, 0)) + theme(panel.grid = element_blank()) + facet_wrap(~ id, ncol = 4, labeller = label_both) 6.4.3 A survey of truly nonlinear change trajectories. By now you should realize that you can represent individual change using a virtually limitless number of mathematical functions…. How then can you possibly specify a suitable model for your data and purposes? Clearly, you need more than empirical evidence. Among a group of well-fitting growth models, blind numeric comparison of descriptive statistics, goodness-of-fit, and regression diagnostics will rarely pick out the best one. As you might expect, we recommend that you blend theory and empirical evidence, articulating a rationale that you can translate into a statistical model. This recommendation underscores an important point that can often be overlooked in the heat of data analysis: substance is paramount. The best way to select an appropriate individual growth model is to work within an explicit theoretical framework. We suggest that you ask not “What is the best model for the job?” but, rather, “What model is most theoretically sound?” (pp. 232–233, emphasis in the original) Based on the work of Mead and Pike (1975), Singer and Willett suggested we might divide up the range of nonlinear models into four bins: polynomial, hyperbolic, inverse polynomial, and exponential. 6.4.3.1 Hyperbolic growth. The rectangular hyperbola is one of the simplest nonlinear models for individual change. TIME enters as a reciprocal in the denominator of the model’s right side. This model possesses an important property for modeling biological and agricultural growth: over time, its outcome smoothly approaches–but never reaches–an asymptote. (p. 234, emphasis in the original) The example of a hyperbolic growth from Table 6.7 (p. 234) followed the form \\[Y_{ij} = \\alpha_i - \\frac{1}{\\pi_{1i} TIME_{ij}} + \\epsilon_{ij},\\] where \\(\\pi_{1i}\\) is kinda like a growth slope and there is no \\(\\pi_{0i}\\) because \\(\\pi_{0i}\\) parameters are for losers. To get a sense of what this model provides, here’s our version of the upper-left panel of Figure 6.11. text &lt;- tibble(time = c(0, 1.2, 4.6, 8), y = c(102.5, 90, 87, 85), label = c(&quot;alpha==100&quot;, &quot;pi[1]==0.01&quot;, &quot;pi[1]==0.02&quot;, &quot;pi[1]==0.1&quot;)) p1 &lt;- crossing(alpha = 100, pi1 = c(0.01, 0.02, 0.1)) %&gt;% expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% mutate(y = alpha - (1 / (pi1 * time))) %&gt;% ggplot(aes(x = time, y = y, group = pi1)) + geom_hline(yintercept = 100, color = &quot;white&quot;) + geom_line() + geom_text(data = text, aes(label = label), size = 3, hjust = 0, parse = T) + labs(title = &quot;Hyperbola&quot;, y = expression(E(italic(Y)))) + coord_cartesian(ylim = c(0, 100)) + theme(panel.grid = element_blank()) p1 Note how, in this plot, \\(\\alpha\\) is the upper asymptote. 6.4.3.2 Inverse polynomial growth. “The family of inverse polynomials extends the rectangular hyperbola by adding higher powers of TIME to the denominator of the quotient on the model’s right side” (p. 236, emphasis in the original). The example of a hyperbolic growth from Table 6.7 followed the form \\[Y_{ij} = \\alpha_i - \\frac{1}{\\pi_{1i} TIME_{ij} + \\pi_{2i} TIME_{ij}^2} + \\epsilon_{ij}.\\] To get a sense of what this model provides, here’s our version of the upper-right panel of Figure 6.11. text &lt;- tibble(time = c(0, 3.5, 7.5, 10, 1.5), y = c(102.5, 94, 91, 77, 8), label = c(&quot;alpha==100&quot;, &quot;pi[2]==0.015&quot;, &quot;pi[2]==0&quot;, &quot;pi[2]==-0.0015&quot;, &quot;pi[1]==0.02~(&#39;for all curves&#39;)&quot;), hjust = c(0, 0, 0, 1, 0)) p2 &lt;- crossing(alpha = 100, pi1 = 0.02, pi2 = c(0.015, 0, -0.0015)) %&gt;% expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% mutate(y = alpha - (1 / (pi1 * time + pi2 * time^2))) %&gt;% ggplot(aes(x = time, y = y, group = pi2)) + geom_hline(yintercept = 100, color = &quot;white&quot;) + geom_line() + geom_text(data = text, aes(label = label, hjust = hjust), size = 3, parse = T) + labs(title = &quot;Inverse polynomial&quot;, y = expression(E(italic(Y)))) + coord_cartesian(ylim = c(0, 100)) + theme(panel.grid = element_blank()) p2 6.4.3.3 Exponential growth. Exponential growth is probably the most widely used class of truly nonlinear models. This theoretically compelling group has been used for centuries to model biological, agricultural, and physical growth. This class includes a wide range of different functional forms, but all contain an exponent of \\(e\\), the base of the natural logarithm. (p. 237, emphasis in the original) The example of simple exponential growth from Table 6.7 followed the form \\[Y_{ij} = \\pi_{0i} e^{\\pi_{1i} TIME_{ij}} + \\epsilon_{ij},\\] where now we have the triumphant return of \\(\\pi_{0i}\\), which still isn’t quite an intercept but is close to one. The of negative exponential growth from Table 6.7 followed the form \\[Y_{ij} = \\alpha_i -(\\alpha_i - \\pi_{0i}) e^{\\pi_{1i} TIME_{ij}} + \\epsilon_{ij}.\\] Let’s make the panels of the lower row of Figure 6.11. # left text &lt;- tibble(time = 10, y = c(100.5, 39, 16, 1), label = c(&quot;pi[1]==0.3&quot;, &quot;pi[1]==0.2&quot;, &quot;pi[1]==0.1&quot;, &quot;pi[0]==5~(&#39;for all curves&#39;)&quot;), vjust = c(0, .5, .5, 0)) p3 &lt;- crossing(pi0 = 5, pi1 = c(0.1, 0.2, 0.3)) %&gt;% expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% mutate(y = pi0 * exp(pi1 * time)) %&gt;% ggplot(aes(x = time, y = y, group = pi1)) + geom_hline(yintercept = 100, color = &quot;white&quot;) + geom_line() + geom_text(data = text, aes(label = label, vjust = vjust), size = 3, hjust = 1, parse = T) + labs(title = &quot;Exponential (simple)&quot;, y = expression(E(italic(Y)))) + coord_cartesian(ylim = c(0, 100)) + theme(panel.grid = element_blank()) # right text &lt;- tibble(time = c(0, 10, 10, 10, 7), y = c(102.5, 98, 83, 63, 20), label = c(&quot;alpha==100&quot;, &quot;pi[1]==0.3&quot;, &quot;pi[1]==0.2&quot;, &quot;pi[1]==0.1&quot;, &quot;pi[0]==20~(&#39;for all curves&#39;)&quot;), hjust = c(0, 1, 1, 1, 1)) p4 &lt;- crossing(alpha = 100, pi0 = 20, pi1 = c(0.1, 0.2, 0.3)) %&gt;% expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% mutate(y = alpha - (alpha - pi0) * exp(-pi1 * time)) %&gt;% ggplot(aes(x = time, y = y, group = pi1)) + geom_hline(yintercept = 100, color = &quot;white&quot;) + geom_line() + geom_text(data = text, aes(label = label, hjust = hjust), size = 3, parse = T) + labs(title = &quot;Negative exponential&quot;, y = expression(E(italic(Y)))) + coord_cartesian(ylim = c(0, 100)) + theme(panel.grid = element_blank()) Now combine the ggplots to make the full version of Figure 6.11. (p1 + p2) / (p3 + p4) As covered, above, the logistic trajectory is great when you have need of both lower and upper asymptotic. I might note that the logistic models we fit were not what you’d expect when thinking of what is typically called logistic regression. Conventional logistic regression models don’t add a normally-distributed error term. Rather, error is simply a function of the expected value. It’s something of a shame Singer and Willett didn’t cover logistic multilevel growth models. It’s such a shame, in fact, that we’ll go rogue and cover them in a bonus section a the end of this chapter. 6.4.4 From substantive theory to mathematical representations of individual growth. Here’s how to make Figure 6.12. text &lt;- tibble(time = c(0.2, 9.8, 9.8, 9.8, 1), y = c(102.5, 87, 63, 46, 10), label = c(&quot;alpha==100&quot;, &quot;pi[1]==1&quot;, &quot;pi[1]==5&quot;, &quot;pi[1]==10&quot;, &quot;pi[0]==10~(&#39;for all curves&#39;)&quot;), hjust = c(0, 1, 1, 1, 0)) crossing(alpha = 100, pi0 = 10, pi1 = c(1, 5, 10)) %&gt;% expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% mutate(y = pi0 + ((alpha - pi0) * time) / (pi1 + time)) %&gt;% ggplot(aes(x = time, y = y, group = pi1)) + geom_hline(yintercept = 100, color = &quot;white&quot;) + geom_line() + geom_text(data = text, aes(label = label, hjust = hjust), size = 3, parse = T) + scale_x_continuous(breaks = 0:5 * 2, expand = c(0, 0), limits = c(0, 10)) + scale_y_continuous(expression(E(italic(Y))), expand = c(0, 0), limits = c(0, 110)) + labs(title = &quot;Thurstone&#39;s learning equation&quot;) + theme(panel.grid = element_blank()) Just to get in a little more practice with non-linear models and the brms non-liner syntax, we might practice applying Thurstone’s learning equation to our children’s behavioral data from the last section. As before, our preliminary task to to pick our prior. Based on what we already know about the data and also based on the curves we see in the plot, above, I recommend we select something like \\(\\gamma_{00} \\sim \\operatorname{Normal}(2, 1)\\) and \\(\\gamma_{10} \\sim \\operatorname{Normal}(5, 2)\\). To get a sense of what that’d mean, here’s a prior predictive check. # how many do you want? n &lt;- 100 # simulate alpha &lt;- 20 set.seed(6) tibble(n = 1:n, pi0 = rnorm(n, mean = 2, sd = 1), pi1 = rnorm(n, mean = 5, sd = 2)) %&gt;% expand_grid(game = 0:30) %&gt;% mutate(y = pi0 + (((alpha - pi0) * game) / (pi1 + game))) %&gt;% # plot! ggplot(aes(x = game, y = y, group = n)) + geom_hline(yintercept = c(1, 20), color = &quot;white&quot;) + geom_line(linewidth = 1/4, alpha = 1/2) + theme(panel.grid = element_blank()) I recommend you play around with different priors on your own. Here’s how to fit the model to the foxngeese_pp data. fit6.17 &lt;- brm(data = foxngeese_pp, family = gaussian, bf(nmoves ~ g0 + (((20 - g0) * game) / (g1 + game)), g0 + g1 ~ 1 + (1 |i| id), nl = TRUE), prior = c(prior(normal(2, 1), nlpar = g0), prior(normal(5, 2), nlpar = g1), prior(exponential(1), class = sd, nlpar = g0), prior(exponential(1), class = sd, nlpar = g1), prior(exponential(1), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, cores = 3, chains = 3, init = 0, control = list(adapt_delta = .99), file = &quot;fits/fit06.17&quot;) Check the parameter summary. print(fit6.17) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: nmoves ~ g0 + (((20 - g0) * game)/(g1 + game)) ## g0 ~ 1 + (1 | i | id) ## g1 ~ 1 + (1 | i | id) ## Data: foxngeese_pp (Number of observations: 445) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Group-Level Effects: ## ~id (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(g0_Intercept) 0.38 0.33 0.01 1.22 1.00 1433 1505 ## sd(g1_Intercept) 17.06 3.09 11.68 23.68 1.00 1583 1689 ## cor(g0_Intercept,g1_Intercept) -0.01 0.33 -0.65 0.61 1.01 230 306 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## g0_Intercept 0.53 0.52 -0.44 1.59 1.00 1453 1985 ## g1_Intercept 9.32 2.05 5.41 13.43 1.00 2292 1994 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 4.30 0.15 4.01 4.62 1.00 4197 2013 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We might use our Thurstone model to an updated version of Figure 6.8. # define the new data nd &lt;- foxngeese_pp %&gt;% distinct(id) %&gt;% filter(id %in% subset) %&gt;% expand_grid(game = seq(from = 1, to = 30, by = 0.1)) # extract the fitted trajectories fitted(fit6.17, newdata = nd) %&gt;% # wrangle data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = game)) + geom_hline(yintercept = c(1, 20), color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/4, linewidth = 0) + geom_line(aes(y = Estimate)) + geom_point(data = foxngeese_pp %&gt;% filter(id %in% subset), aes(y = nmoves), size = 2/3) + scale_y_continuous(breaks = 0:5 * 5, expand = c(0, 0)) + coord_cartesian(ylim = c(0, 25)) + theme(panel.grid = element_blank()) + facet_wrap(~ id, ncol = 4, labeller = label_both) Doesn’t look great. Notice how the parameters in this model did not restrict the fitted trajectories to respect the lower asymptote. And indeed, Thurstone’s model has no lower asymptote. Let’s see how this model compares with the unconditional logistic model, fit6.15, by way of the WAIC. fit6.17 &lt;- add_criterion(fit6.17, criterion = &quot;waic&quot;) loo_compare(fit6.15, fit6.17, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit6.15 0.0 0.0 -1236.1 18.4 28.8 3.3 2472.2 36.7 ## fit6.17 -55.0 10.5 -1291.1 13.9 17.2 1.2 2582.2 27.9 Yep, the logistic model captured the patterns in these data better than Thurstone’s learning model. 6.5 Bonus: The logistic growth model In the social sciences, binary data are a widely-collected data type which are often best analyzed with a nonlinear model. In this section, we’ll practice fitting a multilevel growth model on binary data using logistic regression. This will differ from the logistic curve models, above, in that we will not be presuming normally distributed errors, \\(\\epsilon_{ij} \\sim \\operatorname N(0, \\sigma_\\epsilon)\\). Rather, we’ll be using a different likelihood altogether. 6.5.1 We need data. In this section, we’ll be borrowing a data file from the supplemental material from Gelman &amp; Hill (2006), Data analysis using regression and multilevel/hierarchical models. dogs &lt;- read_delim(&quot;extra_data/dogs.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) %&gt;% rename(dog = Dog) glimpse(dogs) ## Rows: 30 ## Columns: 26 ## $ dog &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, … ## $ T.0 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ T.1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0 ## $ T.2 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 ## $ T.3 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0 ## $ T.4 &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0 ## $ T.5 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1 ## $ T.6 &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0 ## $ T.7 &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1 ## $ T.8 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0 ## $ T.9 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1 ## $ T.10 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1 ## $ T.11 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0 ## $ T.12 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1 ## $ T.13 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0 ## $ T.14 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0 ## $ T.15 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0 ## $ T.16 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1 ## $ T.17 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1 ## $ T.18 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1 ## $ T.19 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ T.20 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ T.21 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0 ## $ T.22 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ T.23 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ T.24 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0 The data are initially in the wide format. Here we make the dogs data long. dogs &lt;- dogs %&gt;% pivot_longer(-dog, values_to = &quot;y&quot;) %&gt;% mutate(trial = str_remove(name, &quot;T.&quot;) %&gt;% as.double()) head(dogs) ## # A tibble: 6 × 4 ## dog name y trial ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 T.0 0 0 ## 2 1 T.1 0 1 ## 3 1 T.2 1 2 ## 4 1 T.3 0 3 ## 5 1 T.4 1 4 ## 6 1 T.5 0 5 As Gelman and Hill described (p. 515), these data were taken from a behavioral experiment in the 1950’s. Thirty dogs learned a new trick. Each of the 30 dogs was given 25 trials to learn the trick. In these data, time is captured by the tial column, which ranges from 0 (the first trial) to 24 (the final 25th trial). The criterion variable is y, which is coded 0 = fail, 1 = success. To give a sense of the data, here’s a descriptive plot of a randomly-chosen subset of eight of the dogs. # define the dog subset set.seed(6) subset &lt;- sample(1:30, size = 8) # subset the data dogs %&gt;% filter(dog %in% subset) %&gt;% # plot! ggplot(aes(x = trial, y = y)) + geom_point() + scale_y_continuous(breaks = 0:1) + theme(panel.grid = element_blank()) + facet_wrap(~ dog, ncol = 4, labeller = label_both) At the beginning of the experiment (trial == 0), none of the dogs new how to do the trick (i.e., for each one, y == 0). However, the dogs tended to learn the trick (i.e., get y == 1) by around the fifth or tenth trial. When modeling data of this kind, the conventional \\(\\epsilon_{ij} \\sim \\operatorname N(0, \\sigma_\\epsilon)\\) assumption has no hope of working. It’s just inappropriate. To get a sense of why, we’ll need to get technical. 6.5.2 Have you heard of the generalized linear [mixed] model? This wasn’t really addressed in Singer and Willett, but not all data kinds produce nicely Gaussian residuals. Though multilevel models can help with this, they won’t always do the job, particularly with binary data. Happily, the family of approaches described as the generalized linear mixed model (GLMM) will allow you to handle all kinds of wacky time-series data types. I’m not really going to offer a thorough introduction to the GLMM, here. For that, see Gelman &amp; Hill (2006) or McElreath (2020a). However, we will explore one strategy from the GLMM framework, which replaces the Gaussian likelihood with the Bernoulli. 6.5.2.1 Sometimes data come in 0’s and 1’s. Say you have a set of binary data \\(y_i\\). You can summarize those data as \\(N\\) trials, for which \\(z\\) were \\(1\\)’s. So if we had five coin flips, for which we assigned heads as 1 and tails as 0, we might describe those data as \\(N = 5\\) Bernoulli trials with \\(z = 4\\) successes. In ‘Bernoulli trials talk’, success is often a synonym for \\(y_i = 1\\). Further, you can use the Bernoulli likelihood to describe the probability \\(y_i = 1\\), given an underlying probability \\(p\\), as \\[\\Pr(y_i = 1 \\mid p) = p^z (1 - p)^{N - z}.\\] Let’s see how this plays out. First, we’ll make a custom bernoulli_likelihood() function. bernoulli_likelihood &lt;- function(p, data) { n &lt;- length(data) z &lt;- sum(data) p^z * (1 - p)^(n - sum(data)) } Now we’ll apply this function to a range of possible \\(p\\) values and data of the kind we just discussed, \\(N = 5\\) trials, for which \\(z = 4\\). data &lt;- c(1, 1, 0, 1, 1) tibble(p = seq(from = 0, to = 1, length.out = 200)) %&gt;% mutate(d = bernoulli_likelihood(p, data)) %&gt;% ggplot(aes(x = p, y = d)) + geom_line() + geom_vline(xintercept = .8, linetype = 3) + scale_x_continuous(expression(italic(p)), breaks = 0:5 / 5) + ylab(&quot;likelihood&quot;) + theme(panel.grid = element_blank()) Note how the maximum value for this likelihood, given our data, is at \\(p = .8\\). It’s also the case that the sample mean of a set of Bernoulli trials is the same as the sample estimate of \\(p\\). Let’s confirm. mean(data) ## [1] 0.8 Although the sample mean gave us a point estimate for \\(p\\), it was our use of the Bernoulli likelihood function that gave us a sense of the relative likelihoods of all possible values of \\(p\\). It also gave us a sense of the certainty for our sample estimate of \\(p\\). As is usually the case, our certainty will increase when \\(N\\) increases. For example, consider the case of \\(N = 100\\) and \\(z = 80\\). n &lt;- 100 z &lt;- 80 data &lt;- rep(1:0, times = c(z, n - z)) tibble(p = seq(from = 0, to = 1, length.out = 200)) %&gt;% mutate(d = bernoulli_likelihood(p, data)) %&gt;% ggplot(aes(x = p, y = d)) + geom_line() + geom_vline(xintercept = z / n, linetype = 3) + scale_x_continuous(expression(italic(p)), breaks = 0:5 / 5) + ylab(&quot;likelihood&quot;) + theme(panel.grid = element_blank()) The same ratio of \\(z/N\\) produced the same point estimate of \\(p\\), but the overall shape of the likelihood was more concentrated around that point estimate (the mode) than in our first example. 6.5.2.2 Recall how we can express models in terms of the Gaussian likelihood. Throughout the text, Singer and Willett express their models in terms true scores and stochastic elements. For a simple single-level model of one predictor \\(x_i\\) for a criterion \\(y_i\\), Singer and Willett might express that model as \\[ \\begin{align*} y_i &amp; = [b_0 + b_1 x_i] + \\epsilon_i \\\\ \\epsilon_i &amp; \\sim \\operatorname{N}(0, \\sigma_\\epsilon^2), \\end{align*} \\] where the \\(y_i = [b_0 + b_1 x_i]\\) portion describes the deterministic part of the model (the true score) and the \\(\\epsilon_i\\) portion describes the stochastic part of the model (the residual variance). As we briefly covered in Section 3.2.2, it turns out that another way to express this model is \\[ \\begin{align*} y_i &amp; \\sim \\operatorname{N}(\\mu_i, \\sigma_\\epsilon^2)\\\\ \\mu_i &amp; = b_0 + b_1 x_i, \\end{align*} \\] where we more explicitly declare that the criterion \\(y_i\\) follows a conditional Normal distribution for which we model the mean with the equation \\(\\mu_i = b_0 + b_1 x_i\\) and we express the variation around the conditional mean as \\(\\sigma_\\epsilon^2\\). The advantage of adopting this style of model notation is it generalizes well to data of other kinds for which we might want to use other likelihoods, such as the Bernoulli. 6.5.2.3 The simple logistic regression model using the Bernoulli likelihood. Now consider our example of \\(N = 100\\) Bernoulli trials data, \\(y_i\\), for which \\(z = 80\\). We can use the likelihood-based notation to express that as \\[ \\begin{align*} y_i &amp; \\sim \\operatorname{Bernoulli}(p_i)\\\\ \\operatorname{logit}(p_i) &amp; = b_0, \\end{align*} \\] where, since we don’t have a predictor variable, the intercept \\(b_0\\) is the log-odds probability. Why ‘log-odds’? you say. Well, the problem of fitting regression models for probabilities (\\(p_i\\)) is that probabilities are logically bounded between 0 and 1. Yet there’s nothing inherent in the machinery of the conventional linear regression model to prevent predictions outside of those bound. Happily, our friends the statisticians can help us get around that with the aid of link functions. If we model the logit of \\(p\\), \\(\\operatorname{logit}(p_i)\\), instead of modeling \\(p\\) directly, we end up with an inherently nonlinear model that will always produce estimates within the bounds of 0 and 1. Another way of expressing this model is by nesting the right-hand part of the equation within the inverse logit function, \\[ \\begin{align*} y_i &amp; \\sim \\operatorname{Bernoulli}(p_i) \\\\ p_i &amp; = \\operatorname{logit}^{-1}(b_0), \\end{align*} \\] where \\(\\operatorname{logit}^{-1}\\) is called the inverse logit. This use of the logit/inverse-logit function is where logistic regression get’s its name. Getting to the functions themselves, the logistic function is \\[\\operatorname{logit}(x) = \\log \\left ( \\frac{x}{1 - x} \\right ).\\] You may recall that the odds of a probability is defined as \\(\\left ( \\frac{p}{1 - p}\\right )\\). Therefore, we can describe \\(\\operatorname{logit}(p)\\) as the log odds of the probability, or \\(p\\) in a log-odds metric. Anyway, the inverse of the logistic function is \\[\\operatorname{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}.\\] To bring this all down to earth, here’s what happens when we apply the logistic function to a series of values ranging between 0 and 1. tibble(p = seq(from = .001, to = .999, by = .001)) %&gt;% mutate(odds = p / (1 - p)) %&gt;% mutate(log_odds = log(odds)) %&gt;% ggplot(aes(x = p, y = log_odds)) + geom_vline(xintercept = .5, color = &quot;white&quot;, linetype = 2) + geom_hline(yintercept = 0, color = &quot;white&quot;, linetype = 2) + geom_line() + scale_x_continuous(expression(italic(p)), breaks = 0:4 / 4, labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;), expand = c(0, 0), limits = 0:1) + scale_y_continuous(expression(italic(p)~(log~odds)), breaks = -2:2 * 2) + coord_cartesian(ylim = c(-4.5, 4.5)) + theme(panel.grid = element_blank()) The consequence of the logistic function is it can take the probability values, which are necessarily bounded within 0 and 1, and transform then to the unbounded log-odds space. Thus, if you fit a linear regression model on \\(\\operatorname{logit}(p)\\), you will avoid making predictions that \\(p\\) is less than 0 or greater than 1. Hopefully this is all just review. Now let’s consider the longitudinal multilevel model version of this model type for our dogs data. 6.5.3 Define the simple logistic multilevel growth model. If we have \\(j\\) Bernoulli trials nested within \\(i\\) participants over time, we can express the generic logistic multilevel growth model as $$ \\[\\begin{align*} y_{ij} &amp; \\sim \\operatorname{Bernoulli}(p_{ij}) \\\\ \\operatorname{logit}(p_{ij}) &amp; = \\pi_{0i} + \\pi_{1i} \\text{time}_{ij} \\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\zeta_{0i} \\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\zeta_{1i} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf \\Omega \\mathbf D&#39; \\end{pmatrix} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1\\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{10} &amp; 1 \\end{bmatrix}, \\end{align*}\\] $$ where we make the usual multivariate-normal assumptions about \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\). Yet because we are modeling the \\(y_{ij}\\) with the Bernoulli likelihood, we end up setting the linear model to \\(\\operatorname{logit}(p_{ij})\\). And also, because the Bernoulli distribution doesn’t itself have a \\(\\sigma\\) parameter, we have no \\(\\sigma_\\epsilon\\) term. That is, though the \\(\\zeta\\) parameters get a multivariate normal distribution, the \\(y_{ij}\\) data is considered Bernoulli. We have multiple distributional assumptions within a single model and this is totally okay. Now let’s see how this will work with our dogs data. 6.5.4 Ease into the model with a prior predictive check. If we focus just on the top parts of the model, we can express the logistic multilevel growth model for our dogs data as \\[ \\begin{align*} y_{ij} &amp; \\sim \\operatorname{Bernoulli}(p_{ij}) \\\\ \\operatorname{logit}(p_{ij}) &amp; = \\gamma_{00} + \\gamma_{10} \\text{trial}_{ij} + [\\zeta_{0i} + \\zeta_{1i} \\text{trial}_{ij}], \\end{align*} \\] where the Bernoulli trials \\(y\\) are nested within \\(i\\) dogs across \\(j\\) trials. As with all Bayesian models, we have to place priors on all model parameters. Before we get all technical with the model, let’s review what we know about the data: there were 25 trials; the trial variable is coded such that the first of the trials is trial == 0 and the 25th is trial == 24; the dogs didn’t know how to do the trick, at first; and the dogs all tended to learn the trick after several trials. Now if we were the actual scientists who collected these data and wanted to model them for a scientific publication, we wouldn’t have known that last bullet point before collecting the data. But perhaps we could have hypothesized that the dogs would tend to learn after a handful of trials. Anyway, this information tells us that because of the scaling of the predictor variable trial, the \\(\\gamma_{00}\\) parameter will be the expected value at the first trial. Since we know the dogs didn’t know how to do the trick at this point in the experiment, the probability value should be close to zero. Also, since we know the trials are scaled from 0 to 24, we also know that the coefficient increases in time, \\(\\gamma_{10}\\), should be positive since we expect improvement, but it shouldn’t be overly large since we don’t expect a lot of learning from one trial to the next. Rather, we expect to see learning unfold across several trials. The difficulty is how to encode these insights into parameters that will make good predictions in the log-odds probability space. If you look up at our last plot, you’ll see that \\(p = .5\\) is the same as \\(\\operatorname{logit}(p) = 0\\). You’ll also notice that as \\(p \\rightarrow 0\\), \\(\\operatorname{logit}(p) \\rightarrow -\\infty\\). Further, \\(p = .25 \\approx \\operatorname{logit}(p) = -1.1\\) and \\(p = .1 \\approx \\operatorname{logit}(p) = -2.2\\). Thus, we might express our expectation that the initial probability of success will be low with a prior centered somewhere around -2. I propose we consider \\(\\gamma_{00} \\sim \\operatorname N(-2, 1)\\). Now we consider our effect size from trial to trial, \\(\\gamma_{10}\\). This should be modest on the log-odds space. Somewhere between 0 and 1 would be weakly regularizing. I propose we consider \\(\\gamma_{10} \\sim \\operatorname N(0.25, 0.25)\\). To get a sense of what these priors would predict, let’s do a prior predictive check. McElreath covered this strategy extensively in his (2020a) text. Here we’ll simulate 1,000 draws using these two priors, given 25 trials. # how many simulations? n &lt;- 1e3 set.seed(6) tibble(iter = 1:n, # notice we set the parameters on the log-odds scale b0 = rnorm(n, mean = -2, sd = 1), b1 = rnorm(n, mean = 0.25, sd = 0.25)) %&gt;% expand_grid(trial = seq(from = 0, to = 24, length.out = 50)) %&gt;% # here we use the inverse logit function to convert the liner model to the probability metric mutate(y = inv_logit_scaled(b0 + b1 * trial)) %&gt;% # plot! ggplot(aes(x = trial, y = y, group = iter)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line(size = 1/4, alpha = 1/4) + scale_y_continuous(expression(italic(p[i][j])), breaks = 0:4 / 4, labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;), expand = c(0, 0), limits = 0:1) + labs(subtitle = &quot;prior predictive check&quot;) + theme(panel.grid = element_blank()) To my eye, this is about what we want. Our \\(\\gamma_{00} \\sim \\operatorname N(-2, 1)\\) prior concentrated the probability values close to zero at the start, but it was permissive enough to allow for the possibility that the success probability could be nearly as high as .5, or so. Also, our \\(\\gamma_{10} \\sim \\operatorname N(0.25, 0.25)\\) prior was largely consistent with moderate growth across trials, but it was not so heavy-handed that it prevented the possibility of no growth or even getting worse, over time.` Especially if you’re new to this, see what happens if you fool with the priors, a little. Next, we’ll need to think about the priors for our \\(\\sigma\\)’s. Since we generally like those to be only weakly regularizing, I suggest we continue on with \\(\\operatorname{Exponential}(1)\\), which puts an expected value at 1, easily allows for values as low as zero, and gently discourages very large values. As to the \\(\\mathbf \\Omega\\) matrix, we’ll stick with the good old \\(\\operatorname{LKJ}(4)\\). Thus, we can express the full model as \\[ \\begin{align*} y_{ij} &amp; \\sim \\operatorname{Bernoulli}(p_{ij}) \\\\ \\operatorname{logit}(p_{ij}) &amp; = \\gamma_{00} + \\gamma_{10} \\text{time}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\sim \\operatorname{Normal}(\\mathbf 0, \\mathbf D \\mathbf \\Omega \\mathbf D&#39;) \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf\\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{10} &amp; 1 \\end{bmatrix} \\\\ \\gamma_{00} &amp; \\sim \\operatorname{Normal}(-2, 1) \\\\ \\gamma_{10} &amp; \\sim \\operatorname{Normal}(0.25, 0.25) \\\\ \\sigma_0, \\sigma_1 &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf \\Omega &amp; \\sim \\operatorname{LKJ}(4). \\end{align*} \\] To fit this model with brms, the big new trick is setting family = bernoulli. By default, brm() will use the logit link. fit6.18 &lt;- brm(data = dogs, family = bernoulli, y ~ 0 + Intercept + trial + (1 + trial | dog), prior = c(prior(normal(-2, 1), class = b, coef = Intercept), prior(normal(0.25, 0.25), class = b), prior(exponential(1), class = sd), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 6, file = &quot;fits/fit06.18&quot;) Check the parameter summary. print(fit6.18) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 0 + Intercept + trial + (1 + trial | dog) ## Data: dogs (Number of observations: 750) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Group-Level Effects: ## ~dog (Number of levels: 30) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.34 0.24 0.01 0.90 1.00 1141 1384 ## sd(trial) 0.08 0.03 0.04 0.14 1.00 785 1431 ## cor(Intercept,trial) -0.06 0.32 -0.63 0.58 1.00 849 1099 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -2.32 0.24 -2.81 -1.88 1.00 2782 1759 ## trial 0.35 0.03 0.29 0.42 1.00 2106 1614 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It can take a while to make sense of the output of a logistic regression model, multilevel or otherwise. Perhaps for now, you might compare the sizes of our posterior means with the priors we used. To me, it’s helpful to see what this model predicts. For that, we’ll want to extract the posterior draws. draws &lt;- as_draws_df(fit6.18) If we focus on the \\(\\gamma\\)’s, here’s the population trajectory in \\(p\\), across the trials. draws %&gt;% select(b_Intercept, b_trial) %&gt;% expand_grid(trial = seq(from = 0, to = 25, by = 0.1)) %&gt;% # the magic happens here mutate(prob = inv_logit_scaled(b_Intercept + b_trial * trial)) %&gt;% ggplot(aes(x = trial, y = prob)) + geom_hline(yintercept = 0:1, color = &quot;white&quot;) + stat_lineribbon(.width = .95, fill = alpha(&quot;grey67&quot;, .5)) + scale_y_continuous(&quot;probability of success&quot;, breaks = 0:1, limits = 0:1) + labs(title = &quot;The population-level learning curve&quot;, subtitle = expression(We~get~the~overall~success~probability~over~time~with~logit^{-1}*(gamma[0][0]+gamma[1][0]*italic(trial[j]))*&#39;.&#39;)) + theme(panel.grid = element_blank()) In the mutate() line, note our use of the brms::inv_logit_scaled() function (i.e., the \\(\\operatorname{logit}^{-1}\\) function). That line was the equivalent of \\(p_j = \\operatorname{logit}^{-1}(\\gamma_{00} + \\gamma_{10} \\text{time}_j)\\). Another thing to keep in mind: The conventional Gaussian regression paradigm, particularly when using the Singer and Willet style \\(\\epsilon_i \\sim \\operatorname{N}(0, \\sigma_\\epsilon^2)\\) notation, can give you a sense that the regression models are supposed to be in the metric of the criterion. That is, we expect to see \\(y_{ij}\\) on the \\(y\\)-axis of our trajectory plots. That isn’t the case when we switch to logistic regression. Now, we’re predicting \\(p(y_{ij} = 1)\\), not \\(y_{ij}\\) itself. This is why our trajectory can take on all values between 0 and 1, whereas our criterion \\(y_{ij}\\) can only take on 0’s and 1’s. Another question we might ask of our model is at what point across the trial axis do we cross the threshold of a \\(p = .5\\) success rate? For a simple model, like ours, the formula for that threshold is \\(-\\gamma_{00}/ \\gamma_{10}\\). Here’s how to compute and summarize the threshold with our posterior draws. threshold &lt;- draws %&gt;% transmute(threshold = -b_Intercept / b_trial) %&gt;% mean_qi() threshold ## # A tibble: 1 × 6 ## threshold .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 6.69 5.73 7.71 0.95 mean qi The population threshold is about 6.7 trials, give or take one. Here’s what that looks like in a plot. # adjust for plotting threshold &lt;- threshold %&gt;% expand_grid(prob = 0:1) # wrangle draws %&gt;% select(b_Intercept, b_trial) %&gt;% expand_grid(trial = seq(from = 0, to = 25, by = 0.1)) %&gt;% mutate(prob = inv_logit_scaled(b_Intercept + b_trial * trial)) %&gt;% # plot ggplot(aes(x = trial, y = prob)) + geom_hline(yintercept = 0:1, color = &quot;white&quot;) + geom_hline(yintercept = .5, linetype = 2, color = &quot;white&quot;) + stat_lineribbon(.width = .95, fill = alpha(&quot;grey67&quot;, .5)) + geom_smooth(data = threshold, aes(x = threshold, xmin = .lower, xmax = .upper), stat = &quot;identity&quot;, fill = &quot;black&quot;, color = &quot;black&quot;, alpha = 1/8) + scale_y_continuous(&quot;probability of success&quot;, breaks = 0:2 / 2, labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + labs(title = &quot;The population-level threshold&quot;, subtitle = expression(-gamma[0][0]/gamma[1][0]*&quot; marks the typical half-way point for learning.&quot;), x = &quot;trial&quot;) + theme(panel.grid = element_blank()) The plot is a little cumbersome, but hopefully it clarifies the idea. Next, we might want to better understand our model at the level of the dogs. As a first step, we might make a plot of the 30 dog-level \\(\\pi\\) parameters. # wrangle pi &lt;- rbind( # pi0 coef(fit6.18, summary = F)$dog[, , &quot;Intercept&quot;], # pi1 coef(fit6.18, summary = F)$dog[, , &quot;trial&quot;] ) %&gt;% data.frame() %&gt;% set_names(1:30) %&gt;% mutate(draw = rep(1:3000, times = 2), pi = rep(c(&quot;p0&quot;, &quot;p1&quot;), each = n() / 2)) %&gt;% pivot_longer(-c(draw, pi)) %&gt;% pivot_wider(names_from = pi, values_from = value) # plot pi %&gt;% ggplot(aes(x = p0, y = p1, group = name)) + stat_ellipse(geom = &quot;polygon&quot;, level = 0.1, alpha = 1/2) + labs(title = &quot;Dog-level intercepts and slopes&quot;, subtitle = &quot;Each ellipse marks off the 10% posterior interval.&quot;, x = expression(pi[0][italic(i)]~(log~odds)), y = expression(pi[1][italic(i)]~(log~odds))) + theme(panel.grid = element_blank()) The full bivariate posterior for each dog’s \\(\\pi\\)’s is wider than we’ve depicted with our little 10% ellipses. But even those ellipses give you a sense of their distributions better than simple posterior mean points. It can be hard to interpret these, directly, since both the axes are on the log-odds scale. But hopefully you can at least get a sense of two things: There’s more variability across the \\(\\pi_{0i}\\) parameters than across the \\(\\pi_{1i}\\) parameters (look at the ranges in the axes). Also, there is more variability within the \\(\\pi_{0i}\\) parameters than within the \\(\\pi_{1i}\\) parameters (look at the shapes of the ellipses, themselves). This is the essence of the posterior means and standard deviations of the level-2 \\(\\sigma\\) parameters. posterior_summary(fit6.18)[3:4, 1:2] ## Estimate Est.Error ## sd_dog__Intercept 0.34270694 0.24333458 ## sd_dog__trial 0.08477086 0.02558093 All this is still quite abstract. It might be helpful if we plotted a handful of the dog-specific trajectories. nd &lt;- crossing(dog = subset, trial = seq(from = 0, to = 25, length.out = 100)) fitted(fit6.18, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = trial)) + geom_hline(yintercept = 0:1, color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/4) + geom_line(aes(y = Estimate)) + geom_point(data = dogs %&gt;% filter(dog %in% subset), aes(y = y)) + scale_y_continuous(&quot;y&quot;, breaks = 0:1) + theme(panel.grid = element_blank()) + facet_wrap(~ dog, ncol = 4, labeller = label_both) Keep in mind that whereas the \\(y\\)-axis is on the scale of the data, the trajectories themselves are actually of \\(p_{ij}\\). To my eye, the dog-level trajectories all look very similar. It might be easier to detect the variation across the dogs by way of their thresholds. To compute the dog-level thresholds, we use the formula \\(-\\pi_{0i} / \\pi_{1i}\\). pi %&gt;% mutate(threshold = -p0 / p1) %&gt;% group_by(name) %&gt;% tidybayes::mean_qi(threshold) %&gt;% ggplot(aes(x = threshold, xmin = .lower, xmax = .upper, y = reorder(name, threshold))) + geom_pointrange(fatten = 1) + scale_y_discrete(&quot;dogs, ranked by threshold&quot;, breaks = NULL) + xlim(0, 25) + labs(title = expression(50*&#39;%&#39;~threshold), subtitle = expression(-pi[0][italic(i)]/pi[1][italic(i)]*&quot; marks the point where the &quot;*italic(i)*&quot;th dog has acquired the skill half way.&quot;), x = &quot;trial&quot;) + theme(panel.grid = element_blank()) The dog-specific thresholds range from \\(5 \\pm 2\\) to \\(12 \\pm 3.5\\). Here’s what it looks like if we adjust our trajectory plots to include the threshold information. # compute the dog-level thresholds threshold &lt;- pi %&gt;% mutate(threshold = -p0 / p1) %&gt;% group_by(name) %&gt;% tidybayes::mean_qi(threshold) %&gt;% mutate(dog = name %&gt;% as.double()) %&gt;% select(dog, threshold) %&gt;% filter(dog %in% subset) %&gt;% expand(nesting(dog, threshold), point = 1:3) %&gt;% mutate(trial = ifelse(point == 1, -Inf, threshold), prob = ifelse(point == 3, -Inf, .5)) # go through the usual fitted() steps nd &lt;- crossing(dog = subset, trial = seq(from = 0, to = 25, length.out = 100)) fitted(fit6.18, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = trial)) + geom_hline(yintercept = 0:1, color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/4) + geom_line(aes(y = Estimate)) + geom_path(data = threshold, aes(y = prob), linetype = 2) + scale_y_continuous(&quot;success probability&quot;, breaks = 0:2 / 2, labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + theme(panel.grid = element_blank()) + facet_wrap(~ dog, ncol = 4, labeller = label_both) To keep things simple, we only focused on threshold posterior means. 6.5.5 You may want more. If you’d like more practice with logistic regression, or with other aspects of the generalized linear model, here are a few resources to consider: Gelman &amp; Hill (2006) covered the generalized linear model, including logistic regression, in both single-level and multilevel contexts. You can find their supplemental materials at https://www.stat.columbia.edu/~gelman/arm/software/. Heads up: some of their Bayesian model code is getting a dated. Gelman et al. (2020) is something of a second edition of the first half of Gelman &amp; Hill (2006). This text covered the generalized linear model, but only from a single-level context. You can find supporting materials at https://avehtari.github.io/ROS-Examples/. Both editions of McElreath’s text (2020a, 2015) cover the generalized linear model, including logistic regression, from both single-level and multilevel contexts. You can find all kinds of supporting material at https://xcelab.net/rm/statistical-rethinking/. I have tidyverse + brms ebook translations of both of McElreath’s texts (Kurz, 2021, 2020). I’m also slowly working through Gelman et al. (2020), which you can find on GitHub at https://github.com/ASKurz/Working-through-Regression-and-other-stories. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.4 GGally_2.1.2 brms_2.19.0 Rcpp_1.0.10 patchwork_1.1.2 lubridate_1.9.2 ## [7] forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [13] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] svUnit_1.0.6 shinythemes_1.2.0 splines_4.3.0 later_1.3.1 ## [5] gamm4_0.2-6 xts_0.13.1 lifecycle_1.0.3 StanHeaders_2.26.25 ## [9] processx_3.8.1 lattice_0.21-8 vroom_1.6.3 MASS_7.3-58.4 ## [13] crosstalk_1.2.0 ggdist_3.3.0 backports_1.4.1 magrittr_2.0.3 ## [17] sass_0.4.6 rmarkdown_2.21 jquerylib_0.1.4 httpuv_1.6.11 ## [21] zip_2.3.0 askpass_1.1 pkgbuild_1.4.0 minqa_1.2.5 ## [25] RColorBrewer_1.1-3 multcomp_1.4-23 abind_1.4-5 TH.data_1.1-2 ## [29] tensorA_0.36.2 sandwich_3.0-2 gdtools_0.3.3 inline_0.3.19 ## [33] crul_1.4.0 xslt_1.4.4 bridgesampling_1.1-2 codetools_0.2-19 ## [37] DT_0.27 xml2_1.3.4 tidyselect_1.2.0 bayesplot_1.10.0 ## [41] httpcode_0.3.0 farver_2.1.1 lme4_1.1-33 matrixStats_0.63.0 ## [45] stats4_4.3.0 base64enc_0.1-3 jsonlite_1.8.4 ellipsis_0.3.2 ## [49] survival_3.5-5 emmeans_1.8.6 systemfonts_1.0.4 projpred_2.5.0 ## [53] tools_4.3.0 ragg_1.2.5 glue_1.6.2 gridExtra_2.3 ## [57] xfun_0.39 mgcv_1.8-42 distributional_0.3.2 loo_2.6.0 ## [61] withr_2.5.0 fastmap_1.1.1 boot_1.3-28.1 fansi_1.0.4 ## [65] shinyjs_2.1.0 openssl_2.0.6 callr_3.7.3 digest_0.6.31 ## [69] timechange_0.2.0 R6_2.5.1 mime_0.12 estimability_1.4.1 ## [73] textshaping_0.3.6 colorspace_2.1-0 gtools_3.9.4 markdown_1.7 ## [77] threejs_0.3.3 utf8_1.2.3 generics_0.1.3 fontLiberation_0.1.0 ## [81] data.table_1.14.8 prettyunits_1.1.1 htmlwidgets_1.6.2 pkgconfig_2.0.3 ## [85] dygraphs_1.1.1.6 gtable_0.3.3 htmltools_0.5.5 fontBitstreamVera_0.1.1 ## [89] bookdown_0.34 scales_1.2.1 posterior_1.4.1 knitr_1.42 ## [93] rstudioapi_0.14 tzdb_0.4.0 reshape2_1.4.4 uuid_1.1-0 ## [97] coda_0.19-4 checkmate_2.2.0 nlme_3.1-162 curl_5.0.0 ## [101] nloptr_2.0.3 cachem_1.0.8 zoo_1.8-12 flextable_0.9.1 ## [105] parallel_4.3.0 miniUI_0.1.1.1 pillar_1.9.0 grid_4.3.0 ## [109] reshape_0.8.9 vctrs_0.6.2 shinystan_2.6.0 promises_1.2.0.1 ## [113] arrayhelpers_1.1-0 xtable_1.8-4 evaluate_0.21 mvtnorm_1.1-3 ## [117] cli_3.6.1 compiler_4.3.0 rlang_1.1.1 crayon_1.5.2 ## [121] rstantools_2.3.1 labeling_0.4.2 ps_1.7.5 plyr_1.8.8 ## [125] stringi_1.7.12 rstan_2.21.8 viridisLite_0.4.2 munsell_0.5.0 ## [129] colourpicker_1.2.0 Brobdingnag_1.2-9 V8_4.3.0 fontquiver_0.2.1 ## [133] Matrix_1.5-4 hms_1.1.3 bit64_4.0.5 gfonts_0.2.0 ## [137] shiny_1.7.4 highr_0.10 igraph_1.4.2 RcppParallel_5.1.7 ## [141] bslib_0.4.2 bit_4.0.5 officer_0.6.2 katex_1.4.1 ## [145] equatags_0.2.0 Footnote References Bürkner, P.-C. (2021a). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942 Gelman, A., Hill, J., &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879 Head, R., &amp; Pike, D. (1975). A review of response surface methodology from a biometric point of view. Biometrics, 31, 803–851. Keiley, Margaret Kraatz, Bates, J. E., Dodge, K. A., &amp; Pettit, G. S. (2000). A cross-domain growth analysis: Externalizing and internalizing behaviors during 8 years of childhood. Journal of Abnormal Child Psychology, 28(2), 161–179. https://doi.org/10.1023/A:1005122814723 Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 McElreath, R. (2020a). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ Sandberg, D. E., Meyer-Bahlburg, H. F. L., &amp; Yager, T. J. (1991). The Child Behavior Checklist nonclinical standardization samples: Should they be utilized as norms? Journal of the American Academy of Child &amp; Adolescent Psychiatry, 30(1), 124–134. https://doi.org/10.1097/00004583-199101000-00019 Schloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Larmarange, J. (2021). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 You might also use our sw_logistic() to investigate the claims on what happens when \\(\\text{game}_{ij} \\rightarrow \\infty\\) or \\(\\text{game}_{ij} \\rightarrow -\\infty\\).↩︎ "],["examining-the-multilevel-models-error-covariance-structure.html", "7 Examining the Multilevel Model’s Error Covariance Structure 7.1 The “standard” specification of the multilevel model for change 7.2 Using the composite model to understand assumptions about the error covariance matrix 7.3 Postulating an alternative error covariance structure Session info", " 7 Examining the Multilevel Model’s Error Covariance Structure In this chapter…, we focus on the model’s random effects as embodied in its error covariance structure. Doing so allows us to both describe the particular error covariance structure that the “standard” multilevel model for change invokes and it also allows us to broaden its representation to other—sometimes more tenable assumptions—about its behavior. (Singer &amp; Willett, 2003, p. 243, emphasis in the original) 7.1 The “standard” specification of the multilevel model for change Load the opposites_pp data (Willett, 1988). library(tidyverse) opposites_pp &lt;- read_csv(&quot;~/Dropbox/Recoding Applied Longitudinal Data Analysis/data/opposites_pp.csv&quot;) glimpse(opposites_pp) ## Rows: 140 ## Columns: 6 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,… ## $ time &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3,… ## $ opp &lt;dbl&gt; 205, 217, 268, 302, 219, 243, 279, 302, 142, 212, 250, 289, 206, 230, 248, 273, 190, 220, 229, … ## $ cog &lt;dbl&gt; 137, 137, 137, 137, 123, 123, 123, 123, 129, 129, 129, 129, 125, 125, 125, 125, 81, 81, 81, 81,… ## $ ccog &lt;dbl&gt; 23.5429, 23.5429, 23.5429, 23.5429, 9.5429, 9.5429, 9.5429, 9.5429, 15.5429, 15.5429, 15.5429, … ## $ wave &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4,… Willett (1988) clarified these data were simulated for pedagogical purposes, which will become important later on. For now, here’s our version of Table 7.1. opposites_pp %&gt;% mutate(name = str_c(&quot;opp&quot;, wave)) %&gt;% select(id, name, opp, cog) %&gt;% pivot_wider(names_from = name, values_from = opp) %&gt;% select(id, starts_with(&quot;opp&quot;), cog) %&gt;% head(n = 10) ## # A tibble: 10 × 6 ## id opp1 opp2 opp3 opp4 cog ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 205 217 268 302 137 ## 2 2 219 243 279 302 123 ## 3 3 142 212 250 289 129 ## 4 4 206 230 248 273 125 ## 5 5 190 220 229 220 81 ## 6 6 165 205 207 263 110 ## 7 7 170 182 214 268 99 ## 8 8 96 131 159 213 113 ## 9 9 138 156 197 200 104 ## 10 10 216 252 274 298 96 The data are composed of 35 people’s scores on a cognitive task, across 4 time points. opposites_pp %&gt;% count(id) ## # A tibble: 35 × 2 ## id n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 4 ## 2 2 4 ## 3 3 4 ## 4 4 4 ## 5 5 4 ## 6 6 4 ## 7 7 4 ## 8 8 4 ## 9 9 4 ## 10 10 4 ## # ℹ 25 more rows Our first model will serve as a comparison model for all that follow. We’ll often refer to it as the standard multilevel model for change. You’ll see why in a bit. It follows the form \\[ \\begin{align*} \\text{opp}_{ij} &amp; = \\pi_{0i} + \\pi_{1i} \\text{time}_{ij} + \\epsilon_{ij}\\\\ \\pi_{0i} &amp; = \\gamma_{00} + \\gamma_{01} (\\text{cog}_i - \\overline{\\text{cog}}) + \\zeta_{0i}\\\\ \\pi_{1i} &amp; = \\gamma_{10} + \\gamma_{11} (\\text{cog}_i - \\overline{\\text{cog}}) + \\zeta_{1i} \\\\ \\epsilon_{ij} &amp; \\stackrel{iid}{\\sim} \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &amp; \\stackrel{iid}{\\sim} \\operatorname{Normal} \\left ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D&#39; \\right ) \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho_{01} \\\\ \\rho_{01} &amp; 1 \\end{bmatrix}, \\end{align*} \\] where the \\((\\text{cog}_i - \\overline{\\text{cog}})\\) notation is meant to indicate we are mean centering the time-invariant covariate \\(\\text{cog}_i\\). In the data, the mean-centered version is saved as ccog. opposites_pp %&gt;% filter(wave == 1) %&gt;% ggplot(aes(ccog)) + geom_histogram(binwidth = 5) + theme(panel.grid = element_blank()) To keep things simple, we’ll be using brms default priors for all the models in this chapter. Fit the model. library(brms) fit7.1 &lt;- brm(data = opposites_pp, family = gaussian, opp ~ 0 + Intercept + time + ccog + time:ccog + (1 + time | id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.01&quot;) Check the summary. print(fit7.1, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: opp ~ 0 + Intercept + time + ccog + time:ccog + (1 + time | id) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~id (Number of levels: 35) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 36.042 5.020 27.666 47.287 1.003 1325 1922 ## sd(time) 10.687 1.806 7.547 14.642 1.001 1327 2307 ## cor(Intercept,time) -0.443 0.159 -0.711 -0.098 1.002 1798 2536 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 164.108 6.453 151.639 177.065 1.004 1153 1783 ## time 26.988 2.093 22.810 31.235 1.004 1629 2086 ## ccog -0.129 0.521 -1.162 0.881 1.002 1337 2211 ## time:ccog 0.434 0.167 0.095 0.761 1.002 1997 2570 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.898 1.112 10.968 15.285 1.000 1702 2746 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you’re curious, here’s the summary of our \\(\\sigma\\) parameters transformed to the variance/covariance metric. library(tidybayes) levels &lt;- c(&quot;sigma[epsilon]^2&quot;, &quot;sigma[0]^2&quot;, &quot;sigma[1]^2&quot;, &quot;sigma[0][1]&quot;) sigma &lt;- as_draws_df(fit7.1) %&gt;% transmute(`sigma[0]^2` = sd_id__Intercept^2, `sigma[1]^2` = sd_id__time^2, `sigma[epsilon]^2` = sigma^2, `sigma[0][1]` = sd_id__Intercept * sd_id__time * cor_id__Intercept__time) sigma %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 × 7 ## name value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[epsilon]^2 164. 120. 234. 0.95 median qi ## 2 sigma[0]^2 1261. 765. 2236. 0.95 median qi ## 3 sigma[1]^2 112. 57.0 214. 0.95 median qi ## 4 sigma[0][1] -166. -399. -28.1 0.95 median qi 7.2 Using the composite model to understand assumptions about the error covariance matrix Dropping the terms specifying the distributional assumptions, we can reexpress the model formula, from above, in the composite format as \\[ \\begin{align*} \\text{opp}_{ij} &amp; = [\\gamma_{00} + \\gamma_{10} \\text{time}_{ij} + \\gamma_{01} (\\text{cog}_i - \\overline{\\text{cog}}) + \\gamma_{11} (\\text{cog}_i - \\overline{\\text{cog}}) \\times \\text{time}_{ij}] \\\\ &amp; \\;\\;\\; + [\\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij} + \\epsilon_{ij}], \\end{align*} \\] where we’ve divided up the structural and stochastic components with our use of brackets. We might think of the terms of the stochastic portion as a composite residual, \\(r_{ij} = [\\epsilon_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij}]\\). Thus, we can rewrite the composite equation with the composite residual term \\(r_{ij}\\) as \\[ \\text{opp}_{ij} = [\\gamma_{00} + \\gamma_{10} \\text{time}_{ij} + \\gamma_{01} (\\text{cog}_i - \\overline{\\text{cog}}) + \\gamma_{11} (\\text{cog}_i - \\overline{\\text{cog}}) \\times \\text{time}_{ij}] + r_{ij}. \\] If we were willing to presume, as in OLS or single-level Bayesian regression, that all residuals are independent and normally distributed, we could express that in statistical notation as \\[\\begin{align*} \\begin{bmatrix} r_{11} \\\\ r_{12} \\\\ r_{13} \\\\ r_{14} \\\\ r_{21} \\\\ r_{22} \\\\ r_{23} \\\\ r_{24} \\\\ \\vdots \\\\ r_{n1} \\\\ r_{n2} \\\\ r_{n3} \\\\ r_{n4} \\end{bmatrix} &amp; \\sim \\mathcal{N} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}, \\begin{bmatrix} \\sigma_r^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_r^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_r^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma_r^2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\sigma_r^2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\sigma_r^2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 \\end{bmatrix} \\end{pmatrix}, \\end{align*}\\] where \\(r_{ij}\\) is the \\(i\\)th person’s residual on the \\(j\\)th time point. The variance/covariance matrix \\(\\mathbf \\Sigma\\) is diagonal (i.e., all the off-diagonal elements are 0’s) and homoscedastic (i.e., all the diagonal elements are the same value, \\(\\sigma_r^2\\)). These assumptions are absurd for longitudinal data, which is why we don’t analyze such data with single-level models. If we were to make the less restrictive assumptions that, within people, the residuals were correlated over time and were heteroscedastic, we could express that as \\[\\begin{align*} \\begin{bmatrix} r_{11} \\\\ r_{12} \\\\ r_{13} \\\\ r_{14} \\\\ r_{21} \\\\ r_{22} \\\\ r_{23} \\\\ r_{24} \\\\ \\vdots \\\\ r_{n1} \\\\ r_{n2} \\\\ r_{n3} \\\\ r_{n4} \\end{bmatrix} &amp; \\sim \\mathcal{N} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}, \\begin{bmatrix} \\sigma_{r_1}^2 &amp; \\sigma_{r_1 r_2} &amp; \\sigma_{r_1 r_3} &amp; \\sigma_{r_1 r_4} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma_{r_2 r_1} &amp; \\sigma_{r_2}^2 &amp; \\sigma_{r_2 r_3} &amp; \\sigma_{r_2 r_4} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma_{r_3 r_1} &amp; \\sigma_{r_3 r_2} &amp; \\sigma_{r_3}^2 &amp; \\sigma_{r_3 r_4} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma_{r_4 r_1} &amp; \\sigma_{r_4 r_2} &amp; \\sigma_{r_4 r_3} &amp; \\sigma_{r_4}^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{r_1}^2 &amp; \\sigma_{r_1 r_2} &amp; \\sigma_{r_1 r_3} &amp; \\sigma_{r_1 r_4} &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{r_2 r_1} &amp; \\sigma_{r_2}^2 &amp; \\sigma_{r_2 r_3} &amp; \\sigma_{r_2 r_4} &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{r_3 r_1} &amp; \\sigma_{r_3 r_2} &amp; \\sigma_{r_3}^2 &amp; \\sigma_{r_3 r_4} &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{r_4 r_1} &amp; \\sigma_{r_4 r_2} &amp; \\sigma_{r_4 r_3} &amp; \\sigma_{r_4}^2 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma_{r_1}^2 &amp; \\sigma_{r_1 r_2} &amp; \\sigma_{r_1 r_3} &amp; \\sigma_{r_1 r_4} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma_{r_2 r_1} &amp; \\sigma_{r_2}^2 &amp; \\sigma_{r_2 r_3} &amp; \\sigma_{r_2 r_4} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma_{r_3 r_1} &amp; \\sigma_{r_3 r_2} &amp; \\sigma_{r_3}^2 &amp; \\sigma_{r_3 r_4} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma_{r_4 r_1} &amp; \\sigma_{r_4 r_2} &amp; \\sigma_{r_4 r_3} &amp; \\sigma_{r_4}^2 \\end{bmatrix} \\end{pmatrix}. \\end{align*}\\] this kind of structure can be called block diagonal, which means the off-diagonal elements are zero between persons, but allowed to be non-zero within persons (i.e., within blocks). The zero elements between person blocks explicate how the residuals are independent between persons. Notice that the variances on the diagonal vary across the four time points (i.e., \\(\\sigma_{r_1}^2, \\dots, \\sigma_{r_4}^2\\)). Yet also notice that the block for one person is identical to the block for all others. Thus, this model allows for heterogeneity across time within persons, but homogeneity between persons. We can express this in the more compact notation, \\[\\begin{align*} r &amp; \\sim \\mathcal{N} \\begin{pmatrix} \\mathbf 0, \\begin{bmatrix} \\mathbf{\\Sigma}_r &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\dots &amp; \\mathbf 0 \\\\ \\mathbf 0 &amp; \\mathbf{\\Sigma}_r &amp; \\mathbf 0 &amp; \\dots &amp; \\mathbf 0 \\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf{\\Sigma}_r &amp; \\dots &amp; \\mathbf 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\mathbf 0 \\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf{\\Sigma}_r \\end{bmatrix} \\end{pmatrix}, \\;\\;\\; \\text{where} \\\\ \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_{r_1}^2 &amp; \\sigma_{r_1 r_2} &amp; \\sigma_{r_1 r_3} &amp; \\sigma_{r_1 r_4} \\\\ \\sigma_{r_2 r_1} &amp; \\sigma_{r_2}^2 &amp; \\sigma_{r_2 r_3} &amp; \\sigma_{r_2 r_4} \\\\ \\sigma_{r_3 r_1} &amp; \\sigma_{r_3 r_2} &amp; \\sigma_{r_3}^2 &amp; \\sigma_{r_3 r_4} \\\\ \\sigma_{r_4 r_1} &amp; \\sigma_{r_4 r_2} &amp; \\sigma_{r_4 r_3} &amp; \\sigma_{r_4}^2 \\end{bmatrix}. \\end{align*}\\] The bulk of the rest of the material in this chapter will focus around how different models handle \\(\\mathbf{\\Sigma}_r\\). The standard multilevel growth model has one way. There are many others. 7.2.1 Variance of the composite residual. Under the conventional multilevel growth model \\[ \\begin{align*} \\sigma_{r_j}^2 &amp; = \\operatorname{Var} \\left ( \\epsilon_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{time}_j \\right ) \\\\ &amp; = \\sigma_\\epsilon^2 + \\sigma_0^2 + 2 \\sigma_{01} \\text{time}_j + \\sigma_1^2 \\text{time}_j^2. \\end{align*} \\] Here’s how to use our posterior samples to compute \\(\\sigma_{r_1}^2, \\dots, \\sigma_{r_4}^2\\). sigma %&gt;% mutate(iter = 1:n()) %&gt;% expand(nesting(iter, `sigma[epsilon]^2`, `sigma[0]^2`, `sigma[1]^2`, `sigma[0][1]`), time = 0:3) %&gt;% mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * time + `sigma[1]^2` * time^2) %&gt;% mutate(name = str_c(&quot;sigma[italic(r)[&quot;, time + 1, &quot;]]^2&quot;)) %&gt;% ggplot(aes(x = r, y = name)) + stat_halfeye(.width = .95, size = 1) + scale_x_continuous(&quot;marginal posterior&quot;, expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + coord_cartesian(ylim = c(1.5, 4.2)) + theme(panel.grid = element_blank()) As is often the case with variance parameters, the posteriors show marked right skew. Here are the numeric summaries. sigma %&gt;% mutate(iter = 1:n()) %&gt;% expand(nesting(iter, `sigma[epsilon]^2`, `sigma[0]^2`, `sigma[1]^2`, `sigma[0][1]`), time = 0:3) %&gt;% mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * time + `sigma[1]^2` * time^2) %&gt;% mutate(name = str_c(&quot;sigma[italic(r)[&quot;, time + 1, &quot;]]^2&quot;)) %&gt;% group_by(name) %&gt;% median_qi(r) ## # A tibble: 4 × 7 ## name r .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[italic(r)[1]]^2 1430. 938. 2388. 0.95 median qi ## 2 sigma[italic(r)[2]]^2 1201. 807. 2011. 0.95 median qi ## 3 sigma[italic(r)[3]]^2 1203. 798. 1972. 0.95 median qi ## 4 sigma[italic(r)[4]]^2 1416. 923. 2424. 0.95 median qi Though our precise numeric values are different from those in the text, we see the same overall pattern. Using our posterior medians, we can update \\(\\mathbf{\\Sigma}_r\\) to \\[ \\begin{align*} \\hat{\\mathbf{\\Sigma}}_r &amp; = \\begin{bmatrix} 1430 &amp; \\hat{\\sigma}_{r_1 r_2} &amp; \\hat{\\sigma}_{r_1 r_3} &amp; \\hat{\\sigma}_{r_1 r_4} \\\\ \\hat{\\sigma}_{r_2 r_1} &amp; 1201 &amp; \\hat{\\sigma}_{r_2 r_3} &amp; \\hat{\\sigma}_{r_2 r_4} \\\\ \\hat{\\sigma}_{r_3 r_1} &amp; \\hat{\\sigma}_{r_3 r_2} &amp; 1203 &amp; \\hat{\\sigma}_{r_3 r_4} \\\\ \\hat{\\sigma}_{r_4 r_1} &amp; \\hat{\\sigma}_{r_4 r_2} &amp; \\hat{\\sigma}_{r_4 r_3} &amp; 1416 \\end{bmatrix}. \\end{align*} \\] For the opposites-naming data, composite residual variance is greatest at the beginning and end of data collection and smaller in between. And, while not outrageously heteroscedastic, this situation is clearly beyond the bland homoscedasticity that we routinely assume for residuals in cross-sectional data. (p. 252) If you work through the equation at the beginning of this section–which I am not going to do, here–, you’ll see that the standard multilevel growth model is set up such that the residual variance follows a quadratic function with time. To give a sense, here we plot the expected \\(\\sigma_r^2\\) values over a wider and more continuous range of time values. set.seed(7) sigma %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = 50) %&gt;% expand(nesting(iter, `sigma[epsilon]^2`, `sigma[0]^2`, `sigma[1]^2`, `sigma[0][1]`), time = seq(from = -4.2, to = 6.6, length.out = 200)) %&gt;% mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * time + `sigma[1]^2` * time^2) %&gt;% ggplot(aes(x = time, y = r, group = iter)) + geom_line(linewidth = 1/6, alpha = 1/2) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expression(sigma[italic(r)]^2), expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) + labs(subtitle = expression(&quot;50 posterior draws showing the quadratic shape of &quot;*sigma[italic(r)[time]]^2)) + theme(panel.grid = element_blank()) Since we have 4,000 posterior draws for all the parameters, we also have 4,000 posterior draws for the quadratic curve. Here we just show 50. The curve is at its minimum at \\(\\text{time} = -(\\sigma_{01} / \\sigma_1^2)\\). Since we have posterior distributions for \\(\\sigma_{01}\\) and \\(\\sigma_1^2\\), we’ll also have a posterior distribution for the minimum point. Here it is. sigma %&gt;% mutate(minimum = -`sigma[0][1]` / `sigma[1]^2`) %&gt;% ggplot(aes(x = minimum, y = 0)) + stat_halfeye(.width = .95) + scale_x_continuous(&quot;time&quot;, expand = c(0, 0), limits = c(-4.2, 6.6)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(Minimum~value~(-sigma[0][1]/sigma[1]^2))) + theme(panel.grid = element_blank()) If we plug those minimum time values into the equation for \\(\\sigma_{r_\\text{time}}^2\\), we’ll get the posterior distribution for the minimum variance value. sigma %&gt;% mutate(minimum = -`sigma[0][1]` / `sigma[1]^2`) %&gt;% mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * minimum + `sigma[1]^2` * minimum^2) %&gt;% ggplot(aes(x = r, y = 0)) + stat_halfeye(.width = .95) + scale_x_continuous(expression(sigma[italic(r)[time]]^2), limits = c(0, NA)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Minimum variance&quot;) + theme(panel.grid = element_blank()) Here’s the numeric summary. sigma %&gt;% mutate(minimum = -`sigma[0][1]` / `sigma[1]^2`) %&gt;% mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * minimum + `sigma[1]^2` * minimum^2) %&gt;% median_qi(r) ## # A tibble: 1 × 6 ## r .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1136. 765. 1838. 0.95 median qi 7.2.2 Covariance of the composite residuals. In addition to the variances in \\(\\mathbf{\\Sigma}_r\\), we might focus on the off-diagonal covariances, too. We can define the covariance between two time points \\(\\sigma_{r_j, r_{j&#39;}}\\) as \\[ \\sigma_{r_j, r_{j&#39;}} = \\sigma_0^2 + \\sigma_{01} (t_j + t_{j&#39;}) + \\sigma_1^2 t_j t_{j&#39;}, \\] where \\(\\sigma_0^2\\), \\(\\sigma_{01}\\) and \\(\\sigma_1^2\\) all have their usual interpretation, and \\(t_j\\) and \\(t_{j&#39;}\\) are the numeric values for whatever variable is used to index time in the model, which is time in the case of fit7.1. Here we compute and plot the marginal posteriors for all \\(4 \\times 4 = 16\\) parameters. # arrange the panels levels &lt;- c(&quot;sigma[italic(r)[1]]^2&quot;, &quot;sigma[italic(r)[1]][italic(r)[2]]&quot;, &quot;sigma[italic(r)[1]][italic(r)[3]]&quot;, &quot;sigma[italic(r)[1]][italic(r)[4]]&quot;, &quot;sigma[italic(r)[2]][italic(r)[1]]&quot;, &quot;sigma[italic(r)[2]]^2&quot;, &quot;sigma[italic(r)[2]][italic(r)[3]]&quot;, &quot;sigma[italic(r)[2]][italic(r)[4]]&quot;, &quot;sigma[italic(r)[3]][italic(r)[1]]&quot;, &quot;sigma[italic(r)[3]][italic(r)[2]]&quot;, &quot;sigma[italic(r)[3]]^2&quot;, &quot;sigma[italic(r)[3]][italic(r)[4]]&quot;, &quot;sigma[italic(r)[4]][italic(r)[1]]&quot;, &quot;sigma[italic(r)[4]][italic(r)[2]]&quot;, &quot;sigma[italic(r)[4]][italic(r)[3]]&quot;, &quot;sigma[italic(r)[4]]^2&quot;) # wrangle sigma &lt;- sigma %&gt;% mutate(`sigma[italic(r)[1]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 0 + `sigma[1]^2` * 0^2, `sigma[italic(r)[2]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 1 + `sigma[1]^2` * 1^2, `sigma[italic(r)[3]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 2 + `sigma[1]^2` * 2^2, `sigma[italic(r)[4]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 3 + `sigma[1]^2` * 3^2, `sigma[italic(r)[2]][italic(r)[1]]` = `sigma[0]^2` + `sigma[0][1]` * (1 + 0) + `sigma[1]^2` * 1 * 0, `sigma[italic(r)[3]][italic(r)[1]]` = `sigma[0]^2` + `sigma[0][1]` * (2 + 0) + `sigma[1]^2` * 2 * 0, `sigma[italic(r)[4]][italic(r)[1]]` = `sigma[0]^2` + `sigma[0][1]` * (3 + 0) + `sigma[1]^2` * 3 * 0, `sigma[italic(r)[3]][italic(r)[2]]` = `sigma[0]^2` + `sigma[0][1]` * (2 + 1) + `sigma[1]^2` * 2 * 1, `sigma[italic(r)[4]][italic(r)[2]]` = `sigma[0]^2` + `sigma[0][1]` * (3 + 1) + `sigma[1]^2` * 3 * 1, `sigma[italic(r)[4]][italic(r)[3]]` = `sigma[0]^2` + `sigma[0][1]` * (3 + 2) + `sigma[1]^2` * 3 * 2, `sigma[italic(r)[1]][italic(r)[2]]` = `sigma[0]^2` + `sigma[0][1]` * (0 + 1) + `sigma[1]^2` * 0 * 1, `sigma[italic(r)[1]][italic(r)[3]]` = `sigma[0]^2` + `sigma[0][1]` * (0 + 2) + `sigma[1]^2` * 0 * 2, `sigma[italic(r)[2]][italic(r)[3]]` = `sigma[0]^2` + `sigma[0][1]` * (1 + 2) + `sigma[1]^2` * 1 * 2, `sigma[italic(r)[1]][italic(r)[4]]` = `sigma[0]^2` + `sigma[0][1]` * (0 + 3) + `sigma[1]^2` * 0 * 3, `sigma[italic(r)[2]][italic(r)[4]]` = `sigma[0]^2` + `sigma[0][1]` * (1 + 3) + `sigma[1]^2` * 1 * 3, `sigma[italic(r)[3]][italic(r)[4]]` = `sigma[0]^2` + `sigma[0][1]` * (2 + 3) + `sigma[1]^2` * 2 * 3) sigma %&gt;% select(contains(&quot;italic&quot;)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% # plot! ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, size = 1) + scale_x_continuous(&quot;marginal posterior&quot;, expand = expansion(mult = c(0, 0.05))) + scale_y_discrete(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 4000), ylim = c(0.5, NA)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, labeller = label_parsed) It might be helpful to reduce the complexity of this plot by focusing on the posterior medians. With a little help from geom_tile() and geom_text(), we’ll make a plot version of the matrix at the top of page 255 in the text. sigma %&gt;% select(contains(&quot;italic&quot;)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate(label = round(value, digits = 0)) %&gt;% ggplot(aes(x = 0, y = 0)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label)) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, NA)) + scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Sigma)[italic(r)]*&quot; for the standard multilevel model for change&quot;)) + theme(legend.text = element_text(hjust = 1)) + facet_wrap(~ name, labeller = label_parsed) Although our posterior median values differ a bit from the REML values reported in the text, the overall pattern holds. Hopefully the coloring in the plot helps highlight what Singer and Willett described as a “‘band diagonal’ structure, in which the overall magnitude of the residual covariances tends to decline in diagonal ‘bands’ the further you get from the main diagonal” (p. 255). One of the consequences for this structure is that in cases where both \\(\\sigma_1^2 \\rightarrow 0\\) and \\(\\sigma_{01} \\rightarrow 0\\), the residual covariance matrix becomes compound symmetric, which is: \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_\\epsilon^2 + \\sigma_0^2 &amp; \\sigma_0^2 &amp; \\sigma_0^2 &amp; \\sigma_0^2 \\\\ \\sigma_0^2 &amp; \\sigma_\\epsilon^2 + \\sigma_0^2 &amp; \\sigma_0^2 &amp; \\sigma_0^2 \\\\ \\sigma_0^2 &amp; \\sigma_0^2 &amp; \\sigma_\\epsilon^2 + \\sigma_0^2 &amp; \\sigma_0^2 \\\\ \\sigma_0^2 &amp; \\sigma_0^2 &amp; \\sigma_0^2 &amp; \\sigma_\\epsilon^2 + \\sigma_0^2 \\end{bmatrix}. \\end{align*} \\] Compound symmetric error covariance structures are particularly common in longitudinal data, especially if the slopes of the change trajectories do not differ much asroc people. Regardless of these special cases, however, the most sensible question to ask of your data is whether the error covariance structure that the “standard” multilevel model for change demands is realistic when applied to data in practice? The answer to this question will determine whether the standard model can be applied ubiquitously, as question we soon address. (pp. 255–256) 7.2.3 Autocorrelation of the composite residuals. We can use the following equation to convert our \\(\\mathbf{\\Sigma}_r\\) into a correlation matrix: \\[\\rho_{r_j r_{j^\\prime}} = \\sigma_{r_j r_{j^\\prime}} \\Big / \\sqrt{\\sigma_{r_j}^2 \\sigma_{r_{j^\\prime}}^2}.\\] Here we use the formula and plot the posteriors. # arrange the panels levels &lt;- c(&quot;sigma[italic(r)[1]]&quot;, &quot;rho[italic(r)[1]][italic(r)[2]]&quot;, &quot;rho[italic(r)[1]][italic(r)[3]]&quot;, &quot;rho[italic(r)[1]][italic(r)[4]]&quot;, &quot;rho[italic(r)[2]][italic(r)[1]]&quot;, &quot;sigma[italic(r)[2]]&quot;, &quot;rho[italic(r)[2]][italic(r)[3]]&quot;, &quot;rho[italic(r)[2]][italic(r)[4]]&quot;, &quot;rho[italic(r)[3]][italic(r)[1]]&quot;, &quot;rho[italic(r)[3]][italic(r)[2]]&quot;, &quot;sigma[italic(r)[3]]&quot;, &quot;rho[italic(r)[3]][italic(r)[4]]&quot;, &quot;rho[italic(r)[4]][italic(r)[1]]&quot;, &quot;rho[italic(r)[4]][italic(r)[2]]&quot;, &quot;rho[italic(r)[4]][italic(r)[3]]&quot;, &quot;sigma[italic(r)[4]]&quot;) sigma &lt;- sigma %&gt;% select(contains(&quot;italic&quot;)) %&gt;% mutate(`sigma[italic(r)[1]]` = `sigma[italic(r)[1]]^2` / sqrt(`sigma[italic(r)[1]]^2`^2), `rho[italic(r)[2]][italic(r)[1]]` = `sigma[italic(r)[2]][italic(r)[1]]` / sqrt(`sigma[italic(r)[2]]^2` * `sigma[italic(r)[1]]^2`), `rho[italic(r)[3]][italic(r)[1]]` = `sigma[italic(r)[3]][italic(r)[1]]` / sqrt(`sigma[italic(r)[3]]^2` * `sigma[italic(r)[1]]^2`), `rho[italic(r)[4]][italic(r)[1]]` = `sigma[italic(r)[4]][italic(r)[1]]` / sqrt(`sigma[italic(r)[4]]^2` * `sigma[italic(r)[1]]^2`), `rho[italic(r)[1]][italic(r)[2]]` = `sigma[italic(r)[1]][italic(r)[2]]` / sqrt(`sigma[italic(r)[1]]^2` * `sigma[italic(r)[2]]^2`), `sigma[italic(r)[2]]` = `sigma[italic(r)[2]]^2` / sqrt(`sigma[italic(r)[2]]^2`^2), `rho[italic(r)[3]][italic(r)[2]]` = `sigma[italic(r)[3]][italic(r)[2]]` / sqrt(`sigma[italic(r)[3]]^2` * `sigma[italic(r)[2]]^2`), `rho[italic(r)[4]][italic(r)[2]]` = `sigma[italic(r)[4]][italic(r)[2]]` / sqrt(`sigma[italic(r)[4]]^2` * `sigma[italic(r)[2]]^2`), `rho[italic(r)[1]][italic(r)[3]]` = `sigma[italic(r)[1]][italic(r)[3]]` / sqrt(`sigma[italic(r)[1]]^2` * `sigma[italic(r)[3]]^2`), `rho[italic(r)[2]][italic(r)[3]]` = `sigma[italic(r)[2]][italic(r)[3]]` / sqrt(`sigma[italic(r)[2]]^2` * `sigma[italic(r)[3]]^2`), `sigma[italic(r)[3]]` = `sigma[italic(r)[3]]^2` / sqrt(`sigma[italic(r)[3]]^2`^2), `rho[italic(r)[4]][italic(r)[3]]` = `sigma[italic(r)[4]][italic(r)[3]]` / sqrt(`sigma[italic(r)[4]]^2` * `sigma[italic(r)[3]]^2`), `rho[italic(r)[1]][italic(r)[4]]` = `sigma[italic(r)[1]][italic(r)[4]]` / sqrt(`sigma[italic(r)[1]]^2` * `sigma[italic(r)[4]]^2`), `rho[italic(r)[2]][italic(r)[4]]` = `sigma[italic(r)[2]][italic(r)[4]]` / sqrt(`sigma[italic(r)[2]]^2` * `sigma[italic(r)[4]]^2`), `rho[italic(r)[3]][italic(r)[4]]` = `sigma[italic(r)[3]][italic(r)[4]]` / sqrt(`sigma[italic(r)[3]]^2` * `sigma[italic(r)[4]]^2`), `sigma[italic(r)[4]]` = `sigma[italic(r)[4]]^2` / sqrt(`sigma[italic(r)[4]]^2`^2)) sigma %&gt;% select(`sigma[italic(r)[1]]`:`sigma[italic(r)[4]]`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% # plot! ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, size = 1) + scale_x_continuous(&quot;marginal posterior&quot;, expand = c(0, 0), limits = c(0, 1), breaks = 0:4 / 4, labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_discrete(NULL, breaks = NULL) + coord_cartesian(ylim = c(0.5, NA)) + theme(panel.grid = element_blank()) + facet_wrap(~ name, labeller = label_parsed) As before, it might be helpful to reduce the complexity of this plot by focusing on the posterior medians. We’ll make a plot version of the correlation matrix in the middle of page 256 in the text. sigma %&gt;% select(`sigma[italic(r)[1]]`:`sigma[italic(r)[4]]`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate(label = round(value, digits = 2)) %&gt;% ggplot(aes(x = 0, y = 0)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label)) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, 1)) + scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Omega)[italic(r)]*&quot; for the standard multilevel model for change&quot;)) + theme(legend.text = element_text(hjust = 1)) + facet_wrap(~ name, labeller = label_parsed) The correlation matrix has an even more pronounced band-diagonal structure. Thus even though the standard multilevel model of change does not contain an explicit autocorrelation parameter \\(\\rho\\), the model does account for residual autocorrelation. We might take this even further. Notice the parameters \\(\\rho_{r_2, r_1}\\), \\(\\rho_{r_3, r_2}\\), and \\(\\rho_{r_4, r_3}\\) are all autocorrelations of the first order, and further note how similar their posterior medians all are. Here’s a summary of the average of those three parameters, which we’ll just call \\(\\rho\\). sigma %&gt;% # take the average of the three parameters transmute(rho = (`rho[italic(r)[2]][italic(r)[1]]` + `rho[italic(r)[3]][italic(r)[2]]` + `rho[italic(r)[4]][italic(r)[3]]`) / 3) %&gt;% # summarize median_qi() ## # A tibble: 1 × 6 ## rho .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.827 0.736 0.897 0.95 median qi If you look at the “estimate” column in Table 7.3 (pp. 258–259), you’ll see the \\(\\hat \\rho\\) values for the autoregressive and heterogeneous-autoregressive models are very similar to the \\(\\hat \\rho\\) posterior we just computed for the standard multilevel model of change. The standard multilevel model of change accounts for autocorrelation, but it does so without an explicit \\(\\rho\\) parameter. 7.3 Postulating an alternative error covariance structure Singer and Willett wrote: it is easy to specify alternative covariance structures for the composite residual and determine analytically which specification—the “standard” or an alternative—fits best. You already possess the analytic tools and skills needed for this work. (p. 257) Frequentest R users would likely do this with the nlme package (Pinheiro et al., 2021). Recent additions to brms makes this largely possible, but not completely so. The six error structures listed in this section were: unstructured, compound symmetric, heterogeneous compound symmetric, autoregressive, heterogeneous autoregressive, and Toeplitz. The Toeplitz structure is not currently available with brms, but we can experiment with the first five. For details, see issue #403 in the brms GitHub repo. 7.3.1 Unstructured error covariance matrix. Models using an unstructured error covariance matrix include \\(k (k + 1) / 2\\) variance/covariance parameters, where \\(k\\) is the number of time waves in the data. In the case of our opposites_pp data, we can compute the number of parameters as follows. k &lt;- 4 k * (k + 1) / 2 ## [1] 10 We end up with 4 variances and 6 covariances. Following the notation Singer and Willett used in the upper left corner of Table 7.3 (p. 258), we can express this as \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\sigma_{13} &amp; \\sigma_{14} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\sigma_{23} &amp; \\sigma_{24} \\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_3^2 &amp; \\sigma_{34} \\\\ \\sigma_{41} &amp; \\sigma_{42} &amp; \\sigma_{43} &amp; \\sigma_4^2 \\end{bmatrix}. \\end{align*} \\] The great appeal of an unstructured error covariance structure is that it places no restrictions on the structure of \\(\\mathbf{\\Sigma}_r\\). For a given set of fixed effects, its deviance statistic will always be the smallest of any error covariance structure. If you have just a few waves of data, this choice can be attractive. But if you have many waves, it can require an exorbitant number of parameters… [However,] the “standard” model requires only 3 variance components (\\(\\sigma_0^2\\), \\(\\sigma_1^2\\), and \\(\\sigma_\\epsilon^2\\)) and one covariance component, \\(\\sigma_{01}\\). (p. 260) To fit the unstructured model with brms, we use the unstr() function. Notice how we’ve dropped the usual (1 + time | id) syntax. Instead, we indicate the data are temporally structured by the time variable by setting time = time within unstr(), and we indicate the data are grouped by id by setting gr = id, also within unstr(). Also, notice we’ve wrapped the entire model formula within the bf() function. The second line within the bf() function has sigma on the left side of the ~ operator, which is something we haven’t seen before. With that line, we have allowed the residual variance \\(\\sigma_\\epsilon\\) to vary across time points. By default, brms will use the log link, to insure the model for \\(\\sigma_{\\epsilon j}\\) will never predict negative variances. fit7.2 &lt;- brm(data = opposites_pp, family = gaussian, bf(opp ~ 0 + Intercept + time + ccog + time:ccog + unstr(time = time, gr = id), sigma ~ 0 + factor(time)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.02&quot;) Check the summary. print(fit7.2, digits = 3, robust = T) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: opp ~ 0 + Intercept + time + ccog + time:ccog + unstr(time = time, gr = id) ## sigma ~ 0 + factor(time) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cortime(0,1) 0.748 0.072 0.573 0.864 1.001 2397 2618 ## cortime(0,2) 0.657 0.091 0.433 0.800 1.001 2268 2688 ## cortime(1,2) 0.806 0.058 0.660 0.895 1.002 2384 3055 ## cortime(0,3) 0.332 0.142 0.027 0.575 1.000 2444 2994 ## cortime(1,3) 0.638 0.094 0.415 0.787 1.000 2570 2949 ## cortime(2,3) 0.733 0.076 0.540 0.847 1.001 2693 2998 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 165.633 5.743 154.338 176.999 1.001 3294 2707 ## time 26.571 2.076 22.396 30.535 1.005 2907 2548 ## ccog -0.088 0.477 -1.005 0.829 1.000 3473 2811 ## time:ccog 0.457 0.162 0.118 0.798 1.000 3401 2863 ## sigma_factortime0 3.571 0.121 3.354 3.824 1.000 2127 2671 ## sigma_factortime1 3.454 0.114 3.248 3.685 1.000 1863 2632 ## sigma_factortime2 3.497 0.108 3.291 3.723 1.001 1794 2332 ## sigma_factortime3 3.518 0.115 3.308 3.763 1.001 2177 2812 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There’s a lot of exciting things going on in that output. We’ll start with the bottom 4 rows in the Population-Level Effects section, which contains our the summaries for our \\(\\log (\\sigma_{\\epsilon j})\\) parameters. To get them out of the log metric, we exponentiate. Here’s a quick conversion. fixef(fit7.2)[5:8, -2] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## sigma_factortime0 35.74347 28.62747 45.79190 ## sigma_factortime1 31.77577 25.72651 39.85576 ## sigma_factortime2 33.04557 26.86645 41.40652 ## sigma_factortime3 33.87739 27.32708 43.06749 Now let’s address the new Correlation Structures section of the print() output. Just as brms decomposes the typical multilevel model level-2 variance/covariance matrix \\(\\mathbf{\\Sigma}\\) as \\[ \\begin{align*} \\mathbf \\Sigma &amp; = \\mathbf D \\mathbf \\Omega \\mathbf D, \\text{where} \\\\ \\mathbf D &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf \\Omega &amp; = \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}, \\end{align*} \\] the same kind of thing happens when we fit a model with an unstructured variance/covariance matrix with the unstr() function. The print() output returned posterior summaries for the elements of the correlation matrix \\(\\mathbf \\Omega\\) in the Correlation Structures section, and it returned posterior summaries for the elements of the diagonal matrix of standard deviations \\(\\mathbf D\\) in the last four rows of the Population-Level Effects section. But notice that instead of \\(2 \\times 2\\) matrices like we got with our conventional growth model fit7.1, both \\(\\mathbf D\\) and \\(\\mathbf \\Omega\\) are now \\(4 \\times 4\\) matrices right out of the gate. Thus if we use the posterior medians from the print() output as our point estimates, we can express the \\(\\hat{\\mathbf \\Sigma}\\) matrix from our unstructured fit7.2 model as \\[ \\begin{align*} \\mathbf \\Sigma &amp; = \\mathbf D \\mathbf \\Omega \\mathbf D \\\\ \\hat{\\mathbf D} &amp; = \\begin{bmatrix} 35.6 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 31.6 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 33.0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 33.7 \\end{bmatrix} \\\\ \\hat{\\mathbf \\Omega} &amp; = \\begin{bmatrix} 1 &amp; .75 &amp; .66 &amp; .33 \\\\ .75 &amp; 1 &amp; .81 &amp; .64 \\\\ .66 &amp; .81 &amp; 1 &amp; .73 \\\\ .33 &amp; .64 &amp; .73 &amp; 1 \\end{bmatrix}. \\end{align*} \\] Here’s how to compute and summarize the \\(\\mathbf D\\) and \\(\\mathbf \\Omega\\) parameters with the as_draws_df() output. # wrangle sigma.us &lt;- as_draws_df(fit7.2) %&gt;% transmute(`sigma[1]` = exp(b_sigma_factortime0), `sigma[2]` = exp(b_sigma_factortime1), `sigma[3]` = exp(b_sigma_factortime2), `sigma[4]` = exp(b_sigma_factortime3), `rho[12]` = cortime__0__1, `rho[13]` = cortime__0__2, `rho[14]` = cortime__0__3, `rho[23]` = cortime__1__2, `rho[24]` = cortime__1__3, `rho[34]` = cortime__2__3, `rho[21]` = cortime__0__1, `rho[31]` = cortime__0__2, `rho[32]` = cortime__1__2, `rho[41]` = cortime__0__3, `rho[42]` = cortime__1__3, `rho[43]` = cortime__2__3) # summarize sigma.us %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 16 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rho[12] 0.75 0.57 0.86 0.95 median qi ## 2 rho[13] 0.66 0.43 0.8 0.95 median qi ## 3 rho[14] 0.33 0.03 0.57 0.95 median qi ## 4 rho[21] 0.75 0.57 0.86 0.95 median qi ## 5 rho[23] 0.81 0.66 0.89 0.95 median qi ## 6 rho[24] 0.64 0.42 0.79 0.95 median qi ## 7 rho[31] 0.66 0.43 0.8 0.95 median qi ## 8 rho[32] 0.81 0.66 0.89 0.95 median qi ## 9 rho[34] 0.73 0.54 0.85 0.95 median qi ## 10 rho[41] 0.33 0.03 0.57 0.95 median qi ## 11 rho[42] 0.64 0.42 0.79 0.95 median qi ## 12 rho[43] 0.73 0.54 0.85 0.95 median qi ## 13 sigma[1] 35.6 28.6 45.8 0.95 median qi ## 14 sigma[2] 31.6 25.7 39.9 0.95 median qi ## 15 sigma[3] 33 26.9 41.4 0.95 median qi ## 16 sigma[4] 33.7 27.3 43.1 0.95 median qi Since Singer and Willett preferred the variance/covariance parameterization for \\(\\mathbf \\Sigma\\), we’ll practice wrangling the posterior draws to transform our results into that metric, too. # transform sigma.us &lt;- sigma.us %&gt;% transmute(`sigma[1]^2` = `sigma[1]`^2, `sigma[12]` = `sigma[1]` * `sigma[2]` * `rho[12]`, `sigma[13]` = `sigma[1]` * `sigma[3]` * `rho[13]`, `sigma[14]` = `sigma[1]` * `sigma[4]` * `rho[14]`, `sigma[21]` = `sigma[2]` * `sigma[1]` * `rho[21]`, `sigma[2]^2` = `sigma[2]`^2, `sigma[23]` = `sigma[2]` * `sigma[3]` * `rho[23]`, `sigma[24]` = `sigma[2]` * `sigma[4]` * `rho[24]`, `sigma[31]` = `sigma[3]` * `sigma[1]` * `rho[31]`, `sigma[32]` = `sigma[3]` * `sigma[2]` * `rho[32]`, `sigma[3]^2` = `sigma[3]`^2, `sigma[34]` = `sigma[3]` * `sigma[4]` * `rho[34]`, `sigma[41]` = `sigma[4]` * `sigma[1]` * `rho[41]`, `sigma[42]` = `sigma[4]` * `sigma[2]` * `rho[42]`, `sigma[43]` = `sigma[4]` * `sigma[3]` * `rho[43]`, `sigma[4]^2` = `sigma[4]`^2) # summarize sigma.us %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 16 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sigma[12] 834. 493. 1421. 0.95 median qi ## 2 sigma[13] 757. 418. 1302. 0.95 median qi ## 3 sigma[14] 391. 29.4 874. 0.95 median qi ## 4 sigma[1]^2 1264. 820. 2097. 0.95 median qi ## 5 sigma[21] 834. 493. 1421. 0.95 median qi ## 6 sigma[23] 834. 515. 1373. 0.95 median qi ## 7 sigma[24] 674. 362. 1174. 0.95 median qi ## 8 sigma[2]^2 1000. 662. 1588. 0.95 median qi ## 9 sigma[31] 757. 418. 1302. 0.95 median qi ## 10 sigma[32] 834. 515. 1373. 0.95 median qi ## 11 sigma[34] 807. 471. 1373. 0.95 median qi ## 12 sigma[3]^2 1089. 722. 1714. 0.95 median qi ## 13 sigma[41] 391. 29.4 874. 0.95 median qi ## 14 sigma[42] 674. 362. 1174. 0.95 median qi ## 15 sigma[43] 807. 471. 1373. 0.95 median qi ## 16 sigma[4]^2 1136. 747. 1855. 0.95 median qi But again, I suspect it will be easier to appreciate our posterior \\(\\hat{\\mathbf \\Sigma}\\) in a tile plot. Here’s a summary using the posterior medians, similar to what Singer and Willett reported in the rightmost column of Table 7.3. levels &lt;- c(&quot;sigma[1]^2&quot;, &quot;sigma[12]&quot;, &quot;sigma[13]&quot;, &quot;sigma[14]&quot;, &quot;sigma[21]&quot;, &quot;sigma[2]^2&quot;, &quot;sigma[23]&quot;, &quot;sigma[24]&quot;, &quot;sigma[31]&quot;, &quot;sigma[32]&quot;, &quot;sigma[3]^2&quot;, &quot;sigma[34]&quot;, &quot;sigma[41]&quot;, &quot;sigma[42]&quot;, &quot;sigma[43]&quot;, &quot;sigma[4]^2&quot;) sigma.us %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate(label = round(value, digits = 0)) %&gt;% ggplot(aes(x = 0, y = 0)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label, color = value &lt; 1), show.legend = F) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, NA)) + scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Sigma)[italic(r)]*&quot; for the unstructured model&quot;)) + theme(legend.text = element_text(hjust = 1)) + facet_wrap(~ name, labeller = label_parsed) In case you’re curious, here’s the variance/covariance matrix from the sample data. fit7.2$data %&gt;% select(id, time, opp) %&gt;% pivot_wider(names_from = time, values_from = opp) %&gt;% select(-id) %&gt;% cov() %&gt;% round(digits = 0) ## 0 1 2 3 ## 0 1309 976 922 563 ## 1 976 1125 1021 856 ## 2 922 1021 1292 1081 ## 3 563 856 1081 1415 The brms default priors are weakly regularizing, particularly the LKJ prior for the correlation matrix \\(\\mathbf \\Omega\\), and I believe this is why the values from our model are systemically lower that the sample statistics. If you find this upsetting, collect more data, which hill help the likelihood dominate the prior. 7.3.2 Compound symmetric error covariance matrix. “A compound symmetric error covariance matrix requires just two parameters, labeled \\(\\sigma^2\\) and \\(\\sigma_1^2\\) in table 7.3” (p. 260). From the table, we see that matrix follows the form \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma^2 + \\sigma_1^2 &amp; \\sigma_1^2 &amp; \\sigma_1^2 &amp; \\sigma_1^2 \\\\ \\sigma_1^2 &amp; \\sigma^2 + \\sigma_1^2 &amp; \\sigma_1^2 &amp; \\sigma_1^2 \\\\ \\sigma_1^2 &amp; \\sigma_1^2 &amp; \\sigma^2 + \\sigma_1^2 &amp; \\sigma_1^2 \\\\ \\sigma_1^2 &amp; \\sigma_1^2 &amp; \\sigma_1^2 &amp; \\sigma^2 + \\sigma_1^2 \\end{bmatrix}, \\end{align*} \\] where \\(\\sigma_1^2\\) does not have the same meaning we’ve become accustomed to (i.e., the level-2 variance in linear change over time). The meaning of \\(\\sigma^2\\) might also be a little opaque. Happily, there’s another way to express this matrix, which is a modification of the heterogeneous compound symmetric matrix we see listed in Table 7.3. That alternative is: \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_\\epsilon^2 &amp; \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\rho \\\\ \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 &amp; \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\rho \\\\ \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 &amp; \\sigma_\\epsilon^2 \\rho \\\\ \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\rho &amp; \\sigma_\\epsilon^2 \\end{bmatrix}, \\end{align*} \\] where the term on the diagonal, \\(\\sigma_\\epsilon^2\\), is the residual variance, which is constrained to equality across all four time points. In all cells in the off-diagonal, we see \\(\\sigma_\\epsilon^2\\) multiplied by \\(\\rho\\). In this parameterization, \\(\\rho\\) is the correlation between time points, and that correlation is constrained to equality across all possible pairs of time points. Although this notation is a little different from the notation used in the text, I believe it will help us interpret our model. As we’ll see, brms uses this alternative parameterization. To fit the compound symmetric model with brms, we use the cosy() function. Notice how like with the unstructured model fit7.2, we’ve dropped the usual (1 + time | id) syntax. Instead, we impose compound symmetry within persons by setting gr = id within cosy(). fit7.3 &lt;- brm(data = opposites_pp, family = gaussian, opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.03&quot;) Check the model summary. print(fit7.3, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cosy 0.721 0.062 0.589 0.833 1.000 2265 2832 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 164.433 5.687 152.939 175.457 1.002 4023 3118 ## time 26.958 1.381 24.257 29.657 1.000 4178 3075 ## ccog -0.107 0.460 -0.997 0.787 1.001 4436 3065 ## time:ccog 0.432 0.113 0.215 0.649 1.001 4604 3215 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.540 3.650 29.615 43.813 1.001 2442 2406 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See that new cosy row? That’s \\(\\rho\\), the residual correlation among the time points. The sigma row on the bottom has it’s typical interpretation, it’s the residual standard deviation, what we typically call \\(\\sigma_\\epsilon\\). Square it and you’ll have what we called \\(\\sigma_\\epsilon^2\\) in the matrix, above. Okay, since our brms model is parameterized differently from what Singer and Willett reported in the text (see Table 7.3, p. 258), we’ll wrangle the posterior draws a bit. sigma.cs &lt;- as_draws_df(fit7.3) %&gt;% transmute(rho = cosy, sigma_e = sigma, `sigma^2 + sigma[1]^2` = sigma^2) %&gt;% mutate(`sigma[1]^2` = rho * sigma_e^2) %&gt;% mutate(`sigma^2` = `sigma^2 + sigma[1]^2` - `sigma[1]^2`) # what did we do? head(sigma.cs) ## # A tibble: 6 × 5 ## rho sigma_e `sigma^2 + sigma[1]^2` `sigma[1]^2` `sigma^2` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.696 31.9 1018. 709. 309. ## 2 0.707 32.5 1057. 748. 309. ## 3 0.668 32.8 1073. 717. 356. ## 4 0.756 37.6 1411. 1067. 344. ## 5 0.763 36.0 1294. 987. 307. ## 6 0.746 36.0 1294. 966. 328. Here’s the numeric summary. sigma.cs %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 5 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rho 0.73 0.59 0.83 0.95 median qi ## 2 sigma[1]^2 888. 532. 1572. 0.95 median qi ## 3 sigma^2 337. 263. 451. 0.95 median qi ## 4 sigma^2 + sigma[1]^2 1232. 877. 1920. 0.95 median qi ## 5 sigma_e 35.1 29.6 43.8 0.95 median qi To simplify, we might pull the posterior medians for \\(\\sigma^2 + \\sigma_1^2\\) and \\(\\sigma_1^2\\). We’ll call them diagonal and off_diagonal, respectively. diagonal &lt;- median(sigma.cs$`sigma^2 + sigma[1]^2`) off_diagonal &lt;- median(sigma.cs$`sigma[1]^2`) Now we have them, we can make our colored version of the \\(\\mathbf{\\Sigma}_r\\) Singer and Willett reported in the rightmost column of Table 7.3. crossing(row = 1:4, col = factor(1:4)) %&gt;% mutate(value = if_else(row == col, diagonal, off_diagonal)) %&gt;% mutate(label = round(value, digits = 0), col = fct_rev(col)) %&gt;% ggplot(aes(x = row, y = col)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label)) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, NA)) + scale_x_continuous(NULL, breaks = NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Sigma)[italic(r)]*&quot; for the compound symmetric model&quot;)) + theme(legend.text = element_text(hjust = 1)) 7.3.3 Heterogeneous compound symmetric error covariance matrix . Now we extend the compound symmetric matrix by allowing the residual variances to vary across the time waves. Thus, instead of a single \\(\\sigma_\\epsilon^2\\) parameter, we’ll have \\(\\sigma_1^2\\) through \\(\\sigma_4^2\\). However, we still have a single correlation parameter \\(\\rho\\). We can express this as \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_1 \\sigma_2 \\rho &amp; \\sigma_1 \\sigma_3 \\rho &amp; \\sigma_1 \\sigma_4 \\rho \\\\ \\sigma_2 \\sigma_1 \\rho &amp; \\sigma_1^2 &amp; \\sigma_2 \\sigma_3 \\rho &amp; \\sigma_2 \\sigma_4 \\rho \\\\ \\sigma_3 \\sigma_1 \\rho &amp; \\sigma_3 \\sigma_2 \\rho &amp; \\sigma_3^2 &amp; \\sigma_3 \\sigma_4 \\rho \\\\ \\sigma_4 \\sigma_1 \\rho &amp; \\sigma_4 \\sigma_2 \\rho &amp; \\sigma_4 \\sigma_3 \\rho &amp; \\sigma_4^2 \\end{bmatrix}, \\end{align*} \\] where, even though the correlation is the same in all cells, the covariances will differ because they are based on different combinations of the \\(\\sigma\\) parameters. To fit this model with brms, we will continue to use cosy(gr = id). But now we wrap the entire model formula within the bf() function and allow the residual standard deviations to vary across he waves with the line sigma ~ 0 + factor(time). fit7.4 &lt;- brm(data = opposites_pp, family = gaussian, bf(opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id), sigma ~ 0 + factor(time)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.04&quot;) Check the summary. print(fit7.4, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id) ## sigma ~ 0 + factor(time) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cosy 0.720 0.060 0.594 0.828 1.003 1851 2390 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 164.125 5.670 153.113 175.163 1.001 3548 3002 ## time 26.969 1.458 24.104 29.786 1.000 3560 3165 ## ccog -0.186 0.480 -1.133 0.752 1.000 3543 2918 ## time:ccog 0.440 0.122 0.208 0.683 1.001 3485 3164 ## sigma_factortime0 3.640 0.122 3.413 3.896 1.002 2136 2899 ## sigma_factortime1 3.494 0.117 3.277 3.741 1.003 2184 2614 ## sigma_factortime2 3.528 0.116 3.310 3.767 1.004 2147 2631 ## sigma_factortime3 3.594 0.121 3.378 3.849 1.003 2165 2560 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you look at the second row in the output, you’ll see that the brms default was to model \\(\\log(\\sigma_j)\\). Thus, you’ll have to exponentiate those posteriors to get them in their natural metric. Here’s a quick conversion. fixef(fit7.4)[5:8, -2] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## sigma_factortime0 38.10989 30.36652 49.18362 ## sigma_factortime1 32.90615 26.48896 42.11996 ## sigma_factortime2 34.07236 27.37209 43.26927 ## sigma_factortime3 36.38293 29.30780 46.93631 To get the marginal posteriors for the full \\(\\mathbf{\\Sigma}_r\\) matrix, we’ll want to work directly with the output from as_draws_df(). sigma.hcs &lt;- as_draws_df(fit7.4) %&gt;% transmute(`sigma[1]` = exp(b_sigma_factortime0), `sigma[2]` = exp(b_sigma_factortime1), `sigma[3]` = exp(b_sigma_factortime2), `sigma[4]` = exp(b_sigma_factortime3), rho = cosy, `sigma[1]^2` = exp(b_sigma_factortime0)^2, `sigma[2]^2` = exp(b_sigma_factortime1)^2, `sigma[3]^2` = exp(b_sigma_factortime2)^2, `sigma[4]^2` = exp(b_sigma_factortime3)^2) %&gt;% mutate(`sigma[2]*sigma[1]*rho` = `sigma[2]` * `sigma[1]` * rho, `sigma[3]*sigma[1]*rho` = `sigma[3]` * `sigma[1]` * rho, `sigma[4]*sigma[1]*rho` = `sigma[4]` * `sigma[1]` * rho, `sigma[1]*sigma[2]*rho` = `sigma[1]` * `sigma[2]` * rho, `sigma[3]*sigma[2]*rho` = `sigma[3]` * `sigma[2]` * rho, `sigma[4]*sigma[2]*rho` = `sigma[4]` * `sigma[2]` * rho, `sigma[1]*sigma[3]*rho` = `sigma[1]` * `sigma[3]` * rho, `sigma[2]*sigma[3]*rho` = `sigma[2]` * `sigma[3]` * rho, `sigma[4]*sigma[3]*rho` = `sigma[4]` * `sigma[3]` * rho, `sigma[1]*sigma[4]*rho` = `sigma[1]` * `sigma[4]` * rho, `sigma[2]*sigma[4]*rho` = `sigma[2]` * `sigma[4]` * rho, `sigma[3]*sigma[4]*rho` = `sigma[3]` * `sigma[4]` * rho) # what did we do? glimpse(sigma.hcs) ## Rows: 4,000 ## Columns: 21 ## $ `sigma[1]` &lt;dbl&gt; 41.13724, 33.60224, 42.36335, 45.88432, 32.06087, 34.74853, 38.26811, 29.828… ## $ `sigma[2]` &lt;dbl&gt; 30.65009, 34.93099, 34.32485, 35.16067, 31.28381, 29.23170, 29.89308, 26.083… ## $ `sigma[3]` &lt;dbl&gt; 38.48143, 34.95699, 46.66269, 43.97816, 31.19888, 31.98744, 30.18341, 26.872… ## $ `sigma[4]` &lt;dbl&gt; 36.18515, 33.68567, 44.92193, 42.97743, 34.08200, 31.70020, 29.48331, 33.833… ## $ rho &lt;dbl&gt; 0.7486510, 0.7430958, 0.7551732, 0.7636550, 0.6220704, 0.5995972, 0.6115675,… ## $ `sigma[1]^2` &lt;dbl&gt; 1692.2721, 1129.1104, 1794.6532, 2105.3707, 1027.8993, 1207.4601, 1464.4485,… ## $ `sigma[2]^2` &lt;dbl&gt; 939.4279, 1220.1739, 1178.1951, 1236.2728, 978.6765, 854.4923, 893.5961, 680… ## $ `sigma[3]^2` &lt;dbl&gt; 1480.8203, 1221.9914, 2177.4063, 1934.0790, 973.3700, 1023.1961, 911.0379, 7… ## $ `sigma[4]^2` &lt;dbl&gt; 1309.3648, 1134.7245, 2017.9800, 1847.0591, 1161.5825, 1004.9030, 869.2656, … ## $ `sigma[2]*sigma[1]*rho` &lt;dbl&gt; 943.9440, 872.2157, 1098.1090, 1232.0225, 623.9279, 609.0460, 699.6037, 515.… ## $ `sigma[3]*sigma[1]*rho` &lt;dbl&gt; 1185.1291, 872.8650, 1492.8170, 1540.9857, 622.2341, 666.4621, 706.3983, 530… ## $ `sigma[4]*sigma[1]*rho` &lt;dbl&gt; 1114.4096, 841.1205, 1437.1274, 1505.9200, 679.7354, 660.4776, 690.0136, 668… ## $ `sigma[1]*sigma[2]*rho` &lt;dbl&gt; 943.9440, 872.2157, 1098.1090, 1232.0225, 623.9279, 609.0460, 699.6037, 515.… ## $ `sigma[3]*sigma[2]*rho` &lt;dbl&gt; 883.0033, 907.3812, 1209.5530, 1180.8411, 607.1529, 560.6517, 551.8020, 464.… ## $ `sigma[4]*sigma[2]*rho` &lt;dbl&gt; 830.3123, 874.3814, 1164.4306, 1153.9707, 663.2605, 555.6173, 539.0031, 584.… ## $ `sigma[1]*sigma[3]*rho` &lt;dbl&gt; 1185.1291, 872.8650, 1492.8170, 1540.9857, 622.2341, 666.4621, 706.3983, 530… ## $ `sigma[2]*sigma[3]*rho` &lt;dbl&gt; 883.0033, 907.3812, 1209.5530, 1180.8411, 607.1529, 560.6517, 551.8020, 464.… ## $ `sigma[4]*sigma[3]*rho` &lt;dbl&gt; 1042.4636, 875.0323, 1582.9775, 1443.3602, 661.4600, 607.9966, 544.2380, 601… ## $ `sigma[1]*sigma[4]*rho` &lt;dbl&gt; 1114.4096, 841.1205, 1437.1274, 1505.9200, 679.7354, 660.4776, 690.0136, 668… ## $ `sigma[2]*sigma[4]*rho` &lt;dbl&gt; 830.3123, 874.3814, 1164.4306, 1153.9707, 663.2605, 555.6173, 539.0031, 584.… ## $ `sigma[3]*sigma[4]*rho` &lt;dbl&gt; 1042.4636, 875.0323, 1582.9775, 1443.3602, 661.4600, 607.9966, 544.2380, 601… Here’s the numeric summary. sigma.hcs %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 21 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rho 0.72 0.59 0.83 0.95 median qi ## 2 sigma[1] 37.9 30.4 49.2 0.95 median qi ## 3 sigma[1]*sigma[2]*rho 892. 533. 1561. 0.95 median qi ## 4 sigma[1]*sigma[3]*rho 924. 547. 1607. 0.95 median qi ## 5 sigma[1]*sigma[4]*rho 992. 585. 1714. 0.95 median qi ## 6 sigma[1]^2 1439. 922. 2419. 0.95 median qi ## 7 sigma[2] 32.7 26.5 42.1 0.95 median qi ## 8 sigma[2]*sigma[1]*rho 892. 533. 1561. 0.95 median qi ## 9 sigma[2]*sigma[3]*rho 802. 476. 1384. 0.95 median qi ## 10 sigma[2]*sigma[4]*rho 854. 512. 1486. 0.95 median qi ## # ℹ 11 more rows That’s a lot of information to wade through. Here we simplify the picture by making our plot version of the matrix Singer and Willett reported in the rightmost column of Table 7.3. # arrange the panels levels &lt;- c(&quot;sigma[1]^2&quot;, &quot;sigma[1]*sigma[2]*rho&quot;, &quot;sigma[1]*sigma[3]*rho&quot;, &quot;sigma[1]*sigma[4]*rho&quot;, &quot;sigma[2]*sigma[1]*rho&quot;, &quot;sigma[2]^2&quot;, &quot;sigma[2]*sigma[3]*rho&quot;, &quot;sigma[2]*sigma[4]*rho&quot;, &quot;sigma[3]*sigma[1]*rho&quot;, &quot;sigma[3]*sigma[2]*rho&quot;, &quot;sigma[3]^2&quot;, &quot;sigma[3]*sigma[4]*rho&quot;, &quot;sigma[4]*sigma[1]*rho&quot;, &quot;sigma[4]*sigma[2]*rho&quot;, &quot;sigma[4]*sigma[3]*rho&quot;, &quot;sigma[4]^2&quot;) sigma.hcs %&gt;% select(`sigma[1]^2`:`sigma[3]*sigma[4]*rho`) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate(label = round(value, digits = 0)) %&gt;% ggplot(aes(x = 0, y = 0)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label)) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, NA)) + scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Sigma)[italic(r)]*&quot; for the heterogeneous compound symmetric model&quot;)) + theme(legend.text = element_text(hjust = 1)) + facet_wrap(~ name, labeller = label_parsed) 7.3.4 Autoregressive error covariance matrix. The first-order autoregressive has a strict “band-diagonal” structure governed by two parameters, which Singer and Willett called \\(\\sigma^2\\) and \\(\\rho\\). From Table 7.3 (p. 260), we see that matrix follows the form \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma^2 &amp; \\sigma^2 \\rho &amp; \\sigma^2 \\rho^2 &amp; \\sigma^2 \\rho^3 \\\\ \\sigma^2 \\rho &amp; \\sigma^2 &amp; \\sigma^2 \\rho &amp; \\sigma^2 \\rho^2 \\\\ \\sigma^2 \\rho^2 &amp; \\sigma^2 \\rho &amp; \\sigma^2 &amp; \\sigma^2 \\rho \\\\ \\sigma^2 \\rho^3 &amp; \\sigma^2 \\rho^2 &amp; \\sigma^2 \\rho &amp; \\sigma^2 \\end{bmatrix}, \\end{align*} \\] where \\(\\rho\\) is the correlation of one time point to the one immediately before or after, after conditioning on the liner model. In a similar way, \\(\\rho^2\\) is the correlation between time points with one degree of separation (e.g., time 1 with time 3) and \\(\\rho^3\\) is the correlation between the first and fourth time point. The other parameter, \\(\\sigma^2\\) is the residual variance after conditioning on the linear model. Once can fit this model with brms using a version of the ar() syntax. However, the model will follow a slightly different parameterization, following the form: \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^3 \\\\ \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 \\\\ \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho \\\\ \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^3 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho &amp; \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\end{bmatrix},\\\\ \\end{align*} \\] where \\(\\sigma_\\epsilon\\) is the residual variance after conditioning on both the linear model AND the autoregressive correlation \\(\\rho\\). It’s not clear to me why brms is parameterized this way, but this is what we’ve got. The main point to get is that what Singer and Willett called \\(\\sigma\\) in their autoregressive model, we’ll have to call \\(\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2}\\). Thus, if you substitute our verbose brms term \\(\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2}\\) for Singer and Willett’s compact term \\(\\sigma\\), you’ll see the hellish matrix above is the same as the much simpler one before it. To fit the first-order autoregressive model with brms, we use the ar() function. As with the last few models, notice how we continue to omit the (1 + time | id) syntax. Instead, we impose the autoregressive structure within persons by setting gr = id within ar(). We also set cov = TRUE. fit7.5 &lt;- brm(data = opposites_pp, family = gaussian, opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.05&quot;) Check the summary. print(fit7.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## ar[1] 0.816 0.041 0.730 0.889 1.000 3651 2814 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 164.394 6.122 152.165 176.435 1.002 4574 3109 ## time 27.217 1.908 23.488 30.865 1.003 4100 2987 ## ccog -0.030 0.497 -0.990 0.942 1.000 4652 3058 ## time:ccog 0.420 0.153 0.123 0.718 1.000 4815 2911 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 20.319 1.370 17.874 23.232 1.003 4084 2672 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The ar[1] row in our summary is \\(\\rho\\). As we discussed just before fitting the model, the sigma line is the summary for what I’m calling \\(\\sigma_\\epsilon\\), which is the residual standard deviation after conditioning on both the linear model AND \\(\\rho\\). If we rename the \\(\\sigma^2\\) parameter in the text as \\(\\sigma_\\text{Singer &amp; Willett (2003)}^2\\), we can convert our \\(\\sigma_\\epsilon\\) parameter to that metric using the formula \\[\\sigma_\\text{Singer &amp; Willett (2003)}^2 = \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2.\\] With that formula in hand, we’re ready to compute the marginal posteriors for the full \\(\\mathbf{\\Sigma}_r\\) matrix, saving the results as sigma.ar. sigma.ar &lt;- as_draws_df(fit7.5) %&gt;% mutate(sigma_e = sigma, sigma = sigma_e / sqrt(1 - `ar[1]`^2)) %&gt;% transmute(rho = `ar[1]`, `sigma^2` = sigma^2, `sigma^2 rho` = sigma^2 * rho, `sigma^2 rho^2` = sigma^2 * rho^2, `sigma^2 rho^3` = sigma^2 * rho^3) Here’s the numeric summary. sigma.ar %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 5 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rho 0.82 0.73 0.89 0.95 median qi ## 2 sigma^2 1247. 890. 1903. 0.95 median qi ## 3 sigma^2 rho 1019. 668 1687. 0.95 median qi ## 4 sigma^2 rho^2 831. 499. 1484. 0.95 median qi ## 5 sigma^2 rho^3 680. 369. 1311 0.95 median qi To simplify, we might pull the posterior medians for \\(\\sigma^2\\) through \\(\\sigma^2 \\rho^3\\). s2 &lt;- median(sigma.ar$`sigma^2`) s2p &lt;- median(sigma.ar$`sigma^2 rho`) s2p2 &lt;- median(sigma.ar$`sigma^2 rho^2`) s2p3 &lt;- median(sigma.ar$`sigma^2 rho^3`) Now we have them, we can make our colored version of the \\(\\mathbf{\\Sigma}_r\\) Singer and Willett reported in the rightmost column of Table 7.3. crossing(row = 1:4, col = factor(1:4)) %&gt;% mutate(value = c(s2, s2p, s2p2, s2p3, s2p, s2, s2p, s2p2, s2p2, s2p, s2, s2p, s2p3, s2p2, s2p, s2)) %&gt;% mutate(label = round(value, digits = 0), col = fct_rev(col)) %&gt;% ggplot(aes(x = row, y = col)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label)) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, NA)) + scale_x_continuous(NULL, breaks = NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Sigma)[italic(r)]*&quot; for the autoregressive model&quot;)) + theme(legend.text = element_text(hjust = 1)) With this presentation, that strict band-diagonal structure really pops. 7.3.5 Heterogeneous autoregressive error covariance matrix. For the heterogeneous autoregressive error covariance matrix, we relax the assumption that the variances on the diagonal of the \\(\\mathbf{\\Sigma}_r\\) matrix are constant across waves. From Table 7.3 (p. 260), we see that matrix follows the form \\[ \\begin{align*} \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_1 \\sigma_2 \\rho &amp; \\sigma_1 \\sigma_3 \\rho^2 &amp; \\sigma_1 \\sigma_4 \\rho^3 \\\\ \\sigma_2 \\sigma_1 \\rho &amp; \\sigma_2^2 &amp; \\sigma_2 \\sigma_3 \\rho &amp; \\sigma_2 \\sigma_4 \\rho^2 \\\\ \\sigma_3 \\sigma_1 \\rho^2 &amp; \\sigma_3 \\sigma_2 \\rho &amp; \\sigma_3^2 &amp; \\sigma_3 \\sigma_4 \\rho \\\\ \\sigma_4 \\sigma_1 \\rho^3 &amp; \\sigma_4 \\sigma_2 \\rho^2 &amp; \\sigma_4 \\sigma_3 \\rho &amp; \\sigma_4^2 \\end{bmatrix}, \\end{align*} \\] where, as before, \\(\\rho\\) is the correlation of one time point to the one immediately before or after, after conditioning on the liner model. To fit this model with brms, we continue to use the ar(gr = id, cov = TRUE) syntax. The only adjustment is we now wrap the formula within the bf() function and add a second line for sigma. fit7.6 &lt;- brm(data = opposites_pp, family = gaussian, bf(opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE), sigma ~ 0 + factor(time)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.06&quot;) Inspect the parameter summary. print(fit7.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE) ## sigma ~ 0 + factor(time) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## ar[1] 0.808 0.041 0.721 0.881 1.001 3564 2753 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 164.572 6.174 152.309 176.850 1.001 4594 3340 ## time 27.171 1.998 23.184 31.162 1.001 3796 2622 ## ccog -0.121 0.484 -1.078 0.823 1.002 3890 2999 ## time:ccog 0.431 0.157 0.134 0.738 1.000 4145 3186 ## sigma_factortime0 3.068 0.106 2.873 3.290 1.001 3789 2765 ## sigma_factortime1 2.969 0.087 2.807 3.147 1.000 3088 2704 ## sigma_factortime2 3.014 0.086 2.854 3.190 1.001 3286 3145 ## sigma_factortime3 3.028 0.102 2.839 3.238 1.002 3863 3005 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here are summaries for the four \\(\\sigma_\\epsilon\\) posteriors, after exponentiation. fixef(fit7.6)[5:8, -2] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## sigma_factortime0 21.49858 17.68502 26.83905 ## sigma_factortime1 19.46950 16.55493 23.25568 ## sigma_factortime2 20.36495 17.35231 24.29690 ## sigma_factortime3 20.64833 17.09583 25.47358 Extending our workflow from the last section, here how we might compute the marginal posteriors for the full \\(\\mathbf{\\Sigma}_r\\) matrix, saving the results as sigma.har. sigma.har &lt;- as_draws_df(fit7.6) %&gt;% mutate(sigma_1e = exp(b_sigma_factortime0), sigma_2e = exp(b_sigma_factortime1), sigma_3e = exp(b_sigma_factortime2), sigma_4e = exp(b_sigma_factortime3)) %&gt;% mutate(sigma_1 = sigma_1e / sqrt(1 - `ar[1]`^2), sigma_2 = sigma_2e / sqrt(1 - `ar[1]`^2), sigma_3 = sigma_3e / sqrt(1 - `ar[1]`^2), sigma_4 = sigma_4e / sqrt(1 - `ar[1]`^2)) %&gt;% transmute(rho = `ar[1]`, `sigma_1^2` = sigma_1^2, `sigma_2^2` = sigma_2^2, `sigma_3^2` = sigma_3^2, `sigma_4^2` = sigma_4^2, `sigma_2 sigma_1 rho` = sigma_2 * sigma_1 * rho, `sigma_3 sigma_1 rho^2` = sigma_3 * sigma_1 * rho^2, `sigma_4 sigma_1 rho^3` = sigma_4 * sigma_1 * rho^3, `sigma_3 sigma_2 rho` = sigma_3 * sigma_2 * rho, `sigma_4 sigma_2 rho^2` = sigma_4 * sigma_2 * rho^2, `sigma_4 sigma_3 rho` = sigma_4 * sigma_3 * rho) Here’s the numeric summary. sigma.har %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 11 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rho 0.81 0.72 0.88 0.95 median qi ## 2 sigma_1^2 1348. 885. 2241. 0.95 median qi ## 3 sigma_2 sigma_1 rho 983. 625. 1678. 0.95 median qi ## 4 sigma_2^2 1106. 727. 1814. 0.95 median qi ## 5 sigma_3 sigma_1 rho^2 831. 496. 1498. 0.95 median qi ## 6 sigma_3 sigma_2 rho 932. 587. 1605. 0.95 median qi ## 7 sigma_3^2 1211. 793. 1972. 0.95 median qi ## 8 sigma_4 sigma_1 rho^3 680. 374. 1276. 0.95 median qi ## 9 sigma_4 sigma_2 rho^2 762. 451. 1352. 0.95 median qi ## 10 sigma_4 sigma_3 rho 993. 632. 1637. 0.95 median qi ## 11 sigma_4^2 1248. 809. 2028. 0.95 median qi As in the last section, we might pull the posterior medians for \\(\\sigma1^2\\) through \\(\\sigma_4^2\\). s12 &lt;- median(sigma.har$`sigma_1^2`) s22 &lt;- median(sigma.har$`sigma_2^2`) s32 &lt;- median(sigma.har$`sigma_3^2`) s42 &lt;- median(sigma.har$`sigma_4^2`) s2s1p &lt;- median(sigma.har$`sigma_2 sigma_1 rho`) s3s1p2 &lt;- median(sigma.har$`sigma_3 sigma_1 rho^2`) s3s2p &lt;- median(sigma.har$`sigma_3 sigma_2 rho`) s4s1p3 &lt;- median(sigma.har$`sigma_4 sigma_1 rho^3`) s4s2p2 &lt;- median(sigma.har$`sigma_4 sigma_2 rho^2`) s4s3p &lt;- median(sigma.har$`sigma_4 sigma_3 rho`) Now we have them, we can make our colored version of the \\(\\mathbf{\\Sigma}_r\\) Singer and Willett reported in the rightmost column of Table 7.3. crossing(row = 1:4, col = factor(1:4)) %&gt;% mutate(value = c(s12, s2s1p, s3s1p2, s4s1p3, s2s1p, s22, s3s2p, s4s2p2, s3s1p2, s3s2p, s32, s4s3p, s4s1p3, s4s2p2, s4s3p, s42)) %&gt;% mutate(label = round(value, digits = 0), col = fct_rev(col)) %&gt;% ggplot(aes(x = row, y = col)) + geom_tile(aes(fill = value)) + geom_text(aes(label = label)) + scale_fill_viridis_c(&quot;posterior\\nmedian&quot;, option = &quot;A&quot;, limits = c(0, NA)) + scale_x_continuous(NULL, breaks = NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = expression(hat(Sigma)[italic(r)]*&quot; for the heterogeneous autoregressive model&quot;)) + theme(legend.text = element_text(hjust = 1)) Even though there’s still a strict band diagonal correlation structure, the heterogeneous variances allow for differences among the covariances within the bands. 7.3.6 Toeplitz error covariance matrix. Handy as it is, brms is not yet set up to fit models with the Toeplitz error covariance matrix, at this time. For details, see brms GitHub issue #403. 7.3.7 Does choosing the “correct” error covariance structure really matter? Now we have all our models, we might compare them with information criteria, as was done in the text. Here we’ll use the WAIC. First, compute and save the WAIC estimates. fit7.1 &lt;- add_criterion(fit7.1, criterion = &quot;waic&quot;) fit7.2 &lt;- add_criterion(fit7.2, criterion = &quot;waic&quot;) fit7.3 &lt;- add_criterion(fit7.3, criterion = &quot;waic&quot;) fit7.4 &lt;- add_criterion(fit7.4, criterion = &quot;waic&quot;) fit7.5 &lt;- add_criterion(fit7.5, criterion = &quot;waic&quot;) fit7.6 &lt;- add_criterion(fit7.6, criterion = &quot;waic&quot;) Now compare the models using WAIC differences and WAIC weights. loo_compare(fit7.1, fit7.2, fit7.3, fit7.4, fit7.5, fit7.6, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit7.1 0.0 0.0 -585.5 6.1 46.5 3.4 1171.1 12.2 ## fit7.5 -22.2 4.3 -607.7 8.5 3.4 0.5 1215.4 17.1 ## fit7.2 -22.9 2.9 -608.4 7.7 14.5 1.7 1216.8 15.3 ## fit7.6 -25.0 4.3 -610.5 8.8 6.7 1.1 1221.0 17.6 ## fit7.3 -40.6 6.0 -626.2 8.1 4.5 0.7 1252.3 16.2 ## fit7.4 -42.4 5.5 -627.9 7.8 6.8 0.8 1255.8 15.6 model_weights(fit7.1, fit7.2, fit7.3, fit7.4, fit7.5, fit7.6, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit7.1 fit7.2 fit7.3 fit7.4 fit7.5 fit7.6 ## 1 0 0 0 0 0 By both methods of comparison, the standard multilevel model for change was the clear winner. Perhaps most important, consider how choice of an error covariance structure affects our ability to address our research questions, especially given that it is the fixed effects–and not the variance components–that usually embody these questions. Some might say that refining the error covariance structure for the multilevel model for change is akin to rearranging the deck chairs on the Titanic–it rarely fundamentally changes our parameter estimates. Indeed, regardless of the error structure chosen, estimates of the fixed effects are unbiased and may not be affected much by choices made in the stochastic part of the model (providing that neither the data, nor the error structure, are idiosyncratic). (p. 264, emphasis in the original) For more on the idea that researchers generally just care about fixed effects, see the paper by McNeish et al. (2017), On the unnecessary ubiquity of hierarchical linear modeling. Although I can’t disagree with the logic presented by Singer and Willett, or by McNeish and colleagues, I’m uneasy with this perspective for a couple reasons. First, I suspect part of the reason researchers don’t theorize about variances and covariances is because those are difficult metrics for many of us to think about. Happily, brms makes these more accessible by parameterizing them as standard deviations and correlations. Second, in many disciplines, including my own (clinical psychology), multilevel models are still exotic and researchers just aren’t used to thinking in their terms. But I see this as more of a reason to spread the multilevel gospel than to down emphasize variance parameters. In my opinion, which is heavily influenced by McElreath’s, it would be great if someday soon, researchers used multilevel models (longitudinal or otherwise) as the default rather than the exception. Third, I actually care about random effects. If you go back and compare the models from this chapter, it was only the multilevel growth model (fit7.1) that assigned person-specific intercepts and slopes. In clinical psychology, this matters! I want a model that allows me to make plots like this: # 35 person-specific growth trajectories nd &lt;- opposites_pp %&gt;% distinct(id, ccog) %&gt;% expand(nesting(id, ccog), time = c(0, 3)) fitted(fit7.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = time + 1, y = Estimate, group = id)) + geom_line(linewidth = 1/4, alpha = 2/3) + labs(subtitle = &quot;35 person-specific growth trajectories&quot;, x = &quot;day&quot;, y = &quot;opposites naming task&quot;) + theme(panel.grid = element_blank()) # 35 person-specific intercept and slope parameters rbind(coef(fit7.1)$id[, , &quot;Intercept&quot;], coef(fit7.1)$id[, , &quot;time&quot;]) %&gt;% data.frame() %&gt;% mutate(type = rep(c(&quot;intercepts&quot;, &quot;slopes&quot;), each = n() / 2)) %&gt;% group_by(type) %&gt;% arrange(Estimate) %&gt;% mutate(id = 1:35) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = id)) + geom_pointrange(fatten = 1) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;35 person-specific intercept and slope parameters&quot;, x = &quot;marginal posterior&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ type, scales = &quot;free_x&quot;) Of all the models we fit, only the standard multilevel model for change allows us to drill down all the way to the individual participants and this was accomplished by how it parameterized \\(\\mathbf{\\Sigma}_r\\). Even your substantive theory isn’t built around the subtleties in the \\(4 \\times 4 = 16\\) cells in the \\(\\mathbf{\\Sigma}_r\\), it still matters that our fit7.1 parameterized them by way of \\(\\sigma_\\epsilon\\), \\(\\sigma_0\\), \\(\\sigma_1\\), and \\(\\rho_{01}\\). But Singer and Willett went on: But refining our hypotheses about the error covariance structure does affect the precision of estimates of the fixed effects and will therefore impact hypothesis testing and confidence interval construction. (p. 264, emphasis in the original) I’m going to showcase this in a coefficient plot, rather than the way the authors did in their Table 7.4. First, we’ll want a custom wrangling function. Let’s call it gamma_summary(). gamma_summary &lt;- function(brmsfit) { fixef(brmsfit)[1:4, ] %&gt;% data.frame() %&gt;% mutate(gamma = c(&quot;gamma[0][0]&quot;, &quot;gamma[1][0]&quot;, &quot;gamma[0][1]&quot;, &quot;gamma[1][1]&quot;)) } gamma_summary(fit7.1) ## Estimate Est.Error Q2.5 Q97.5 gamma ## Intercept 164.1077730 6.4531957 151.63917320 177.0652489 gamma[0][0] ## time 26.9878037 2.0932513 22.80959495 31.2354433 gamma[1][0] ## ccog -0.1287983 0.5206337 -1.16248699 0.8813408 gamma[0][1] ## time:ccog 0.4337499 0.1672927 0.09452261 0.7611580 gamma[1][1] Now wrangle and plot. type &lt;- c(&quot;standard growth model&quot;, &quot;unstructured&quot;, &quot;compound symmetric&quot;, &quot;heterogeneous compound symmetric&quot;, &quot;autoregressive&quot;, &quot;heterogeneous autoregressive&quot;) tibble(fit = str_c(&quot;fit7.&quot;, c(1:6)), type = factor(type, levels = type)) %&gt;% mutate(type = fct_rev(type), gamma = map(fit, ~get(.) %&gt;% gamma_summary())) %&gt;% unnest(gamma) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = type)) + geom_pointrange(fatten = 1.25) + scale_x_continuous(&quot;marginal posterior&quot;, expand = expansion(mult = 0.25)) + labs(subtitle = &quot;Parameter precision can vary across model types.&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~ gamma, scales = &quot;free_x&quot;, labeller = label_parsed, nrow = 1) Yep, the way we parameterize \\(\\mathbf{\\Sigma}_r\\) might have subtle consequences for the widths of our marginal \\(\\gamma\\) posteriors. This can matter a lot when you’re analyzing data from, say, a clinical trial where lives may depend on the results of your analysis. Rearrange the deck chairs on your Titanic with caution, friends. Someone might trip and hurt themselves. 7.3.8 Bonus: Did we trade multilevel for multivariate? So far in this chapter, only the first model fit7.1 used the conventional brms multilevel syntax, such as (1 + time | id). The other five models used the unstr(), cosy(), or ar() helper functions, instead. Singer and Willett didn’t exactly come out and say it this way, but in a sense, fit7.1 is the only multilevel model we’ve fit so far in this chapter. Mallinckrod et al. (2008) made the point more clearly: A simple formulation of the general linear mixed model… can be implemented in which the random effects are not explicitly modeled, but rather are included as part of the marginal covariance matrix… leading them to what could alternatively be described as a multivariate normal model. (p. 306) Thus we might think of all our models after fit7.1 as special kinds of multivariate panel models. To help make this easier to see, let’s first fit a simplified version of the unstructured model, following the form \\[ \\begin{align*} \\text{opp}_{ij} &amp; \\sim \\operatorname{Normal}(\\mu_j, \\mathbf{\\Sigma}_r) \\\\ \\mu_j &amp; = \\pi_{0j} \\\\ \\mathbf{\\Sigma}_r &amp; = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\sigma_{13} &amp; \\sigma_{14} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\sigma_{23} &amp; \\sigma_{24} \\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_3^2 &amp; \\sigma_{34} \\\\ \\sigma_{41} &amp; \\sigma_{42} &amp; \\sigma_{43} &amp; \\sigma_4^2 \\end{bmatrix}, \\end{align*} \\] where \\(\\pi_{0j}\\) is the intercept, which is estimated separately for each of the 4 \\(j\\) time points, and \\(\\mathbf{\\Sigma}_r\\) is the unstructured variance/covariance matrix. To fit those separate \\(\\pi_{0j}\\) intercepts with brm(), we use the syntax of 0 + factor(time) in the \\(\\mu_j\\) portion of the model. Otherwise, the syntax is much the same as with the first unstructured model fit7.2. fit7.7 &lt;- brm(data = opposites_pp, family = gaussian, bf(opp ~ 0 + factor(time) + unstr(time = time, gr = id), sigma ~ 0 + factor(time)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.07&quot;) Check the parameter summary. print(fit7.7, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: opp ~ 0 + factor(time) + unstr(time = time, gr = id) ## sigma ~ 0 + factor(time) ## Data: opposites_pp (Number of observations: 140) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cortime(0,1) 0.739 0.078 0.556 0.858 1.002 1975 2336 ## cortime(0,2) 0.624 0.098 0.407 0.787 1.001 2190 2566 ## cortime(1,2) 0.780 0.066 0.628 0.884 1.002 2666 3057 ## cortime(0,3) 0.288 0.144 -0.014 0.550 1.001 2263 2490 ## cortime(1,3) 0.585 0.105 0.352 0.762 1.001 2471 3004 ## cortime(2,3) 0.734 0.076 0.565 0.854 1.001 3141 3107 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## factortime0 164.394 6.069 153.024 176.217 1.002 2371 2917 ## factortime1 192.009 5.515 181.216 202.614 1.002 1972 2296 ## factortime2 216.654 5.865 204.734 228.130 1.002 1961 2089 ## factortime3 246.064 6.443 233.261 258.609 1.000 2426 2532 ## sigma_factortime0 3.570 0.113 3.364 3.801 1.000 2453 2397 ## sigma_factortime1 3.464 0.108 3.264 3.694 1.002 2202 2341 ## sigma_factortime2 3.527 0.108 3.326 3.750 1.002 2236 2461 ## sigma_factortime3 3.611 0.112 3.401 3.835 1.002 2558 2808 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Once again, those 4 sigma_ parameters were fit using the log link. Here they are after exponentiation. fixef(fit7.7)[5:8, -2] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## sigma_factortime0 35.50063 28.89688 44.72506 ## sigma_factortime1 31.93331 26.14629 40.19886 ## sigma_factortime2 34.01411 27.83327 42.54170 ## sigma_factortime3 36.99593 29.99889 46.29403 All we’ve really done is fit an unconditional multivariate normal model for opp, across the four time points. To see, compare the posterior point estimates in the Population-Level Effects section (or their exponentiated values, above), to the sample means and SD’s for opp over time. opposites_pp %&gt;% group_by(time) %&gt;% summarise(m = mean(opp), s = sd(opp)) ## # A tibble: 4 × 3 ## time m s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 164. 36.2 ## 2 1 192 33.5 ## 3 2 217. 35.9 ## 4 3 246. 37.6 In a similar way, the correlations in the Correlation Structures part of the print() output are really just regularized sample correlations. Compare them to the un-regularized Pearson’s correlation estimates. opposites_pp %&gt;% select(id, time, opp) %&gt;% pivot_wider(names_from = time, values_from = opp) %&gt;% select(-id) %&gt;% cor() %&gt;% round(digits = 3) ## 0 1 2 3 ## 0 1.000 0.804 0.709 0.414 ## 1 0.804 1.000 0.847 0.678 ## 2 0.709 0.847 1.000 0.800 ## 3 0.414 0.678 0.800 1.000 The reason for the regularization, by the way, is because of the default LKJ(1) prior. That prior is more strongly regularizing as you increase the dimensions of the correlation matrix. In the case of a \\(4 \\times 4\\) matrix, the LKJ will definitely push the individual correlations toward zero, particularly in the case of a modestly-sized data set like opposites_pp. But anyway, we can fit basically the same model by explicitly using the brms multivariate syntax, as outlined in Bürkner’s (2022a) vignette, Estimating multivariate models with brms. Before we can fit a more conventional multivariate model to the data, we’ll need to convert opposites_pp to the wide format, which we’ll call opposites_wide. opposites_wide &lt;- opposites_pp %&gt;% mutate(time = str_c(&quot;t&quot;, time)) %&gt;% select(-wave) %&gt;% pivot_wider(names_from = time, values_from = opp) # what? head(opposites_wide) ## # A tibble: 6 × 7 ## id cog ccog t0 t1 t2 t3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 137 23.5 205 217 268 302 ## 2 2 123 9.54 219 243 279 302 ## 3 3 129 15.5 142 212 250 289 ## 4 4 125 11.5 206 230 248 273 ## 5 5 81 -32.5 190 220 229 220 ## 6 6 110 -3.46 165 205 207 263 Now we’re ready to fit a multivariate model to our 4 variables t0 through t3. To do so, we place all 4 variables within the mvbind() function, and we nest the entire model formula within the bf() function. Also notice we are explicitly asking for residual correlations by setting set_rescor(TRUE). fit7.8 &lt;- brm(data = opposites_wide, family = gaussian, bf(mvbind(t0, t1, t2, t3) ~ 1) + set_rescor(TRUE), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.08&quot;) Review the model summary. print(fit7.8, digits = 3) ## Family: MV(gaussian, gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: t0 ~ 1 ## t1 ~ 1 ## t2 ~ 1 ## t3 ~ 1 ## Data: opposites_wide (Number of observations: 35) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## t0_Intercept 164.637 6.108 152.708 176.571 1.000 1966 2289 ## t1_Intercept 192.169 5.475 181.626 203.058 1.002 1941 2259 ## t2_Intercept 216.885 5.734 205.411 228.061 1.001 1919 2133 ## t3_Intercept 246.290 6.256 233.790 258.523 1.003 2237 2729 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_t0 35.874 4.214 28.927 45.298 1.000 2484 2589 ## sigma_t1 32.181 3.643 25.959 40.146 1.000 2220 2729 ## sigma_t2 34.295 3.830 27.770 43.252 1.001 2147 2341 ## sigma_t3 37.500 4.437 29.950 47.075 1.001 2521 2859 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(t0,t1) 0.738 0.078 0.558 0.862 1.002 2173 3269 ## rescor(t0,t2) 0.622 0.099 0.402 0.788 1.001 2014 2169 ## rescor(t1,t2) 0.781 0.067 0.633 0.886 1.000 2621 3241 ## rescor(t0,t3) 0.285 0.146 -0.020 0.551 1.001 2125 2457 ## rescor(t1,t3) 0.584 0.108 0.343 0.763 1.000 2729 2909 ## rescor(t2,t3) 0.737 0.077 0.564 0.862 1.000 2697 3068 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The results are very similar to those for fit7.7, above, with a few small differences. First, we now have a Residual Correlations section instead of a Correlation Structures. The output of the two is basically the same, though. The various sigma_ parameters are now in their own Family Specific Parameters, instead of in the bottom fo the Population-Level Effects section. You’ll also note the sigma_ parameters are all in their natural metric, rather than the log metric. This is because, by default, brms used the log link for the syntax we used for fit7.7, but used the identity link for the syntax we used for fit7.8. Even if you take these small formatting-type differences into account, a careful eye will notice the parameter summaries are still a little different between these two models. The reason is the defualt priors were a little different. Take a look: fit7.7$prior ## prior class coef group resp dpar nlpar lb ub source ## (flat) b default ## (flat) b factortime0 (vectorized) ## (flat) b factortime1 (vectorized) ## (flat) b factortime2 (vectorized) ## (flat) b factortime3 (vectorized) ## (flat) b sigma default ## (flat) b factortime0 sigma (vectorized) ## (flat) b factortime1 sigma (vectorized) ## (flat) b factortime2 sigma (vectorized) ## (flat) b factortime3 sigma (vectorized) ## lkj_corr_cholesky(1) Lcortime default fit7.8$prior ## prior class coef group resp dpar nlpar lb ub source ## student_t(3, 166, 35.6) Intercept t0 default ## student_t(3, 196, 43) Intercept t1 default ## student_t(3, 215, 32.6) Intercept t2 default ## student_t(3, 246, 47.4) Intercept t3 default ## lkj_corr_cholesky(1) Lrescor default ## student_t(3, 0, 35.6) sigma t0 0 default ## student_t(3, 0, 43) sigma t1 0 default ## student_t(3, 0, 32.6) sigma t2 0 default ## student_t(3, 0, 47.4) sigma t3 0 default I’m not going to break down exactly why brms made different default prior choices for these models, but experienced brms users should find these results very predictable, particularly due to our use of the 0 + factor(time) syntax in the fit7.7 model. Know your software defaults, friends. So let’s try fitting these two models one more time, but with a couple adjustments. First, we will explicitly set the priors based on the defaults used by fit7.8. We will manually request the identity link for the sigma_ parameters in fit7.7. We will also increase the post-warmup draws for both models, to help account for MCMC sampling error. Now fit the models. # unstructures fit7.7b &lt;- brm(data = opposites_pp, family = brmsfamily(family = &quot;gaussian&quot;, link = &quot;identity&quot;, link_sigma = &quot;identity&quot;), bf(opp ~ 0 + factor(time) + unstr(time = time, gr = id), sigma ~ 0 + factor(time)), prior = c(prior(student_t(3, 166, 35.6), class = b, coef = factortime0), prior(student_t(3, 196, 43.0), class = b, coef = factortime1), prior(student_t(3, 215, 32.6), class = b, coef = factortime2), prior(student_t(3, 246, 47.4), class = b, coef = factortime3), prior(student_t(3, 0, 35.6), class = b, coef = factortime0, dpar = sigma), prior(student_t(3, 0, 43.0), class = b, coef = factortime1, dpar = sigma), prior(student_t(3, 0, 32.6), class = b, coef = factortime2, dpar = sigma), prior(student_t(3, 0, 47.4), class = b, coef = factortime3, dpar = sigma), prior(lkj(1), class = cortime)), iter = 13500, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.07b&quot;) # multivariate fit7.8b &lt;- brm(data = opposites_wide, family = gaussian, bf(mvbind(t0, t1, t2, t3) ~ 1) + set_rescor(TRUE), prior = c(prior(student_t(3, 166, 35.6), class = Intercept, resp = t0), prior(student_t(3, 196, 43.0), class = Intercept, resp = t1), prior(student_t(3, 215, 32.6), class = Intercept, resp = t2), prior(student_t(3, 246, 47.4), class = Intercept, resp = t3), prior(student_t(3, 0, 35.6), class = sigma, resp = t0), prior(student_t(3, 0, 43.0), class = sigma, resp = t1), prior(student_t(3, 0, 32.6), class = sigma, resp = t2), prior(student_t(3, 0, 47.4), class = sigma, resp = t3), prior(lkj(1), class = rescor)), iter = 13500, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/fit07.08b&quot;) This time, let’s just compare the \\(\\mu\\) and \\(\\sigma\\) posteriors in a coefficient plot. # combine bind_rows( as_draws_df(fit7.7b) %&gt;% select(contains(&quot;factortime&quot;)) %&gt;% set_names(c(str_c(&quot;mu_&quot;, 0:3), str_c(&quot;sigma_&quot;, 0:3))), as_draws_df(fit7.8b) %&gt;% select(starts_with(&quot;b_&quot;), starts_with(&quot;sigma_&quot;)) %&gt;% set_names(c(str_c(&quot;mu_&quot;, 0:3), str_c(&quot;sigma_&quot;, 0:3))) ) %&gt;% # wrangle mutate(fit = rep(c(&quot;fit7.7b&quot;, &quot;fit7.8b&quot;), each = n() / 2)) %&gt;% pivot_longer(-fit) %&gt;% separate(name, into = c(&quot;par&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% group_by(fit, par, time) %&gt;% mean_qi(value) %&gt;% # plot! ggplot(aes(x = value, xmin = .lower, xmax = .upper, y = time, group = fit, color = fit)) + geom_pointrange(position = position_dodge(width = -0.5)) + scale_color_viridis_d(NULL, end = .5) + xlab(&quot;posterior&quot;) + facet_wrap(~ par, labeller = label_parsed, scales = &quot;free_x&quot;) + theme(panel.grid = element_blank()) Now the results between the two methods are very similar. If you wanted to, you could make a similar kind of plot for the two versions of the correlation matrix, too. Thus, the various models Singer and Willett introduced in this chapter are special multivariate alternatives to the standard multilevel approach we’ve preferred up to this point. Cool, huh? Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.4 brms_2.19.0 Rcpp_1.0.10 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 ## [7] dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 ggplot2_3.4.2 ## [13] tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] tensorA_0.36.2 rstudioapi_0.14 jsonlite_1.8.4 magrittr_2.0.3 TH.data_1.1-2 ## [6] estimability_1.4.1 farver_2.1.1 nloptr_2.0.3 rmarkdown_2.21 vctrs_0.6.2 ## [11] minqa_1.2.5 base64enc_0.1-3 htmltools_0.5.5 distributional_0.3.2 sass_0.4.6 ## [16] StanHeaders_2.26.25 bslib_0.4.2 htmlwidgets_1.6.2 plyr_1.8.8 sandwich_3.0-2 ## [21] emmeans_1.8.6 zoo_1.8-12 cachem_1.0.8 igraph_1.4.2 mime_0.12 ## [26] lifecycle_1.0.3 pkgconfig_2.0.3 colourpicker_1.2.0 Matrix_1.5-4 R6_2.5.1 ## [31] fastmap_1.1.1 shiny_1.7.4 digest_0.6.31 colorspace_2.1-0 ps_1.7.5 ## [36] crosstalk_1.2.0 projpred_2.5.0 labeling_0.4.2 fansi_1.0.4 timechange_0.2.0 ## [41] abind_1.4-5 mgcv_1.8-42 compiler_4.3.0 bit64_4.0.5 withr_2.5.0 ## [46] backports_1.4.1 inline_0.3.19 shinystan_2.6.0 gamm4_0.2-6 pkgbuild_1.4.0 ## [51] highr_0.10 MASS_7.3-58.4 gtools_3.9.4 loo_2.6.0 tools_4.3.0 ## [56] httpuv_1.6.11 threejs_0.3.3 glue_1.6.2 callr_3.7.3 nlme_3.1-162 ## [61] promises_1.2.0.1 grid_4.3.0 checkmate_2.2.0 reshape2_1.4.4 generics_0.1.3 ## [66] diffobj_0.3.5 gtable_0.3.3 tzdb_0.4.0 hms_1.1.3 utf8_1.2.3 ## [71] ggdist_3.3.0 pillar_1.9.0 markdown_1.7 vroom_1.6.3 posterior_1.4.1 ## [76] later_1.3.1 splines_4.3.0 lattice_0.21-8 survival_3.5-5 bit_4.0.5 ## [81] tidyselect_1.2.0 miniUI_0.1.1.1 knitr_1.42 arrayhelpers_1.1-0 gridExtra_2.3 ## [86] bookdown_0.34 stats4_4.3.0 xfun_0.39 bridgesampling_1.1-2 matrixStats_0.63.0 ## [91] DT_0.27 rstan_2.21.8 stringi_1.7.12 boot_1.3-28.1 evaluate_0.21 ## [96] codetools_0.2-19 cli_3.6.1 RcppParallel_5.1.7 shinythemes_1.2.0 xtable_1.8-4 ## [101] munsell_0.5.0 processx_3.8.1 jquerylib_0.1.4 coda_0.19-4 svUnit_1.0.6 ## [106] parallel_4.3.0 rstantools_2.3.1 ellipsis_0.3.2 prettyunits_1.1.1 dygraphs_1.1.1.6 ## [111] bayesplot_1.10.0 Brobdingnag_1.2-9 lme4_1.1-33 viridisLite_0.4.2 mvtnorm_1.1-3 ## [116] scales_1.2.1 xts_0.13.1 crayon_1.5.2 rlang_1.1.1 multcomp_1.4-23 ## [121] shinyjs_2.1.0 References Bürkner, P.-C. (2022a). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Mallinckrod, C. H., Lane, P. W., Schnell, D., Peng, Y., &amp; Mancuso, J. P. (2008). Recommendations for the primary analysis of continuous endpoints in longitudinal clinical trials. Drug Information Journal, 42(4), 303–319. https://doi.org/10.1177/009286150804200402 McNeish, D., Stapleton, L. M., &amp; Silverman, R. D. (2017). On the unnecessary ubiquity of hierarchical linear modeling. Psychological Methods, 22(1), 114. https://doi.org/10.1037/met0000078 Pinheiro, J., Bates, D., &amp; R-core. (2021). nlme: Linear and nonlinear mixed effects models [Manual]. https://CRAN.R-project.org/package=nlme Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Willett, J. B. (1988). Chapter 9: Questions and answers in the measurement of change. Review of Research in Education, 15, 345–422. https://doi.org/10.2307/1167368 "],["modeling-change-using-covariance-structure-analysis.html", "8 Modeling Change Using Covariance Structure Analysis Session info", " 8 Modeling Change Using Covariance Structure Analysis To be fleshed out later Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] digest_0.6.31 R6_2.5.1 bookdown_0.34 fastmap_1.1.1 ## [5] xfun_0.39 cachem_1.0.8 knitr_1.42 htmltools_0.5.5 ## [9] rmarkdown_2.21 cli_3.6.1 sass_0.4.6 jquerylib_0.1.4 ## [13] compiler_4.3.0 rstudioapi_0.14 tools_4.3.0 evaluate_0.21 ## [17] bslib_0.4.2 rlang_1.1.1 jsonlite_1.8.4 "],["a-framework-for-investigating-event-occurrence.html", "9 A Framework for Investigating Event Occurrence 9.1 Should you conduct a survival analysis? The “whether” and “when” test 9.2 Framing a research question about event occurrence 9.3 Censoring: How complete are the data on event occurrence? Session info", " 9 A Framework for Investigating Event Occurrence Researchers who want to study event occurrence must learn how to think about their data in new and unfamiliar ways. Even traditional methods for data description–the use of means and standard deviations–fail to serve researchers well. In this chapter we introduce the essential features of event occurrence data, explaining how and why they create the need for new analytic methods. (Singer &amp; Willett, 2003, pp 305–306) 9.1 Should you conduct a survival analysis? The “whether” and “when” test To determine whether a research question calls for survival analysis, we find it helpful to apply a simple mnemonic we refer to as “the whether and when test.” If your research questions include either word–whether or when–you probably need to use survival methods. (p. 306, emphasis added) 9.1.1 Time to relapse among recently treated alcoholics. Within the addictive-behaviors literature, researchers often study if and when participants relapse (i.e., begin using the substance(s) again). 9.1.2 Length of stay in teaching. Education researchers can use survival analysis to study whether and for how long newly-hired teachers stay in their positions. 9.1.3 Age at first suicide ideation. Suicide is a major health risk and clinical researchers sometimes use survival analysis to whether and when participants have first considered killing themselves. 9.2 Framing a research question about event occurrence Survival analyses share three common characteristics. Each has a clearly defined: Target event, whose occurrence is being studies Beginning of time, an initial starting point when no one under study has yet experienced the target event Metrics for clocking time, a meaningful scale in which event occurrence is recorded (p. 310, emphasis in the original) 9.2.1 Defining event occurrence. “Event occurrence represents an individual’s transition from one ‘state’ to another ‘state’” (p. 310). Though our primary focus will be on binary states (e.g., drinking/abstinent), survival analyses can handle more categories (e.g., whether/when marriages end in divorce or death). 9.2.2 Identifying the “beginning of time.” The “beginning of time” is a moment when everyone in the population occupies one, and only one, of the possible states… Over time, as individuals move from the original state to the next, they experience the target event. The timing of this transition–the distance from the “beginning of time” until the event occurrence–is referred to as the event time. To identify the “beginning of time” in a given study, imagine placing everyone in the population on a time-line, an axis with the “beginning of time” at one end and the last moment when event occurrence could be observed at the other. The goal is to “start the clock” when on one in the population has yet experienced the event but everyone is at least (theoretically) eligible to do so. In the language of survival analysis, you want to start the clock when everyone in the population is at risk of experiencing the event. (pp. 311–312, emphasis in the original) 9.2.3 Specifying a metric for time. We distinguish between data recorded in thin precise units and those recorded in thicker intervals by calling the former continuous time and the latter discrete time. [Though survival methods can handle both discrete and continuous time,] time should be recorded in the smallest possible units relevant to the process under study. No single metric is universally appropriate, and even different studies of the identical event might use different scales. (p. 313, emphasis in the original) 9.3 Censoring: How complete are the data on event occurrence? No matter when data collection begins, and no matter how long it lasts, some sample members are likely to have unknown event times. Statisticians call this problem censoring and they label the people with the unknown event times censored observations. Because censoring is inevitable–and a fundamental conundrum in the study of event occurrence–we now explore it in detail. (p. 316, emphasis in the original) 9.3.1 How and why does censoring arise? Censoring occurs whenever a researcher does not know an individual’s event time. There are two major reasons for censoring: (1) some individuals will never experience the target event; and (2) others will experience the event, but not during the study’s data collection. Some of these latter individuals will experience the event shortly after data collection ends while others will do so at a much later time. As a practical matter, though, these distinctions matter little because you cannot distinguish among them. That, unfortunately, is the nature of censoring: it prevents you from knowing the very quantity of interest–whether and, if so, when the target event occurs for a subset of the sample. (pp. 316–317, emphasis in the original) 9.3.2 Different types of censoring. “Methodologists make two major types of distinctions: first, between non-informative and informative censoring mechanisms, and second, between right- and left-censoring” (p. 318, emphasis in the original). 9.3.2.1 Noninformative versus informative censoring. A noninformative censoring mechanism operates independent of event occurrence and the risk of event occurrence. If censoring is under an investigator’s control, determined in advance by design–as it usually is–then it is noninformative… [Under this mechanism] we can therefore assume that all individuals who remain in the study after the censoring date are representative of everyone who would have remained in the study had censoring not occurred. If censoring occurs because individuals have experienced the event or are likely to do so in the future, the censoring mechanism is informative… Under these circumstances, we can no longer assume that those people who remain in the study after this tie are representative of all individuals who would have remained in the study had censoring not occurred. The noncensored individuals differ systematically from the censored individuals. (pp. 318–319, emphasis in the original) 9.3.2.2 Right- versus left-censoring. Right-censoring arises when an event time is unknown because event occurrence is not observed. Left-censoring arises when an event time is unknown because the beginning of time is not observed…. Because [right-censoring] is the one typically encountered in practice, and because it is the type for which survival methods were developed, references to censoring, unencumbered by a directional modifier, usually refer to right-censoring. How to left-censored observations arise? Often they arise because researchers have not paid sufficient attention to identifying the beginning of time during the design phase. If the beginning of time is defined well–as that moment when all individuals in the population are eligible to experience the event but none have yet done so–left-censoring can be eliminated…. Left-censoring presents challenges not easily addressed even with the most sophisticated of survival methods (Hu &amp; Lawless, 1996). Little progress has been made in this area since Turnbull Turnbull (1976) offered some basic descriptive approaches and Flinn and Heckman (1982) and Cox and Oakes (1984) offered some directions for fitting models under a restrictive set of assumptions. The most common advice, followed by Fichman, is to set the left-censored spells aside from analysis…. Redefining the beginning of time to coincide with a precipitating event… is often the best way of resolving the otherwise intractable problems that left-censored data pose. Whenever possible, we suggest that researchers consider such a redefinition or otherwise eliminate left-censored data through design. (pp. 319–320, emphasis in the original) 9.3.3 How does censoring affect statistical analysis? Here we load the teachers.csv data Singer (1992). library(tidyverse) teachers &lt;- read_csv(&quot;data/teachers.csv&quot;) glimpse(teachers) ## Rows: 3,941 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, … ## $ t &lt;dbl&gt; 1, 2, 1, 1, 12, 1, 12, 1, 2, 2, 7, 12, 1, 12, 12, 2, 12, 1, 3, 2, 12, 12, 9, 12, 2,… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,… Make a version of Figure 9.1. teachers %&gt;% count(censor, t) %&gt;% mutate(censor = if_else(censor == &quot;0&quot;, &quot;not censored&quot;, &quot;censored&quot;)) %&gt;% ggplot(aes(x = t)) + geom_col(aes(y = n)) + geom_text(aes(y = n + 25, label = n)) + scale_x_continuous(&quot;years&quot;, breaks = 1:12) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~censor, nrow = 2) Here’s a descriptive breakdown of those censored or not. teachers %&gt;% group_by(censor) %&gt;% summarise(n = n(), mean = mean(t), sd = sd(t)) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 × 5 ## censor n mean sd percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2207 3.73 2.41 56.0 ## 2 1 1734 9.60 1.78 44.0 Whereas the distribution of the censored occasions is flattish with a bit of a spike at 12, the distribution of the non-censored times has a bit of an exponential look to it. Recall that the exponential distribution is controlled by a single parameter, its rate, and the mean of the exponential distribution is the reciprocal of that rate. If we take the empirical mean and \\(n\\) of the non-censored data and plot those in to the rexp() function, we can simulate exponential data and plot. set.seed(9) tibble(years = rexp(n = 2207, rate = 1 / 3.7)) %&gt;% ggplot(aes(x = years)) + geom_histogram(binwidth = 1, boundary = 0) + scale_x_continuous(breaks = 1:12) + coord_cartesian(xlim = c(0, 12)) + theme(panel.grid = element_blank()) That simulation looks pretty similar to our non-censored data. If we stopped there, we might naïvely presume \\(\\operatorname{Exponential}(1/3.7)\\) is a good model for our data. But this would ignore the censored data. One of the solutions researchers have used is to assign the censored cases the event time they possess at the end of the data collection (e.g., Frank &amp; Keith, 1984). Applying this to our teacher career data (e.g., assigning a career length of 7 years to the 280 teachers censored in the year 7, etc.) yields an estimated mean career duration of 7.5 years. (pp. 322–323) Here’s what that looks like. teachers %&gt;% summarise(mean = mean(t), median = median(t), sd = sd(t)) ## # A tibble: 1 × 3 ## mean median sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6.31 7 3.63 I have no idea where the 7.5 value Singer and Willett presented came from. It’s larger than both the mean and the median in the data. But anyway, this method is patently wrong, so it doesn’t matter: Imputing event times for censored cases simply changes all “nonevents” into “events” and further assumes that all these new “events” occur at the earliest time possible–that is, at the moment of censoring. Surely these decisions are most likely wrong. (p. 323) Stay tuned for methods that are better than patently wrong. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 ## [7] tidyr_1.3.0 tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] sass_0.4.6 utf8_1.2.3 generics_0.1.3 stringi_1.7.12 hms_1.1.3 ## [6] digest_0.6.31 magrittr_2.0.3 evaluate_0.21 grid_4.3.0 timechange_0.2.0 ## [11] bookdown_0.34 fastmap_1.1.1 jsonlite_1.8.4 fansi_1.0.4 scales_1.2.1 ## [16] jquerylib_0.1.4 cli_3.6.1 rlang_1.1.1 crayon_1.5.2 bit64_4.0.5 ## [21] munsell_0.5.0 withr_2.5.0 cachem_1.0.8 tools_4.3.0 parallel_4.3.0 ## [26] tzdb_0.4.0 colorspace_2.1-0 vctrs_0.6.2 R6_2.5.1 lifecycle_1.0.3 ## [31] bit_4.0.5 vroom_1.6.3 pkgconfig_2.0.3 pillar_1.9.0 bslib_0.4.2 ## [36] gtable_0.3.3 glue_1.6.2 xfun_0.39 tidyselect_1.2.0 highr_0.10 ## [41] rstudioapi_0.14 knitr_1.42 farver_2.1.1 htmltools_0.5.5 labeling_0.4.2 ## [46] rmarkdown_2.21 compiler_4.3.0 References Cox, David Roxbee, &amp; Oakes, D. (1984). Analysis of survival data (Vol. 21). CRC Press. https://www.routledge.com/Analysis-of-Survival-Data/Cox-Oakes/p/book/9780412244902 Flinn, C. J., &amp; Heckman, J. J. (1982). New methods for analyzing individual event histories. Sociological Methodology, 13, 99–140. https://doi.org/10.2307/270719 Frank, A. R., &amp; Keith, T. Z. (1984). Academic abilities of persons entering and remaining in special education. Exceptional Children, 51(1), 76–77. https://eric.ed.gov/?id=EJ306852 Hu, X. J., &amp; Lawless, J. F. (1996). Estimation from truncated lifetime data with supplementary information on covariates and censoring times. Biometrika, 83(4), 747–761. https://doi.org/10.1093/biomet/83.4.747 Singer, J. D. (1992). Are special educators’ career paths special? Results from a 13-year longitudinal study. Exceptional Children, 59(3), 262–279. https://doi.org/10.1177/001440299305900309 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Turnbull, B. W. (1976). The empirical distribution function with arbitrarily grouped, censored and truncated data. Journal of the Royal Statistical Society: Series B (Methodological), 38(3), 290–295. https://doi.org/10.1111/j.2517-6161.1976.tb01597.x "],["describing-discrete-time-event-occurrence-data.html", "10 Describing Discrete-Time Event Occurrence Data 10.1 The life table 10.2 A framework for characterizing the distribution of discrete-time event occurrence data 10.3 Developing intuition about hazard functions, survivor functions, and median lifetimes 10.4 Quantifying the effects of sampling variation 10.5 A simple and useful strategy for constructing the life table 10.6 Bonus: Fit the discrete-time hazard models with brms Session info", " 10 Describing Discrete-Time Event Occurrence Data In this chapter, [Singer and Willett presented] a framework for describing discrete-time event occurrence data…. As we will [see], the conceptual linchpin for all subsequent survival methods is to approach the analysis on a period-by-period basis. This allows you to examine event occurrence sequentially among those individuals eligible to experience the event at each discrete point in time. (Singer &amp; Willett, 2003, p. 325) 10.1 The life table The fundamental tool for summarizing the sample distribution of event occurrence is the life table. As befits its name, a life table tracks the event histories (the “lives”) of a sample of individuals from the beginning of time (when no one has yet experienced the target event) through the end of data collection. (p. 326, emphasis in the original) To make a life table as presented in Table 10.1, we need to load the teachers.csv data. library(tidyverse) teachers &lt;- read_csv(&quot;data/teachers.csv&quot;) glimpse(teachers) ## Rows: 3,941 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, … ## $ t &lt;dbl&gt; 1, 2, 1, 1, 12, 1, 12, 1, 2, 2, 7, 12, 1, 12, 12, 2, 12, 1, 3, 2, 12, 12, 9, 12, 2,… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,… Perhaps the easiest way to make a life table as presented in Table 10.1 is with help from the survival package (Terry M. Therneau, 2021c; Terry M. Therneau &amp; Grambsch, 2000). # install.packages(&quot;survival&quot;, dependencies = T) library(survival) Here we’ll use the survfit() function to compute survival curves. Within the survfit() function, we’ll use the Surv() function to make a survival object, which will become the criterion within the model formula. It takes two basic arguments, time and event. With the teachers data, t is time in years. In the data, events are encoded in censor. However, it’s important to understand how the event argument expects the data. From the survival reference manual (Terry M. Therneau, 2021b), we read that event is “the status indicator, normally 0=alive, 1=dead. Other choices are TRUE/FALSE (TRUE = death) or 1/2 (2=death).” Note that whereas within our data censor is coded 0 = event 1 = censored, the event argument expects the opposite. A quick way to solve that is to enter 1 - censor. fit10.1 &lt;- survfit(data = teachers, Surv(t, 1 - censor) ~ 1) Use the str() function to survey the results. fit10.1 %&gt;% str() ## List of 16 ## $ n : int 3941 ## $ time : num [1:12] 1 2 3 4 5 6 7 8 9 10 ... ## $ n.risk : num [1:12] 3941 3485 3101 2742 2447 ... ## $ n.event : num [1:12] 456 384 359 295 218 184 123 79 53 35 ... ## $ n.censor : num [1:12] 0 0 0 0 0 0 280 307 255 265 ... ## $ surv : num [1:12] 0.884 0.787 0.696 0.621 0.566 ... ## $ std.err : num [1:12] 0.00576 0.00829 0.01053 0.01245 0.01396 ... ## $ cumhaz : num [1:12] 0.116 0.226 0.342 0.449 0.538 ... ## $ std.chaz : num [1:12] 0.00542 0.00781 0.00992 0.01173 0.01319 ... ## $ type : chr &quot;right&quot; ## $ logse : logi TRUE ## $ conf.int : num 0.95 ## $ conf.type: chr &quot;log&quot; ## $ lower : num [1:12] 0.874 0.774 0.682 0.606 0.55 ... ## $ upper : num [1:12] 0.894 0.8 0.71 0.636 0.581 ... ## $ call : language survfit(formula = Surv(t, 1 - censor) ~ 1, data = teachers) ## - attr(*, &quot;class&quot;)= chr &quot;survfit&quot; We can retrieve the values for the “Year” column from fit10.1$time. The values in the “Time interval” column are a simple transformation from there. fit10.1$time ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 We can find the values in the “Employed at the beginning of the year” column in fit10.1$n.risk and those in the “Who left during the year” column in fit10.1$n.event. fit10.1$n.risk ## [1] 3941 3485 3101 2742 2447 2229 2045 1642 1256 948 648 391 fit10.1$n.event ## [1] 456 384 359 295 218 184 123 79 53 35 16 5 We’ll have to work a little harder to compute the values in the “Censored at the end of the year column.” Here we’ll walk it through in a data frame format. data.frame(n_risk = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(n_risk_1 = lead(n_risk, default = 0)) %&gt;% mutate(n_censored = n_risk - n_event - n_risk_1) ## n_risk n_event n_risk_1 n_censored ## 1 3941 456 3485 0 ## 2 3485 384 3101 0 ## 3 3101 359 2742 0 ## 4 2742 295 2447 0 ## 5 2447 218 2229 0 ## 6 2229 184 2045 0 ## 7 2045 123 1642 280 ## 8 1642 79 1256 307 ## 9 1256 53 948 255 ## 10 948 35 648 265 ## 11 648 16 391 241 ## 12 391 5 0 386 That is, to get the number of those censored at the end of a given year, you take the number employed at the beginning of that year, subtract the number of those who left (i.e., the number who experienced the “event”), and then subtract the number of those employed at the beginning of the next year. Notice our use of the dplyr::lead() function to get the number employed in the next year (learn more about that function here). To get the values in the “Teachers at the beginning of the year who left during the year” column, which is in a proportion metric, we use division. fit10.1$n.event / fit10.1$n.risk ## [1] 0.11570667 0.11018651 0.11576911 0.10758570 0.08908868 0.08254823 0.06014670 0.04811206 ## [9] 0.04219745 0.03691983 0.02469136 0.01278772 Finally, to pull the values in the “All teachers still employed at the end of the year” column, we just execute fit10.1$surv. fit10.1$surv ## [1] 0.8842933 0.7868561 0.6957625 0.6209084 0.5655925 0.5189038 0.4876935 0.4642295 0.4446402 ## [10] 0.4282242 0.4176508 0.4123100 Let’s put that all together in a tibble. most_rows &lt;- tibble(year = fit10.1$time) %&gt;% mutate(time_int = str_c(&quot;[&quot;, year, &quot;, &quot;, year + 1, &quot;)&quot;), n_employed = fit10.1$n.risk, n_left = fit10.1$n.event) %&gt;% mutate(n_censored = n_employed - n_left - lead(n_employed, default = 0), hazard_fun = n_left / n_employed, survivor_fun = fit10.1$surv) most_rows ## # A tibble: 12 × 7 ## year time_int n_employed n_left n_censored hazard_fun survivor_fun ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 [1, 2) 3941 456 0 0.116 0.884 ## 2 2 [2, 3) 3485 384 0 0.110 0.787 ## 3 3 [3, 4) 3101 359 0 0.116 0.696 ## 4 4 [4, 5) 2742 295 0 0.108 0.621 ## 5 5 [5, 6) 2447 218 0 0.0891 0.566 ## 6 6 [6, 7) 2229 184 0 0.0825 0.519 ## 7 7 [7, 8) 2045 123 280 0.0601 0.488 ## 8 8 [8, 9) 1642 79 307 0.0481 0.464 ## 9 9 [9, 10) 1256 53 255 0.0422 0.445 ## 10 10 [10, 11) 948 35 265 0.0369 0.428 ## 11 11 [11, 12) 648 16 241 0.0247 0.418 ## 12 12 [12, 13) 391 5 386 0.0128 0.412 The only thing missing from our version of Table 10.1 is we don’t have a row for Year 0. Here’s a quick and dirty way to manually insert those values. row_1 &lt;- tibble(year = 0, time_int = &quot;[0, 1)&quot;, n_employed = fit10.1$n.risk[1], n_left = NA, n_censored = NA, hazard_fun = NA, survivor_fun = 1) d &lt;- bind_rows(row_1, most_rows) d ## # A tibble: 13 × 7 ## year time_int n_employed n_left n_censored hazard_fun survivor_fun ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 [0, 1) 3941 NA NA NA 1 ## 2 1 [1, 2) 3941 456 0 0.116 0.884 ## 3 2 [2, 3) 3485 384 0 0.110 0.787 ## 4 3 [3, 4) 3101 359 0 0.116 0.696 ## 5 4 [4, 5) 2742 295 0 0.108 0.621 ## 6 5 [5, 6) 2447 218 0 0.0891 0.566 ## 7 6 [6, 7) 2229 184 0 0.0825 0.519 ## 8 7 [7, 8) 2045 123 280 0.0601 0.488 ## 9 8 [8, 9) 1642 79 307 0.0481 0.464 ## 10 9 [9, 10) 1256 53 255 0.0422 0.445 ## 11 10 [10, 11) 948 35 265 0.0369 0.428 ## 12 11 [11, 12) 648 16 241 0.0247 0.418 ## 13 12 [12, 13) 391 5 386 0.0128 0.412 We might walk out the notation in our time_int column a bit. Those intervals reflect a standard partition of time, in which each interval includes the initial time and excludes the concluding time. Adopting common mathematical notation, [brackets] denote inclusions and (parentheses) denote exclusions. Thus, we bracket each interval’s initial time and place a parenthesis around its concluding time. (p. 328, emphasis in the original) The values in the n_employed column the risk set, those who are “eligible to experience the event during that interval” (p. 329, emphasis in the original). 10.2 A framework for characterizing the distribution of discrete-time event occurrence data The fundamental quantity used to assess the risk of event occurrence in each discrete time period is known as hazard. Denoted by \\(h(t_{ij})\\), discrete time hazard is the conditional probability that individual \\(i\\) will experience the event time in period \\(j\\), given that he or she did not experience it in any earlier time period. Because hazard represents the risk of the event occurrence in each discrete time period among those people eligible to experience the event (those in the risk set) hazard tells us precisely what we want to know: whether and when events occurs. (p. 330, emphasis in the original) If we let \\(T_i\\) stand for the discrete value in time person \\(i\\) experiences the event, we can express the conditional probability the event might occur in the \\(j^\\text{th}\\) interval as \\[h(t_{ij}) = \\Pr[T_i = j \\mid T \\geq j].\\] That last part, \\(T \\geq j\\), clarifies the event can only occur once and, therefore, cannot have occurred in any of the prior levels of \\(j\\). More plainly put, imagine the event is death and person \\(i\\) died during the period of \\(T_j = 20\\). In such a case, it’s nonsensical to speak of that \\(i^\\text{th}\\) person’s hazard for the period of \\(T_j = 25\\). They’re already dead. Also, “the discrete-time hazard probabilities expressed as a function of time–labeled \\(h(t_{ij})\\)–is known as the population discrete-time hazard function” (p 330, emphasis in the original). That was expressed in the 6th column in Table 10.1, which we called hazard_fun in our d tibble. d %&gt;% select(year, hazard_fun) ## # A tibble: 13 × 2 ## year hazard_fun ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 NA ## 2 1 0.116 ## 3 2 0.110 ## 4 3 0.116 ## 5 4 0.108 ## 6 5 0.0891 ## 7 6 0.0825 ## 8 7 0.0601 ## 9 8 0.0481 ## 10 9 0.0422 ## 11 10 0.0369 ## 12 11 0.0247 ## 13 12 0.0128 You might notice \\(h(t_{ij})\\) is in a proportion metric and it is not cumulative. If you look above in the code, you’ll see we computed that by hazard_fun = n_left / n_employed. More formally and generally, this is an operationalization of \\[\\hat h(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j},\\] where \\(n \\text{ events}_j\\) is the number of individuals who experienced the event in the \\(j^{th}\\) period and \\(n \\text{ at risk}_j\\) is the number within the period who have not (a) already experienced the event and (b) been censored. Also note that by \\(\\hat h(t_{j})\\), we’re indicating we’re talking about the maximum likelihood estimate for \\(h(t_{j})\\). Because no one is at risk during the initial time point, \\(h(t_0)\\) is undefined (i.e., NA). Here we mimic the top panel of Figure 10.1 and plot our \\(\\hat h(t_{j})\\) over time. d %&gt;% ggplot(aes(x = year, y = hazard_fun)) + geom_line() + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated hazard probability, &quot;*hat(italic(h))(italic(t[j]))), breaks = c(0, .05, .1, .15), limits = c(0, .15)) + theme(panel.grid = element_blank()) 10.2.1 Survivor function. The survivor function provides another way of describing the distribution of event occurrence over time. Unlike the hazard function, which assesses the unique risk associated with each time period, the survivor function cumulates these period-by-period risks of event occurrence (or more properly, nonoccurrence) together to assess the probability that a randomly selected individual will survive will not experience the event. We can formally define the survivor function, \\(S(t_{ij})\\), as \\[S(t_{ij}) = \\Pr[T &gt; j],\\] where \\(S\\) is survival as a function of time, \\(t\\). But since our data are finite, we can only have an estimate of the “true” survivor function, which we call \\(\\hat S(t_{ij})\\). Here it is in a plot, our version of the bottom panel of Figure 10.1. d %&gt;% ggplot(aes(x = year, y = survivor_fun)) + geom_hline(yintercept = .5, color = &quot;white&quot;, linetype = 2) + geom_line() + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated survival probability, &quot;*hat(italic(S))(italic(t[j]))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) 10.2.2 Median lifetime. Having characterized the distribution of event times using the hazard and survivor functions, we often want to identify the distribution’s center. Were there no censoring, all event times would be known, and we could compute a sample mean. But because of censoring, another estimate of central tendency is preferred: the median lifetime. The estimated median lifetime identifies that value for \\(T\\) for which the value of the estimated survivor function is .5. It is the point in time by which we estimate that half of the sample has experienced the target event, half has not. (p. 337, emphasis in the original) If we use filter(), well see our median lifetime rests between years 6 and 7. d %&gt;% filter(year %in% c(6, 7)) %&gt;% # this just simplifies the output select(year, time_int, survivor_fun) ## # A tibble: 2 × 3 ## year time_int survivor_fun ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6 [6, 7) 0.519 ## 2 7 [7, 8) 0.488 Using a simple descriptive approach, we’d just say the median lifetime was between years 6 and 7. We could also follow Miller (1981) and linearly interpolate between the two values of \\(S(t_j)\\) bracketing .5. If we let \\(m\\) be the time interval just before the median lifetime, \\(\\hat S(t_m)\\) be the value of the survivor function in that \\(m^\\text{th}\\) interval, and \\(\\hat S(t_{m + 1})\\) be the survival value in the next interval, the can write \\[\\text{Estimated median lifetime} = m + \\Bigg [\\frac{\\hat S(t_m) - .5}{\\hat S(t_m) - \\hat S(t_{m + 1})} \\Bigg ] \\big ((m + 1) - m \\big).\\] We can compute that by hand like so. m &lt;- 6 m_plus_1 &lt;- 7 stm &lt;- d %&gt;% filter(year == m) %&gt;% pull(survivor_fun) stm_plus_1 &lt;- d %&gt;% filter(year == m_plus_1) %&gt;% pull(survivor_fun) # compute the interpolated median lifetime and save it as `iml` iml &lt;- m + ((stm - .5) / (stm - stm_plus_1)) * ((m + 1) - m) iml ## [1] 6.605691 Now we have the iml value, we can add that information to our version of the lower panel of Figure 10.1. line &lt;- tibble(year = c(0, iml, iml), survivor_fun = c(.5, .5, 0)) d %&gt;% ggplot(aes(x = year, y = survivor_fun)) + geom_path(data = line, color = &quot;white&quot;, linetype = 2) + geom_line() + annotate(geom = &quot;text&quot;, x = iml, y = .55, label = &quot;All teachers (6.6 years)&quot;, hjust = 0) + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated survival probability, &quot;*hat(italic(S))(italic(t[j]))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) We can compute the estimates for the 5- and 10-year survival rates as a direct algebraic transformation of the survival function from those years. d %&gt;% filter(year %in% c(5, 10)) %&gt;% select(year, survivor_fun) %&gt;% mutate(`survival rate (%)` = (100 * survivor_fun) %&gt;% round(digits = 0)) ## # A tibble: 2 × 3 ## year survivor_fun `survival rate (%)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0.566 57 ## 2 10 0.428 43 10.3 Developing intuition about hazard functions, survivor functions, and median lifetimes Developing intuition about these sample statistics requires exposure to estimates computed from a wide range of studies. To jump-start this process, we review results from four studies that differ across three salient dimensions–the type of event investigated, the metric used to record discrete time, and most important, the underlying profile of risk–and discuss how we would examine, and describe, the estimated hazard functions, survivor functions, and median lifetimes. (p. 339) Here we load the four relevant data sets. cocaine &lt;- read_csv(&quot;data/cocaine_relapse.csv&quot;) sex &lt;- read_csv(&quot;data/firstsex.csv&quot;) suicide &lt;- read_csv(&quot;data/suicide.csv&quot;) congress &lt;- read_csv(&quot;data/congress.csv&quot;) # glimpse(cocaine) # glimpse(sex) # glimpse(suicide) # glimpse(congress) We have a lot of leg work in front of use before we can recreate Figure 10.2. First, we’ll feed each of the four data sets into the survfit() function. fit10.2 &lt;- survfit(data = cocaine, Surv(week, 1 - censor) ~ 1) fit10.3 &lt;- survfit(data = sex, Surv(time, 1 - censor) ~ 1) fit10.4 &lt;- survfit(data = suicide, Surv(time, 1 - censor) ~ 1) fit10.5 &lt;- survfit(data = congress, Surv(time, 1 - censor) ~ 1) Given the four fits all follow the same basic form and given our end point is to make the same basic plots for each, we can substantially streamline our code by making a series of custom functions. For our first custom function, make_lt(), we’ll save the general steps for making life tables for each data set. make_lt &lt;- function(fit) { # arrange the lt data for all rows but the first most_rows &lt;- tibble(time = fit$time) %&gt;% mutate(time_int = str_c(&quot;[&quot;, time, &quot;, &quot;, time + 1, &quot;)&quot;), n_risk = fit$n.risk, n_event = fit$n.event) %&gt;% mutate(hazard_fun = n_event / n_risk, survivor_fun = fit$surv) # define the values for t = 2 and t = 1 time_1 &lt;- fit$time[1] time_0 &lt;- time_1 - 1 # define the values for the row for which t = 1 row_1 &lt;- tibble(time = time_0, time_int = str_c(&quot;[&quot;, time_0, &quot;, &quot;, time_1, &quot;)&quot;), n_risk = fit$n.risk[1], n_event = NA, hazard_fun = NA, survivor_fun = 1) # make the full life table lt &lt;- bind_rows(row_1, most_rows) lt } Use make_lt() to make the four life tables. lt_cocaine &lt;- make_lt(fit10.2) lt_sex &lt;- make_lt(fit10.3) lt_suicide &lt;- make_lt(fit10.4) lt_congress &lt;- make_lt(fit10.5) You’ll note that the four survival-curve plots in Figure 10.2 all show the median lifetime using the interpolation method. Here we’ll save the necessary steps to compute that for each model as the make_iml() function. make_iml &lt;- function(lt) { # lt is a generic name for a life table of the # kind we made with our `make_lt()` function # determine the mth row lt_m &lt;- lt %&gt;% filter(survivor_fun &gt; .5) %&gt;% slice(n()) # determine the row for m + 1 lt_m1 &lt;- lt %&gt;% filter(survivor_fun &lt; .5) %&gt;% slice(1) # pull the value for m m &lt;- pull(lt_m, time) # pull the two survival function values stm &lt;- pull(lt_m, survivor_fun) stm1 &lt;- pull(lt_m1, survivor_fun) # plug the values into Equation 10.6 (page 338) iml &lt;- m + ((stm - .5) / (stm - stm1)) * ((m + 1) - m) iml } If you want, you can use make_iml() directly like this. make_iml(lt_cocaine) ## [1] 7.5 However, our approach will be to wrap it in another function, line_tbl(), with which we will save the coordinates necessary for marking off the median lifetimes and them save them in a tibble. line_tbl &lt;- function(lt) { iml &lt;- make_iml(lt) tibble(time = c(lt[1, 1] %&gt;% pull(), iml, iml), survivor_fun = c(.5, .5, 0)) } It works like this. line_tbl(lt_cocaine) ## # A tibble: 3 × 2 ## time survivor_fun ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.5 ## 2 7.5 0.5 ## 3 7.5 0 If you look closely at the hazard function plots in the left column of Figure 10.2, you’ll note they share many common settings (e.g., the basic shape, the label of the \\(y\\)-axis). But there are several parameters we’ll need to set custom settings for. To my eye, those are: the data; the \\(x\\)-axis label, break points, and limits; and the \\(y\\)-axis break points, and limits. With our custom h_plot() function, we’ll leave those parameters free while keeping all the other ggplot2 parameters the same. h_plot &lt;- function(data = data, xlab = xlab, xbreaks = xbreaks, xlimits = xlimits, ybreaks = ybreaks, ylimits = ylimits) { ggplot(data = data, mapping = aes(x = time, y = hazard_fun)) + geom_line() + scale_x_continuous(xlab, breaks = xbreaks, limits = xlimits) + scale_y_continuous(expression(widehat(italic(h(t)))), breaks = ybreaks, limits = ylimits) + theme(panel.grid = element_blank()) } Now we’ll make a similar custom plotting function, s_plot(), for the hazard function plots on the right column of Figure 10.2. s_plot &lt;- function(data = data, xlab = xlab, xbreaks = xbreaks, xlimits = xlimits) { # compute the interpolated median life value iml &lt;- make_iml(data) # make the imp line values line &lt;- data %&gt;% line_tbl() ggplot(data = data, mapping = aes(x = time, y = survivor_fun)) + geom_path(data = line, color = &quot;white&quot;, linetype = 2) + geom_line() + annotate(geom = &quot;text&quot;, x = iml, y = .6, label = str_c(&quot;widehat(ML)==&quot;, iml %&gt;% round(1)), size = 3, hjust = 0, parse = T) + scale_x_continuous(xlab, breaks = xbreaks, limits = xlimits) + scale_y_continuous(expression(widehat(italic(S(t)))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) } Now we make the eight subplots in bulk, naming them p1, p2, and so on. # cocaine p1 &lt;- lt_cocaine %&gt;% h_plot(xlab = &quot;Weeks after release&quot;, xbreaks = 0:12, xlimits = c(0, 12), ybreaks = c(0, .05, .1, .15), ylimits = c(0, .15)) p2 &lt;- lt_cocaine %&gt;% s_plot(xlab = &quot;Weeks after release&quot;, xbreaks = 0:12, xlimits = c(0, 12)) # sex p3 &lt;- lt_sex %&gt;% h_plot(xlab = &quot;Grade&quot;, xbreaks = 6:12, xlimits = c(6, 12), ybreaks = 0:3 / 10, ylimits = c(0, .325)) p4 &lt;- lt_sex %&gt;% s_plot(xlab = &quot;Grade&quot;, xbreaks = 6:12, xlimits = c(6, 12)) # suicide p5 &lt;- lt_suicide %&gt;% h_plot(xlab = &quot;Age&quot;, xbreaks = 1:9 * 2 + 3, xlimits = c(5, 22), ybreaks = c(0, .05, .1, .15), ylimits = c(0, .16)) p6 &lt;- lt_suicide %&gt;% s_plot(xlab = &quot;Age&quot;, xbreaks = 1:9 * 2 + 3, xlimits = c(5, 22)) # congress p7 &lt;- lt_congress %&gt;% h_plot(xlab = &quot;Terms in office&quot;, xbreaks = 0:8, xlimits = c(0, 8), ybreaks = 0:3 / 10, ylimits = c(0, .3)) p8 &lt;- lt_congress %&gt;% s_plot(xlab = &quot;Terms in office&quot;, xbreaks = 0:8, xlimits = c(0, 8)) Now we’ll use some functions and syntax from the patchwork package to combine the subplots and make Figure 10.2. library(patchwork) p12 &lt;- (p1 + p2) + plot_annotation(title = &quot;A&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) p34 &lt;- (p3 + p4) + plot_annotation(title = &quot;B&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) p56 &lt;- (p5 + p6) + plot_annotation(title = &quot;C&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) p78 &lt;- (p7 + p8) + plot_annotation(title = &quot;D&quot;) &amp; theme(plot.margin = margin(0, 5.5, 0, 5.5)) (wrap_elements(p12) / wrap_elements(p34) / wrap_elements(p56) / wrap_elements(p78)) Boom! Looks like a dream. 10.3.1 Identifying periods of high and low risk using hazard functions. It can be useful to evaluate hazard functions based on whether they are monotonic (i.e., have a single distinctive peak and single distinctive trough) and nonmonotonic (i.e., have multiple distinctive peaks or troughs). Globally speaking, the hazard functions for rows A and B are monotonic and the remaining two are nonmonotonic. Singer and Willett remarked “monotonically increasing hazard functions are common when studying events that are ultimately inevitable (or near universal)…. [However,] nonmonotonic hazard functions, like those in Panels C and D, generally arise in studies of long duration” (p. 342). However, when risk is constant over time, hazard functions will not have peaks or troughs. 10.3.2 Survivor functions as a context for evaluating the magnitude of hazard. Unlike with hazard functions, all survivor functions decrease or stay constant over time. They are monotonic (i.e., they never switch direction, they never increase). From the text (p. 344), we learn three ways hazard functions relate to survival functions: When hazard is high, the survivor function drops rapidly. When hazard is low, the survivor function drops slowly. When hazard is zero, the survivor function remains unchanged. 10.3.3 Strengths and limitations of estimated median lifetimes. When examining a median lifetime, we find it helpful to remember three important limitations on its interpretation. First, it identifies only an “average” event time; it tells us little about the distribution of even times and is relatively insensitive to extreme values. Second, the median lifetime is not necessarily a moment when the target event is especially likely to occur…. Third, the median lifetime reveals little about the distribution of risk over time; identical median lifetimes can result from dramatically different survivor and hazard functions. (pp. 345–346, emphasis in the original) Without access to Singer and Willett’s hypothetical data, we’re not in a good position to recreate their Figure 10.3. Even the good folks at IDRE gave up on that one. 10.4 Quantifying the effects of sampling variation We can quantify the uncertainty in the estimates with standard errors. 10.4.1 The standard error of the estimated hazard probabilities. The formula for the frequentist standard errors for the hazard probabilities follows the form \\[se \\left (\\hat h(t_j) \\right) = \\sqrt{\\frac{\\hat h(t_j) \\left (1 - \\hat h(t_j) \\right)}{n \\text{ at risk}_j}}.\\] We can express that equation R code to recreate the first four columns of Table 10.2. We’ll be pulling much of the information from fit10.1. But to show our work within a tibble format, we’ll be adding a column after \\(n_j\\). Our additional n_event column will contain the information pulled from fit10.1$n.event, which we’ll use to compute the \\(\\hat h(t_j)\\). se_h_hat &lt;- tibble(year = fit10.1$time, n_j = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(h_hat = n_event / n_j) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j)) se_h_hat ## # A tibble: 12 × 5 ## year n_j n_event h_hat se_h_hat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3941 456 0.116 0.00510 ## 2 2 3485 384 0.110 0.00530 ## 3 3 3101 359 0.116 0.00575 ## 4 4 2742 295 0.108 0.00592 ## 5 5 2447 218 0.0891 0.00576 ## 6 6 2229 184 0.0825 0.00583 ## 7 7 2045 123 0.0601 0.00526 ## 8 8 1642 79 0.0481 0.00528 ## 9 9 1256 53 0.0422 0.00567 ## 10 10 948 35 0.0369 0.00612 ## 11 11 648 16 0.0247 0.00610 ## 12 12 391 5 0.0128 0.00568 As in the text, our standard errors are pretty small. To get a better sense, here they are in a rug plot. se_h_hat %&gt;% ggplot(aes(x = se_h_hat)) + geom_rug(length = unit(0.25, &quot;in&quot;)) + scale_x_continuous(expression(italic(se)(hat(italic(h))(italic(t[j])))), limits = c(.004, .007)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) Standard errors for discrete hazards probabilities share a property with those for other probabilities: they are less certain (i.e., larger) for probability values near .5 and increasingly certain (i.e., smaller) for probability values approaching 0 and 1. To give a sense of that, here are the corresponding \\(se \\big (\\hat h(t_j) \\big)\\) for a series of \\(\\hat h(t_j)\\) values ranging from 0 to 1, with \\(n_j\\) held constant at 1,000. tibble(n_j = 1000, h_hat = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j)) %&gt;% ggplot(aes(x = h_hat, y = se_h_hat)) + geom_point() + labs(x = expression(hat(italic(h))(italic(t[j]))), y = expression(italic(se)(hat(italic(h))(italic(t[j]))))) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) Also, as the size of the risk set, \\(n_j\\), influences the standard errors in the typical way. All things equal, a larger \\(n\\) will make for a smaller \\(se\\). To give a sense, here’s the same basic plot from above, but this time with \\(n_j = 100, 1{,}000, \\text{ and } 10{,}000\\). crossing(n_j = c(100, 1000, 10000), h_hat = seq(from = 0, to = 1, by = .01)) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j), n_j = str_c(&quot;italic(n[j])==&quot;, n_j)) %&gt;% ggplot(aes(x = h_hat, y = se_h_hat)) + geom_point() + labs(x = expression(hat(italic(h))(italic(t[j]))), y = expression(italic(se)(hat(italic(h))(italic(t[j]))))) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + facet_wrap(~ n_j, nrow = 1, labeller = label_parsed) 10.4.2 Standard error of the estimated survival probabilities. Computing the frequentist standard errors for estimated survival probabilities is more difficult because these are the products of (1 - hazard) for the current and all previous survival probabilities. Computing them is such a pain, Singer and Willett recommend you rely on Greenwood’s (1926) approximation. This follows the form \\[se \\big (\\hat S(t_j) \\big) = \\hat S(t_j) \\sqrt{\\frac{\\hat h(t_1)}{n_1 \\big (1 - \\hat h(t_1) \\big)} + \\frac{\\hat h(t_2)}{n_2 \\big (1 - \\hat h(t_2) \\big)} + \\cdots + \\frac{\\hat h(t_j)}{n_j \\big (1 - \\hat h(t_j) \\big)}}.\\] Here we put the formula to work and finish our version of Table 10.2. For the sake of sanity, we’re simply calling our “Term under the square root sign” column term. Note our use of the cumsum() function. # suspend scientific notation options(scipen = 999) tibble(year = fit10.1$time, n_j = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(h_hat = n_event / n_j) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j), s_hat = fit10.1$surv, term = cumsum(h_hat / (n_j * (1 - h_hat)))) %&gt;% mutate(se_s_hat = s_hat * sqrt(term), std.err = fit10.1$std.err) ## # A tibble: 12 × 9 ## year n_j n_event h_hat se_h_hat s_hat term se_s_hat std.err ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3941 456 0.116 0.00510 0.884 0.0000332 0.00510 0.00576 ## 2 2 3485 384 0.110 0.00530 0.787 0.0000687 0.00652 0.00829 ## 3 3 3101 359 0.116 0.00575 0.696 0.000111 0.00733 0.0105 ## 4 4 2742 295 0.108 0.00592 0.621 0.000155 0.00773 0.0124 ## 5 5 2447 218 0.0891 0.00576 0.566 0.000195 0.00790 0.0140 ## 6 6 2229 184 0.0825 0.00583 0.519 0.000235 0.00796 0.0153 ## 7 7 2045 123 0.0601 0.00526 0.488 0.000267 0.00796 0.0163 ## 8 8 1642 79 0.0481 0.00528 0.464 0.000297 0.00800 0.0172 ## 9 9 1256 53 0.0422 0.00567 0.445 0.000332 0.00811 0.0182 ## 10 10 948 35 0.0369 0.00612 0.428 0.000373 0.00827 0.0193 ## 11 11 648 16 0.0247 0.00610 0.418 0.000412 0.00848 0.0203 ## 12 12 391 5 0.0128 0.00568 0.412 0.000445 0.00870 0.0211 For comparisson, we also added the \\(se \\big (\\hat S(t_j) \\big)\\) values coputed by the survivor package in the final column, std.err. tibble(year = fit10.1$time, n_j = fit10.1$n.risk, n_event = fit10.1$n.event) %&gt;% mutate(h_hat = n_event / n_j) %&gt;% mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j), s_hat = fit10.1$surv, term = cumsum(h_hat / (n_j * (1 - h_hat)))) %&gt;% mutate(Greenwood = s_hat * sqrt(term), `survival package` = fit10.1$std.err) %&gt;% pivot_longer(Greenwood:`survival package`) %&gt;% mutate(name = factor(name, levels = c(&quot;survival package&quot;, &quot;Greenwood&quot;))) %&gt;% ggplot(aes(x = year, y = value, color = name)) + geom_point(size = 4) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .55) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(expression(italic(se)), limits = c(0, 0.025)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.125, .9), panel.grid = element_blank()) It’s out of my expertise to comment on which should we should trust more. But Singer and Willett did note that “as an approximation, Greenwood’s formula is accurate only asymptotically” (p. 351). # turn scientific notation back on (R default) options(scipen = 0) 10.5 A simple and useful strategy for constructing the life table How can you construct a life table for your data set? For preliminary analyses, it is easy to use the prepackaged routines available in the major statistical packages. If you choose this approach, be sure to check whether your package allows you to: (1) select the partition of time; and (2) ignore any actuarial corrections invoked due to continuous-time assumptions (that do not hold in discrete time). When event times have been measured using a discrete-time scale, actuarial corrections (discussed in chapter13) are inappropriate. Although most packages clearly document the algorithm being used, we suggest that you double-check by comparing results with one or two estimates computed by hand. (p. 351, emphasis in the original) For more information about the methods we’ve been using via the survival package, browse through the documentation listed on the CRAN page, https://CRAN.R-project.org/package=survival, with a particular emphasis on the reference manual and A package for survival analysis in R (Terry M. Therneau, 2021a). But back to the text: Despite the simplicity of preprogrammed algorithms, [Singer and Willett] prefer an alternative approach for life table construction. This approach requires construction of a person-period data set, much like the period-period data set used for growth modeling. Once you create the person-period data set, you can compute descriptive statistics sing any standard cross-tabulation routine. (p. 351) 10.5.1 The person-period data set. Here is the person-level data set displayed in Figure 10.4; it’s just a subset of the teachers data. teachers %&gt;% filter(id %in% c(20, 126, 129)) ## # A tibble: 3 × 3 ## id t censor ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20 3 0 ## 2 126 12 0 ## 3 129 12 1 You can transform the person-level survival data set into the person-period variant shown on the right panel of Figure 10.4 with a workflow like this. teachers_pp &lt;- teachers %&gt;% uncount(weights = t) %&gt;% group_by(id) %&gt;% mutate(period = 1:n()) %&gt;% mutate(event = if_else(period == max(period) &amp; censor == 0, 1, 0)) %&gt;% select(-censor) %&gt;% ungroup() teachers_pp %&gt;% filter(id %in% c(20, 126, 129)) ## # A tibble: 27 × 3 ## id period event ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 20 1 0 ## 2 20 2 0 ## 3 20 3 1 ## 4 126 1 0 ## 5 126 2 0 ## 6 126 3 0 ## 7 126 4 0 ## 8 126 5 0 ## 9 126 6 0 ## 10 126 7 0 ## # ℹ 17 more rows You don’t necessarily need to use ungroup() at the end, but it’s probably a good idea. Anyway, note how the information previously contained in the censor column has been transformed to the event column, which is coded 0 = no event, 1 = event. With this coding, we know a participant has been censored when event == 0 on their max(period) row. We can count the number of teachers in the sample like this. teachers %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 3941 To get a sense of the difference in the data structures, here are the number of rows for the original person-level teachers data and for our person-period transformation. # person-level teachers %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 3941 # person-period teachers_pp %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 24875 Here is the breakdown of the number of rows in the person-period teachers data for which event == 1 or event == 0. teachers_pp %&gt;% count(event) ## # A tibble: 2 × 2 ## event n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 22668 ## 2 1 2207 10.5.2 Using the person-period data set to construct the life table. “All the life table’s essential elements can be computed through cross-tabulation of PERIOD and EVENT in the person-period data set” (p. 354, emphasis in the original). Here’s how we might use the tidyverse do that with teachers_pp. teachers_lt &lt;- teachers_pp %&gt;% # change the coding for `event` in anticipation of the final format mutate(event = str_c(&quot;event = &quot;, event)) %&gt;% group_by(period) %&gt;% count(event) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(total = `event = 0` + `event = 1`) %&gt;% mutate(prop_e_1 = (`event = 1` / total) %&gt;% round(digits = 4)) teachers_lt ## # A tibble: 12 × 5 ## period `event = 0` `event = 1` total prop_e_1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 3485 456 3941 0.116 ## 2 2 3101 384 3485 0.110 ## 3 3 2742 359 3101 0.116 ## 4 4 2447 295 2742 0.108 ## 5 5 2229 218 2447 0.0891 ## 6 6 2045 184 2229 0.0825 ## 7 7 1922 123 2045 0.0601 ## 8 8 1563 79 1642 0.0481 ## 9 9 1203 53 1256 0.0422 ## 10 10 913 35 948 0.0369 ## 11 11 632 16 648 0.0247 ## 12 12 386 5 391 0.0128 Here are the totals Singer and Willett displayed in the bottom row. teachers_lt %&gt;% pivot_longer(`event = 0`:total) %&gt;% group_by(name) %&gt;% summarise(total = sum(value)) %&gt;% pivot_wider(names_from = name, values_from = total) ## # A tibble: 1 × 3 ## `event = 0` `event = 1` total ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 22668 2207 24875 The ability to construct a life table using the person-period data set provides a simple strategy for conducting the descriptive analyses outlined in this chapter. This strategy yields appropriate statistics regardless of the amount, or pattern, of censoring. Perhaps even more important, the person-period data set is the fundamental tool for fitting discrete-time hazard models to data, using methods that we describe in the next chapter. (p. 356) 10.6 Bonus: Fit the discrete-time hazard models with brms The frequentists aren’t the only ones who can discrete-time hazard models. Bayesians can get in on the fun, too. The first step is to decide on an appropriate likelihood function. Why? Because Bayes’ formula requires that we define the likelihood and the priors. As all the parameters in the model are seen through the lens of the likelihood, it’s important we consider it with care. Happily, the exercises in the last section did a great job preparing us for the task. In Table 10.3, Singer and Willett hand computed the discrete hazards (i.e., the values in the “Proportion EVENT = 1” column) by dividing the valued in the “EVENT = 1” column by those in the “Total” column. Discrete hazards are proportions. Proportions have two important characteristics; they are continuous and necessarily range between 0 and 1. You know what else has those characteristics? Probabilities. So far in this text, we have primarily focused on models using the Gaussian likelihood. Though it’s a workhorse, the Gaussian is inappropriate for modeling proportions/probabilities. Good old Gauss is great at modeling unbounded continuous data, but it can fail miserably when working with bounded data and our proportions/probabilities are most certainly bounded. The binomial likelihood, however, is well-suited for handling probabilities. Imagine you have data that can take on values of 0 and 1, such as failures/successes, no’s/yesses, fails/passes, and no-events/events. If you sum up all the 1’s and divide them by the total cases, you get a proportion/probability. The simple binomial model takes just that kind of data–the number of 1’s and the number of total cases. The formula for the binomial likelihood is \\[\\Pr (z \\mid n, p) = \\frac{n!}{z!(n - z)!} p^z (1 - p)^{n - z},\\] where \\(z\\) is the number of cases for which the value is 1, \\(n\\) is the total number of cases, and \\(p\\) is the probability of a 1 across cases. As the data provide us with \\(z\\) and \\(n\\), we end up estimating \\(p\\). If we’re willing to use what’s called a link function, we can estimate \\(p\\) with a linear model with any number of predictors. When fitting binomial regression models, you can take your choice among several link functions, the most popular of which is the logit. This will be our approach. As you may have guesses, using the logit link to fit a binomial model is often termed logistic regression. Welcome to the generalized linear model. Let’s fire up brms. library(brms) Before we fit the model, it will make our lives easier if we redefine period as a factor and rename our event = 1 column as event. We defined period as a factor because we want to fit a model with discrete time. Renaming event = 1 column as event just makes it easier on the brm() function to read the variable. teachers_lt &lt;- teachers_lt %&gt;% mutate(period = factor(period), event = `event = 1`) With this formulation, event is our \\(z\\) term and total is our \\(n\\) term. We’re estimating \\(p\\). Behold the mode syntax. fit10.6 &lt;- brm(data = teachers_lt, family = binomial, event | trials(total) ~ 0 + period, prior(normal(0, 4), class = b), chains = 4, cores = 1, iter = 2000, warmup = 1000, seed = 10, file = &quot;fits/fit10.06&quot;) Check the summary. print(fit10.6) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + period ## Data: teachers_lt (Number of observations: 12) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## period1 -2.03 0.05 -2.13 -1.94 1.00 8978 2633 ## period2 -2.09 0.05 -2.20 -1.99 1.00 8535 3148 ## period3 -2.03 0.06 -2.15 -1.92 1.00 8693 2762 ## period4 -2.12 0.06 -2.23 -2.00 1.00 8914 2761 ## period5 -2.33 0.07 -2.46 -2.19 1.00 8628 3003 ## period6 -2.41 0.08 -2.56 -2.26 1.00 8058 2956 ## period7 -2.75 0.10 -2.95 -2.56 1.00 9576 2837 ## period8 -2.99 0.11 -3.21 -2.77 1.00 8576 3107 ## period9 -3.13 0.14 -3.40 -2.86 1.00 7823 2712 ## period10 -3.27 0.18 -3.62 -2.93 1.00 9520 3361 ## period11 -3.69 0.26 -4.21 -3.21 1.00 8064 3155 ## period12 -4.38 0.46 -5.36 -3.57 1.00 7540 2664 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because we used the 0 + Intercept syntax in the presence of a factor predictor, period, we suppressed the default intercept. Instead, we have separate “intercepts” for each of the 12 levels period. Because we used the conventional logit link, the parameters are all on the log-odds scale. Happily, we can use the brms::inv_logit_scaled() function to convert them back into the probability metric. Here’s a quick and dirty conversion using the output from fixef(). fixef(fit10.6) %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## period1 0.11558590 0.5126268 0.105992384 0.12595926 ## period2 0.11013198 0.5132931 0.100171819 0.12057547 ## period3 0.11560177 0.5144565 0.104104675 0.12754441 ## period4 0.10748598 0.5150173 0.096763627 0.11921332 ## period5 0.08904255 0.5172470 0.078503161 0.10030839 ## period6 0.08238098 0.5194902 0.071493616 0.09455987 ## period7 0.06004397 0.5240282 0.049683837 0.07168711 ## period8 0.04796623 0.5280987 0.038694290 0.05875754 ## period9 0.04206871 0.5344391 0.032238297 0.05423039 ## period10 0.03666806 0.5439246 0.026160527 0.05083425 ## period11 0.02437227 0.5643922 0.014637046 0.03895152 ## period12 0.01233845 0.6127540 0.004657292 0.02736638 Compare these Estimate values with the values in the “Estimated hazard probability” column from Table 10.2 in the text (p. 349). They are very close. We can go further and look at these hazard estimates in a plot. We’ll use the tidybayes::stat_lineribbon() function to plot their posterior means atop their 50% and 95% intervals. library(tidybayes) as_draws_df(fit10.6) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(1:12) %&gt;% pivot_longer(everything(), names_to = &quot;period&quot;, values_to = &quot;hazard&quot;) %&gt;% mutate(period = period %&gt;% as.double()) %&gt;% ggplot(aes(x = period, y = hazard)) + stat_lineribbon(.width = c(.5, .95), size = 1/3) + # add the hazard estimates from `survival::survfit()` geom_point(data = tibble(period = fit10.1$time, hazard = fit10.1$n.event / fit10.1$n.risk), aes(y = hazard), size = 2, color = &quot;violetred1&quot;) + scale_fill_manual(&quot;CI&quot;, values = c(&quot;grey75&quot;, &quot;grey60&quot;)) + scale_x_continuous(breaks = 1:12) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.925, .825), panel.grid = element_blank()) For comparison sake, those violet dots in the foreground are the hazard estimates from the frequentist survival::survfit() function. Turns out our Bayesian results are very similar to the frequentist ones. Hopefully this isn’t a surprise. There was a lot of data and we used fairly weak priors. For simple models under those conditions, frequentist and Bayesian results should be pretty close. Remember that \\(se \\big (\\hat h(t_j) \\big)\\) formula from back in section 10.4.1? That doesn’t quite apply to the posterior standard deviations from our Bayesian model. Even so, our posterior \\(\\textit{SD}\\)s will be very similar to the ML standard errors. Let’s compare those in a plot, too. as_draws_df(fit10.6) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(1:12) %&gt;% pivot_longer(everything(), names_to = &quot;period&quot;, values_to = &quot;hazard&quot;) %&gt;% mutate(period = period %&gt;% as.double()) %&gt;% group_by(period) %&gt;% summarise(sd = sd(hazard)) %&gt;% bind_cols(se_h_hat %&gt;% select(se_h_hat)) %&gt;% pivot_longer(-period) %&gt;% mutate(name = factor(name, levels = c(&quot;sd&quot;, &quot;se_h_hat&quot;), labels = c(&quot;Bayesian&quot;, &quot;ML&quot;))) %&gt;% ggplot(aes(x = period, y = value, color = name)) + geom_point(size = 3, position = position_dodge(width = .25)) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .55) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(expression(italic(se)), limits = c(0, 0.01)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.09, .9), panel.grid = element_blank()) Man those are close. We can also use our brms output to depict the survivor function. On page 337 in the text (Equation 10.5), Singer and Willett demonstrated how to define the survivor function in terms of the hazard function. It follows the form \\[\\hat S (t_j) = [1 - \\hat h (t_j)][1 - \\hat h (t_{j - 1})][1 - \\hat h (t_{j - 2})] \\dots [1 - \\hat h (t_1)].\\] In words, “each year’s estimated survival probability is the successive product of the complement of the estimated hazard function probabilities across this and all previous years” (p. 337, emphasis in the original). Here’s how you might do that with the output from as_draws_df(). draws &lt;- as_draws_df(fit10.6) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% # transform the hazards from the log-odds metric to probabilities mutate_all(inv_logit_scaled) %&gt;% set_names(str_c(&quot;h&quot;, 1:12)) %&gt;% # take the &quot;complement&quot; of each hazard mutate_all(~1 - .) %&gt;% # apply Equation 10.5 transmute(s0 = 1, s1 = h1, s2 = h1 * h2, s3 = h1 * h2 * h3, s4 = h1 * h2 * h3 * h4, s5 = h1 * h2 * h3 * h4 * h5, s6 = h1 * h2 * h3 * h4 * h5 * h6, s7 = h1 * h2 * h3 * h4 * h5 * h6 * h7, s8 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8, s9 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9, s10 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10, s11 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10 * h11, s12 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10 * h11 * h12) glimpse(draws) ## Rows: 4,000 ## Columns: 13 ## $ s0 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ s1 &lt;dbl&gt; 0.8873979, 0.8803308, 0.8835959, 0.8859869, 0.8801158, 0.8850150, 0.8890101, 0.8800389… ## $ s2 &lt;dbl&gt; 0.7806517, 0.7919493, 0.7760261, 0.7967607, 0.7775297, 0.7968146, 0.7907930, 0.7746211… ## $ s3 &lt;dbl&gt; 0.6862845, 0.7044593, 0.6819337, 0.7088261, 0.6793964, 0.7115761, 0.7011122, 0.6774138… ## $ s4 &lt;dbl&gt; 0.6182967, 0.6222400, 0.6165402, 0.6235120, 0.6172917, 0.6261048, 0.6299567, 0.6022196… ## $ s5 &lt;dbl&gt; 0.5622619, 0.5674013, 0.5585957, 0.5712784, 0.5604331, 0.5722488, 0.5767957, 0.5450130… ## $ s6 &lt;dbl&gt; 0.5127794, 0.5228326, 0.5117748, 0.5240422, 0.5177996, 0.5222464, 0.5344377, 0.4920744… ## $ s7 &lt;dbl&gt; 0.4816586, 0.4913180, 0.4828236, 0.4903100, 0.4895234, 0.4893107, 0.5054531, 0.4615354… ## $ s8 &lt;dbl&gt; 0.4555664, 0.4693745, 0.4579987, 0.4683243, 0.4644157, 0.4690895, 0.4731413, 0.4430076… ## $ s9 &lt;dbl&gt; 0.4370529, 0.4502755, 0.4397470, 0.4474608, 0.4447764, 0.4486741, 0.4542936, 0.4262741… ## $ s10 &lt;dbl&gt; 0.4191644, 0.4339902, 0.4244117, 0.4310308, 0.4268689, 0.4319676, 0.4438548, 0.4050520… ## $ s11 &lt;dbl&gt; 0.4064542, 0.4240468, 0.4111878, 0.4224947, 0.4119500, 0.4238517, 0.4321744, 0.3919878… ## $ s12 &lt;dbl&gt; 0.4020233, 0.4192921, 0.4035911, 0.4196204, 0.4037077, 0.4195607, 0.4230202, 0.3886072… We’ll learn how to simplify that syntax with help from the cumprod() function in the next chapter. Now we have our survival estimates, we can make our Bayesian version of the lower panel of Figure 10.1. # this will help us depict the median lifetime line &lt;- tibble(period = c(0, iml, iml), survival = c(.5, .5, 0)) # wrangle draws %&gt;% pivot_longer(everything(), values_to = &quot;survival&quot;) %&gt;% mutate(period = str_remove(name, &quot;s&quot;) %&gt;% as.double()) %&gt;% # plot! ggplot(aes(x = period, y = survival)) + geom_path(data = line, color = &quot;white&quot;, linetype = 2) + stat_lineribbon(.width = .95, size = 1/3, fill = &quot;grey75&quot;) + # add the survival estimates from `survival::survfit()` geom_point(data = tibble(period = fit10.1$time, survival = fit10.1$surv), size = 2, color = &quot;violetred1&quot;) + scale_x_continuous(&quot;years in teaching&quot;, breaks = 0:13, limits = c(0, 13)) + scale_y_continuous(expression(&quot;estimated survival probability, &quot;*hat(italic(S))(italic(t[j]))), breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) Like we did with our hazard plot above, we superimposed the frequentist estimates as violet dots atop the Bayesian posterior means and intervals. Because of how narrow the posteriors were, we only showed the 95% intervals, here. As with the hazard estimates, our Bayesian survival estimates are very close to the ones we computed with ML. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.4 brms_2.19.0 Rcpp_1.0.10 patchwork_1.1.2 survival_3.5-5 lubridate_1.9.2 ## [7] forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [13] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] tensorA_0.36.2 rstudioapi_0.14 jsonlite_1.8.4 magrittr_2.0.3 ## [5] TH.data_1.1-2 estimability_1.4.1 farver_2.1.1 nloptr_2.0.3 ## [9] rmarkdown_2.21 vctrs_0.6.2 minqa_1.2.5 base64enc_0.1-3 ## [13] htmltools_0.5.5 distributional_0.3.2 sass_0.4.6 StanHeaders_2.26.25 ## [17] bslib_0.4.2 htmlwidgets_1.6.2 plyr_1.8.8 sandwich_3.0-2 ## [21] emmeans_1.8.6 zoo_1.8-12 cachem_1.0.8 igraph_1.4.2 ## [25] mime_0.12 lifecycle_1.0.3 pkgconfig_2.0.3 colourpicker_1.2.0 ## [29] Matrix_1.5-4 R6_2.5.1 fastmap_1.1.1 shiny_1.7.4 ## [33] digest_0.6.31 colorspace_2.1-0 ps_1.7.5 crosstalk_1.2.0 ## [37] projpred_2.5.0 labeling_0.4.2 fansi_1.0.4 timechange_0.2.0 ## [41] abind_1.4-5 mgcv_1.8-42 compiler_4.3.0 bit64_4.0.5 ## [45] withr_2.5.0 backports_1.4.1 inline_0.3.19 shinystan_2.6.0 ## [49] gamm4_0.2-6 pkgbuild_1.4.0 highr_0.10 MASS_7.3-58.4 ## [53] gtools_3.9.4 loo_2.6.0 tools_4.3.0 httpuv_1.6.11 ## [57] threejs_0.3.3 glue_1.6.2 callr_3.7.3 nlme_3.1-162 ## [61] promises_1.2.0.1 grid_4.3.0 checkmate_2.2.0 reshape2_1.4.4 ## [65] generics_0.1.3 gtable_0.3.3 tzdb_0.4.0 hms_1.1.3 ## [69] utf8_1.2.3 ggdist_3.3.0 pillar_1.9.0 markdown_1.7 ## [73] vroom_1.6.3 posterior_1.4.1 later_1.3.1 splines_4.3.0 ## [77] lattice_0.21-8 bit_4.0.5 tidyselect_1.2.0 miniUI_0.1.1.1 ## [81] knitr_1.42 arrayhelpers_1.1-0 gridExtra_2.3 bookdown_0.34 ## [85] stats4_4.3.0 xfun_0.39 bridgesampling_1.1-2 matrixStats_0.63.0 ## [89] DT_0.27 rstan_2.21.8 stringi_1.7.12 boot_1.3-28.1 ## [93] evaluate_0.21 codetools_0.2-19 cli_3.6.1 RcppParallel_5.1.7 ## [97] shinythemes_1.2.0 xtable_1.8-4 munsell_0.5.0 processx_3.8.1 ## [101] jquerylib_0.1.4 coda_0.19-4 svUnit_1.0.6 parallel_4.3.0 ## [105] rstantools_2.3.1 ellipsis_0.3.2 prettyunits_1.1.1 dygraphs_1.1.1.6 ## [109] bayesplot_1.10.0 Brobdingnag_1.2-9 lme4_1.1-33 viridisLite_0.4.2 ## [113] mvtnorm_1.1-3 scales_1.2.1 xts_0.13.1 crayon_1.5.2 ## [117] rlang_1.1.1 multcomp_1.4-23 shinyjs_2.1.0 References Greenwood, M. (1926). The natural duration of cancer. Reports on Public Health and Medical Subjects, 33, 1–26. Miller, R. G. (1981). Survival analysis. John Wiley &amp; Sons. Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Therneau, Terry M. (2021a). A package for survival analysis in R. https://CRAN.R-project.org/package=survival/vignettes/survival.pdf Therneau, Terry M. (2021b). survival reference manual, Version 3.2-10. https://CRAN.R-project.org/package=survival/survival.pdf Therneau, Terry M. (2021c). survival: Survival analysis [Manual]. https://github.com/therneau/survival Therneau, Terry M., &amp; Grambsch, P. M. (2000). Modeling survival data: Extending the Cox model. Springer. https://link.springer.com/book/10.1007/978-1-4757-3294-8 "],["fitting-basic-discrete-time-hazard-models.html", "11 Fitting Basic Discrete-Time Hazard Models 11.1 Toward a statistical model for discrete-time hazard 11.2 Formal representation of the population discrete-time hazard model 11.3 Fitting a discrete-time hazard model to data 11.4 Interpreting parameter estimates 11.5 Displaying fitted hazard and survivor functions 11.6 Comparing models using deviance statistics and information criteria 11.7 Statistical inference using [uncertainty in the Bayesian posterior] Session info Footnote", " 11 Fitting Basic Discrete-Time Hazard Models In this chapter and the next, we present statistical models of hazard for data collected in discrete time. The relative simplicity of these models makes them an ideal entrée into the world of survival analysis. In subsequent chapters, we extend these basic ideas to situations in which event occurrence is recorded in continuous time. Good data analysis involves more than using a computer package to fit a statistical model to data. To conduce a credible discrete-time survival analysis, you must: (1) specify a suitable model for hazard and understand its assumptions; (2) use sample data to estimate the model parameters; (3) interpret results in terms of your research questions; (4) evaluate model fit and [express the uncertainty in the] model parameters; and (5) communicate your findings. (Singer &amp; Willett, 2003, pp. 357–358) 11.1 Toward a statistical model for discrete-time hazard Time to load Capaldi, Crosby, and Stoolmiller’s (1996) firstsex.csv data. library(tidyverse) sex &lt;- read_csv(&quot;data/firstsex.csv&quot;) glimpse(sex) ## Rows: 180 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, … ## $ time &lt;dbl&gt; 9, 12, 12, 12, 11, 9, 12, 11, 12, 11, 12, 11, 9, 12, 10, 12, 7, 12, 10, 12, 11, 12, 11, 12, 1… ## $ censor &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, … ## $ pt &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, … ## $ pas &lt;dbl&gt; 1.9788670, -0.5454916, -1.4049800, 0.9741806, -0.6356313, -0.2428857, -0.8685001, 0.4535947, … Here are the cases broken down by time and censor status. sex %&gt;% count(time, censor) ## # A tibble: 7 × 3 ## time censor n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7 0 15 ## 2 8 0 7 ## 3 9 0 24 ## 4 10 0 29 ## 5 11 0 25 ## 6 12 0 26 ## 7 12 1 54 Since these data show no censoring before the final time point, it is straightforward to follow along with the text (p. 358) and compute the percent who had already had sex by the 12th grade. sex %&gt;% count(censor) %&gt;% mutate(percent = 100 * (n / sum(n))) ## # A tibble: 2 × 3 ## censor n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 126 70 ## 2 1 54 30 Here we break the data down by our central predictor, pt, which is coded “0 for boys who lived with both biological parents” and “1 for boys who experienced one or more parenting transitions” before the 7th grade. sex %&gt;% count(pt) %&gt;% mutate(percent = 100 * (n / sum(n))) ## # A tibble: 2 × 3 ## pt n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 72 40 ## 2 1 108 60 11.1.1 Plots of within-group hazard functions and survivor functions. Plots of sample hazard functions and survivor functions estimates separately for groups distinguished by their predictor values are invaluable exploratory tools. If a predictor is categorical, like PT, construction of these displays is straightforward. If a predictor is continuous, you should just temporarily categorize its values for plotting purposes. (pp. 358–359, emphasis in the original) To make our version of the descriptive plots in Figure 11.1, we’ll need to first load the survival package. library(survival) fit11.1 will be of the cases for which pt == 0 and fit11.2 will be of the cases for which pt == 1. With fit11.3, we use all cases regardless of pt status. fit11.1 &lt;- survfit(data = sex %&gt;% filter(pt == 0), Surv(time, 1 - censor) ~ 1) fit11.2 &lt;- survfit(data = sex %&gt;% filter(pt == 1), Surv(time, 1 - censor) ~ 1) fit11.3 &lt;- survfit(data = sex, Surv(time, 1 - censor) ~ 1) Before we plot the results, it might be handy to arrange the fit results in life tables. We can streamline that code with the custom make_lt() function from last chapter. make_lt &lt;- function(fit) { # arrange the lt data for all rows but the first most_rows &lt;- tibble(time = fit$time) %&gt;% mutate(time_int = str_c(&quot;[&quot;, time, &quot;, &quot;, time + 1, &quot;)&quot;), n_risk = fit$n.risk, n_event = fit$n.event) %&gt;% mutate(n_censored = n_risk - n_event - lead(n_risk, default = 0), hazard_fun = n_event / n_risk, survivor_fun = fit$surv) # define the values for t = 2 and t = 1 time_1 &lt;- fit$time[1] time_0 &lt;- time_1 - 1 # define the values for the row for which t = 1 row_1 &lt;- tibble(time = time_0, time_int = str_c(&quot;[&quot;, time_0, &quot;, &quot;, time_1, &quot;)&quot;), n_risk = fit$n.risk[1], n_event = NA, n_censored = NA, hazard_fun = NA, survivor_fun = 1) # make the full life table lt &lt;- bind_rows(row_1, most_rows) lt } We’ll use make_lt() separately for each fit, stack the results from the first on top of those from the second, and add a pt column to index the rows. This will be our version of Table 11.1 (p. 360). lt &lt;- bind_rows(make_lt(fit11.1), make_lt(fit11.2), make_lt(fit11.3)) %&gt;% mutate(pt = factor(rep(c(&quot;pt = 0&quot;, &quot;pt = 1&quot;, &quot;overall&quot;), each = n() / 3))) %&gt;% select(pt, everything()) lt ## # A tibble: 21 × 8 ## pt time time_int n_risk n_event n_censored hazard_fun survivor_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 6 [6, 7) 72 NA NA NA 1 ## 2 pt = 0 7 [7, 8) 72 2 0 0.0278 0.972 ## 3 pt = 0 8 [8, 9) 70 2 0 0.0286 0.944 ## 4 pt = 0 9 [9, 10) 68 8 0 0.118 0.833 ## 5 pt = 0 10 [10, 11) 60 8 0 0.133 0.722 ## 6 pt = 0 11 [11, 12) 52 10 0 0.192 0.583 ## 7 pt = 0 12 [12, 13) 42 8 34 0.190 0.472 ## 8 pt = 1 6 [6, 7) 108 NA NA NA 1 ## 9 pt = 1 7 [7, 8) 108 13 0 0.120 0.880 ## 10 pt = 1 8 [8, 9) 95 5 0 0.0526 0.833 ## # ℹ 11 more rows Here is the code for the top panel of Figure 11.1. p1 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = hazard_fun, color = pt, group = pt)) + geom_line() + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .5) + scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) + scale_y_continuous(&quot;estimated hazard probability&quot;, limits = c(0, .5)) + theme(panel.grid = element_blank()) Now make the plot for the bottom panel. p2 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = survivor_fun, color = pt, group = pt)) + geom_hline(yintercept = .5, color = &quot;white&quot;, linetype = 2) + geom_line() + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .5) + scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) + scale_y_continuous(&quot;estimated survival probability&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) Combine the two ggplot2 objects with patchwork syntax to make our version of Figure 11.1. library(patchwork) (p1 / p2) + plot_layout(guides = &quot;collect&quot;) On page 361, Singer and Willett compared the hazard probabilities at grades 8 and 11 for boys in the two pt groups. We can make that comparison with filter(). lt %&gt;% filter(time %in% c(8, 11) &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, hazard_fun) ## # A tibble: 4 × 3 ## pt time hazard_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 8 0.0286 ## 2 pt = 0 11 0.192 ## 3 pt = 1 8 0.0526 ## 4 pt = 1 11 0.283 Compare the two groups on the hazard probabilities at grade 9. lt %&gt;% filter(time == 9 &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, hazard_fun) ## # A tibble: 2 × 3 ## pt time hazard_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 9 0.118 ## 2 pt = 1 9 0.178 Now compare them on their hazard probabilities in grade 12. lt %&gt;% filter(time == 12 &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, hazard_fun) ## # A tibble: 2 × 3 ## pt time hazard_fun ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 12 0.190 ## 2 pt = 1 12 0.474 At the top of page 362, Singer and Willett compared the percentages of boys who were virgins at grades 9 and 12, by pt status. Those percentages are straight algebraic transformations of the corresponding survival function values. lt %&gt;% filter(time %in% c(9, 12) &amp; pt != &quot;overall&quot;) %&gt;% select(pt:time, survivor_fun) %&gt;% mutate(percent_virgins = (100 * survivor_fun) %&gt;% round(digits = 1)) ## # A tibble: 4 × 4 ## pt time survivor_fun percent_virgins ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 9 0.833 83.3 ## 2 pt = 0 12 0.472 47.2 ## 3 pt = 1 9 0.685 68.5 ## 4 pt = 1 12 0.185 18.5 Now let’s finish off by computing the interpolated median lifetime values for each with our custom make_iml() function. make_iml &lt;- function(lt) { # lt is a generic name for a life table of the # kind we made with our `make_lt()` function # determine the mth row lt_m &lt;- lt %&gt;% filter(survivor_fun &gt; .5) %&gt;% slice(n()) # determine the row for m + 1 lt_m1 &lt;- lt %&gt;% filter(survivor_fun &lt; .5) %&gt;% slice(1) # pull the value for m m &lt;- pull(lt_m, time) # pull the two survival function values stm &lt;- pull(lt_m, survivor_fun) stm1 &lt;- pull(lt_m1, survivor_fun) # plug the values into Equation 10.6 (page 338) iml &lt;- m + ((stm - .5) / (stm - stm1)) * ((m + 1) - m) iml } Put make_iml() to work. make_iml(lt %&gt;% filter(pt == &quot;pt = 0&quot;)) ## [1] 11.75 make_iml(lt %&gt;% filter(pt == &quot;pt = 1&quot;)) ## [1] 9.952381 11.1.2 What kind of statistical model do these graphs suggest? To postulate a statistical model to represent the relationship between the population discrete-time hazard function and predictors, we must deal with two complications apparent in these displays. One is that any hypothesized model must describe the shape of the entire discrete-time hazard function over time, not just its value in any one period, in much the same way that a multilevel model for change characterizes the shape of entire individual growth trajectories over time. A second complication is that, as a conditional probability, the value of discrete-time hazard must lie between 0 and 1. Any reasonable statistical model for hazard must recognize this constraint, precluding the occurrence of theoretically impossible values. (p. 362, emphasis in the original) 11.1.2.1 The bounded nature of hazard. A conventional way to handle the bounded nature of probabilities is transform the scale of the data. David R. Cox (1972) recommended either the odds and log-odds (i.e., logit) transformations. Given a probability, \\(p\\), we compute the odds as \\[\\text{odds} = \\frac{p}{1 - p}.\\] Log-odds is a minor extension; you simply take the log of the odds, which we can formally express as \\[\\text{log-odds} = \\log \\left (\\frac{p}{1 - p} \\right ).\\] To make the conversions easy, we’ll define2 a couple convenience functions: odds() and log_odds(). odds &lt;- function(p) { p / (1 - p) } log_odds &lt;- function(p) { log(p / (1 - p)) } Here’s how they work. tibble(p = seq(from = 0, to = 1, by = .1)) %&gt;% mutate(odds = odds(p), log_odds = log_odds(p)) ## # A tibble: 11 × 3 ## p odds log_odds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 -Inf ## 2 0.1 0.111 -2.20 ## 3 0.2 0.25 -1.39 ## 4 0.3 0.429 -0.847 ## 5 0.4 0.667 -0.405 ## 6 0.5 1 0 ## 7 0.6 1.5 0.405 ## 8 0.7 2.33 0.847 ## 9 0.8 4 1.39 ## 10 0.9 9 2.20 ## 11 1 Inf Inf Before we make our version of Figure 11.2, it might be instructive to see how odds and log-odds compare to probabilities in a plot. Here we’ll compare them to probabilities ranging from .01 to .99. tibble(p = seq(from = .01, to = .99, by = .01)) %&gt;% mutate(odds = odds(p), `log(odds)` = log_odds(p)) %&gt;% pivot_longer(-p) %&gt;% mutate(name = factor(name, levels = c(&quot;odds&quot;, &quot;log(odds)&quot;))) %&gt;% ggplot(aes(x = p, y = value)) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;transformed scale&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Odds are bounded to values of zero and above and have an inflection at 1. Log-odds are unbounded and have an inflection point at 0. Here we’ll save the odds and log-odds for our hazard functions within the lt life table. lt &lt;- lt %&gt;% mutate(odds = odds(hazard_fun), log_odds = log_odds(hazard_fun)) lt ## # A tibble: 21 × 10 ## pt time time_int n_risk n_event n_censored hazard_fun survivor_fun odds log_odds ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pt = 0 6 [6, 7) 72 NA NA NA 1 NA NA ## 2 pt = 0 7 [7, 8) 72 2 0 0.0278 0.972 0.0286 -3.56 ## 3 pt = 0 8 [8, 9) 70 2 0 0.0286 0.944 0.0294 -3.53 ## 4 pt = 0 9 [9, 10) 68 8 0 0.118 0.833 0.133 -2.01 ## 5 pt = 0 10 [10, 11) 60 8 0 0.133 0.722 0.154 -1.87 ## 6 pt = 0 11 [11, 12) 52 10 0 0.192 0.583 0.238 -1.44 ## 7 pt = 0 12 [12, 13) 42 8 34 0.190 0.472 0.235 -1.45 ## 8 pt = 1 6 [6, 7) 108 NA NA NA 1 NA NA ## 9 pt = 1 7 [7, 8) 108 13 0 0.120 0.880 0.137 -1.99 ## 10 pt = 1 8 [8, 9) 95 5 0 0.0526 0.833 0.0556 -2.89 ## # ℹ 11 more rows We’re ready to make and combine the subplots for our version of Figure 11.2. # hazard p1 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = hazard_fun, color = pt, group = pt)) + geom_line() + scale_y_continuous(NULL, breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;Estimated hazard&quot;) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # odds p2 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = odds, color = pt, group = pt)) + geom_line() + scale_y_continuous(NULL, breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;Estimated odds&quot;) + theme(legend.position = &quot;none&quot;) # log-odds p3 &lt;- lt %&gt;% filter(pt != &quot;overall&quot;) %&gt;% ggplot(aes(x = time, y = log_odds, color = pt, group = pt)) + geom_line() + scale_y_continuous(NULL, limits = c(-4, 0)) + labs(subtitle = &quot;Estimated logit(hazard)&quot;) + theme(legend.position = &quot;none&quot;) (p1 / p2 / p3 ) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .5) &amp; scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) 11.1.2.2 What statistical model could have generated these sample data? With the survival models from the prior sections, we were lazy and just used the survival package. But recall from the end of the last chapter that we can fit the analogous models brms using the binomial likelihood. This subsection is a great place to practice those some more. The fitted lines Singer and Willett displayed in Figure 11.3 can all be reproduced with binomial regression. However, the sex data are not in a convenient form to fit those models. Just like we did in last chapter, we’ll want to take a two-step process whereby we first convert the data to the long (i.e., person-period) format and then summarize. Happily, we can accomplish that first step by uploading the data in the firstsex_pp.csv file, which are already in the long format. sex_pp &lt;- read_csv(&quot;data/firstsex_pp.csv&quot;) glimpse(sex_pp) ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, … ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8… ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, … ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, … ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, … ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, … ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, … Now we’ll compute the desired summary values and wrangle a bit. sex_aggregated &lt;- sex_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(period) %&gt;% count(event, pt) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(total = event + no_event, period_center = period - mean(period), peroid_factor = factor(period), pt = factor(pt)) sex_aggregated ## # A tibble: 12 × 7 ## period pt event no_event total period_center peroid_factor ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 0 2 70 72 -2.5 7 ## 2 7 1 13 95 108 -2.5 7 ## 3 8 0 2 68 70 -1.5 8 ## 4 8 1 5 90 95 -1.5 8 ## 5 9 0 8 60 68 -0.5 9 ## 6 9 1 16 74 90 -0.5 9 ## 7 10 0 8 52 60 0.5 10 ## 8 10 1 21 53 74 0.5 10 ## 9 11 0 10 42 52 1.5 11 ## 10 11 1 15 38 53 1.5 11 ## 11 12 0 8 34 42 2.5 12 ## 12 12 1 18 20 38 2.5 12 Note how we saved the grade values in three columns: period has them as continuous values, which will be hand for plotting; period_center has them as mean-centered continuous values, which will make fitting the linear model in the middle panel easier; and period_factor has them saved as a factor, which will help us fit the model in the bottom panel. Fire up brms. library(brms) Before we fit the models, it might be good to acknowledge we’re jumping ahead of the authors, a bit. Singer and Willett didn’t discuss fitting discrete time hazard models until section 11.3.2. Sure, their focus was on the frequentist approach using maximum likelihood. But the point still stands. If these model fitting details feel a bit rushed, they are. Any anxious feelings aside, now fit the three binomial models. We continue to use weakly-regularizing priors for each. # top panel fit11.4 &lt;- brm(data = sex_aggregated, family = binomial, event | trials(total) ~ 0 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.04&quot;) # middle panel fit11.5 &lt;- brm(data = sex_aggregated, family = binomial, event | trials(total) ~ 0 + pt + period_center, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.05&quot;) # bottom panel fit11.6 &lt;- brm(data = sex_aggregated, family = binomial, event | trials(total) ~ 0 + pt + peroid_factor, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.06&quot;) Check the model summaries. print(fit11.4) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + pt ## Data: sex_aggregated (Number of observations: 12) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## pt0 -2.15 0.17 -2.50 -1.83 1.00 3668 2824 ## pt1 -1.44 0.12 -1.68 -1.21 1.00 4237 2963 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.5) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + pt + period_center ## Data: sex_aggregated (Number of observations: 12) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## pt0 -2.23 0.18 -2.60 -1.89 1.00 3779 3003 ## pt1 -1.35 0.12 -1.60 -1.12 1.00 3270 2585 ## period_center 0.43 0.06 0.31 0.56 1.00 3635 3089 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.6) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(total) ~ 0 + pt + peroid_factor ## Data: sex_aggregated (Number of observations: 12) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## pt0 -3.00 0.32 -3.63 -2.38 1.00 986 1718 ## pt1 -2.12 0.27 -2.68 -1.61 1.00 912 1224 ## peroid_factor8 -0.77 0.48 -1.76 0.12 1.00 1442 1993 ## peroid_factor9 0.69 0.35 0.03 1.37 1.00 1096 1527 ## peroid_factor10 1.15 0.34 0.50 1.83 1.00 1064 1732 ## peroid_factor11 1.32 0.36 0.61 2.03 1.00 1142 1581 ## peroid_factor12 1.80 0.36 1.11 2.50 1.00 1180 1759 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can extract the fitted values and their summaries for each row in the data with fitted(). To get them in the log-odds metric, we need to set scale = \"linear\". Here’s a quick example with fit11.4. fitted(fit11.4, scale = &quot;linear&quot;) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] -2.153394 0.1698095 -2.501172 -1.829720 ## [2,] -1.440906 0.1215326 -1.682504 -1.210137 ## [3,] -2.153394 0.1698095 -2.501172 -1.829720 ## [4,] -1.440906 0.1215326 -1.682504 -1.210137 ## [5,] -2.153394 0.1698095 -2.501172 -1.829720 ## [6,] -1.440906 0.1215326 -1.682504 -1.210137 ## [7,] -2.153394 0.1698095 -2.501172 -1.829720 ## [8,] -1.440906 0.1215326 -1.682504 -1.210137 ## [9,] -2.153394 0.1698095 -2.501172 -1.829720 ## [10,] -1.440906 0.1215326 -1.682504 -1.210137 ## [11,] -2.153394 0.1698095 -2.501172 -1.829720 ## [12,] -1.440906 0.1215326 -1.682504 -1.210137 If we convert that output to a data frame, tack on the original data values, and wrangle a bit, we’ll be in good shape to make the top panel of Figure 11.3. Below we’ll do that for each of the three panels, saving them as p1, p2, and p3. # logit(hazard) is horizontal with time p1 &lt;- fitted(fit11.4, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/4) + geom_line(aes(y = Estimate), alpha = 1/2) + geom_point(aes(y = log_odds(event / total))) + scale_y_continuous(NULL, limits = c(-4, 0)) + labs(subtitle = &quot;logit(hazard) is horizontal with time&quot;) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # logit(hazard) is linear with time p2 &lt;- fitted(fit11.5, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/4) + geom_line(aes(y = Estimate), alpha = 1/2) + geom_point(aes(y = log_odds(event / total))) + labs(subtitle = &quot;logit(hazard) is linear with time&quot;, y = &quot;logit(hazard)&quot;) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.position = &quot;none&quot;) # logit(hazard) is completely general with time p3 &lt;- fitted(fit11.6, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/4) + geom_line(aes(y = Estimate), alpha = 1/2) + geom_point(aes(y = log_odds(event / total))) + labs(subtitle = &quot;logit(hazard) is completely general with time&quot;, y = NULL) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.position = &quot;none&quot;) Now combine the plots with patchwork syntax. (p1 / p2 / p3) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) In addition to the posterior means (i.e., our analogues to the fitted values in the text), we added the 95% Bayesian intervals to give a better sense of the uncertainty in each model. Singer and Willett mused the unconstrained model (fit6) was a better fit to the data than the other two. We can quantify that with a LOO comparison. fit11.4 &lt;- add_criterion(fit11.4, &quot;loo&quot;) fit11.5 &lt;- add_criterion(fit11.5, &quot;loo&quot;) fit11.6 &lt;- add_criterion(fit11.6, &quot;loo&quot;) loo_compare(fit11.4, fit11.5, fit11.6) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit11.6 0.0 0.0 -31.6 1.7 5.1 0.7 63.1 3.5 ## fit11.5 -0.7 3.2 -32.2 3.2 3.5 1.2 64.5 6.4 ## fit11.4 -27.7 10.5 -59.3 10.4 9.3 2.6 118.6 20.7 Here are the LOO weights. model_weights(fit11.4, fit11.5, fit11.6, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit11.4 fit11.5 fit11.6 ## 0.000 0.338 0.662 11.2 Formal representation of the population discrete-time hazard model Earlier equations for the hazard function omitted substantive predictors. Now consider the case where \\(X_{1ij}, X_{2ij}, \\dots , X_{Pij}\\) stand for the \\(P\\) predictors which may or may not vary across individuals \\(i\\) and time periods \\(j\\). Thus \\(x_{pij}\\) is the value for the \\(i^\\text{th}\\) individual on the \\(p^\\text{th}\\) variable during the \\(j^\\text{th}\\) period. We can use this to define the conditional hazard function as \\[h(t_{ij}) = \\Pr [T_i = j \\mid T \\geq j \\text{ and } X_{1ij} = x_{1ij}, X_{2ij} = x_{2ij}, \\dots , X_{Pij} = x_{pij}].\\] Building further and keeping the baseline shape of the discrete hazard function flexible, we want a method that allows each of the \\(j\\) time periods to have its own value. Imagine a set of \\(J\\) dummy variables, \\(D_1, D_2, \\dots, D_J\\), marking off each of the time periods. For example, say \\(J = 6\\), we could depict this in a tibble like so. tibble(period = 1:6) %&gt;% mutate(d1 = if_else(period == 1, 1, 0), d2 = if_else(period == 2, 1, 0), d3 = if_else(period == 3, 1, 0), d4 = if_else(period == 4, 1, 0), d5 = if_else(period == 5, 1, 0), d6 = if_else(period == 6, 1, 0)) ## # A tibble: 6 × 7 ## period d1 d2 d3 d4 d5 d6 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 0 0 ## 2 2 0 1 0 0 0 0 ## 3 3 0 0 1 0 0 0 ## 4 4 0 0 0 1 0 0 ## 5 5 0 0 0 0 1 0 ## 6 6 0 0 0 0 0 1 If we were to use a set of dummies of this kind in a model, we would omit the conventional regression intercept, replacing it with the \\(J\\) dummies. Now presume we’re fitting a hazard model using the logit link, \\(\\operatorname{logit} h(t_{ij})\\). We can express the discrete conditional hazard model with a general functional form with respect to time as \\[\\operatorname{logit} h(t_{ij}) = [\\alpha_1 D_{1ij} + \\alpha_2 D_{2ij} + \\cdots + \\alpha_J D_{Jij}] + [\\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\cdots + \\beta_P X_{Pij}],\\] where the \\(\\alpha\\) parameters are the \\(J\\) time-period dummies and the \\(\\beta\\) parameters are for other time-varying or time-invariant predictors. This is just the type of model we used to fit fit116. For that model, the basic equation was \\[\\operatorname{logit} h(t_{ij}) = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ],\\] where the only substantive predictor was the time-invariant pt. However, that formula could be a little misleading. Recall the formula: fit11.6$formula ## event | trials(total) ~ 0 + pt + peroid_factor We suppressed the default regression intercept with the ~ 0 + syntax and the only two predictors were pt and peroid_factor. Both were saved as factor variables. Functionally, that’s why period_factor worked as \\(\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}\\), a series of 5 dummy variables with no reference category. The same basic thing goes for pt. Because pt was a factor used in a model formula with no conventional intercept, it acted as if it was a series of 2 dummy variables with no reference category. Thus, we might rewrite the model equation for fit6 as \\[\\operatorname{logit} h(t_{ij}) = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_0 \\text{PT}_{0i} + \\beta_1 \\text{PT}_{1i} ].\\] And since we’re practicing fitting these models as Bayesians, the fit6 equation with a fuller expression of the likelihood and the priors looks like \\[ \\begin{align*} \\text{event}_{ij} &amp; = \\operatorname{Binomial}(n = \\text{trials}_{ij}, p_{ij}) \\\\ \\operatorname{logit} (p_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_0 \\text{PT}_{0i} + \\beta_1 \\text{PT}_{1i} ] \\\\ \\alpha_7, \\alpha_8, \\dots, \\alpha_{12} &amp; \\sim \\operatorname{Normal}(0, 4) \\\\ \\beta_0 \\text{ and } \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 4), \\end{align*} \\] where we’re describing the model in terms of the criterion, event, rather than in terms of \\(h(t_{ij})\\). And what is the criterion, event? It’s a vector of counts. The binomial likelihood allows us to model vectors of counts in terms of the number of trials, as indexed by our trials vector, and the (conditional) probability of a “1” in a given trial. In this context, \\(h(t_{ij}) = p_{ij}\\). 11.2.1 What do the parameters represent? Given our factor coding of pt, our two submodels for the equations in the last section are \\[ \\begin{align*} \\text{when PT = 0: } \\operatorname{logit} h(t_j) &amp; = [\\alpha_7 D_7 + \\alpha_8 D_8 + \\cdots + \\alpha_{12} D_{12}] + \\beta_0 \\\\ \\text{when PT = 1: } \\operatorname{logit} h(t_j) &amp; = [\\alpha_7 D_7 + \\alpha_8 D_8 + \\cdots + \\alpha_{12} D_{12}] + \\beta_1, \\end{align*} \\] where we used Singer and Willett’s simplified notation and dropped all the \\(i\\) subscripts and most of the \\(j\\) subscripts. 11.2.2 An alternative representation of the model. In the previous sections, we expressed the model in terms of the logit of the criterion or the \\(p\\) parameter of the likelihood. Another strategy is the express the criterion (or \\(p\\)) in its natural metric and put the nonlinear portion on the right side of the equation. If we consider the generic discrete conditional hazard function, that would follow the form \\[ h(t_{ij}) = \\frac{1}{1 + e^{-([\\alpha_1 D_{1ij} + \\alpha_2 D_{2ij} + \\cdots + \\alpha_J D_{Jij}] + [\\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\cdots + \\beta_P X_{Pij}])}}. \\] This is just a particular kind of logistic regression model. It also clarifies that “by specifying a linear relationship between predictors and logit hazard we imply a nonlinear relationship between predictors and raw hazard” (p. 377, emphasis in the original). We can explore what that might look like with our version of Figure 11.4. Here we continue to use fit6, but this time we’ll save the output from fitted() before plotting. f &lt;- fitted(fit11.6, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(sex_aggregated) f ## Estimate Est.Error Q2.5 Q97.5 period pt event no_event total period_center peroid_factor ## 1 -2.9970126 0.3176158 -3.6294093 -2.3832815 7 0 2 70 72 -2.5 7 ## 2 -2.1180003 0.2735893 -2.6848898 -1.6111410 7 1 13 95 108 -2.5 7 ## 3 -3.7684632 0.4287806 -4.6415348 -2.9903587 8 0 2 68 70 -1.5 8 ## 4 -2.8894508 0.3951568 -3.7108161 -2.1724589 8 1 5 90 95 -1.5 8 ## 5 -2.3082801 0.2743845 -2.8655483 -1.7835310 9 0 8 60 68 -0.5 9 ## 6 -1.4292677 0.2334189 -1.9071366 -0.9685727 9 1 16 74 90 -0.5 9 ## 7 -1.8435758 0.2576237 -2.3635566 -1.3492209 10 0 8 52 60 0.5 10 ## 8 -0.9645635 0.2215530 -1.4188340 -0.5291742 10 1 21 53 74 0.5 10 ## 9 -1.6761430 0.2710102 -2.2258520 -1.1397280 11 0 10 42 52 1.5 11 ## 10 -0.7971307 0.2476662 -1.2959928 -0.3393064 11 1 15 38 53 1.5 11 ## 11 -1.2014513 0.2681393 -1.7501715 -0.6750968 12 0 8 34 42 2.5 12 ## 12 -0.3224389 0.2584378 -0.8478826 0.1663958 12 1 18 20 38 2.5 12 Make the subplots. # logit(hazard) p1 &lt;- f %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;logit(hazard)&quot;, y = NULL) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # odds p2 &lt;- f %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = exp) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;odds&quot;, y = NULL) + coord_cartesian(ylim = c(0, .8)) + theme(legend.position = &quot;none&quot;) # hazard p3 &lt;- f %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = inv_logit_scaled) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;hazard (i.e., probability)&quot;, y = NULL) + coord_cartesian(ylim = c(0, .5)) + theme(legend.position = &quot;none&quot;) Combine the subplots with patchwork. (p1 / p2 / p3) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) The mutate_at() conversions we made for p2 and p3 were based on the guidelines in Table 11.2. Those were: tibble(`original scale` = c(&quot;logit&quot;, &quot;odds&quot;, &quot;logit&quot;), `desired scale` = c(&quot;odds&quot;, &quot;probability&quot;, &quot;probability&quot;), transformation = c(&quot;exp(logit)&quot;, &quot;odds / (1 + odds)&quot;, &quot;1 / (1 + exp(-1 * logit))&quot;)) %&gt;% flextable::flextable() %&gt;% flextable::width(width = c(1.5, 1.5, 2)) .cl-01979a08{}.cl-017baf5a{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0191d190{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0191ee14{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0191ee1e{width:2in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0191ee28{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0191ee32{width:2in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0191ee33{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0191ee3c{width:2in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}original scaledesired scaletransformationlogitoddsexp(logit)oddsprobabilityodds / (1 + odds)logitprobability1 / (1 + exp(-1 * logit)) We accomplished the transformation in the bottom row with the brms::inv_logit_scaled() function. 11.3 Fitting a discrete-time hazard model to data As Singer and Willett wrote, “with data collected on a random sample of individuals from a target population, you can easily fit a discrete-time hazard model, estimate its parameters using maximum likelihood methods, and evaluate goodness-of-fit” (pp. 378–379. As we’ve already demonstrated, you can fit them with Bayesian software, too. Though we’ll be focusing on brms, you might also want to check out the rstanarm package, about which you can learn more from Brilleman, Elci, Novik, and Wolfe’s (2020) preprint, Bayesian survival analysis Using the rstanarm R package, Brilleman’s (2019) vignette, Estimating survival (time-to-event) models with rstanarm, and the Survival models in rstanarm thread in the Stan forums. 11.3.1 Adding predictors to the person-period data set. At the beginning of section 11.1.2.2, we already loaded a version of the person-period data including the discrete-time dummies. It has our substantive predictors pt and pas, too. We saved it as sex_pp. Here’s a glimpse(). sex_pp %&gt;% glimpse() ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, … ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8… ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, … ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, … ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, … ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, … ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, … 11.3.2 Maximum likelihood estimates [and Bayesian posteriors] for the discrete-time hazard model. We’re not going to walk through all the foundational equations for the likelihood and log-likelihood functions (Equations 11.11 through 11.13). For our purposes, just note that “it turns out that the standard logistic regression routines widely available in all major statistical packages, when applied appropriately in the person-period data set, actually provide estimates of the parameters of the discrete-time hazard model” (p. 383, emphasis in the original). Happily, this is what we’ve been doing. Bayesian logistic regression via the binomial likelihood has been our approach. And since we’re Bayesians, the same caveat applies to survival models as applied to the other longitudinal models we fit in earlier chapters. We’re not just maximizing likelihoods, here. Bayes’s formula requires us to multiply the likelihood by the prior. \\[ \\\\underbrace{p(\\theta \\mid d)}_\\text{posterior} \\propto \\underbrace{p(d \\mid \\theta)}_\\text{likelihood} \\; \\underbrace{p(\\theta)}_\\text{prior} \\] 11.3.3 Fitting the discrete-time hazard model to data. In one sense, fitting discrete-hazard models with Bayesian logistic regression is old hat, for us. We’ve been doing that since the end of last chapter. But one thing I haven’t clarified is, up to this point, we have been using the aggregated binomial format. To show what I mean, we might look at the data we used for our last model, fit11.6. sex_aggregated ## # A tibble: 12 × 7 ## period pt event no_event total period_center peroid_factor ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 0 2 70 72 -2.5 7 ## 2 7 1 13 95 108 -2.5 7 ## 3 8 0 2 68 70 -1.5 8 ## 4 8 1 5 90 95 -1.5 8 ## 5 9 0 8 60 68 -0.5 9 ## 6 9 1 16 74 90 -0.5 9 ## 7 10 0 8 52 60 0.5 10 ## 8 10 1 21 53 74 0.5 10 ## 9 11 0 10 42 52 1.5 11 ## 10 11 1 15 38 53 1.5 11 ## 11 12 0 8 34 42 2.5 12 ## 12 12 1 18 20 38 2.5 12 Now recall the formula for the binomial likelihood from the end of last chapter: \\[\\Pr (z \\mid n, p) = \\frac{n!}{z!(n - z)!} p^z (1 - p)^{n - z},\\] where \\(z\\) is the number of cases for which the value is 1, \\(n\\) is the total number of cases, and \\(p\\) is the constant chance of a 1 across cases. We refer to binomial data as aggregated with \\(n &gt; 1\\). Our \\(n\\) vector in the sex_aggregated, total, ranged from 38 to 108. Accordingly, our \\(z\\) vector, event, was always some value equal or lower to that in the same row for total. The person-period data, sex_pp, contain the same information but in a different format. Instead, each event cell only takes on a value of 0 or 1 (i.e., \\(n = 1\\)). If you were to sum up all the values in the total column of the sex_aggregated data, you’d return 822. sex_aggregated %&gt;% summarise(sum = sum(total)) ## # A tibble: 1 × 1 ## sum ## &lt;int&gt; ## 1 822 This is also the total number of rows in the sex_pp data. sex_pp %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 822 It’s also the case that when \\(n = 1\\), the right side of the equation for the binomial function reduces to \\[p^z (1 - p)^{1 - z}.\\] Whether you are working with aggregated or un-aggregated data, both are suited to fit logistic regression models with the binomial likelihood. Just specify the necessary information in the model syntax. For brms, the primary difference is how you use the trials() function. When we fit our logistic regression models using the aggregated data, we specified trials(total), which informed the brm() function what the \\(n\\) values were. In the case of unaggregated binomial data, we can just state trials(1). Each cell is the outcome \\(z\\) for a single trial. Before we fit the models, we might talk a bit about priors. When we fit the first model of this kind at the end of Chapter 10, we just used prior(normal(0, 4), class = b) without comment. Recall we’re modeling probabilities in the log-odds space. In Section 11.1.2.1 we used a plot to compare probability values to their log-odds counterparts. Let’s take a more focused look. tibble(log_odds = -8:8) %&gt;% mutate(p = inv_logit_scaled(log_odds)) %&gt;% ggplot(aes(x = log_odds, y = p)) + geom_hline(yintercept = 0:5 / 5, color = &quot;white&quot;) + geom_point() + scale_x_continuous(breaks = -8:8) + scale_y_continuous(breaks = 0:5 / 5) + theme(panel.grid = element_blank()) When \\(\\operatorname{log-odds} p = 0\\), \\(p = .5\\). Once \\(\\operatorname{log-odds} p\\) approaches the \\(\\mp 4\\) neighborhood, the corresponding values for \\(p\\) asymptote at the boundaries \\([0, 1]\\). By using a \\(\\operatorname{Normal} (0, 4)\\) prior for \\(\\operatorname{log-odds} p\\), we’re putting bulk of the prior mass in the \\(\\operatorname{log-odds} p\\) space between, say, -8 and 8. In the absence of other information, this might be a good place to start. A little further down, we’ll reexamine this set-up. For now, here’s how to use brm() to fit Models A through D from page 385. library(brms) # model a fit11.7 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.07&quot;) # model b fit11.8 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.08&quot;) # model c fit11.9 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pas, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.09&quot;) # model d fit11.10 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.10&quot;) 11.4 Interpreting parameter estimates Here are the model summaries in bulk. print(fit11.7) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 ## Data: sex_pp (Number of observations: 822) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.43 0.27 -2.99 -1.92 1.00 6107 3046 ## d8 -3.15 0.39 -3.96 -2.45 1.00 5698 3058 ## d9 -1.73 0.23 -2.19 -1.29 1.00 6071 3032 ## d10 -1.30 0.21 -1.72 -0.90 1.00 5271 2893 ## d11 -1.18 0.24 -1.64 -0.73 1.00 5793 3056 ## d12 -0.74 0.24 -1.20 -0.28 1.00 6444 3114 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.8) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt ## Data: sex_pp (Number of observations: 822) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -3.02 0.32 -3.65 -2.41 1.00 3073 2485 ## d8 -3.73 0.43 -4.66 -2.95 1.00 3922 2734 ## d9 -2.29 0.28 -2.87 -1.78 1.00 2565 2672 ## d10 -1.83 0.26 -2.34 -1.34 1.00 2747 2757 ## d11 -1.65 0.27 -2.21 -1.14 1.00 3296 2575 ## d12 -1.18 0.27 -1.72 -0.67 1.00 3041 2491 ## pt 0.86 0.22 0.43 1.29 1.00 1908 2457 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.9) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pas ## Data: sex_pp (Number of observations: 822) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.49 0.28 -3.05 -1.97 1.00 6497 3093 ## d8 -3.19 0.39 -4.02 -2.48 1.00 6204 2791 ## d9 -1.75 0.23 -2.22 -1.32 1.00 6580 2693 ## d10 -1.30 0.21 -1.71 -0.90 1.00 6685 2950 ## d11 -1.15 0.23 -1.61 -0.71 1.00 7435 2591 ## d12 -0.64 0.24 -1.14 -0.17 1.00 7176 3290 ## pas 0.45 0.11 0.22 0.66 1.00 7266 3341 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit11.10) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas ## Data: sex_pp (Number of observations: 822) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.91 0.33 -3.58 -2.30 1.00 2602 2358 ## d8 -3.61 0.42 -4.47 -2.86 1.00 3035 2921 ## d9 -2.15 0.28 -2.72 -1.63 1.00 2148 2448 ## d10 -1.69 0.27 -2.23 -1.17 1.00 2187 2907 ## d11 -1.52 0.28 -2.11 -0.98 1.00 2639 2409 ## d12 -1.01 0.28 -1.58 -0.45 1.00 2571 2681 ## pt 0.64 0.24 0.16 1.11 1.01 1358 1532 ## pas 0.30 0.12 0.06 0.54 1.00 2849 2810 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Although the text distinguishes between \\(\\alpha\\) and \\(\\beta\\) parameters (i.e., intercept and slope parameters, respectively), our brms output makes no such distinction. These are all of class = b, population-level \\(\\beta\\) parameters. When viewed in bulk, all those print() calls yield a lot of output. We can arrange the parameter summaries similar to those in Table 11.3 with a little tricky wrangling. tibble(model = str_c(&quot;model &quot;, letters[1:4]), fit = str_c(&quot;fit11.&quot;, 7:10)) %&gt;% mutate(f = map(fit, ~ get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(f) %&gt;% mutate(e_sd = str_c(round(Estimate, digits = 2), &quot; (&quot;, round(Est.Error, digits = 2), &quot;)&quot;)) %&gt;% select(model, parameter, e_sd) %&gt;% pivot_wider(names_from = model, values_from = e_sd) %&gt;% flextable::flextable() %&gt;% flextable::width(width = 1) .cl-02dd06be{}.cl-02d26592{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-02d7cc12{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-02d7e576{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02d7e58a{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02d7e58b{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}parametermodel amodel bmodel cmodel dd7-2.43 (0.27)-3.02 (0.32)-2.49 (0.28)-2.91 (0.33)d8-3.15 (0.39)-3.73 (0.43)-3.19 (0.39)-3.61 (0.42)d9-1.73 (0.23)-2.29 (0.28)-1.75 (0.23)-2.15 (0.28)d10-1.3 (0.21)-1.83 (0.26)-1.3 (0.21)-1.69 (0.27)d11-1.18 (0.24)-1.65 (0.27)-1.15 (0.23)-1.52 (0.28)d12-0.74 (0.24)-1.18 (0.27)-0.64 (0.24)-1.01 (0.28)pt0.86 (0.22)0.64 (0.24)pas0.45 (0.11)0.3 (0.12) 11.4.1 The time indicators. As a group, the \\(\\hat \\alpha\\)s are [Bayesian] estimates for the baseline logit hazard function. The amount and direction of variation in their values describe the shape of this function and tell us whether risk increases, decreases, or remains steady over time. (p. 387) A coefficient plot might help us get a sense of that across the four models. tibble(model = str_c(&quot;model &quot;, letters[1:4]), fit = str_c(&quot;fit11.&quot;, 7:10)) %&gt;% mutate(f = map(fit, ~ get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(f) %&gt;% filter(str_detect(parameter, &quot;d&quot;)) %&gt;% mutate(parameter = factor(str_remove(parameter, &quot;b_&quot;), levels = str_c(&quot;d&quot;, 12:7))) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = parameter)) + geom_pointrange(fatten = 2.5) + labs(x = &quot;posterior (log-odds scale)&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~ model, nrow = 1) “The fairly steady increase over time in the magnitude of the \\(\\hat \\alpha\\)s in each model [in the coefficient plot] shows that, in this sample of boys, the risk of first intercourse increases over time” (p. 387). When comparing the \\(\\hat \\alpha\\)s across models, it’s important to recall that the presence/absence of substantive covariates means each model has a different baseline group. Because they were in the log-odds scale, the model output and our coefficient plot can be difficult to interpret. With the brms::inv_logit_scaled(), we can convert the \\(\\hat \\alpha\\)s to the hazard (i.e., probability) metric. tibble(model = str_c(&quot;model &quot;, letters[1:4]), fit = str_c(&quot;fit11.&quot;, 7:10)) %&gt;% mutate(f = map(fit, ~ get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(f) %&gt;% filter(str_detect(parameter, &quot;d&quot;)) %&gt;% mutate(parameter = factor(str_remove(parameter, &quot;b_&quot;), levels = str_c(&quot;d&quot;, 12:7))) %&gt;% mutate_at(vars(Estimate:Q97.5), .funs = inv_logit_scaled) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = parameter)) + geom_pointrange(fatten = 2.5) + labs(x = &quot;posterior (hazard scale)&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) + facet_wrap(~ model, nrow = 1) Building further, here’s our version of Table 11.4. fixef(fit11.7) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;predictor&quot;) %&gt;% mutate(`time period` = str_remove(predictor, &quot;d&quot;) %&gt;% as.double()) %&gt;% select(`time period`, predictor, Estimate) %&gt;% mutate(`fitted odds` = exp(Estimate), `fitted hazard` = inv_logit_scaled(Estimate)) %&gt;% mutate_if(is.double, round, digits = 4) %&gt;% flextable::flextable() %&gt;% flextable::width(width = 1) .cl-037072e6{}.cl-03675404{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-036b4c9e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-036b4ca8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-036b6788{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-036b679c{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-036b679d{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-036b67a6{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-036b67a7{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-036b67b0{width:1in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}time periodpredictorEstimatefitted oddsfitted hazard7d7-2.42530.08840.08138d8-3.15040.04280.04119d9-1.72940.17740.150710d10-1.29730.27330.214611d11-1.17590.30860.235812d12-0.74190.47620.3226 11.4.2 Dichotomous substantive predictors. Here’s the summary for pt from fit11.8 (i.e., Model B). fixef(fit11.8)[&quot;pt&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.8631546 0.2188621 0.4319023 1.2944748 If we take the anti-log (i.e., exponentiate) of that coefficient, we’ll return an odds ratio. Here’s the conversion with just the posterior mean. fixef(fit11.8)[&quot;pt&quot;, 1] %&gt;% exp() ## [1] 2.370627 To get a better sense of the conversion, here it is in a plot. library(tidybayes) posterior_samples(fit11.8) %&gt;% transmute(`log-odds` = b_pt, `hazard ratio` = exp(b_pt)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;log-odds&quot;, &quot;hazard ratio&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior for pt&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) This tells us that, in every grade, the estimated odds of first intercourse are nearly two and one half times higher for boys who experienced a parenting transition in comparison to boys raised with both biological parents. In substantive terms, an odds ratio of this magnitude represents a substantial, and potentially important, effect. (p. 398) To reframe the odds ratio in terms of the other group (i.e., pt == 0), take the reciprocal. 1 / exp(fixef(fit11.8)[7, 1]) ## [1] 0.4218293 “This tells us that the estimated odds of first intercourse for boys who did not experience a parenting transition are approximately 40% of the odds for boys who did. These complimentary ways of reporting effect sizes are equivalent” (p. 389) 11.4.3 Continuous substantive predictors. Here’s the conditional effect of pas from fit11.9 (i.e., Model C). fixef(fit11.9)[&quot;pas&quot;, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.4451985 0.1128906 0.2244570 0.6648630 To understand pas, our measure of parental antisocial behavior, it will help to look at its range. range(sex_pp$pas) ## [1] -1.716180 2.781413 Exponentiating (i.e., taking the anti-log) the posterior of a continuous predictor is a legitimate way to convert it to a hazard ratio. as_draws_df(fit11.9) %&gt;% transmute(`log-odds` = b_pas, `hazard ratio` = exp(b_pas)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;log-odds&quot;, &quot;hazard ratio&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior for pas&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Here’s how to compute the hazard ratio for a 2-unit difference in pas. exp(fixef(fit11.9)[7, 1] * 2) ## [1] 2.436097 Here’s what that looks like in a plot. as_draws_df(fit11.9) %&gt;% transmute(`log-odds` = b_pas, `hazard ratio` = exp(b_pas), `hr for a 2-unit difference` = exp(b_pas * 2)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;log-odds&quot;, &quot;hazard ratio&quot;, &quot;hr for a 2-unit difference&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;marginal posterior for pas&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) 11.4.4 Polytomous substantive predictors. Unfortunately, neither the sex nor the sex_pp data sets contain the polytomous version of pt as Singer described in this section. However, we can simulate a similar set of dummy variables which will allow us to trace the basic steps in Singer and Willett’s workflow. Since we’ve been working with the sex_pp data for the past few models, we’ll continue using it here. However, this creates a minor challenge. What we want to do is use the sample() function to randomly assign cases with values of 1, 2, or 3 conditional on whether pt == 0. The catch is, we need to make sure that random value is constant for each case. Our solution will be to first nest the data such that each case only has one row. Here’s what that looks like. set.seed(11) sex_pp &lt;- sex_pp %&gt;% nest(data = c(period, event, d7, d8, d9, d10, d11, d12, pas)) %&gt;% mutate(random = sample(1:3, size = n(), replace = T)) %&gt;% mutate(pt_cat = ifelse(pt == 0, pt, random)) %&gt;% mutate(pt1 = ifelse(pt_cat == 1, 1, 0), pt2 = ifelse(pt_cat == 2, 1, 0), pt3 = ifelse(pt_cat == 3, 1, 0)) %&gt;% select(id, pt, random, pt_cat:pt3, data) sex_pp %&gt;% head() ## # A tibble: 6 × 8 ## id pt random pt_cat pt1 pt2 pt3 data ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 1 0 2 0 0 0 0 &lt;tibble [3 × 9]&gt; ## 2 2 1 2 2 0 1 0 &lt;tibble [6 × 9]&gt; ## 3 3 0 1 0 0 0 0 &lt;tibble [6 × 9]&gt; ## 4 5 1 1 1 1 0 0 &lt;tibble [6 × 9]&gt; ## 5 6 0 1 0 0 0 0 &lt;tibble [5 × 9]&gt; ## 6 7 1 2 2 0 1 0 &lt;tibble [3 × 9]&gt; Here are the number of cases for each of the four levels of our pseudovariable pt_cat. sex_pp %&gt;% count(pt_cat) ## # A tibble: 4 × 2 ## pt_cat n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 72 ## 2 1 41 ## 3 2 30 ## 4 3 37 Our breakdown isn’t exactly the same as the one in the text (p. 391), but we’re in the ballpark. Before we’re ready to fit our next model, we’ll need to unnest() the data, which will transform sex_pp back into the familiar long format. sex_pp &lt;- sex_pp %&gt;% unnest(data) Before we fit the model with the pt* dummies, let’s backup and talk about priors, again. When we fit the last four models, our discussion about priors for \\(p\\) focused on the posterior implications for those parameters in the log-odds space. Things get odd when we consider the implications in the probability space. To demonstrate, let’s simulate from \\(\\operatorname{Normal}(0, 4)\\) and see what it looks like when we transform the draws back into the probability metric. set.seed(11) tibble(log_odds = rnorm(1e6, mean = 0, sd = 4)) %&gt;% mutate(p = inv_logit_scaled(log_odds)) %&gt;% ggplot(aes(x = p)) + geom_histogram(bins = 50) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Perhaps unexpectedly, our log-odds \\(\\operatorname{Normal}(0, 4)\\) prior ended up bunching up the prior mass at the boundaries. Depending on what we want, this may or may not make sense. If we want to regularize the coefficients toward zero in the probability space, something closer to \\(\\operatorname{Normal}(0, 1)\\) would be a better idea. Here’s a look at what happens when we compare three simulations in that range. set.seed(11) tibble(sd = c(2, 1.5, 1)) %&gt;% mutate(log_odds = map(sd, ~rnorm(1e6, mean = 0, sd = .))) %&gt;% unnest(log_odds) %&gt;% mutate(sd = str_c(&quot;sd = &quot;, sd), p = inv_logit_scaled(log_odds)) %&gt;% ggplot(aes(x = p)) + geom_histogram(bins = 50) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ sd) It looks like the log-odds \\(\\operatorname{Normal}(0, 1)\\) gently regularizes \\(p\\) toward .5, but still allows for stronger values. This might be a good prior to use for our substantive covariates, what Singer and Willett referred to as the \\(\\beta\\) parameters. But I don’t know that this makes sense for our \\(\\alpha\\) parameters. If you’ve been following along with the model output, the life tables, and so on, you’ll have noticed those tend to drift toward the lower end of the probability range. Regularizing toward \\(p = .5\\) might not be a good idea. In the absence of good substantive or statistical theory, perhaps it’s best to use a flat prior. The log-odds \\(\\operatorname{Normal} (0, 1.5)\\) prior is nearly flat on the probability space, but it does still push the mass away from the boundaries. Maybe we can come up with something better. What if we simulated a large number of draws from the \\(\\operatorname{Uniform}(0, 1)\\) distribution, converted those draws to the log-odds metric, and fit a simple Student’s \\(t\\) model? If we wanted to stay within the Student-\\(t\\) family of priors, of which the normal is a special case, that would give us a sense of the what prior values would approximate a uniform distribution on the probability scale. set.seed(11) dat &lt;- tibble(p = runif(1e5, 0, 1)) %&gt;% mutate(g = log_odds(p)) fit11.11 &lt;- brm(data = dat, family = student, g ~ 1, chains = 4, cores = 4, file = &quot;fits/fit11.11&quot;) Check the summary. print(fit11.11) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: g ~ 1 ## Data: dat (Number of observations: 100000) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.01 -0.01 0.02 1.00 2592 2306 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.57 0.01 1.56 1.58 1.00 2155 2743 ## nu 7.61 0.17 7.28 7.94 1.00 1923 2551 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we can reverse the process. Here’s what it would look like if we simulated from the Student \\(t\\)-distribution based on those posterior means and then converted the results into the probability metric. set.seed(11) tibble(g = rt(1e6, df = 7.61) * 1.57) %&gt;% mutate(p = inv_logit_scaled(g)) %&gt;% ggplot(aes(x = p)) + geom_histogram(bins = 50) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Now here’s what it might look like if fit the next model with the \\(\\operatorname{Normal}(0, 1)\\) prior for the \\(\\beta\\) parameters and \\(\\text{Student-} t (7.61, 0, 1.57)\\) prior for the \\(\\alpha\\) parameters. fit11.12 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt1 + pt2 + pt3, prior = c(prior(student_t(7.61, 0, 1.57), class = b), prior(normal(0, 1), class = b, coef = pt1), prior(normal(0, 1), class = b, coef = pt2), prior(normal(0, 1), class = b, coef = pt3)), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.12&quot;) Here is the model summary. print(fit11.12) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt1 + pt2 + pt3 ## Data: sex_pp (Number of observations: 822) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.84 0.30 -3.47 -2.27 1.00 4056 2801 ## d8 -3.49 0.40 -4.34 -2.79 1.00 5087 2533 ## d9 -2.13 0.26 -2.65 -1.65 1.00 3731 2963 ## d10 -1.66 0.24 -2.15 -1.20 1.00 3672 2985 ## d11 -1.50 0.25 -2.01 -1.01 1.00 4069 2804 ## d12 -1.04 0.26 -1.56 -0.54 1.00 4511 3143 ## pt1 0.47 0.26 -0.03 0.97 1.00 3423 3383 ## pt2 0.52 0.27 -0.01 1.04 1.00 3571 3392 ## pt3 1.01 0.26 0.52 1.53 1.00 3168 2937 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can use the tidy() function and a few lines of wrangling code to make a version of the table in the middle of page 391. fixef(fit11.12)[7:9, ] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;predictor&quot;) %&gt;% mutate(`odds ratio` = exp(Estimate)) %&gt;% select(predictor, Estimate, `odds ratio`) %&gt;% mutate_if(is.double, round, digits = 3) ## predictor Estimate odds ratio ## 1 pt1 0.474 1.606 ## 2 pt2 0.522 1.685 ## 3 pt3 1.013 2.754 Because our data did not include the original values for pt1 through pt3, the results in our table will not match those in the text. We did get pretty close, though, eh? Hopefully this gives a sense of the workflow. 11.5 Displaying fitted hazard and survivor functions This will be an extension of what we’ve already been doing. 11.5.1 A strategy for a single categorical substantive predictor. We can make our version of Table 11.5 like so. To reduce clutter, we will use abbreviated column names. tibble(time = 7:12, alpha = fixef(fit11.8)[1:6, 1], beta = fixef(fit11.8)[7, 1]) %&gt;% mutate(lh0 = alpha, lh1 = alpha + beta) %&gt;% mutate(h0 = inv_logit_scaled(lh0), h1 = inv_logit_scaled(lh1)) %&gt;% mutate(s0 = cumprod(1 - h0), s1 = cumprod(1 - h1)) %&gt;% # this just simplifies the output mutate_if(is.double, round, digits = 4) ## # A tibble: 6 × 9 ## time alpha beta lh0 lh1 h0 h1 s0 s1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 -3.02 0.863 -3.02 -2.15 0.0467 0.104 0.953 0.896 ## 2 8 -3.73 0.863 -3.73 -2.87 0.0234 0.0537 0.931 0.848 ## 3 9 -2.29 0.863 -2.29 -1.43 0.0916 0.193 0.846 0.684 ## 4 10 -1.83 0.863 -1.83 -0.968 0.138 0.275 0.729 0.496 ## 5 11 -1.65 0.863 -1.65 -0.791 0.160 0.312 0.612 0.341 ## 6 12 -1.18 0.863 -1.18 -0.320 0.234 0.421 0.468 0.198 For the alpha and beta columns, we just subset the values from fixef(). The two logit-hazard columns, lh0 and lh1, were simple algebraic transformations of alpha and beta, respectively. To make the two hazard columns, h0 and h1, we applied the inv_logit_scaled() function to lh0 and lh1, respectively. To make the two survival columns, s0 and s1, we applied the cumprod() function to one minus the two hazard columns. Note how all this is based off of the posterior means. There’s enough going on with Table 11.5 that it makes sense to ignore uncertainty But when we’re ready to go beyond table glancing and actually make a plot, we will go beyond posterior means and reintroduce the uncertainty in the model. Two of these plots are quite similar to two of the subplots from Figure 11.4, back in Section 11.2.1. But recall that though those plots were based on fit11.6, which was based on the aggregated data, the plots we are about to make will be based on fit11.8, the analogous model based on the disaggregated person-period data. Regardless of whether the logistic regression model is based on aggregated data, the post-processing approach will involve the fitted() function. However, the specifics of how we use fitted() will differ. For the disaggregated data used to fit fit11.8, here is how we might define the newdata, pump it through the model via fitted(), and wrangle. nd &lt;- crossing(pt = 0:1, period = 7:12) %&gt;% mutate(d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- fitted(fit11.8, newdata = nd, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) f ## Estimate Est.Error Q2.5 Q97.5 pt period d7 d8 d9 d10 d11 d12 ## 1 -3.0158100 0.3226729 -3.6548961 -2.4101270 pt = 0 7 1 0 0 0 0 0 ## 2 -3.7332639 0.4320540 -4.6605608 -2.9515943 pt = 0 8 0 1 0 0 0 0 ## 3 -2.2948031 0.2781025 -2.8705837 -1.7807053 pt = 0 9 0 0 1 0 0 0 ## 4 -1.8307105 0.2561879 -2.3418476 -1.3360186 pt = 0 10 0 0 0 1 0 0 ## 5 -1.6544779 0.2747193 -2.2144784 -1.1401863 pt = 0 11 0 0 0 0 1 0 ## 6 -1.1834418 0.2682602 -1.7184945 -0.6706097 pt = 0 12 0 0 0 0 0 1 ## 7 -2.1526553 0.2819101 -2.7254006 -1.6252754 pt = 1 7 1 0 0 0 0 0 ## 8 -2.8701093 0.4043764 -3.7196783 -2.1488898 pt = 1 8 0 1 0 0 0 0 ## 9 -1.4316484 0.2357858 -1.9315490 -0.9821854 pt = 1 9 0 0 1 0 0 0 ## 10 -0.9675558 0.2256333 -1.4117138 -0.5270430 pt = 1 10 0 0 0 1 0 0 ## 11 -0.7913233 0.2466043 -1.2877582 -0.3239337 pt = 1 11 0 0 0 0 1 0 ## 12 -0.3202872 0.2561600 -0.8262651 0.1804904 pt = 1 12 0 0 0 0 0 1 Here we make and save the upper two panels of Figure 11.6. # logit(hazard) p1 &lt;- f %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;fitted logit(hazard)&quot;, y = NULL) + coord_cartesian(ylim = c(-4, 0)) + theme(legend.background = element_rect(fill = &quot;transparent&quot;), legend.key = element_rect(color = &quot;grey92&quot;), legend.position = c(.1, .825)) # hazard p2 &lt;- f %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = inv_logit_scaled) %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;fitted hazard&quot;, y = NULL) + coord_cartesian(ylim = c(0, .5)) + theme(legend.position = &quot;none&quot;) Before we’re ready to make the last panel, we’ll redo our fitted() work, this time including predicted values for grade 6. nd &lt;- crossing(pt = 0:1, period = 6:12) %&gt;% mutate(d6 = if_else(period == 6, 1, 0), d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- fitted(fit11.8, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt)) f ## Estimate Est.Error Q2.5 Q97.5 pt period d6 d7 d8 d9 d10 d11 d12 ## 1 0.50000000 0.00000000 0.500000000 0.50000000 pt = 0 6 1 0 0 0 0 0 0 ## 2 0.04882018 0.01493324 0.025212098 0.08240372 pt = 0 7 0 1 0 0 0 0 0 ## 3 0.02536701 0.01037586 0.009372481 0.04966121 pt = 0 8 0 0 1 0 0 0 0 ## 4 0.09415150 0.02331905 0.053627034 0.14421607 pt = 0 9 0 0 0 1 0 0 0 ## 5 0.14095429 0.03084918 0.087715959 0.20816556 pt = 0 10 0 0 0 0 1 0 0 ## 6 0.16390446 0.03708717 0.098457842 0.24228622 pt = 0 11 0 0 0 0 0 1 0 ## 7 0.23779436 0.04790263 0.152065178 0.33836034 pt = 0 12 0 0 0 0 0 0 1 ## 8 0.70132350 0.04546198 0.606327819 0.78490363 pt = 1 6 1 0 0 0 0 0 0 ## 9 0.10699011 0.02662588 0.061491062 0.16447861 pt = 1 7 0 1 0 0 0 0 0 ## 10 0.05728882 0.02112107 0.023668020 0.10443502 pt = 1 8 0 0 1 0 0 0 0 ## 11 0.19546013 0.03629156 0.126579224 0.27245837 pt = 1 9 0 0 0 1 0 0 0 ## 12 0.27761544 0.04480749 0.195963894 0.37120682 pt = 1 10 0 0 0 0 1 0 0 ## 13 0.31430028 0.05231257 0.216232504 0.41971737 pt = 1 11 0 0 0 0 0 1 0 ## 14 0.42184774 0.06152185 0.304435381 0.54500051 pt = 1 12 0 0 0 0 0 0 1 The values for grade 6 (i.e., those for when d6 == 1) are nonsensical. The main reason we included d6 in the fitted results and in the nd data is so we’d have the slots in our f object. In the code block below, we’ll fill those slots with the appropriate values (0) and then convert the hazard summaries to the survival (i.e., cumulative probability) metric. f &lt;- f %&gt;% mutate(Estimate = if_else(period == 6, 0, Estimate), Q2.5 = if_else(period == 6, 0, Q2.5), Q97.5 = if_else(period == 6, 0, Q97.5)) %&gt;% group_by(pt) %&gt;% mutate(s = cumprod(1 - Estimate), s_lower = cumprod(1 - Q2.5), s_upper = cumprod(1 - Q97.5)) %&gt;% select(pt:d12, s:s_upper) f %&gt;% glimpse() ## Rows: 14 ## Columns: 12 ## Groups: pt [2] ## $ pt &lt;chr&gt; &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 1&quot;, &quot;pt = 1&quot;, &quot;p… ## $ period &lt;int&gt; 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 10, 11, 12 ## $ d6 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 ## $ d7 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 ## $ d8 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ## $ d9 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0 ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0 ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1 ## $ s &lt;dbl&gt; 1.0000000, 0.9511798, 0.9270512, 0.8397680, 0.7213991, 0.6031585, 0.4597308, 1.0000000, 0.89… ## $ s_lower &lt;dbl&gt; 1.0000000, 0.9747879, 0.9656517, 0.9138667, 0.8337060, 0.7516211, 0.6373257, 1.0000000, 0.93… ## $ s_upper &lt;dbl&gt; 1.00000000, 0.91759628, 0.87202734, 0.74626698, 0.59091990, 0.44774815, 0.29624793, 1.000000… Make and save the final panel. # save the interpolated median lifetime values imls &lt;- c(make_iml(lt %&gt;% filter(pt == &quot;pt = 0&quot;)), make_iml(lt %&gt;% filter(pt == &quot;pt = 1&quot;))) # hazard p3 &lt;- f %&gt;% ggplot(aes(x = period, group = pt, fill = pt, color = pt)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_segment(x = imls[1], xend = imls[1], y = -Inf, yend = .5, color = &quot;white&quot;, linetype = 2) + geom_segment(x = imls[2], xend = imls[2], y = -Inf, yend = .5, color = &quot;white&quot;, linetype = 2) + geom_ribbon(aes(ymin = s_lower, ymax = s_upper), linewidth = 0, alpha = 1/6) + geom_line(aes(y = s)) + scale_y_continuous(NULL, breaks = c(0, .5, 1)) + labs(subtitle = &quot;fitted survival probability&quot;) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;none&quot;) Combine the subplots to finish off our version of Figure 11.6. (p1 / p2 / p3) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank()) Here is the breakdown of what percentage of boys will still be virgins at grades 9 and 12, based on pt status, as indicated by fit11.8. f %&gt;% filter(period %in% c(9, 12)) %&gt;% mutate_if(is.double, ~ (. * 100) %&gt;% round(digits = 0)) %&gt;% mutate(`percent virgins` = str_c(s, &quot; [&quot;, s_lower, &quot;, &quot;, s_upper, &quot;]&quot;)) %&gt;% select(period, pt, `percent virgins`) %&gt;% arrange(period) ## # A tibble: 4 × 3 ## # Groups: pt [2] ## period pt `percent virgins` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 9 pt = 0 84 [91, 75] ## 2 9 pt = 1 68 [80, 54] ## 3 12 pt = 0 46 [64, 30] ## 4 12 pt = 1 19 [35, 9] 11.5.2 Extending this strategy to multiple predictors (some of which are continuous). It is easy to display fitted hazard and survivor functions for model involving multiple predictor by extending these ideas in a straightforward manner. Instead of plotting one fitted function for each predictor value, select several prototypical predictor values (using strategies presented in section 4.5.3 and plot fitted functions for combinations of these values. (p. 394, emphasis in the original) We’ll be focusing on fit11.10, which includes both pt and sas as substantive predictors. pt only takes two values, 0 and 1. For pas, we’ll use the conventional -1, 0, and 1. Here’s the fitted()-related code. nd &lt;- crossing(pt = 0:1, pas = -1:1) %&gt;% expand(nesting(pt, pas), period = 6:12) %&gt;% mutate(d6 = if_else(period == 6, 1, 0), d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- fitted(fit11.10, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) head(f) ## Estimate Est.Error Q2.5 Q97.5 pt pas period d6 d7 d8 d9 d10 d11 d12 ## 1 0.42502258 0.030246749 0.367418550 0.48511371 0 -1 6 1 0 0 0 0 0 0 ## 2 0.04077901 0.013144117 0.019410454 0.07067494 0 -1 7 0 1 0 0 0 0 0 ## 3 0.02119052 0.008558962 0.008440147 0.04130416 0 -1 8 0 0 1 0 0 0 0 ## 4 0.08141420 0.020968559 0.046732027 0.12872256 0 -1 9 0 0 0 1 0 0 0 ## 5 0.12326596 0.029408292 0.072767576 0.18858903 0 -1 10 0 0 0 0 1 0 0 ## 6 0.14293494 0.034472676 0.082039623 0.21718230 0 -1 11 0 0 0 0 0 1 0 Make the two subplots. # logit(hazard) p1 &lt;- f %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt), pas = str_c(&quot;pas = &quot;, pas)) %&gt;% mutate(pas = factor(pas, levels = str_c(&quot;pas = &quot;, 1:-1))) %&gt;% filter(period &gt; 6) %&gt;% ggplot(aes(x = period, group = pas, fill = pas, color = pas)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0, alpha = 1/6) + geom_line(aes(y = Estimate)) + labs(subtitle = &quot;fitted logit(hazard)&quot;, y = NULL) + coord_cartesian(ylim = c(0, .5)) + facet_wrap(~ pt) # hazard p2 &lt;- f %&gt;% mutate(Estimate = if_else(period == 6, 0, Estimate), Q2.5 = if_else(period == 6, 0, Q2.5), Q97.5 = if_else(period == 6, 0, Q97.5)) %&gt;% mutate(pt = str_c(&quot;pt = &quot;, pt), pas = str_c(&quot;pas = &quot;, pas)) %&gt;% mutate(pas = factor(pas, levels = str_c(&quot;pas = &quot;, 1:-1))) %&gt;% group_by(pt, pas) %&gt;% mutate(s = cumprod(1 - Estimate), s_lower = cumprod(1 - Q2.5), s_upper = cumprod(1 - Q97.5)) %&gt;% ggplot(aes(x = period, group = pas, fill = pas, color = pas)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_ribbon(aes(ymin = s_lower, ymax = s_upper), linewidth = 0, alpha = 1/6) + geom_line(aes(y = s)) + scale_y_continuous(NULL, breaks = c(0, .5, 1)) + labs(subtitle = &quot;fitted survival probability&quot;) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ pt) Combine the subplots to make our version of Figure 11.7. ((p1 / p2) &amp; scale_fill_viridis_d(NULL, option = &quot;D&quot;, end = .8, direction = -1) &amp; scale_color_viridis_d(NULL, option = &quot;D&quot;, end = .8, direction = -1) &amp; scale_x_continuous(&quot;Grade&quot;, breaks = 6:12, limits = c(6, 12)) &amp; theme(panel.grid = element_blank())) + plot_layout(guides = &quot;collect&quot;) Here we departed from the text a bit by separating the subplots by pt status. They’re already cluttered enough as is. 11.5.3 Two cautions when interpreting fitted hazard and survivor functions. Beware of inferring statistical interaction of a substantive predictor and time when examining plots if fitted hazard and survivor functions. The root of this difficulty is in our use of a link function. Because the model expresses the linear effect of the predictor on logit hazard, you cannot draw a conclusion about the stability of an effect using graphs plotted on a raw hazard scale. In fact, the logic works in the opposite direction. If the size of the gap between fitted hazard functions is constant over time, [the] effect of the predictor must vary over time! (pp. 396-397, emphasis in the original) Also, please don’t confuse plots of fitted values with descriptive sample-based plots. Hopefully our inclusion of 95% intervals helps prevent this. 11.6 Comparing models using deviance statistics and information criteria We now introduce two important questions that we usually address before interpreting parameters and displaying results: Which of the alternative models fits better: Might a predictor’s observed effect be the result of nothing more than sampling variation? (p. 397) Much of the material in this section will be a refresher from the material we covered in Section 4.6. 11.6.1 The deviance statistic. The log-likelihood, LL, is a summary statistic routinely output (in some form) by any program that provides ML estimates. As discussed in section 4.6, its relative magnitude across a series of models fit to the same set of data can be informative (although its absolute magnitude is not). The larger the LL statistic, the better the fit. (pp. 397–398) Note that in some form part. Frequentist software typically returns the LL for a given model as a single value. As we learned way back in Section 4.6, we can use the log_lik() function to get the LL information from our brms fits. However, form the brms reference manual we discover log_lik() returns an “S x N matrix containing the pointwise log-likelihood samples, where S is the number of samples and N is the number of observations in the data” (p. 112). Using fit11.7 as a test case, here’s what that looks like. log_lik(fit11.7) %&gt;% str() ## num [1:4000, 1:822] -0.0988 -0.0798 -0.088 -0.0665 -0.0827 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL To compute the LL for each HMC iteration, you sum across the rows. Deviance is that sum multiplied by -2. Here’s that in a tibble. ll &lt;- fit11.7 %&gt;% log_lik() %&gt;% as_tibble(.name_repair = ~ str_c(&quot;c&quot;, 1:822)) %&gt;% mutate(ll = rowSums(.)) %&gt;% mutate(deviance = -2 * ll) %&gt;% select(ll, deviance, everything()) ll ## # A tibble: 4,000 × 824 ## ll deviance c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -330. 660. -0.0988 -0.0477 -2.03 -0.0988 -0.0477 -0.141 -0.252 -0.225 -0.234 -0.0988 -0.0477 -0.141 ## 2 -328. 655. -0.0798 -0.0435 -1.91 -0.0798 -0.0435 -0.160 -0.201 -0.348 -0.462 -0.0798 -0.0435 -0.160 ## 3 -331. 661. -0.0880 -0.0230 -1.86 -0.0880 -0.0230 -0.170 -0.210 -0.298 -0.620 -0.0880 -0.0230 -0.170 ## 4 -333. 667. -0.0665 -0.0525 -2.35 -0.0665 -0.0525 -0.0998 -0.182 -0.343 -0.588 -0.0665 -0.0525 -0.0998 ## 5 -337. 675. -0.0827 -0.0564 -2.33 -0.0827 -0.0564 -0.102 -0.140 -0.440 -0.556 -0.0827 -0.0564 -0.102 ## 6 -328. 657. -0.0779 -0.0514 -2.24 -0.0779 -0.0514 -0.113 -0.205 -0.237 -0.376 -0.0779 -0.0514 -0.113 ## 7 -329. 658. -0.0647 -0.0597 -2.03 -0.0647 -0.0597 -0.140 -0.201 -0.358 -0.359 -0.0647 -0.0597 -0.140 ## 8 -327. 654. -0.0931 -0.0428 -1.72 -0.0931 -0.0428 -0.198 -0.241 -0.317 -0.349 -0.0931 -0.0428 -0.198 ## 9 -327. 653. -0.0934 -0.0483 -2.04 -0.0934 -0.0483 -0.139 -0.254 -0.278 -0.443 -0.0934 -0.0483 -0.139 ## 10 -329. 658. -0.0837 -0.0331 -2.19 -0.0837 -0.0331 -0.119 -0.191 -0.332 -0.363 -0.0837 -0.0331 -0.119 ## # ℹ 3,990 more rows ## # ℹ 810 more variables: c13 &lt;dbl&gt;, c14 &lt;dbl&gt;, c15 &lt;dbl&gt;, c16 &lt;dbl&gt;, c17 &lt;dbl&gt;, c18 &lt;dbl&gt;, c19 &lt;dbl&gt;, ## # c20 &lt;dbl&gt;, c21 &lt;dbl&gt;, c22 &lt;dbl&gt;, c23 &lt;dbl&gt;, c24 &lt;dbl&gt;, c25 &lt;dbl&gt;, c26 &lt;dbl&gt;, c27 &lt;dbl&gt;, c28 &lt;dbl&gt;, ## # c29 &lt;dbl&gt;, c30 &lt;dbl&gt;, c31 &lt;dbl&gt;, c32 &lt;dbl&gt;, c33 &lt;dbl&gt;, c34 &lt;dbl&gt;, c35 &lt;dbl&gt;, c36 &lt;dbl&gt;, c37 &lt;dbl&gt;, ## # c38 &lt;dbl&gt;, c39 &lt;dbl&gt;, c40 &lt;dbl&gt;, c41 &lt;dbl&gt;, c42 &lt;dbl&gt;, c43 &lt;dbl&gt;, c44 &lt;dbl&gt;, c45 &lt;dbl&gt;, c46 &lt;dbl&gt;, ## # c47 &lt;dbl&gt;, c48 &lt;dbl&gt;, c49 &lt;dbl&gt;, c50 &lt;dbl&gt;, c51 &lt;dbl&gt;, c52 &lt;dbl&gt;, c53 &lt;dbl&gt;, c54 &lt;dbl&gt;, c55 &lt;dbl&gt;, ## # c56 &lt;dbl&gt;, c57 &lt;dbl&gt;, c58 &lt;dbl&gt;, c59 &lt;dbl&gt;, c60 &lt;dbl&gt;, c61 &lt;dbl&gt;, c62 &lt;dbl&gt;, c63 &lt;dbl&gt;, c64 &lt;dbl&gt;, … Since we have distributions for the LL and deviance, we may as well visualize them in a plot. ll %&gt;% pivot_longer(ll:deviance) %&gt;% mutate(name = factor(name, levels = c(&quot;ll&quot;, &quot;deviance&quot;))) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(point_interval = median_qi, .width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Here’s how to compute the LL and deviance distributions for each of our four models, fit11.7 through fit11.10, in bulk. ll &lt;- tibble(model = str_c(&quot;model &quot;, letters[1:4]), name = str_c(&quot;fit11.&quot;, 7:10)) %&gt;% mutate(fit = map(name, get)) %&gt;% mutate(ll = map(fit, ~log_lik(.) %&gt;% data.frame() %&gt;% transmute(ll = rowSums(.)))) %&gt;% select(-fit) %&gt;% unnest(ll) %&gt;% mutate(deviance = -2 * ll) ll %&gt;% glimpse() ## Rows: 16,000 ## Columns: 4 ## $ model &lt;chr&gt; &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;model a&quot;, &quot;mo… ## $ name &lt;chr&gt; &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fit11.7&quot;, &quot;fi… ## $ ll &lt;dbl&gt; -329.7851, -327.7458, -330.6678, -333.3882, -337.4965, -328.3698, -328.9242, -326.9331, -32… ## $ deviance &lt;dbl&gt; 659.5702, 655.4915, 661.3355, 666.7763, 674.9931, 656.7396, 657.8484, 653.8662, 653.2041, 6… Now plot the LL and deviance distributions for each. ll %&gt;% pivot_longer(ll:deviance, names_to = &quot;statistic&quot;) %&gt;% mutate(statistic = factor(statistic, levels = c(&quot;ll&quot;, &quot;deviance&quot;))) %&gt;% ggplot(aes(x = value, y = model)) + stat_halfeye(point_interval = median_qi, .width = .95, normalize = &quot;panels&quot;) + labs(x = NULL, y = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ statistic, scales = &quot;free_x&quot;) 11.6.2 Deviance-based hypothesis tests for individual predictors. Singer and Willett wrote: “Comparing deviance statistics for pairs of nested models that differ only by a single substantive predictor permits evaluation of the ‘statistical significance’ of that predictor” (p. 399). I’m just not going to appeal to null-hypothesis significance testing in this project and, as an extension, I am not going to appeal to tests using the \\(\\chi^2\\) distribution. But sure, you could take our deviance distributions and compare them with difference distributions. Singer and Willett made four deviance comparisons in this section. Here’s what that might look like using our deviance distributions. ll %&gt;% select(model, deviance) %&gt;% mutate(iter = rep(1:4000, times = 4)) %&gt;% pivot_wider(names_from = model, values_from = deviance) %&gt;% mutate(`a - b` = `model a` - `model b`, `a - c` = `model a` - `model c`, `c - d` = `model c` - `model d`, `b - d` = `model b` - `model d`) %&gt;% pivot_longer(contains(&quot;-&quot;)) %&gt;% ggplot(aes(x = value, y = name)) + stat_halfeye(point_interval = median_qi, .width = .95, normalize = &quot;panels&quot;) + labs(x = &quot;deviance difference distribution&quot;, y = NULL) + theme(panel.grid = element_blank()) But really, like no one does this with Bayesian models. If you think you have a good theoretical reason to use this approach, do not cite this project as a justification. I do not endorse it. 11.6.3 Deviance-based hypothesis tests for groups of predictors. We won’t be doing this. 11.6.4 Comparing nonnested models using [WAIC and LOO]. Now we return to our preferred methods for model comparison. Use the add_criterion() function to compute the WAIC and LOO and add their output to the model fits. fit11.7 &lt;- add_criterion(fit11.7, c(&quot;loo&quot;, &quot;waic&quot;)) fit11.8 &lt;- add_criterion(fit11.8, c(&quot;loo&quot;, &quot;waic&quot;)) fit11.9 &lt;- add_criterion(fit11.9, c(&quot;loo&quot;, &quot;waic&quot;)) fit11.10 &lt;- add_criterion(fit11.10, c(&quot;loo&quot;, &quot;waic&quot;)) First compare Models B and C (i.e., fit11.8 and fit11.9, respectively). loo_compare(fit11.8, fit11.9, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit11.8 0.0 0.0 -324.5 17.6 7.2 0.6 649.0 35.3 ## fit11.9 -1.1 4.6 -325.6 17.6 7.0 0.6 651.3 35.1 loo_compare(fit11.8, fit11.9, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit11.8 0.0 0.0 -324.5 17.6 7.2 0.6 649.0 35.3 ## fit11.9 -1.1 4.6 -325.6 17.6 7.0 0.6 651.2 35.1 In a head-to-head comparison, Model B is a little better than Model C. However, the standard error for their difference score is about three times as large as the difference itself. This is not a difference I would write home about. Now compare Models A through D. loo_compare(fit11.7, fit11.8, fit11.9, fit11.10, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit11.10 0.0 0.0 -322.8 17.6 8.3 0.6 645.6 35.1 ## fit11.8 -1.7 2.6 -324.5 17.6 7.2 0.6 649.0 35.3 ## fit11.9 -2.8 2.8 -325.6 17.6 7.0 0.6 651.3 35.1 ## fit11.7 -9.4 4.8 -332.2 17.6 6.2 0.5 664.3 35.3 loo_compare(fit11.7, fit11.8, fit11.9, fit11.10, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit11.10 0.0 0.0 -322.8 17.6 8.2 0.6 645.6 35.1 ## fit11.8 -1.7 2.6 -324.5 17.6 7.2 0.6 649.0 35.3 ## fit11.9 -2.8 2.8 -325.6 17.6 7.0 0.6 651.2 35.1 ## fit11.7 -9.4 4.8 -332.2 17.6 6.2 0.5 664.3 35.3 Model D (i.e., fit11.10, the full model) has the best (i.e., lowest) WAIC and LOO estimates. However, the standard errors for its difference scores with the other models is on the large side, particularly for Models B and C. So sure, adding either pt or sas to the model helps a bit and adding them both helps a little more, but neither predictor is a huge winner when you take that model complexity penalty into account. As discussed earlier, we can also compare the models using weights. Here we’ll use the WAIC, LOO, and stacking weights to compare all four models. model_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit11.7 fit11.8 fit11.9 fit11.10 ## 0.000 0.146 0.048 0.805 model_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit11.7 fit11.8 fit11.9 fit11.10 ## 0.000 0.146 0.048 0.806 model_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = &quot;stacking&quot;) %&gt;% round(digits = 3) ## fit11.7 fit11.8 fit11.9 fit11.10 ## 0.000 0.382 0.292 0.326 Model D has the best showing across the three weighting schemes. 11.7 Statistical inference using [uncertainty in the Bayesian posterior] I generally take a model-based approach to Bayesian statistics and I prefer to scrutinize marginal posteriors, consider effect sizes, and use graphical depictions of my models (e.g., posterior predictive checks) over hypothesis testing. Further extending that approach, here, puts us at further odds with the content in the test. In addition, the authors spent some time discussing the asymptotic properties of ML standard errors. Our Bayesian approach is not based on asymptotic theory and we just don’t need to concern ourselves with whether our marginal posteriors are Gaussian. They often are, which is nice. But we summarize our posteriors with percentile-based 95% intervals, we are not presuming they are symmetric or Gaussian. 11.7.1 The Wald chi-square statistic. This will not be our approach. On page 404, Singer and Willett wrote: “The logistic regression analysis routines in all major statistical packages routinely output asymptotic standard errors.” This comment presumes we’re focusing on frequentist packages. Our rough analogue to frequentist standard errors is our Bayesian posterior standard deviations. The authors focused on the two substantive predictors from Model D (i.e., fit11.10). Here’s another look at the brms summary. print(fit11.10) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas ## Data: sex_pp (Number of observations: 822) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d7 -2.91 0.33 -3.58 -2.30 1.00 2602 2358 ## d8 -3.61 0.42 -4.47 -2.86 1.00 3035 2921 ## d9 -2.15 0.28 -2.72 -1.63 1.00 2148 2448 ## d10 -1.69 0.27 -2.23 -1.17 1.00 2187 2907 ## d11 -1.52 0.28 -2.11 -0.98 1.00 2639 2409 ## d12 -1.01 0.28 -1.58 -0.45 1.00 2571 2681 ## pt 0.64 0.24 0.16 1.11 1.01 1358 1532 ## pas 0.30 0.12 0.06 0.54 1.00 2849 2810 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Recall that the second column for our ‘Population-Level Effects’, ‘Est.Error’, contains the standard deviation for each dimension of the posteriors listed (i.e., for each parameter ranging from d7 to pas). This is similar, but distinct from, the frequentist standard error. Instead of focusing on \\(p\\)-values connected to standard errors, why not look at the marginal posteriors directly? draws &lt;- as_draws_df(fit11.10) draws %&gt;% pivot_longer(b_pt:b_pas) %&gt;% ggplot(aes(x = value, y = name, fill = stat(x &gt; 0))) + stat_slab() + scale_fill_manual(values = c(&quot;blue3&quot;, &quot;red3&quot;)) + labs(x = &quot;marginal posterior&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 2)) + theme(panel.grid = element_blank()) If we’d like to keep with the NHST perspective, zero is not a particularly credible value for either parameter. But neither are negative values in generals. In terms of uncertainty, look how much wider the posterior for pt is when compared with pas. And don’t forget that these are on the log-odds scale. Looking at those densities might lead one to ask, Exactly what proportion of the posterior draws for each is zero or below? You can compute that like this. draws %&gt;% pivot_longer(b_pt:b_pas) %&gt;% group_by(name) %&gt;% summarise(`percent zero or below` = 100 * mean(value &lt;= 0)) ## # A tibble: 2 × 2 ## name `percent zero or below` ## &lt;chr&gt; &lt;dbl&gt; ## 1 b_pas 0.725 ## 2 b_pt 0.375 Less that 1% of the draws were zero or below for each. 11.7.2 [Asymmetric credible intervals] for parameters and odds ratios. Whether we use percentile-based credible intervals, as we typically do, or use highest posterior density intervals, neither depends on asymptotic theory nor do they depend on the posterior standard deviation. That is, our Bayesian intervals do not presume the marginal posteriors are Gaussian. Let’s look back at the summary output for fit11.10, this time using the fixef() function. fixef(fit11.10) ## Estimate Est.Error Q2.5 Q97.5 ## d7 -2.9070130 0.3296742 -3.57689882 -2.3023789 ## d8 -3.6113414 0.4193496 -4.47419502 -2.8617981 ## d9 -2.1527055 0.2792362 -2.71607751 -1.6276566 ## d10 -1.6864101 0.2708240 -2.22509953 -1.1692466 ## d11 -1.5164174 0.2829950 -2.10973368 -0.9784899 ## d12 -1.0071412 0.2842673 -1.57657489 -0.4529368 ## pt 0.6390128 0.2419286 0.15816324 1.1104594 ## pas 0.3033367 0.1241903 0.05956277 0.5433073 We find the lower- and upper-limits for our percentile-based Bayesian credible intervals in the last two columns. If you’d like HDIs instead, use the convenience functions from tidybayes. draws %&gt;% pivot_longer(b_pt:b_pas) %&gt;% group_by(name) %&gt;% mean_hdi(value) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_pas 0.303 0.0622 0.544 0.95 mean hdi ## 2 b_pt 0.639 0.148 1.09 0.95 mean hdi We can exponentiate our measures of central tendency (e.g., posterior means) and posterior intervals to transform them out of the log-odds metric an into the odds-ratio metric. Here are the results working directly with fixef(). fixef(fit11.10)[c(&quot;pt&quot;, &quot;pas&quot;), -2] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## pt 1.89461 1.171357 3.035753 ## pas 1.35437 1.061372 1.721692 Keep in mind that fixating on just the 95% intervals is a little NHST-centric. Since we have entire posterior distributions to summarize, we might consider other intervals. Here we use another graphical approach by using tidybayes_statintervalh() to mark off the 10, 30, 50, 70, and 90% intervals for both substantive predictors. Both are in the odds-ratio metric. draws %&gt;% pivot_longer(b_pt:b_pas) %&gt;% mutate(`odds ratio` = exp(value)) %&gt;% ggplot(aes(x = `odds ratio`, y = name)) + stat_interval(size = 5, .width = seq(from = .1, to = .9, by = .2)) + scale_color_grey(&quot;CI level:&quot;, start = .8, end = .2) + scale_x_continuous(breaks = 1:3) + ylab(NULL) + coord_cartesian(xlim = c(1, 3)) + theme(legend.position = &quot;top&quot;, panel.grid = element_blank()) The frequentist 95% confidence intervals are asymmetric when expressed in the odds-ratio metric and so are our various Bayesian intervals. However, the asymmetry in our Bayesian intervals is less noteworthy because there was no explicit assumption of symmetry when they were in the log-odds metric. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.4 brms_2.19.0 Rcpp_1.0.10 patchwork_1.1.2 survival_3.5-5 lubridate_1.9.2 ## [7] forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [13] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] svUnit_1.0.6 shinythemes_1.2.0 splines_4.3.0 later_1.3.1 ## [5] gamm4_0.2-6 xts_0.13.1 lifecycle_1.0.3 StanHeaders_2.26.25 ## [9] processx_3.8.1 lattice_0.21-8 vroom_1.6.3 MASS_7.3-58.4 ## [13] crosstalk_1.2.0 ggdist_3.3.0 backports_1.4.1 magrittr_2.0.3 ## [17] sass_0.4.6 rmarkdown_2.21 jquerylib_0.1.4 httpuv_1.6.11 ## [21] zip_2.3.0 askpass_1.1 pkgbuild_1.4.0 minqa_1.2.5 ## [25] multcomp_1.4-23 abind_1.4-5 TH.data_1.1-2 tensorA_0.36.2 ## [29] sandwich_3.0-2 gdtools_0.3.3 inline_0.3.19 crul_1.4.0 ## [33] xslt_1.4.4 bridgesampling_1.1-2 codetools_0.2-19 DT_0.27 ## [37] xml2_1.3.4 tidyselect_1.2.0 bayesplot_1.10.0 httpcode_0.3.0 ## [41] farver_2.1.1 lme4_1.1-33 matrixStats_0.63.0 stats4_4.3.0 ## [45] base64enc_0.1-3 jsonlite_1.8.4 ellipsis_0.3.2 emmeans_1.8.6 ## [49] systemfonts_1.0.4 projpred_2.5.0 tools_4.3.0 ragg_1.2.5 ## [53] glue_1.6.2 gridExtra_2.3 xfun_0.39 mgcv_1.8-42 ## [57] distributional_0.3.2 loo_2.6.0 withr_2.5.0 fastmap_1.1.1 ## [61] boot_1.3-28.1 fansi_1.0.4 shinyjs_2.1.0 openssl_2.0.6 ## [65] callr_3.7.3 digest_0.6.31 timechange_0.2.0 R6_2.5.1 ## [69] mime_0.12 estimability_1.4.1 textshaping_0.3.6 colorspace_2.1-0 ## [73] gtools_3.9.4 markdown_1.7 threejs_0.3.3 utf8_1.2.3 ## [77] generics_0.1.3 fontLiberation_0.1.0 data.table_1.14.8 prettyunits_1.1.1 ## [81] htmlwidgets_1.6.2 pkgconfig_2.0.3 dygraphs_1.1.1.6 gtable_0.3.3 ## [85] htmltools_0.5.5 fontBitstreamVera_0.1.1 bookdown_0.34 scales_1.2.1 ## [89] posterior_1.4.1 knitr_1.42 rstudioapi_0.14 tzdb_0.4.0 ## [93] reshape2_1.4.4 uuid_1.1-0 coda_0.19-4 checkmate_2.2.0 ## [97] nlme_3.1-162 curl_5.0.0 nloptr_2.0.3 cachem_1.0.8 ## [101] zoo_1.8-12 flextable_0.9.1 parallel_4.3.0 miniUI_0.1.1.1 ## [105] pillar_1.9.0 grid_4.3.0 vctrs_0.6.2 shinystan_2.6.0 ## [109] promises_1.2.0.1 arrayhelpers_1.1-0 xtable_1.8-4 evaluate_0.21 ## [113] mvtnorm_1.1-3 cli_3.6.1 compiler_4.3.0 rlang_1.1.1 ## [117] crayon_1.5.2 rstantools_2.3.1 labeling_0.4.2 ps_1.7.5 ## [121] plyr_1.8.8 stringi_1.7.12 rstan_2.21.8 viridisLite_0.4.2 ## [125] munsell_0.5.0 colourpicker_1.2.0 Brobdingnag_1.2-9 V8_4.3.0 ## [129] fontquiver_0.2.1 Matrix_1.5-4 hms_1.1.3 bit64_4.0.5 ## [133] gfonts_0.2.0 shiny_1.7.4 highr_0.10 igraph_1.4.2 ## [137] RcppParallel_5.1.7 bslib_0.4.2 bit_4.0.5 officer_0.6.2 ## [141] katex_1.4.1 equatags_0.2.0 Footnote References Brilleman, S. (2019). Estimating survival (time-to-event) models with rstanarm. https://github.com/stan-dev/rstanarm/blob/feature/frailty-models/vignettes/surv.Rmd Brilleman, S. L., Elci, E. M., Novik, J. B., &amp; Wolfe, R. (2020). Bayesian survival analysis using the rstanarm R package. https://arxiv.org/abs/2002.09633 Capaldi, D. M., Crosby, L., &amp; Stoolmiller, M. (1996). Predicting the timing of first sexual intercourse for at-risk adolescent males. Child Development, 67(2), 344–359. https://doi.org/10.2307/1131818 Cox, David R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society: Series B (Methodological), 34(2), 187–202. https://doi.org/10.1111/j.2517-6161.1972.tb00899.x Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 R already has a built-in function to convert probabilities to the log-odds scale. Somewhat confusingly, it’s called qlogis(). You can learn more by executing ?qlogis or by browsing through this great blog post by Roman Cheplyaka. It’s generally a good idea to stick to the functions in base R rather than make your own, like we did earlier in this chapter (see this twitter thread). Since the name of qlogis() isn’t the easiest to remember, you can always execute something like log_odds &lt;- qlogis or logit &lt;- qlogis at the beginning of your scripts and then use one of those as a thin wrapper for qlogis().↩︎ "],["extending-the-discrete-time-hazard-model.html", "12 Extending the Discrete-Time Hazard Model 12.1 Alternative specification for the “main effect” of TIME 12.2 Using the complementary log-log link to specify a discrete-time hazard model 12.3 Time-varying predictors 12.4 The linear additivity assumption: Uncovering violations and simple solutions 12.5 The proportionality assumption: Uncovering violations and simple solutions 12.6 The no unobserved heterogeneity assumption: No simple solution 12.7 Residual analysis Session info Footnote", " 12 Extending the Discrete-Time Hazard Model Like all statistical models, the basic discrete-time hazard model invokes assumptions about the population that may, or may not, hold in practice. Because no model should be adopted without scrutiny, we devote this chapter to examining its assumptions, demonstrating how to evaluate their tenability and relax their constraints when appropriate. In doing so, we illustrate practical principles of data analysis and offer theoretical insights into the model’s behavior and interpretation. (Singer &amp; Willett, 2003, p. 407) 12.1 Alternative specification for the “main effect” of TIME Use of a completely general specification for TIME [as explored in the last chapter] is an analytic decision, not an integral feature of the model. Nothing about the model or its estimation requires adoption of this, or any other, particular specification for TIME (p. 409, emphasis in the original) In the next page, Singer and Willett listed three circumstances under which we might consider alternatives to the completely general approach to time. They were in studies with many discrete time periods, when hazard is expected to be near zero in some time periods, and when some time periods have small risk sets. In the subsections to follow, we will explore each in turn. 12.1.1 An ordered series of polynomial specifications for TIME. Load the Gamse and Conger’s (1997) tenure_pp.csv data. library(tidyverse) tenure_pp &lt;- read_csv(&quot;data/tenure_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(tenure_pp) ## Rows: 1,474 ## Columns: 12 ## $ id &lt;dbl&gt; 111, 111, 111, 111, 111, 211, 211, 211, 211, 211, 211, 311, 311, 311, 311, 311, 311, 311, 311… ## $ period &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, … ## $ event &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, … ## $ d1 &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, … ## $ d2 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, … ## $ d3 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ d4 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ d5 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, … ## $ d6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ d7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ d8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, … ## $ d9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … Let’s confirm these data are composed of the records of \\(n = 260\\) early-career academics. tenure_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 260 Here’s a way to count how many cases were censored. tenure_pp %&gt;% group_by(id) %&gt;% arrange(desc(period)) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(event) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 × 3 ## event n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 94 36.2 ## 2 1 166 63.8 Let’s fire up brms. library(brms) As discussed in the prose and displayed in Tables 12.1 and 12.2, we will fit seven models, ranging from a constant (i.e., intercept only) model to a general (i.e., discrete factor) model. In the last chapter, we discussed how one can fit a general model with a series of \\(J\\) dummies or equivalently with the time variable, period in these data, set as a factor. Here we’ll do both. In preparation, we’ll make a period_f version of period. tenure_pp &lt;- tenure_pp %&gt;% mutate(period_f = factor(period)) Now fit the models. # constant fit12.1 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 1, prior(normal(0, 4), class = Intercept), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.01&quot;) # linear fit12.2 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.02&quot;) # quadratic fit12.3 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.03&quot;) # cubic fit12.4 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.04&quot;) # fourth order fit12.5 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3) + I(period^4), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, control = list(max_treedepth = 12), seed = 12, file = &quot;fits/fit12.05&quot;) # fifth order fit12.6 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3) + I(period^4) + I(period^5), prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, control = list(max_treedepth = 14), seed = 12, init = 0, file = &quot;fits/fit12.06&quot;) # general fit12.7 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.07&quot;) # general with `factor(period)` fit12.8 &lt;- brm(data = tenure_pp, family = binomial, event | trials(1) ~ 0 + period_f, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.08&quot;) Before we compare the models with information criteria, it might be handy to look at the hazard functions for each. A relatively quick way is with the conditional_effects() function. p2 &lt;- plot(conditional_effects(fit12.2), plot = F)[[1]] + ggtitle(&quot;linear&quot;) p3 &lt;- plot(conditional_effects(fit12.3), plot = F)[[1]] + ggtitle(&quot;quadratic&quot;) p4 &lt;- plot(conditional_effects(fit12.4), plot = F)[[1]] + ggtitle(&quot;cubic&quot;) p5 &lt;- plot(conditional_effects(fit12.5), plot = F)[[1]] + ggtitle(&quot;fourth order&quot;) p6 &lt;- plot(conditional_effects(fit12.6), plot = F)[[1]] + ggtitle(&quot;fifth order&quot;) p7 &lt;- plot(conditional_effects(fit12.8), cat_args = list(size = 3/2), plot = F)[[1]] + ggtitle(&quot;general&quot;) Because it contains no predictors, we cannot use conditional_effects() to make a plot for the constant model (i.e., fit12.1). We’ll have to do that by hand. p1 &lt;- tibble(period = 1:9) %&gt;% ggplot(aes(x = period)) + geom_ribbon(aes(ymin = fixef(fit12.1)[, 3] %&gt;% inv_logit_scaled(), ymax = fixef(fit12.1)[, 4] %&gt;% inv_logit_scaled()), alpha = 1/5) + geom_line(aes(y = fixef(fit12.1)[, 1] %&gt;% inv_logit_scaled()), linewidth = 1, color = &quot;blue1&quot;) + ggtitle(&quot;constant&quot;) + ylab(&quot;event | trials(1)&quot;) Now combine and format the subplots with patchwork. library(patchwork) (((p1 + p2 + p3 + p4 + p5 + p6) &amp; scale_x_continuous(breaks = 1:9)) + p7) &amp; coord_cartesian(ylim = c(0, .5)) &amp; theme(panel.grid = element_blank()) We are going to depart from Singer and Willett and no longer entertain using deviance for Bayesian model comparison. But we will compare then using the WAIC and the LOO. fit12.1 &lt;- add_criterion(fit12.1, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.2 &lt;- add_criterion(fit12.2, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.3 &lt;- add_criterion(fit12.3, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.4 &lt;- add_criterion(fit12.4, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.5 &lt;- add_criterion(fit12.5, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.6 &lt;- add_criterion(fit12.6, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.7 &lt;- add_criterion(fit12.7, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.8 &lt;- add_criterion(fit12.8, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) Before comparing the models in bulk, as in Table 12.2, let’s confirm that whether we use the dummy variable method (fit12.7) or the factor variable method (fit12.8), the results for the general model are the same. loo_compare(fit12.7, fit12.8, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.7 0.0 0.0 -425.1 22.5 9.4 1.1 850.1 45.0 ## fit12.8 0.0 0.0 -425.1 22.5 9.4 1.1 850.1 45.0 loo_compare(fit12.7, fit12.8, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.7 0.0 0.0 -425.0 22.5 9.3 1.1 850.0 45.0 ## fit12.8 0.0 0.0 -425.0 22.5 9.3 1.1 850.0 45.0 Yep, both WAIC and LOO confirm both methods are equivalent. One of the main reasons we used the factor method, here, was because it made it easier to plot the results with conditional_effects(). But with the following model comparisons, we’ll focus on fit12.1 through fit12.7. loo_compare(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.4 0.0 0.0 -420.7 22.0 4.1 0.5 841.4 44.1 ## fit12.3 -0.5 2.2 -421.2 22.0 3.1 0.4 842.3 44.1 ## fit12.5 -0.6 0.8 -421.3 22.2 4.8 0.6 842.5 44.4 ## fit12.6 -0.9 0.8 -421.6 22.2 5.2 0.6 843.3 44.4 ## fit12.7 -4.4 1.5 -425.1 22.5 9.4 1.1 850.1 45.0 ## fit12.2 -14.9 5.9 -435.6 22.0 1.8 0.1 871.2 44.1 ## fit12.1 -99.1 13.2 -519.8 25.1 1.0 0.1 1039.5 50.2 loo_compare(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.4 0.0 0.0 -420.7 22.0 4.1 0.5 841.4 44.1 ## fit12.3 -0.5 2.2 -421.2 22.0 3.1 0.4 842.3 44.1 ## fit12.5 -0.6 0.8 -421.2 22.2 4.8 0.6 842.5 44.4 ## fit12.6 -0.9 0.8 -421.6 22.2 5.2 0.6 843.2 44.4 ## fit12.7 -4.3 1.5 -425.0 22.5 9.3 1.1 850.0 45.0 ## fit12.2 -14.9 5.9 -435.6 22.0 1.8 0.1 871.2 44.1 ## fit12.1 -99.1 13.2 -519.8 25.1 1.0 0.1 1039.5 50.2 Our results are very similar to those in the AIC column in Table 12.2. Just for kicks and giggles, here are the model weights based on the LOO, WAIC, and stacking method. model_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 ## 0.000 0.000 0.240 0.385 0.219 0.150 0.005 model_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 ## 0.000 0.000 0.238 0.384 0.221 0.151 0.005 model_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = &quot;stacking&quot;) %&gt;% round(digits = 3) ## fit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 ## 0.019 0.000 0.487 0.494 0.000 0.000 0.000 Across the comparison methods, the overall pattern is the cubic model (fit12.4) is marginally better than the rest, but that both the quadratic and fourth-order models were quite close. Unlike when you use model deviance, the parsimony corrections used in the information-criteria-based methods all suggest the general model is overfit. Before considering whether these differences in [information criteria] are sufficient to warrant use of an alternative specification for TIME, let us examine the corresponding fitted logit hazard functions. Doing so not only highlights the behavior of logit hazard, it also offers a graphical means of comparing the fit of competing specifications. (p. 413, emphasis in the original) In preparation for our Figure 12.1, we’ll make a custom function called make_fitted(), which will streamline some of the data wrangling code. make_fitted &lt;- function(fit, scale, ...) { fitted(fit, newdata = nd, scale = scale, ...) %&gt;% data.frame() %&gt;% bind_cols(nd) } In addition to taking different brms fit objects as input, make_fitted() will allow us to adjust the scale of the output. As you’ll see, we will need to work with to settings in the coming plots. For our first batch of code, we’ll use scale = \"linear\". Because the newdata for our version of the general model (i.e., fit12.8) requires the predictor to be called period_f and the rest of the models require the predictor to be named period, we’ll make and save the results for the former first, redefine our newdata, apply make_fitted() to the rest of the models, and then combine them all. Here it is in one fell swoop. nd &lt;- tibble(period_f = 1:9) f &lt;- make_fitted(fit12.8, scale = &quot;linear&quot;) %&gt;% rename(period = period_f) # this will simplify the `mutate()` code below models &lt;- c(&quot;constant&quot;, &quot;linear&quot;, &quot;quadratic&quot;, &quot;cubic&quot;, &quot;general&quot;) nd &lt;- tibble(period = 1:9) f &lt;- bind_rows(make_fitted(fit12.1, scale = &quot;linear&quot;), # constant make_fitted(fit12.2, scale = &quot;linear&quot;), # linear make_fitted(fit12.3, scale = &quot;linear&quot;), # quadratic make_fitted(fit12.4, scale = &quot;linear&quot;), # cubic f) %&gt;% # general mutate(model = factor(rep(models, each = 9), levels = models)) # what have we done? glimpse(f) ## Rows: 45 ## Columns: 6 ## $ Estimate &lt;dbl&gt; -2.0652119, -2.0652119, -2.0652119, -2.0652119, -2.0652119, -2.0652119, -2.0652119, -2.065… ## $ Est.Error &lt;dbl&gt; 0.08186425, 0.08186425, 0.08186425, 0.08186425, 0.08186425, 0.08186425, 0.08186425, 0.0818… ## $ Q2.5 &lt;dbl&gt; -2.22771603, -2.22771603, -2.22771603, -2.22771603, -2.22771603, -2.22771603, -2.22771603,… ## $ Q97.5 &lt;dbl&gt; -1.90606492, -1.90606492, -1.90606492, -1.90606492, -1.90606492, -1.90606492, -1.90606492,… ## $ period &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, … ## $ model &lt;fct&gt; constant, constant, constant, constant, constant, constant, constant, constant, constant, … Now we’re in good shape to make and save our version of the top panel of Figure 12.1. p1 &lt;- f %&gt;% ggplot(aes(x = period, y = Estimate, color = model)) + geom_line() + scale_color_viridis_d(option = &quot;A&quot;, direction = -1) + ylab(&quot;Fitted logit(hazard)&quot;) + coord_cartesian(ylim = -c(6, 0)) + theme(panel.grid = element_blank()) The bottom two panels require we redo our make_fitted() code from above, this time setting scale = \"response\". nd &lt;- tibble(period_f = 1:9) f &lt;- make_fitted(fit12.8, scale = &quot;response&quot;) %&gt;% rename(period = period_f) nd &lt;- tibble(period = 1:9) f &lt;- bind_rows(make_fitted(fit12.1, scale = &quot;response&quot;), # constant make_fitted(fit12.2, scale = &quot;response&quot;), # linear make_fitted(fit12.3, scale = &quot;response&quot;), # quadratic make_fitted(fit12.4, scale = &quot;response&quot;), # cubic f) %&gt;% # general mutate(model = factor(rep(models, each = 9), levels = models)) Now make and save the bottom left panel for Figure 12.1. p2 &lt;- f %&gt;% filter(model %in% c(&quot;quadratic&quot;, &quot;general&quot;)) %&gt;% ggplot(aes(x = period, y = Estimate, color = model)) + geom_line() + scale_color_viridis_d(option = &quot;A&quot;, end = .5, direction = -1) + ylab(&quot;Fitted hazard&quot;) + coord_cartesian(ylim = c(0, .4)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Our f data will also work to make the final bottom right panel for the figure, but we’ll need to convert the Estimate values from the hazard metric to the survival-probability metric. In addition, we will need to add in values for when period = 0. Here we wrangle, plot, and save in one block. new_rows &lt;- tibble(Estimate = 0, period = 0, model = factor(c(&quot;quadratic&quot;, &quot;general&quot;), levels = models)) p3 &lt;- f %&gt;% filter(model %in% c(&quot;quadratic&quot;, &quot;general&quot;)) %&gt;% select(Estimate, period, model) %&gt;% # add the `new_rows` data bind_rows(new_rows) %&gt;% arrange(model, period) %&gt;% group_by(model) %&gt;% # convert hazards to survival probabilities mutate(Estimate = cumprod(1 - Estimate)) %&gt;% # plot! ggplot(aes(x = period, y = Estimate, color = model)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line() + scale_color_viridis_d(option = &quot;A&quot;, end = .5, direction = -1) + scale_y_continuous(&quot;Fitted survival probability&quot;, breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 1)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Now combine the subplots and format a little with patchwork to make our version of Figure 12.1. p1 + p2 + p3 + plot_layout(guides = &quot;collect&quot;) &amp; scale_x_continuous(&quot;Years after hire&quot;, breaks = 0:9, limits = c(0, 9)) Instead of the two-row layout in the text, it seemed simpler to arrange the panels all in one row. This way we can let the full version of the line-color legend serve for all three panels. 12.1.2 Criteria for comparing alternative specification. The decline in the deviance statistic across models indicates that fit improves with increasing complexity of the temporal specification. To evaluate the magnitude of this decline, we must also account for the increased number of parameters in the model. You should not adopt a more complex specification if it fits no better than a simpler one. But if an alternative specification is (nearly) as good as the most general one, it may be “good enough.” At the same time, we would not want an alternative that performs measurably worse than we know we can do. (p. 415) We won’t be comparing deviances with \\(\\chi^2\\) tests, here. As to information criteria, we got ahead of the authors a bit and presented those comparisons in the last section. Although our use of the WAIC and the LOO is similar to Singer and Willett’s use of the AIC and BIC in that they yield no formal hypothesis test in the form of a \\(p\\)-value, their estimates and difference scores do come with standard errors. In the middle of page 416, the authors focused on comparing the constant and linear models, the linear and quadratic models, and the quadratic and general models. The LOO and WAIC estimates for each were near identical. For the sake of simplicity, here are those three focused comparisons using the LOO. # the constant and linear models l1 &lt;- loo_compare(fit12.1, fit12.2, criterion = &quot;loo&quot;) # the linear and quadratic models l2 &lt;- loo_compare(fit12.2, fit12.3, criterion = &quot;loo&quot;) # the quadratic and general models l3 &lt;- loo_compare(fit12.3, fit12.7, criterion = &quot;loo&quot;) l1 %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.2 0.0 0.0 -435.6 22.0 1.8 0.1 871.2 44.1 ## fit12.1 -84.2 11.7 -519.8 25.1 1.0 0.1 1039.5 50.2 l2 %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.3 0.0 0.0 -421.2 22.0 3.1 0.4 842.3 44.1 ## fit12.2 -14.4 5.5 -435.6 22.0 1.8 0.1 871.2 44.1 l3 %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.3 0.0 0.0 -421.2 22.0 3.1 0.4 842.3 44.1 ## fit12.7 -3.9 2.3 -425.1 22.5 9.4 1.1 850.1 45.0 If we presume the LOO differences follow a normal distribution, we can use their point estimates and standard errors to plot those distributions using simulated data from good old rnorm(). library(tidybayes) n &lt;- 1e6 models &lt;- c(&quot;linear - constant&quot;, &quot;quadratic - linear&quot;, &quot;quadratic - general&quot;) set.seed(12) # wrangle tibble(loo_difference = c(rnorm(n, mean = l1[2, 1] * -2, sd = l1[2, 2] * 2), rnorm(n, mean = l2[2, 1] * -2, sd = l2[2, 2] * 2), rnorm(n, mean = l3[2, 1] * -2, sd = l3[2, 2] * 2))) %&gt;% mutate(models = factor(rep(models, each = n), levels = models)) %&gt;% # plot! ggplot(aes(x = loo_difference, y = 0)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;LOO-difference simulations based on 1,000,000 draws&quot;, x = &quot;difference distribution&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ models, scales = &quot;free&quot;) The LOO difference for the linear and constant models is decisive. The difference for the quadratic and linear models is fairly large on the information-criteria scale, but the uncertainty in that distribution is fairly large relative to its location (i.e., its mean), which might temper overly-strong conclusions about how much better the quadratic was compared to the linear. The comparison between the quadratic and the general produced a simulation with a modest location and rather large uncertainty relative to the magnitude of that location. All in all, “all signs point to the superiority of the quadratic specification, which fits nearly as well as the general mode, but with fewer parameters” (p. 416). In the next page, Singer and Willett briefly focused on comparing the cubic and quadratic models. Here are their LOO and WAIC caparisons. loo_compare(fit12.3, fit12.4, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.4 0.0 0.0 -420.7 22.0 4.1 0.5 841.4 44.1 ## fit12.3 -0.5 2.2 -421.2 22.0 3.1 0.4 842.3 44.1 loo_compare(fit12.3, fit12.4, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.4 0.0 0.0 -420.7 22.0 4.1 0.5 841.4 44.1 ## fit12.3 -0.5 2.2 -421.2 22.0 3.1 0.4 842.3 44.1 In the text, the results for the AIC and BIC differed. Our LOO and WAIC results both agree with the AIC: the cubic model has a slightly lower LOO and WAIC estimate compared to the quadratic. However, the standard errors for the formal difference score are about twice the size of that difference and the absolute magnitude of the difference is rather small to begin with. Here’s what it looks like if we compare them using LOO weights, WAIC weights, and stacking weights. model_weights(fit12.3, fit12.4, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.3 fit12.4 ## 0.384 0.616 model_weights(fit12.3, fit12.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.3 fit12.4 ## 0.383 0.617 model_weights(fit12.3, fit12.4, weights = &quot;stacking&quot;) %&gt;% round(digits = 3) ## fit12.3 fit12.4 ## 0.404 0.596 Across all three weight comparisons, there was a modest edge for the quadratic model. But back to Singer and Willett: Although decision rules cannot substitute for judgment, intuition, and common sense, we nevertheless conclude by offering two guidelines for selecting among alternative specifications: If a smooth specification works nearly as well as the completely general one, appreciably better than all simpler ones, and no worse than all more complex ones, consider adopting it. If no smooth specifications meet these criteria, retain the completely general specification. If this decision process leads you to a polynomial specification, then you can interpret the model’s parameters easily, as we [will discuss in a bit]. (p. 417, emphasis in the original) Before moving on, we might point out that our Bayesian brms-based framework offers a different option: model averaging. We plot the hazard and survival curves based on weighted averages of multiple models. The weights can be based on various criteria. One approach would be to use the model weights from the LOO or the WAIC. As an example, here we use the LOO weights for the quadratic and cubic models. nd &lt;- tibble(period = 1:9) pp &lt;- pp_average(fit12.3, fit12.4, weights = &quot;loo&quot;, newdata = nd, method = &quot;pp_expect&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) The pp_average() function works much like fitted() or predict(). If you input models and perhaps newdata, it will return estimates that are the weighted averages of the specified models. With the weights = \"loo\" argument, we indicated our desired weights were those from the LOO, just as we computed earlier with the model_weights() function. With the method = \"pp_expect\" argument, we indicated we wanted fitted values like we would get from fitted(). Here we plot the results in terms of hazard and survival. # hazard p1 &lt;- pp %&gt;% ggplot(aes(x = period)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5) + geom_line(aes(y = Estimate)) + scale_x_continuous(&quot;Years after hire&quot;, breaks = 0:9, limits = c(0, 9)) + ylab(&quot;hazard&quot;) + theme(panel.grid = element_blank()) # survival p2 &lt;- pp %&gt;% select(-Est.Error) %&gt;% bind_rows(tibble(Estimate = 0, Q2.5 = 0, Q97.5 = 0, period = 0)) %&gt;% arrange(period) %&gt;% mutate_at(vars(Estimate:Q97.5), .funs = ~ cumprod(1 - .)) %&gt;% ggplot(aes(x = period)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5) + geom_line(aes(y = Estimate)) + scale_x_continuous(&quot;Years after hire&quot;, breaks = 0:9) + scale_y_continuous(&quot;survival&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(panel.grid = element_blank()) # combine (p1 | p2) + plot_annotation(title = &quot;Behold the fitted hazard and survival curves based on a weighted\\naverage of the quadratic and linear models!&quot;) 12.1.3 Interpreting parameters from linear, quadratic, and cubic specifications. “One advantage of a polynomial specification is that you can often interpret its parameters directly” (p. 417). For the polynomial models in this section, Singer and Willett used the \\(TIME - c\\) specification for period where \\(c\\) is a centering constant. They used \\(c = 5\\). Before we can refit our polynomial models with this parameterization, we’ll want to make a new period variable with this centering. We’ll call it period_5. tenure_pp &lt;- tenure_pp %&gt;% mutate(period_5 = period - 5) # how do the two `period` variables compare? tenure_pp %&gt;% distinct(period, period_5) ## # A tibble: 9 × 2 ## period period_5 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -4 ## 2 2 -3 ## 3 3 -2 ## 4 4 -1 ## 5 5 0 ## 6 6 1 ## 7 7 2 ## 8 8 3 ## 9 9 4 Now refit the quadratic model using period_5. fit12.9 &lt;- update(fit12.3, newdata = tenure_pp, event | trials(1) ~ 0 + Intercept + period_5 + I(period_5^2), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.09&quot;) Check the model summary. print(fit12.9) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ Intercept + period_5 + I(period_5^2) - 1 ## Data: tenure_pp (Number of observations: 1474) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.41 0.11 -1.64 -1.19 1.00 2318 2182 ## period_5 0.61 0.06 0.50 0.73 1.00 2379 2147 ## Iperiod_5E2 -0.13 0.03 -0.18 -0.08 1.00 2158 2633 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Focusing just on the posterior means, this yields the following formula for the quadratic discrete-time hazard model: \\(\\operatorname{logit} \\hat h (t_j) = \\;\\) -1.4122 \\(\\text{one } +\\) 0.6150\\((\\text{time}_j - 5)\\) -0.1282\\((\\text{time}_j - 5)^2\\). Singer and Willett used the term “flipover point” for the point at which the quadratic function reaches its peak or trough. If we let \\(c\\) be the centering constant for time variable, \\(\\alpha_1\\) be the linear coefficient for time, and \\(\\alpha_2\\) be the quadratic coefficient for time, we define the flipover point as \\[\\text{flipover point} = [c - 1/2 (\\alpha_1 / \\alpha_2)].\\] Here’s what that looks like using the posterior samples. as_draws_df(fit12.9) %&gt;% transmute(c = 5, a1 = b_period_5, a2 = b_Iperiod_5E2) %&gt;% mutate(`flipover point` = c - 0.5 * (a1 / a2)) %&gt;% ggplot(aes(x = `flipover point`, y = 0)) + stat_halfeye(.width = c(.5, .95)) + scale_x_continuous(breaks = 7:12) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Just as each parameter has a posterior distribution, the flipover point, which is a function of two of the parameters, also has a posterior distribution. To understand what this flipover distribution means, it might be helpful to look at it in another way. For that, we’ll employ fitted(). nd &lt;- tibble(period = seq(from = 0, to = 12, by = .1)) %&gt;% mutate(period_5 = period - 5) f &lt;- fitted(fit12.9, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(nd, iter = 1:4000, nesting(period, period_5))) f %&gt;% glimpse() ## Rows: 484,000 ## Columns: 5 ## $ name &lt;chr&gt; &quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;, &quot;X5&quot;, &quot;X6&quot;, &quot;X7&quot;, &quot;X8&quot;, &quot;X9&quot;, &quot;X10&quot;, &quot;X11&quot;, &quot;X12&quot;, &quot;X13&quot;, &quot;X14&quot;, &quot;X… ## $ value &lt;dbl&gt; -6.718848, -6.570127, -6.423196, -6.278057, -6.134709, -5.993153, -5.853387, -5.715413, -5.… ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ period &lt;dbl&gt; 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1… ## $ period_5 &lt;dbl&gt; -5.0, -4.9, -4.8, -4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4.0, -3.9, -3.8, -3.7, -3.6, -… Now make a logit hazard spaghetti plot. f %&gt;% # how many lines would you like? filter(iter &lt;= 30) %&gt;% ggplot(aes(x = period, y = value, group = iter)) + geom_line(alpha = 1/2) + ylab(&quot;logit hazard&quot;) + coord_cartesian(xlim = c(0, 11), ylim = c(-5, 0)) + theme(panel.grid = element_blank()) To keep the plot manageable, we filtered to just the first 30 posterior draws. Each was depicted with its own hazard line. Note how each of those hazard lines peaks at a different point along the \\(x\\)-axis. Most peak somewhere around 7.4. Some take on notably higher values. Now fit a cubic model using period_5, \\((TIME_j - 5)\\), as the measure of time. fit12.10 &lt;- update(fit12.4, newdata = tenure_pp, event | trials(1) ~ 0 + Intercept + period_5 + I(period_5^2) + I(period_5^3), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.10&quot;) Check the model summary. print(fit12.10) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ Intercept + period_5 + I(period_5^2) + I(period_5^3) - 1 ## Data: tenure_pp (Number of observations: 1474) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.45 0.12 -1.68 -1.23 1.00 2027 2390 ## period_5 0.75 0.10 0.56 0.94 1.00 1981 2332 ## Iperiod_5E2 -0.11 0.03 -0.17 -0.07 1.00 1947 1964 ## Iperiod_5E3 -0.02 0.01 -0.04 0.00 1.00 1927 2027 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Focusing again on just the posterior means, this yields the following formula for the cubic discrete-time hazard model: \\(\\operatorname{logit} \\hat h (t_j) =\\;\\) -1.4535 \\(\\text{one } +\\) 0.7505\\((\\text{time}_j - 5)\\) -0.1149\\((\\text{time}_j - 5)^2\\) -0.0182\\((\\text{time}_j - 5)^3\\). Now if we let \\(c\\), \\(\\alpha_1\\) and \\(\\alpha_2\\) retain their meanings from before and further let \\(\\alpha_3\\) stand for the cubic term for time, we can define the two flipover points in the cubic logit hazard model as \\[\\text{flipover points} = c + \\frac{-\\alpha_2 \\pm \\sqrt{\\alpha_2^2 - 3 \\alpha_1 \\alpha_3}}{3 \\alpha_3}.\\] Do you see that \\(\\pm\\) sign in the numerator? That’s what gives us the two points. Now apply the formula to fit12.10 and plot. # extract the posterior draws draws &lt;- as_draws_df(fit12.10) %&gt;% transmute(c = 5, a1 = b_period_5, a2 = b_Iperiod_5E2, a3 = b_Iperiod_5E3) # flipover point with &quot;+&quot; in the numerator p1 &lt;- draws %&gt;% mutate(`flipover point 1` = c + (- a2 + sqrt(a2^2 - 3 * a1 * a3)) / (3 * a3)) %&gt;% filter(!is.na(`flipover point 1`)) %&gt;% filter(`flipover point 1` &gt; -50 &amp; `flipover point 1` &lt; 50) %&gt;% ggplot(aes(x = `flipover point 1`, y = 0)) + stat_halfeyeh(.width = c(.5, .95),) + annotate(geom = &quot;text&quot;, x = -30, y = .85, label = &quot;italic(c)+frac(-alpha[2]+sqrt(alpha[2]^2-3*alpha[1]*alpha[3]), 3*alpha[3])&quot;, hjust = 0, family = &quot;Times&quot;, parse = T) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-30, 20)) # flipover point with &quot;-&quot; in the numerator p2 &lt;- draws %&gt;% mutate(`flipover point 2` = c + (- a2 - sqrt(a2^2 - 3 * a1 * a3)) / (3 * a3)) %&gt;% filter(!is.na(`flipover point 2`)) %&gt;% ggplot(aes(x = `flipover point 2`, y = 0)) + stat_halfeyeh(.width = c(.5, .95)) + annotate(geom = &quot;text&quot;, x = 8.2, y = .85, label = &quot;italic(c)+frac(-alpha[2]-sqrt(alpha[2]^2-3*alpha[1]*alpha[3]), 3*alpha[3])&quot;, hjust = 0, family = &quot;Times&quot;, parse = T) + scale_y_continuous(NULL, breaks = NULL) # combine! (p1 | p2) &amp; theme(panel.grid = element_blank()) The plot on the right looks similar to the flipover plot from fit12.9. But look at the massive uncertainty in the flipover point in the plot on the left. If you play around with the code, you’ll see the \\(x\\)-axis extends far beyond the boundaries in the plot. Another spaghetti plot might help show what’s going on. # redifine the `newdata` nd &lt;- tibble(period = seq(from = -8, to = 11, by = .1)) %&gt;% mutate(period_5 = period - 5) # employ `fitted()` and wrangle f &lt;- fitted(fit12.10, newdata = nd, summary = F, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(nd, iter = 1:4000, nesting(period, period_5))) # plot! f %&gt;% filter(iter &lt;= 30) %&gt;% ggplot(aes(x = period, y = value, group = iter)) + geom_line(alpha = 1/2) + ylab(&quot;logit hazard&quot;) + coord_cartesian(xlim = c(-7, 10), ylim = c(-13, 0)) + theme(panel.grid = element_blank()) On the region to the right of 0 on the \\(x\\)-axis, the plot looks a lot like the one for the quadratic model. But look at how wildly the lines fan out on the left side of 0. Since that’s the region where we find the second flipover point, all that uncertainty got baked into its marginal posterior. Just because I think it looks cool, here’s a version of that plot with lines corresponding to all 4,000 posterior draws. f %&gt;% ggplot(aes(x = period, y = value, group = iter)) + geom_line(alpha = 1/10, linewidth = 1/10) + ylab(&quot;logit hazard&quot;) + coord_cartesian(xlim = c(-7, 10), ylim = c(-13, 0)) + theme(panel.grid = element_blank()) 12.2 Using the complementary log-log link to specify a discrete-time hazard model So far we’ve been using the logit transformation [which] represented a natural choice because it allowed us to: (1) specify the model using familiar terminology; (2) use widely available software for estimation; and (3) exploit interpretative strategies with which many empirical researchers are comfortable. Just like the choice of a completely general specification for the main effect of TIME, use of a logit link is an analytic decision. Nothing about the way in which the model is postulated or fit requires the adoption of this, or any other, particular link function. (p. 419–420, emphasis in the original) The complimentary log-log transformation–clog-log for short–is a widely-used alternative. It follows the form \\[\\operatorname{clog-log} = \\log \\big (- \\log (1 - p) \\big),\\] where \\(p\\) is a probability value. In words, “while the logit transformation yields the logarithm of the odds of event occurrence, the clog-log transformation yields the logarithm of the negated logarithm of the probability of event nonoccurrence” (p. 420, emphasis in the original). 12.2.1 The clog-log transformation: When and why it is useful. Here’s our version of Figure 12.2, where we compare the logit and clog-log transformations. # simulate the data tibble(p = seq(from = .00001, to = .99999, length.out = 1e4)) %&gt;% mutate(logit = log(p / (1 - p)), cloglog = log(-log(1 - p))) %&gt;% pivot_longer(-p) %&gt;% mutate(name = factor(name, levels = c(&quot;logit&quot;, &quot;cloglog&quot;), labels = c(&quot;Logit&quot;, &quot;Complementary log-log&quot;))) %&gt;% # plot ggplot(aes(x = p, y = value, color = name)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_line(linewidth = 1) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) + scale_y_continuous(&quot;transformed hazard probability&quot;, breaks = -3:3 * 5, limits = c(-15, 15)) + xlab(&quot;hazard probability&quot;) + theme(legend.background = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(fill = &quot;grey92&quot;, color = &quot;grey92&quot;), legend.position = c(.25, .85), panel.grid = element_blank()) Both transformations extend to the full \\(-\\infty\\) to \\(\\infty\\) parameter space. But whereas the logit is symmetric around zero and has a memorable point corresponding to \\(p = .5\\) (i.e., 0), the clog-log is asymmetric and has a less-intuitive point corresponding to \\(p = .5\\) (i.e., -0.3665129). Though somewhat odd, the advantage of the clog-log is it provides a discrete-time statistical model for hazard that has a built-in proportional hazards assumption, and not a proportional odds assumption (as in the case of the logit link). This would be completely unremarkable except for one thing: it provides a conceptual parallelism between the clog-log discrete-time hazard model and the models that we will ultimately describe for continuous-time survival analysis. (p. 421, emphasis in the original) 12.2.2 A discrete-time hazard model using the complementary log-log link. Singer and Willett (p. 422): Any discrete-time hazard model postulated using a logit link can be rewritten using a clog-log link, simply by substituting transformations of the outcome. For example, we can write a general discrete-time hazard model for \\(J\\) time periods and \\(P\\) substantive predictors as: \\[\\begin{align*} \\operatorname{clog-log} h(t_j) &amp; = [\\alpha_1 D_1 + \\alpha_2 D_2 + \\cdots + \\alpha_J D_J] \\\\ &amp; \\;\\; + [\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_P X_P]. \\end{align*}\\] Now reload the firstsex_pp.csv data to test this baby out. sex_pp &lt;- read_csv(&quot;data/firstsex_pp.csv&quot;) glimpse(sex_pp) ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, … ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8… ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, … ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, … ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, … ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, … ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, … Our next task is to recreate Figure 12.3, which shows the “sample hazard functions for the grade of first intercourse data displayed on different scales,” namely the logit and clog-log (p. 423). The key word here is “sample,” meaning we’re not fitting full models. For our version of the corresponding figure from the last chapter (i.e., Figure 11.2 from page 363), we computed the sample logit hazard functions with life tables based on the output from the survival::survfit() function. I am not aware that you can get hazards in the clog-log scale from the survfit() function. The folks at IDRE solved the problem by fitting four maximum likelihood models using the glm() function (see here). We’ll follow a similar approach, but with weakly-regularizing priors using the brms::brm() function. ## logit # pt == 0 fit12.11 &lt;- brm(data = sex_pp %&gt;% filter(pt == 0), family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.11&quot;) # pt == 1 fit12.12 &lt;- brm(data = sex_pp %&gt;% filter(pt == 1), family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.12&quot;) ## clog-log # pt == 0 fit12.13 &lt;- brm(data = sex_pp %&gt;% filter(pt == 0), family = binomial(link = cloglog), event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.13&quot;) # pt == 1 fit12.14 &lt;- brm(data = sex_pp %&gt;% filter(pt == 1), family = binomial(link = cloglog), event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.14&quot;) The primary line of code that distinguishes fit12.11 and fit12.12 from fit12.13 and fit12.14 is family = binomial(link = cloglog). With the family argument, we switched out the default logit link for the clog-log link. Before we can make our version of Figure 12.5, we will redefine our nd data for fitted(), pump our four fit objects into our custom make_fitted() function from earlier, and wrangle a little. nd &lt;- tibble(period = 7:12) %&gt;% mutate(d7 = if_else(period == 7, 1, 0), d8 = if_else(period == 8, 1, 0), d9 = if_else(period == 9, 1, 0), d10 = if_else(period == 10, 1, 0), d11 = if_else(period == 11, 1, 0), d12 = if_else(period == 12, 1, 0)) f &lt;- bind_rows(make_fitted(fit12.11, scale = &quot;linear&quot;), make_fitted(fit12.12, scale = &quot;linear&quot;), make_fitted(fit12.13, scale = &quot;linear&quot;), make_fitted(fit12.14, scale = &quot;linear&quot;)) %&gt;% mutate(pt = rep(str_c(&quot;pt = &quot;, c(0:1, 0:1)), each = n() / 4), link = rep(c(&quot;logit&quot;, &quot;clog-log&quot;), each = n() / 2)) f %&gt;% glimpse() ## Rows: 24 ## Columns: 13 ## $ Estimate &lt;dbl&gt; -3.6706692, -3.6407969, -2.0493384, -1.9018679, -1.4646795, -1.4876617, -2.0033286, -2.938… ## $ Est.Error &lt;dbl&gt; 0.7488902, 0.7365261, 0.3807953, 0.3840227, 0.3501475, 0.4042828, 0.3020423, 0.4622748, 0.… ## $ Q2.5 &lt;dbl&gt; -5.3764207, -5.3293475, -2.8240043, -2.7003814, -2.1926528, -2.3309166, -2.6308878, -3.926… ## $ Q97.5 &lt;dbl&gt; -2.41270860, -2.42506307, -1.33723119, -1.19372384, -0.81299679, -0.74145934, -1.44619911,… ## $ period &lt;int&gt; 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12 ## $ d7 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0 ## $ d10 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0 ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0 ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1 ## $ pt &lt;chr&gt; &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 0&quot;, &quot;pt = 1&quot;, &quot;pt = 1&quot;, &quot;pt = 1&quot;, … ## $ link &lt;chr&gt; &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, &quot;logit&quot;, … Make our version of Figure 12.3. f %&gt;% ggplot(aes(x = period, group = interaction(pt, link), color = pt)) + geom_line(aes(y = Estimate, linetype = link)) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) + scale_x_continuous(&quot;grade&quot;, breaks = 6:12, limits = c(6, 12)) + ylab(&quot;transformed hazard probability&quot;) + coord_cartesian(ylim = c(-4, 0)) + theme(panel.grid = element_blank()) Our next step is to fit proper models to the data. First, we will refit fit11.8 from last chapter. Then we’ll fit the clog-log analogue to the same model. The two models, respectively, follow the form \\[ \\begin{align*} \\operatorname{logit} h(t_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ] \\; \\text{and} \\\\ \\operatorname{clog-log} h(t_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ]. \\end{align*} \\] Fit the models. # logit fit11.8 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.08&quot;) # clog-log fit12.15 &lt;- brm(data = sex_pp, family = binomial(link = cloglog), event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.15&quot;) Both models used the binomial likelihood. They only differed in which link function we used to transform the data. To further explicate, we can more fully write out our Bayesian clog-log model as \\[ \\begin{align*} \\text{event}_{ij} &amp; = \\operatorname{Binomial}(n = 1, p_{ij}) \\\\ \\operatorname{clog-log} (p_{ij}) &amp; = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_{1i} ] \\\\ \\alpha_7, \\alpha_8, ..., \\alpha_{12} &amp; \\sim \\operatorname{Normal}(0, 4) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 4). \\end{align*} \\] Here are the posterior means for fit11.8 and fit12.15, as depicted in the firs three columns of Table 12.3. pars &lt;- bind_rows(fixef(fit11.8) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;par&quot;), fixef(fit12.15) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;par&quot;)) %&gt;% mutate(link = rep(c(&quot;logit&quot;, &quot;clog-log&quot;), each = n() / 2), par = factor(par, levels = c(str_c(&quot;d&quot;, 7:12), &quot;pt&quot;))) pars %&gt;% select(par, link, Estimate) %&gt;% pivot_wider(names_from = link, values_from = Estimate) %&gt;% select(par, `clog-log`, logit) %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 7 × 3 ## par `clog-log` logit ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 d7 -2.98 -3.02 ## 2 d8 -3.69 -3.73 ## 3 d9 -2.32 -2.29 ## 4 d10 -1.90 -1.83 ## 5 d11 -1.77 -1.65 ## 6 d12 -1.35 -1.18 ## 7 pt 0.765 0.863 They might be easier to compare in a coefficient plot. pars %&gt;% ggplot(aes(x = link, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_pointrange() + labs(x = NULL, y = &quot;transformed hazard&quot;) + coord_flip() + theme(axis.text.y = element_text(hjust = 0), panel.grid = element_blank()) + facet_wrap(~ par, ncol = 1) Instead of comparing them with deviance values, we will compare the two models using Bayesian information criteria. For simplicity, we’ll focus on the LOO. fit12.15 &lt;- add_criterion(fit12.15, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(fit12.15, fit11.8, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.15 0.0 0.0 -324.3 17.6 7.1 0.5 648.6 35.2 ## fit11.8 -0.2 0.2 -324.5 17.6 7.2 0.6 649.0 35.3 model_weights(fit12.15, fit11.8, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.15 fit11.8 ## 0.548 0.452 From a LOO perspective, they’re basically the same. The parameter summaries are also quite similar between the two models. A coefficient plot might make it easy to see. “Numerical similarity is common when fitting identical models with alternate link functions (and net risks of event occurrence are low) and suggests that choice of a link function should depend on other considerations” (p. 423). We already know from the last chapter that we can convert logits to probabilities with the function \\[p = \\frac{1}{1 + e^{- \\text{logit}}}.\\] The relevant inverse transformation for the clog-log link is \\[p = 1 - e^{\\left (- e^{( \\text{clog-log})} \\right)}.\\] Now use those formulas to convert our Estimate values into the hazard metric, as shown in the last two columns of Table 12.3. pars %&gt;% filter(par != &quot;pt&quot;) %&gt;% mutate(Estimate = if_else(str_detect(link, &quot;logit&quot;), 1 / (1 + exp(-1 * Estimate)), 1 - exp(-exp(Estimate)))) %&gt;% select(par, link, Estimate) %&gt;% pivot_wider(names_from = link, values_from = Estimate) %&gt;% select(par, `clog-log`, logit) %&gt;% mutate(`clog-log - logit` = `clog-log` - logit) %&gt;% mutate_if(is.double, round, digits = 4) ## # A tibble: 6 × 4 ## par `clog-log` logit `clog-log - logit` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 d7 0.0493 0.0467 0.0026 ## 2 d8 0.0248 0.0234 0.0014 ## 3 d9 0.0938 0.0916 0.0023 ## 4 d10 0.139 0.138 0.0009 ## 5 d11 0.157 0.160 -0.0035 ## 6 d12 0.228 0.234 -0.0061 For kicks, we threw in a column of their differences. The clog-log - logit column highlights how in general, fitted hazard functions from models estimated with both link functions will be indistinguishable unless hazard is high, once again suggesting that the quality of the estimates does not provide a rationale for selecting one of these link functions over the other. (p. 424) Now focus on the pt parameter for both models. In both cases, we antilog [i.e., exponentiate] parameter estimates, but whereas an antilogged [i.e., exponentiated] coefficient from a model with a logit link is an odds ratio, an antilogged coefficient from a model with a clog-log link is a hazard ratio. (p. 424) Here’s that in a plot. as_draws_df(fit11.8) %&gt;% ggplot(aes(x = b_pt %&gt;% exp(), y = 0)) + stat_halfeye(.width = c(.5, .95), na.rm = T) + scale_x_continuous(&quot;b_pt (exponentiated)&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;fit11.8 (logit link)&quot;, subtitle = &quot;Exponentiating this parameter yields an odds ratio.&quot;) + coord_cartesian(xlim = c(1, 5)) + theme(panel.grid = element_blank()) # left p1 &lt;- as_draws_df(fit11.8) %&gt;% ggplot(aes(x = b_pt %&gt;% exp(), y = 0)) + stat_halfeye(.width = c(.5, .95), na.rm = T) + scale_x_continuous(&quot;b_pt (exponentiated)&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;fit11.8 (logit link)&quot;, subtitle = &quot;Exponentiating this parameter yields an odds ratio.&quot;) + coord_cartesian(xlim = c(1, 5)) + theme(panel.grid = element_blank()) # right p2 &lt;- as_draws_df(fit12.15) %&gt;% ggplot(aes(x = b_pt %&gt;% exp(), y = 0)) + stat_halfeye(.width = c(.5, .95)) + scale_x_continuous(&quot;b_pt (exponentiated)&quot;, limits = c(1, 5)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;fit12.15 (clog-log link)&quot;, subtitle = &quot;Exponentiating this parameter yields a hazard ratio.&quot;) + theme(panel.grid = element_blank()) # combine p1 | p2 For the clog-log model, then, in every grade from 7th to 12th, we estimate the hazard of first intercourse for boys who experienced a parenting transition to be [about] 2.2 times the hazard for their peers raised with both biological parents. This interpretation contrasts with that from the model with a logit link, which suggests that the odds of first intercourse are [about] 2.4 times the odds for boys who experienced a parenting transition. (pp. 424–425, emphasis in the original) 12.2.3 Choosing between logit and clog-log links for discrete-time hazard models. The primary advantage of the clog-log link is that in invoking a proportional hazards assumption it yields a direct analog to the continuous time hazard model…. If you believe that the underlying metric for time is truly continuous and that the only reason you observe discretized values is due to measurement difficulties, a model specified with a clog-log link has much to recommend it…. [Yet,] if data are collected in truly discrete time, the clog-log specification has no particular advantage. As N. Beck (1999); Beck, Katz, and Tucker (1998); and Sueyoshi (1995) argue, the proportional hazards assumption is no more sacred than the proportional odds assumption, and while consistency across models is noble, so, too, is simplicity (which decreases the chances of mistake). (p. 426) 12.3 Time-varying predictors “Discrete-time survival analysis adopts naturally to the inclusion of time-varying predictors. Because models are fit using a person-period data set, a time-varying predictor simply takes on its appropriate value for each person in each period” (p. 427). 12.3.1 Assumptions underlying a model with time-varying predictors. Load the depression data from Wheaton, Rozell, and Hall (1997). depression_pp &lt;- read_csv(&quot;data/depression_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(depression_pp) ## Rows: 36,997 ## Columns: 22 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ onset &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ age &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34… ## $ censor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ censage &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34… ## $ aged &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ nsibs &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, … ## $ sibs12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ sibs34 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ sibs56 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ sibs78 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ sibs9plus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ bigfamily &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ period &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, … ## $ event &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pd &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pdnow &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ one &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ age_18 &lt;dbl&gt; -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,… ## $ age_18sq &lt;dbl&gt; 196, 169, 144, 121, 100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64, … ## $ age_18cub &lt;dbl&gt; -2744, -2197, -1728, -1331, -1000, -729, -512, -343, -216, -125, -64, -27, -8, -1, 0, 1, 8… Here is the participant age range. range(depression_pp$age) ## [1] 17 57 We might count how many participants experienced a parental divorce, pd, like this. depression_pp %&gt;% group_by(id) %&gt;% summarise(pd = if_else(sum(pd) &gt; 0, 1, 0)) %&gt;% ungroup() %&gt;% count(pd) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 × 3 ## pd n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1248 89.6 ## 2 1 145 10.4 Here we fit the discrete hazard model based on a quadratic treatment of \\(\\text{age} - 18\\) with and the time-varying predictor pd and without/with the time-invariant predictor female. 1:52 # 1.092947 hours fit12.16 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.16&quot;) # 1.055131 hours fit12.17 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.17&quot;) Check the summaries. print(fit12.16) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd ## Data: depression_pp (Number of observations: 36997) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.25 0.08 -4.40 -4.10 1.00 1743 1884 ## age_18 0.06 0.01 0.04 0.08 1.00 1874 2082 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 2637 2715 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 2290 2451 ## pd 0.42 0.16 0.09 0.73 1.00 2333 1984 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit12.17) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female ## Data: depression_pp (Number of observations: 36997) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.59 0.11 -4.81 -4.38 1.00 1550 1673 ## age_18 0.06 0.01 0.04 0.08 1.00 2095 2161 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 3563 3281 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 2955 2815 ## pd 0.41 0.16 0.08 0.71 1.00 2113 1779 ## female 0.55 0.11 0.33 0.77 1.00 1759 1859 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Before we make our version of Figure 12.4, we’ll need to make an aggregated version of the data, which will allow us to replicate the dots and plus signs. We’ll use the same basic wrangling steps from when we made sex_aggregated from back in Chapter 11. depression_aggregated &lt;- depression_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(age_18) %&gt;% count(event, pd) %&gt;% ungroup() %&gt;% complete(age_18, event, pd) %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0)), age = age_18 + 18, total = event + no_event) %&gt;% mutate(proportion = event / total) %&gt;% mutate(logit = log(proportion / (1 - proportion))) depression_aggregated ## # A tibble: 72 × 8 ## age_18 pd event no_event age total proportion logit ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -14 pd = 0 1 1353 4 1354 0.000739 -7.21 ## 2 -14 pd = 1 NA 39 4 NA NA NA ## 3 -13 pd = 0 NA 1344 5 NA NA NA ## 4 -13 pd = 1 NA 48 5 NA NA NA ## 5 -12 pd = 0 4 1333 6 1337 0.00299 -5.81 ## 6 -12 pd = 1 1 54 6 55 0.0182 -3.99 ## 7 -11 pd = 0 5 1322 7 1327 0.00377 -5.58 ## 8 -11 pd = 1 NA 60 7 NA NA NA ## 9 -10 pd = 0 2 1313 8 1315 0.00152 -6.49 ## 10 -10 pd = 1 1 66 8 67 0.0149 -4.19 ## # ℹ 62 more rows Now make Figure 12.4. nd &lt;- crossing(age_18 = -14:21, pd = 0:1) %&gt;% mutate(age = age_18 + 18) # hazard (top panel) p1 &lt;- fitted(fit12.16, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0))) %&gt;% ggplot(aes(x = age, group = pd, color = pd)) + geom_line(aes(y = Estimate)) + geom_point(data = depression_aggregated, aes(y = proportion, shape = pd), show.legend = F) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 40)) + scale_y_continuous(&quot;proportion experiencing event&quot;, limits = c(0, 0.06)) # logit(hazard) (top panel) p2 &lt;- fitted(fit12.16, newdata = nd, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0))) %&gt;% ggplot(aes(x = age, group = pd, color = pd)) + geom_line(aes(y = Estimate)) + geom_point(data = depression_aggregated, aes(y = logit, shape = pd), show.legend = F) + scale_x_continuous(breaks = 0:8 * 5, limits = c(0, 40)) + scale_y_continuous(&quot;logit(proportion experiencing event)&quot;, limits = c(-8, -2)) # combine ( (p1 / p2) &amp; scale_shape_manual(values = c(3, 16)) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; theme(panel.grid = element_blank()) ) + plot_layout(guides = &quot;collect&quot;) 12.3.2 Interpreting and displaying time-varying predictors’ effects. We jumped the gun a little, but to repeat: you can fit a discrete-time hazard model with time-varying predictors using exactly the same strategies presented in chapter 11. In the person-period data set, use a logistic regression routine to regress the event indicator on variables representing the main effect of TIME and the desired predictors. (pp. 434–435, emphasis in the original) For example, here is the statistical formula we used when we fit fit12.17: \\[ \\begin{align*} \\text{event}_{ij} &amp; = \\operatorname{Binomial}(n = 1, p_{ij}) \\\\ \\operatorname{logit} (p_{ij}) &amp; = [\\alpha_0 + \\alpha_1 (\\text{age}_{ij} - 18) + \\alpha_2 (\\text{age}_{ij} - 18)^2 + \\alpha_3 (\\text{age}_{ij} - 18)^3] \\\\ &amp; \\; \\; + [\\beta_1 \\text{pd}_{ij} + \\beta_2 \\text{female}_{i}] \\\\ \\alpha_0, ..., \\alpha_3 &amp; \\sim \\operatorname{Normal}(0, 4) \\\\ \\beta_1 \\text{ and } \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 4), \\end{align*} \\] where \\(\\operatorname{logit} (p_{ij}) = \\operatorname{logit} \\hat h(t_{ij})\\). If you’d like to compare our results for those displayed by Singer and Willett in Equation 12.8, here are our posterior means. fixef(fit12.17)[, 1] %&gt;% round(digits = 4) ## Intercept age_18 Iage_18E2 Iage_18E3 pd female ## -4.5895 0.0602 -0.0075 0.0002 0.4066 0.5453 The values are pretty close. Though we won’t compare fit12.16 and fit12.17 using deviance values, we will use information criteria. fit12.16 &lt;- add_criterion(fit12.16, criterion = &quot;waic&quot;) fit12.17 &lt;- add_criterion(fit12.17, criterion = &quot;waic&quot;) loo_compare(fit12.16, fit12.17, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.17 0.0 0.0 -2075.6 86.0 6.0 0.4 4151.3 172.0 ## fit12.16 -12.0 5.0 -2087.7 86.4 5.0 0.4 4175.4 172.8 model_weights(fit12.16, fit12.17, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.16 fit12.17 ## 0 1 Based on both the WAIC difference and the WAIC weights, fit12.17 is the clear favorite. From that model, Here’s a plot of the anti-logged (i.e., exponentiated) posteriors for pd and female, which yields their odds ratios. as_draws_df(fit12.17) %&gt;% pivot_longer(b_pd:b_female) %&gt;% mutate(`odds ratio` = exp(value)) %&gt;% ggplot(aes(x = `odds ratio`, y = name)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;xy&quot;) + ylab(NULL) + coord_cartesian(ylim = c(1.4, 2.4)) + theme(panel.grid = element_blank()) Here is our version of Figure 12.5. nd &lt;- crossing(female = 0:1, pd = 0:1) %&gt;% expand_grid(age_18 = -14:21) %&gt;% mutate(age = age_18 + 18) f &lt;- fitted(fit12.17, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(sex = if_else(female == 0, &quot;male&quot;, &quot;female&quot;), pd = factor(str_c(&quot;pd = &quot;, pd), levels = str_c(&quot;pd = &quot;, 1:0))) # hazard (top panel) p1 &lt;- f %&gt;% ggplot(aes(x = age, group = pd, color = pd, fill = pd)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5, linewidth = 0) + geom_line(aes(y = Estimate)) + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 40)) + scale_y_continuous(&quot;fitted hazard&quot;, limits = c(0, 0.04)) # survival (bottom panel) p2 &lt;- f %&gt;% group_by(sex, pd) %&gt;% mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = ~cumprod(1 - .)) %&gt;% ggplot(aes(x = age, group = pd, color = pd, fill = pd)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 1/5, linewidth = 0) + geom_line(aes(y = Estimate)) + scale_x_continuous(breaks = 0:8 * 5, limits = c(0, 40)) + scale_y_continuous(&quot;fitted survival probability&quot;, limits = c(0, 1)) + theme(strip.background.x = element_blank(), strip.text.x = element_blank()) # combine ( (p1 / p2) &amp; scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6) &amp; theme(panel.grid = element_blank()) &amp; facet_wrap(~ sex) ) + plot_layout(guides = &quot;collect&quot;) Note how with this many age levels, our discrete-time models are beginning to look like continuous-time models. 12.3.3 Two caveats: The problems of state and rate dependence. A time-varying predictor is state-dependent if its values at time \\(t_j\\) are affected by an individual’s state (event occurrence status) at time \\(t_j\\): \\(EVENT_{ij}\\). A time-varying predictor is rate-dependent if its values at time \\(t_j\\) are affected by the individuals value of hazard (the “rate”) at time \\(t_j\\): \\(h (t_{ij})\\). (p. 440, emphasis in the original) 12.4 The linear additivity assumption: Uncovering violations and simple solutions Because the focus on hazard causes you to analyze group level summaries, model violations can be more difficult to discern [than in other kinds of regression models]. We therefore devote this section to introducing practical strategies for diagnosing and correcting violations of the linear additivity assumption. (p. 443) 12.4.1 Interactions between substantive predictors. We do not advocate fishing expeditions. Open searches for interactions can be counterproductive, leading to the discovery of many “effects” that are little more than sampling variation. But there are at least two circumstances when a guided search for interactions is crucial: When theory (or common sense!) suggests that two (or more) predictors will interact in the prediction of the outcome. If you hypothesize the existence of interactions a priori, your search will be targeted and efficient. When examining the effects of “question” predictor(s), variables whose effects you intend to emphasize in your report. You need to be certain that these predictors’ effects do not differ according ot levels of other important predictors, lest you misinterpret your major findings. With this in mind, we now demonstrate how to (1) explore your data for the possibility of statistical interactions; and (2) include the additional appropriate terms when necessary. (p. 444, emphasis in the original) Load the data from Keiley and Martin [-M. K. Keiley &amp; Martin (2002)]3. firstarrest_pp &lt;- read_csv(&quot;data/firstarrest_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(firstarrest_pp) ## Rows: 15,834 ## Columns: 19 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, … ## $ time &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 17, 17, 17, 17, 1… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, … ## $ abused &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, … ## $ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ ablack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ d8 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, … ## $ d9 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ d10 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ d11 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, … ## $ d13 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ d14 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ d15 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, … ## $ d16 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … ## $ d17 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, … ## $ d18 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ period &lt;dbl&gt; 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13,… ## $ event &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, … The total \\(N\\) is 1553. firstarrest_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 1553 \\(n = 887\\) were abused. firstarrest_pp %&gt;% filter(abused == 1) %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 887 \\(n = 342\\) were arrested between the ages of 8 and 18. firstarrest_pp %&gt;% filter(censor == 0) %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 342 Since the two focal predictors in this section with be black and abused, here are the counts broken down by both. firstarrest_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(black, abused) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 4 × 4 ## black abused n percent ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0 434 27.9 ## 2 0 1 605 39.0 ## 3 1 0 232 14.9 ## 4 1 1 282 18.2 Here’s how to hand compute the values for the sample logit(hazard) functions depicted in the top panels of Figure 12.6. # wrangle firstarrest_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(period) %&gt;% count(event, black, abused) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% drop_na(event) %&gt;% mutate(total = event + no_event, logit = log(event / total / (1 - event / total)), race = factor(ifelse(black == 1, &quot;Black&quot;, &quot;White&quot;), levels = c(&quot;White&quot;, &quot;Black&quot;)), abused = ifelse(abused == 1, &quot;abused&quot;, &quot;not abused&quot;) %&gt;% factor()) %&gt;% # plot! ggplot(aes(x = period, y = logit)) + geom_line(aes(color = abused, group = abused)) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_x_continuous(&quot;age&quot;, breaks = 7:19, limits = c(7, 19)) + scale_y_continuous(&quot;sample logit(hazard)&quot;, limits = c(-7, -2)) + theme(panel.grid = element_blank()) + facet_wrap(~ race) For kicks and giggles, it might informative to fit a series of subset Bayesian models to make similar versions of those sample hazard functions. Before we fit the models, let’s make our lives easier and make a factor version of our time variable, period. firstarrest_pp &lt;- firstarrest_pp %&gt;% mutate(period_f = factor(period)) Fit the four subset models. # white, not abused fit12.18 &lt;- brm(data = firstarrest_pp %&gt;% filter(black == 0 &amp; abused == 0), family = binomial, event | trials(1) ~ 0 + period_f, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.18&quot;) # white, abused fit12.19 &lt;- update(fit12.18, newdata = firstarrest_pp %&gt;% filter(black == 0 &amp; abused == 1), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.19&quot;) # black, not abused fit12.20 &lt;- update(fit12.18, newdata = firstarrest_pp %&gt;% filter(black == 1 &amp; abused == 0), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.20&quot;) # black, abused fit12.21 &lt;- update(fit12.18, newdata = firstarrest_pp %&gt;% filter(black == 1 &amp; abused == 1), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.21&quot;) We might use fitted() via our custom make_fitted() function to perform some of the pre-plotting computation and wrangling. nd &lt;- tibble(period_f = 8:18) # this will simplify the `mutate()` code below models &lt;- c(&quot;constant&quot;, &quot;linear&quot;, &quot;quadratic&quot;, &quot;cubic&quot;, &quot;general&quot;) f &lt;- bind_rows(make_fitted(fit12.18, scale = &quot;linear&quot;), # white, not abused make_fitted(fit12.19, scale = &quot;linear&quot;), # white, abused make_fitted(fit12.20, scale = &quot;linear&quot;), # black, not abused make_fitted(fit12.21, scale = &quot;linear&quot;)) %&gt;% # black, abused mutate(race = factor(rep(c(&quot;White&quot;, &quot;Black&quot;), each = n() / 2), levels = c(&quot;White&quot;, &quot;Black&quot;)), abuse = rep(c(&quot;not abused&quot;, &quot;abused&quot;, &quot;not abused&quot;, &quot;abused&quot;), each = n() / 4)) # what have we done? glimpse(f) ## Rows: 44 ## Columns: 7 ## $ Estimate &lt;dbl&gt; -6.113737, -7.767396, -5.421275, -4.486981, -7.744434, -4.128871, -3.851241, -3.039893, -3… ## $ Est.Error &lt;dbl&gt; 0.9614576, 1.7504718, 0.6720442, 0.4486308, 1.7498479, 0.3776723, 0.3404120, 0.2335922, 0.… ## $ Q2.5 &lt;dbl&gt; -8.345358, -11.981231, -6.962572, -5.458206, -12.100321, -4.942711, -4.558721, -3.510652, … ## $ Q97.5 &lt;dbl&gt; -4.579547, -5.171702, -4.295233, -3.687680, -5.140699, -3.451226, -3.252522, -2.600993, -3… ## $ period_f &lt;int&gt; 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, … ## $ race &lt;fct&gt; White, White, White, White, White, White, White, White, White, White, White, White, White,… ## $ abuse &lt;chr&gt; &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not abused&quot;, &quot;not a… Now plot. f %&gt;% ggplot(aes(x = period_f, y = Estimate, group = abuse, color = abuse)) + geom_line() + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_x_continuous(&quot;age&quot;, breaks = 7:19, limits = c(7, 19)) + ylab(&quot;sample logit(hazard)&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ race) Notice how this looks different from the version of the plot, above, in that there are five points with very low values on the \\(y\\)-axis. Did you notice the drop_na(event) line in the code when we computed the sample logit(hazard) values by hand? Those points with missing values in the data are what caused those very low log(hazard) estimates in the models. Next we’ll fit the primary statistical models with both black and abused, without and with their interaction term, based on the full data set. You’ll see that when we use all cases, those odd low logit(hazard) values go away. fit12.22 &lt;- brm(data = firstarrest_pp, family = binomial, event | trials(1) ~ 0 + d8 + d9 + d10 + d11 + d12 + d13 + d14 + d15 + d16 + d17 + d18 + abused + black, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.22&quot;) fit12.23 &lt;- update(fit12.22, newdata = firstarrest_pp, event | trials(1) ~ 0 + d8 + d9 + d10 + d11 + d12 + d13 + d14 + d15 + d16 + d17 + d18 + abused + black + abused:black, chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.23&quot;) Let’s compare the models with information criteria. fit12.22 &lt;- add_criterion(fit12.22, criterion = &quot;waic&quot;) fit12.23 &lt;- add_criterion(fit12.23, criterion = &quot;waic&quot;) loo_compare(fit12.22, fit12.23, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.23 0.0 0.0 -1538.7 65.3 13.9 1.0 3077.5 130.6 ## fit12.22 -1.0 2.2 -1539.7 65.3 12.9 1.0 3079.5 130.6 model_weights(fit12.22, fit12.23, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.22 fit12.23 ## 0.268 0.732 Our results diverge a bit from those in the text. For the deviance test on page 446, Singer and Willett reported a difference of \\(4.05 \\; (p &lt; .05, \\textit{df} = 1)\\) in favor of the full model (i.e., fit12.23). Though both our WAIC difference score and the WAIC weights favor fit12.23, it’s by a hair. “As in linear (or logistic) regression, we interpret interaction effects by simultaneously considering all the constituent parameters, for the cross-product term and its main-effect components” (p. 446, emphasis in the original). Rather than the point-estimate table Singer and Willett displayed at the bottom of the page, we’ll present the full posterior distributions odds ratios in a faceted plot. as_draws_df(fit12.23) %&gt;% expand_grid(abused = 0:1, black = 0:1) %&gt;% mutate(`odds ratio` = exp(b_abused * abused + b_black * black + `b_abused:black` * abused * black), abused = str_c(&quot;abused = &quot;, abused), black = str_c(&quot;black = &quot;, black)) %&gt;% ggplot(aes(x = `odds ratio`, y = 0)) + stat_halfeye(.width = .95, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_grid(abused ~ black) Here’s our version of the lower panel of Figure 12.6. # define the `newdata` nd &lt;- firstarrest_pp %&gt;% distinct(abused, black, d8, d9, d10, d11, d12, d13, d14, d15, d16, d17, d18, period) # use `fitted()` and wrangle make_fitted(fit12.23, scale = &quot;linear&quot;) %&gt;% mutate(abused = ifelse(abused == 1, &quot;abused&quot;, &quot;not abused&quot;) %&gt;% factor(), sex = factor(ifelse(black == 1, &quot;Black&quot;, &quot;White&quot;), levels = c(&quot;White&quot;, &quot;Black&quot;))) %&gt;% # plot! ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = abused, color = abused)) + geom_ribbon(alpha = 1/5, linewidth = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_x_continuous(&quot;age&quot;, breaks = 7:19, limits = c(7, 19)) + ylab(&quot;fitted logit(hazard)&quot;) + coord_cartesian(ylim = c(-8, -2)) + theme(panel.grid = element_blank()) + facet_wrap(~ sex) 12.4.2 Nonlinear effects. There are two general strategies for exploring the linearity assumption. The simplest approach–although somewhat limited–is to fit additional models, replacing the raw predictor with a re-expressed version. Although the additional models also invoke a linearity constraint, use of re-expressed predictors guarantees that the effects represent nonlinear relationships for the raw predictors. The ladder of power (section 6.2.1) provides a dizzying array of options. The second approach is to categorize each continuous variable into a small number of groups, create a series of dummy variables representing group membership, and visually examine the pattern of parameter estimates for consecutive dummies to deduce the appropriate functional form. If the pattern is linear, retain the predictor in its raw state; if not, explore an alternative specification. As the first approach is straightforward, we illustrate the second, using the depression onset data presented in section 12.3. (pp. 447–448, emphasis in the original) Here’s another look at those depression_pp data. depression_pp %&gt;% glimpse() ## Rows: 36,997 ## Columns: 22 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ onset &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ age &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34… ## $ censor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ censage &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34… ## $ aged &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ nsibs &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, … ## $ sibs12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ sibs34 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ sibs56 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ sibs78 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ sibs9plus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ bigfamily &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ period &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, … ## $ event &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pd &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pdnow &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ one &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ age_18 &lt;dbl&gt; -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,… ## $ age_18sq &lt;dbl&gt; 196, 169, 144, 121, 100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64, … ## $ age_18cub &lt;dbl&gt; -2744, -2197, -1728, -1331, -1000, -729, -512, -343, -216, -125, -64, -27, -8, -1, 0, 1, 8… Our three models will be cubic with respect to our time variable, age_18. The focal predictor whose form (non)linear form we’re interested in is nsibs. Singer and Willett’s first model, Model A (see Table 12.4, p. 449), treated nsibs as linear. Fit the model with brms. # model a (linear) # 1.598436 hours fit12.24 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + nsibs, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.24&quot;) Check the model summary. print(fit12.24) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + nsibs ## Data: depression_pp (Number of observations: 36997) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.36 0.12 -4.59 -4.12 1.00 2083 2530 ## age_18 0.06 0.01 0.04 0.08 1.00 2065 2112 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 4051 3040 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 2935 2930 ## pd 0.36 0.16 0.05 0.66 1.00 2307 1925 ## female 0.56 0.11 0.35 0.77 1.00 2262 2409 ## nsibs -0.08 0.02 -0.13 -0.04 1.00 2566 2205 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we invert the antilog of nsibs and summarize the posterior with median_qi(). as_draws_df(fit12.24) %&gt;% median_qi(1 / exp(b_nsibs)) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 1 × 6 ## `1/exp(b_nsibs)` .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.09 1.04 1.14 0.95 median qi Singer and Willett described nsibs as “highly skewed” (p. 448). Let’s take a look. depression_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(x = nsibs)) + geom_bar() + theme(panel.grid = element_blank()) Yep, sure is. Here’s a way to save Singer and Willett’s discretized version of nsibs and then break the cases down in terms of \\(n\\) and percentage, which matches nicely with the numbers at the bottom of page 449. depression_pp &lt;- depression_pp %&gt;% mutate(nsibs_cat = case_when( nsibs == 0 ~ &quot;0&quot;, nsibs %in% c(1, 2) ~ &quot;1 or 2&quot;, nsibs %in% c(3, 4) ~ &quot;3 or 4&quot;, nsibs %in% c(5, 6) ~ &quot;5 or 6&quot;, nsibs %in% c(7, 8) ~ &quot;7 or 8&quot;, nsibs &gt;= 9 ~ &quot;9 or more&quot; ) ) depression_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(nsibs_cat) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(digits = 1)) ## # A tibble: 6 × 3 ## nsibs_cat n percent ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 98 7 ## 2 1 or 2 672 48.2 ## 3 3 or 4 330 23.7 ## 4 5 or 6 159 11.4 ## 5 7 or 8 72 5.2 ## 6 9 or more 62 4.5 Now fit our version of Model B, in which we replace the original linear predictor nsibs with a series of dummies: sibs12, sibs34,…, sibs9plus. # model b (nonlinear) # 2.346094 hours fit12.25 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + sibs12 + sibs34 + sibs56 + sibs78 + sibs9plus, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.25&quot;) Here’s the model summary. print(fit12.25) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + sibs12 + sibs34 + sibs56 + sibs78 + sibs9plus ## Data: depression_pp (Number of observations: 36997) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.51 0.21 -4.95 -4.12 1.00 1171 1774 ## age_18 0.06 0.01 0.04 0.08 1.00 3271 2602 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 5517 2781 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 4190 3636 ## pd 0.36 0.16 0.03 0.67 1.00 2489 2324 ## female 0.56 0.11 0.36 0.78 1.00 2673 2847 ## sibs12 0.03 0.20 -0.35 0.44 1.00 1183 1892 ## sibs34 0.02 0.21 -0.38 0.43 1.00 1286 1998 ## sibs56 -0.50 0.26 -1.00 0.02 1.00 1458 2047 ## sibs78 -0.80 0.35 -1.50 -0.15 1.00 1967 2402 ## sibs9plus -0.68 0.35 -1.37 -0.01 1.00 1932 2301 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It might be easier to compare the various sibs* coefficients with a coefficient plot. as_draws_df(fit12.25) %&gt;% pivot_longer(contains(&quot;sibs&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;)) %&gt;% ggplot(aes(x = value, y = name)) + stat_interval(size = 5, .width = c(.1, .5, .9)) + scale_color_grey(&quot;CI level:&quot;, start = .8, end = .2) + labs(x = &quot;sibs coeficients&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Here is the breakdown by bigfamily. depression_pp %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% count(bigfamily) %&gt;% mutate(percent = (100 * n / sum(n)) %&gt;% round(digits = 2)) ## # A tibble: 2 × 3 ## bigfamily n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1100 79.0 ## 2 1 293 21.0 Now fit our version of Model C, the dichotomized bigfamily model. # model c (dichotomized) # 1.560219 hours fit12.26 &lt;- brm(data = depression_pp, family = binomial, event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + bigfamily, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.26&quot;) Check the model summary. print(fit12.26) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + bigfamily ## Data: depression_pp (Number of observations: 36997) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -4.48 0.11 -4.70 -4.28 1.00 1833 1825 ## age_18 0.06 0.01 0.04 0.08 1.00 2532 2480 ## Iage_18E2 -0.01 0.00 -0.01 -0.01 1.00 4095 3489 ## Iage_18E3 0.00 0.00 0.00 0.00 1.00 3390 3365 ## pd 0.36 0.16 0.04 0.67 1.00 2411 2150 ## female 0.56 0.11 0.35 0.77 1.00 1804 1972 ## bigfamily -0.61 0.15 -0.91 -0.34 1.00 3291 2267 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summaries look very similar to the values in the rightmost column of Table 12.4 in the text. Here is a look at the antilog of our bigfamily coefficient. # just to make the x-axis pretty breaks &lt;- c(exp(fixef(fit12.26)[&quot;bigfamily&quot;, c(1, 3:4)]), 1) %&gt;% as.vector() labels &lt;- c(exp(fixef(fit12.26)[&quot;bigfamily&quot;, c(1, 3:4)]), 1) %&gt;% round(digits = 3) %&gt;% as.vector() # plot! as_draws_df(fit12.26) %&gt;% ggplot(aes(x = exp(b_bigfamily), y = 0)) + geom_vline(xintercept = 1, color = &quot;white&quot;) + stat_halfeye(.width = c(.5, .95)) + scale_x_continuous(&quot;bigfamily odds ratio&quot;, breaks = breaks, labels = labels) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Now compute the WAIC estimates for fit12.24 through fit12.26 and compare them by their WAIC differences and WAIC weights. fit12.24 &lt;- add_criterion(fit12.24, criterion = &quot;waic&quot;) fit12.25 &lt;- add_criterion(fit12.25, criterion = &quot;waic&quot;) fit12.26 &lt;- add_criterion(fit12.26, criterion = &quot;waic&quot;) loo_compare(fit12.24, fit12.25, fit12.26, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.26 0.0 0.0 -2066.3 85.6 6.9 0.5 4132.7 171.3 ## fit12.24 -2.8 2.9 -2069.2 85.8 7.1 0.5 4138.3 171.6 ## fit12.25 -3.8 1.1 -2070.2 85.8 11.1 0.7 4140.3 171.7 model_weights(fit12.24, fit12.25, fit12.26, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.24 fit12.25 fit12.26 ## 0.054 0.020 0.926 Our WAIC estimates are very similar to the AIC estimates presented in Table 12.4. By both the differences and the weights, fit12.16 (Model C) is the best among the three. Though its rank is decisive with the weights, it’s less impressive if you compare the se_diff values with the elpd_diff values. Those suggest there’s a lot of uncertainty in our WAIC differences. 12.5 The proportionality assumption: Uncovering violations and simple solutions All the discrete hazard models postulated so far invoke another common, but restrictive assumption: that each predictor has an identical effect in every time period under study. This constraint, known as the proportionality assumption, stipulates that a predictor’s effect does not depend on the respondent’s duration in the initial state…. Yet is it not possible, even likely, that the effects of some predictors will vary over time? (p. 451, emphasis in the original) 12.5.1 Discrete-time hazard models that do not invoke a proportionality assumption. “There are dozens of ways of violating the proportionality assumption” (p. 452). We see three such examples in panels B through D in Figure 12.7. Here’s our version of the plot. p1 &lt;- crossing(z = 0:1, x = 1:8) %&gt;% mutate(y = -2.1 + -0.2 * (x - 1) + 1.1 * z, z = factor(z)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. p2 &lt;- crossing(z = 0:1, x = 1:8) %&gt;% mutate(y = -4.8 + 0.28 * (x - 1) + 0.01 * z + 0.25 * x * z, z = factor(z)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z)) p3 &lt;- crossing(z = 0:1, x = 1:8) %&gt;% mutate(y = -4.8 + 0.25 * (x - 1) + 4.3 * z + -0.5 * x * z, z = factor(z)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z)) p4 &lt;- crossing(z1 = 0:1, x = 1:8) %&gt;% mutate(z2 = rep(0:1, times = n() / 2), y = -2.8 + -0.2 * (x - 1) + 1.8 * z1 + -1.4 * z1 * z2, z1 = factor(z1)) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(aes(size = z1)) # combine ( (p1 + p2 + p3 + p4) + plot_annotation(tag_levels = &quot;A&quot;) ) &amp; scale_size_manual(values = c(1, 1/2)) &amp; scale_x_continuous(&quot;time period&quot;, breaks = 0:8, limits = c(0, 8)) &amp; scale_y_continuous(&quot;logit hazard&quot;, limits = c(-5, -0.5)) &amp; theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) In case is wasn’t clear, I just winged it on the equations for the y values in each subplot. If you’d like to annotate the subplots with the arrows and \\(\\beta\\) labels as depicted in the original Figure 12.7, consider it a homework exercise. 12.5.2 Investigating the proportionality assumption in practice. Load the data from Graham’s (1997) dissertation. mathdropout_pp &lt;- read_csv(&quot;data/mathdropout_pp.csv&quot;) %&gt;% # convert the column names to lower case rename_all(tolower) glimpse(mathdropout_pp) ## Rows: 9,558 ## Columns: 19 ## $ id &lt;dbl&gt; 201303, 201303, 201304, 201304, 201304, 201305, 201305, 201311, 201311, 201311, 201311, 20131… ## $ lastpd &lt;dbl&gt; 2, 2, 3, 3, 3, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 1, 5, 5, 5, 5, 5, 2, 2, 3, 3, 3, … ## $ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, … ## $ one &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ hs11 &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, … ## $ hs12 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, … ## $ coll1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, … ## $ coll2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ coll3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ fhs11 &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, … ## $ fhs12 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, … ## $ fcoll1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, … ## $ fcoll2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ fcoll3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ period &lt;dbl&gt; 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, … ## $ event &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, … ## $ ltime &lt;dbl&gt; 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 0, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2, … ## $ fltime &lt;dbl&gt; 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2, … The data are composed of \\(n = 3{,}790\\) high school students. mathdropout_pp %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 3790 Here is the division by female. mathdropout_pp %&gt;% distinct(id, female) %&gt;% count(female) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 × 3 ## female n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1875 49.5 ## 2 1 1915 50.5 Singer and Willett also wrote: “only 93 men and 39 women took a mathematics class in each of the next five terms” (p. 456). I think this is a typo. Here’s the break down by censor and female at the fifth time period (period == 5). mathdropout_pp %&gt;% filter(period == 5) %&gt;% count(censor, female) ## # A tibble: 4 × 3 ## censor female n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 0 39 ## 2 0 1 39 ## 3 1 0 93 ## 4 1 1 51 Fitting the three models displayed in Table 12.5 is a mild extension from our previous models. From a brm() perspective, there’s nothing new, here. fit12.27 &lt;- brm(data = mathdropout_pp, family = binomial, event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.27&quot;) fit12.28 &lt;- brm(data = mathdropout_pp, family = binomial, event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.28&quot;) fit12.29 &lt;- brm(data = mathdropout_pp, family = binomial, event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female + fltime, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 12, file = &quot;fits/fit12.29&quot;) Heads up about the formula for fit12.28 (Model B). Many of the examples in the text and the corresponding data sets we’ve been working with included pre-computed interaction terms. We have been quietly ignoring those and making our interactions by hand in our formula arguments. Here we went ahead with the text and just used event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3. If you wanted a more verbose version of that code, we could have specified either event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female:hs11 + female:hs12 + female:coll1 + female:coll2 + female:coll3 or event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female:(hs11 + hs12 + coll1 + coll2 + coll3). All three return the same results. Anyway, let’s check the model summaries. print(fit12.27) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female ## Data: mathdropout_pp (Number of observations: 9558) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## hs11 -2.13 0.06 -2.24 -2.02 1.00 3274 3212 ## hs12 -0.94 0.05 -1.03 -0.85 1.00 3170 3111 ## coll1 -1.45 0.06 -1.57 -1.33 1.00 3805 2924 ## coll2 -0.62 0.08 -0.77 -0.47 1.00 3927 2854 ## coll3 -0.77 0.14 -1.05 -0.50 1.00 5012 3121 ## female 0.38 0.05 0.28 0.47 1.00 2601 3117 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit12.28) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3 ## Data: mathdropout_pp (Number of observations: 9558) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## hs11 -2.01 0.07 -2.15 -1.86 1.00 3929 2636 ## hs12 -0.96 0.06 -1.08 -0.85 1.00 4242 3118 ## coll1 -1.49 0.09 -1.65 -1.32 1.00 4393 2983 ## coll2 -0.71 0.10 -0.91 -0.52 1.00 4290 3270 ## coll3 -0.87 0.19 -1.25 -0.49 1.00 4140 3233 ## fhs11 0.15 0.10 -0.05 0.35 1.00 3793 2706 ## fhs12 0.42 0.08 0.26 0.57 1.00 4180 3169 ## fcoll1 0.44 0.12 0.22 0.66 1.00 3704 3113 ## fcoll2 0.57 0.14 0.29 0.85 1.00 4466 3450 ## fcoll3 0.60 0.29 0.03 1.16 1.00 3934 3034 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(fit12.29) ## Family: binomial ## Links: mu = logit ## Formula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female + fltime ## Data: mathdropout_pp (Number of observations: 9558) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## hs11 -2.05 0.06 -2.17 -1.92 1.00 2486 3112 ## hs12 -0.92 0.05 -1.02 -0.83 1.00 2807 2921 ## coll1 -1.50 0.07 -1.63 -1.37 1.00 3467 2801 ## coll2 -0.72 0.09 -0.89 -0.55 1.00 3033 2710 ## coll3 -0.92 0.16 -1.23 -0.62 1.00 3308 3165 ## female 0.23 0.08 0.08 0.38 1.00 2014 2564 ## fltime 0.12 0.05 0.03 0.21 1.00 2162 2478 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The results are similar to those in Table 12.5. Based on fit12.27 (Model A), here is the posterior for the odds ratio for female. as_draws_df(fit12.27) %&gt;% ggplot(aes(x = exp(b_female), y = 0)) + stat_halfeye(.width = c(.5, .95)) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;odds ratio for female&quot;) + theme(panel.grid = element_blank()) With regard to the interaction terms for fit12.28 (Model B), Singer and Willett remarked: “Notice how the estimates rise over time” (p. 457). It might be easiest to observe that in a plot. as_draws_df(fit12.28) %&gt;% pivot_longer(starts_with(&quot;b_f&quot;)) %&gt;% mutate(name = factor(str_remove(name, &quot;b_&quot;), levels = c(str_c(&quot;fhs&quot;, 11:12), str_c(&quot;fcoll&quot;, 1:3)))) %&gt;% ggplot(aes(x = value, y = name)) + stat_interval(size = 5, .width = c(.1, .5, .9)) + scale_color_grey(&quot;CI level:&quot;, start = .8, end = .2) + labs(x = &quot;interaction terms&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Make and save the subplots for Figure 12.8. # Within-group sample hazard functions p1 &lt;- mathdropout_pp %&gt;% mutate(event = if_else(event == 1, &quot;event&quot;, &quot;no_event&quot;)) %&gt;% group_by(period) %&gt;% count(event, female) %&gt;% ungroup() %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(total = event + no_event, logit = log(event / total / (1 - event / total))) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = logit)) + geom_line(aes(color = female), show.legend = F) + scale_y_continuous(&quot;sample logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Within-group sample hazard functions&quot;) # Model A: Main effect of female nd &lt;- mathdropout_pp %&gt;% distinct(female, period, hs11, hs12, coll1, coll2, coll3) p2 &lt;- make_fitted(fit12.27, scale = &quot;linear&quot;) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = female, color = female)) + geom_ribbon(alpha = 1/5, linewidth = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_y_continuous(&quot;fitted logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Model A: Main effect of female&quot;) # Model B: Completely general\\ninteraction between female and time nd &lt;- mathdropout_pp %&gt;% distinct(female, period, hs11, hs12, coll1, coll2, coll3, fhs11, fhs12, fcoll1, fcoll2, fcoll3) p3 &lt;- make_fitted(fit12.28, scale = &quot;linear&quot;) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = female, color = female)) + geom_ribbon(alpha = 1/5, linewidth = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_y_continuous(&quot;fitted logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Model B: Completely general\\ninteraction between female and time&quot;) # Model C: Interaction\\nbetween female and time nd &lt;- mathdropout_pp %&gt;% distinct(female, period, hs11, hs12, coll1, coll2, coll3, fltime) p4 &lt;- make_fitted(fit12.29, scale = &quot;linear&quot;) %&gt;% mutate(female = factor(female, levels = 1:0, labels = c(&quot;F&quot;, &quot;M&quot;))) %&gt;% # plot ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = female, color = female)) + geom_ribbon(alpha = 1/5, linewidth = 0) + geom_line() + scale_fill_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) + scale_y_continuous(&quot;fitted logit(hazard)&quot;, breaks = -2:0) + labs(subtitle = &quot;Model C: Interaction\\nbetween female and time&quot;) Now combine the subplots, augment them in bulk, and return our version of Figure 12.8. (p1 + p2 + p3 + p4 + plot_layout(guides = &quot;collect&quot;)) &amp; scale_color_viridis_d(NULL, option = &quot;A&quot;, end = .6, direction = -1) &amp; scale_x_continuous(&quot;term&quot;, breaks = 1:5, labels = c(&quot;HS 11&quot;, &quot;HS 12&quot;, &quot;C 1&quot;, &quot;C 2&quot;, &quot;C 3&quot;)) &amp; coord_cartesian(ylim = c(-2.3, 0)) &amp; theme(panel.grid = element_blank()) Compute the information criteria for the three models. Since these were a bit faster to compute than for some of the earlier models in this chapter, we’ll go ahead and compute both the LOO and the WAIC. fit12.27 &lt;- add_criterion(fit12.27, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.28 &lt;- add_criterion(fit12.28, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit12.29 &lt;- add_criterion(fit12.29, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) Now compare the models with information criteria differences and weights. loo_compare(fit12.27, fit12.28, fit12.29, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## fit12.29 0.0 0.0 -4906.0 51.3 7.1 0.1 9811.9 102.5 ## fit12.27 -2.0 2.6 -4908.0 51.3 5.8 0.1 9816.0 102.6 ## fit12.28 -2.2 1.2 -4908.2 51.3 10.0 0.2 9816.4 102.7 loo_compare(fit12.27, fit12.28, fit12.29, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## fit12.29 0.0 0.0 -4906.0 51.3 7.1 0.1 9811.9 102.5 ## fit12.27 -2.0 2.6 -4908.0 51.3 5.8 0.1 9816.0 102.6 ## fit12.28 -2.2 1.2 -4908.2 51.3 10.0 0.2 9816.3 102.7 model_weights(fit12.27, fit12.28, fit12.29, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## fit12.27 fit12.28 fit12.29 ## 0.105 0.088 0.806 model_weights(fit12.27, fit12.28, fit12.29, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## fit12.27 fit12.28 fit12.29 ## 0.105 0.089 0.806 You’ll note both the LOO and WAIC estimates are very close to those displayed in Table 12.5. When we take their standard errors into account, fit12.29 (Model C) is marginally better than the other two models. fit12.29 also took most of the LOO and WAIC weights. How do we interpret the gender differential implied by Model C? Because we have centered TIME at 1, the coefficient for FEMALE (0.2275) estimates the differential in time period 1, which here is 11th grade. Antilogging yields 1.26, which leads us to estimate that in 11th grade, the odds of ending one’s mathematics course-taking career are 26% higher for females. (p. 460, emphasis in the original) Let’s examine those results with our Bayesian fit. as_draws_df(fit12.29) %&gt;% mutate(`log odds` = b_female, `odds ratio` = exp(b_female)) %&gt;% pivot_longer(contains(&quot;odds&quot;)) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;the effects of female in two metrics&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Now examine the odds ratio for three different educational periods. as_draws_df(fit12.29) %&gt;% mutate(`12th grade` = exp(b_female + b_fltime), `1st semester of college` = exp(b_female + 2 * b_fltime), `3rd semester of college` = exp(b_female + 4 * b_fltime)) %&gt;% pivot_longer(contains(&quot; &quot;)) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;The interaction effect in different periods&quot;, x = &quot;odds ratio&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) 12.6 The no unobserved heterogeneity assumption: No simple solution All the hazard models discussed in this book–both the discrete-time models we are discussing now and the continuous-time models we will soon introduce–impose an additional assumption to which we have alluded: the assumption of no unobserved heterogeneity. Every model assumes that the population hazard function for individual \\(i\\) depends only on his or her predictor values. Any pair of individuals who share identical predictor profiles will have identical population functions…. Many data sets will not conform to this assumption. As in the multilevel model for change (and regular regression for that matter), pairs of individuals who share predictor profiles are very likely to have different outcomes…. Unobserved heterogeneity can have serious consequences. In their classic (1985) paper, Vaupel and Yaskin elegantly demonstrate what they call “heterogeneity’s ruses”–that ability of unobserved heterogeneity to create the misimpression that a hazard function follows a particular form, when in fact it may not…. Is it possible to fit a hazard model that accounts for unobserved heterogeneity? As you might expect, doing so requires that we have either additional data (for example, data on repeated events within individuals) or that we invoke other–perhaps less tenable–assumptions about the distribution of event time errors (Aalen, 1988; Heckman &amp; Singer, 1984; Mare, 1994; Scheike &amp; Jensen, 1997; Vaupel et al., 1979). As a result, most empirical researchers–and we–proceed ahead, if not ignoring the problem, at least not addressing it. In the remainder of this book, we assume that all heterogeneity is observed and attributable to the predictors included in our models. 12.7 Residual analysis Before concluding that your model is sound, you should ascertain how well it performs for individual cases. As in regular regression, we [can] address this question by examining residuals. Residuals compare–usually through subtraction–an outcome’s “observed” value to its model-based “expected” value. For a discrete-time hazard model, a simple difference will not suffice because each person has not a single outcome but a set of outcomes–one for each time period when he or she was at risk. This suggests the need for a residual defined at the person-period level. A further complication is that the observed outcome in every time period has a value of either 0 or 1 while its expected value—the predicted hazard probability—lies between these extremes. (p. 463, emphasis in the original) Starting on page 464, Singer and Willett illustrated a residual analysis using Model D from Table 11.3. In the last chapter, we called that fit11.10. Here it is, again. fit11.10 &lt;- brm(data = sex_pp, family = binomial, event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas, prior(normal(0, 4), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 11, file = &quot;fits/fit11.10&quot;) With brms, users can extract the residuals of a brm() fit with the residuals() function. residuals(fit11.10) %&gt;% str() ## num [1:822, 1:4] -0.0967 -0.0523 0.8177 -0.0837 -0.0445 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; The output is similar to what we get from fitted(). We have a numeric array of 822 rows and 4 columns. There are 822 rows because that is the number of rows in the original data set with which we fit the model, sex_pp. sex_pp %&gt;% glimpse() ## Rows: 822 ## Columns: 11 ## $ id &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, … ## $ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8… ## $ event &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ d7 &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, … ## $ d8 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, … ## $ d9 &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, … ## $ d10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ d11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ d12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pt &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, … ## $ pas &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, … Just as we often express the uncertainty in our Bayesian models with parameter summaries from the posterior, we also express the uncertainty of our residuals. Thus, the four columns returned by the residuals() function are the familiar summary columns of Estimate, Est.Error, Q2.5, and Q97.5. On page 465, Singer and Willett showcased the deviance residuals for eight participants. We’re going to diverge from them a little. In the plot, below, we’ll look at the residuals for the first 10 cases in the data, by period. residuals(fit11.10) %&gt;% data.frame() %&gt;% bind_cols(sex_pp) %&gt;% mutate(event = factor(event), period = factor(str_c(&quot;period &quot;, period), levels = str_c(&quot;period &quot;, 7:12))) %&gt;% # reduce the number of cases filter(id &lt; 11) %&gt;% ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = event)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(fatten = 2/3) + scale_color_viridis_d(option = &quot;A&quot;, end = .6) + scale_x_continuous(breaks = 1:10, labels = rep(c(1, &quot;&quot;, 10), times = c(1, 8, 1))) + ylab(&quot;residual&quot;) + theme(legend.position = &quot;top&quot;, panel.grid = element_blank()) + facet_wrap(~ period, nrow = 1) As is often the case in coefficient plots, the dots are the posterior means and the intersecting lines are the percentile-based 95% intervals. In the sex_pp data, the event variable encodes when participants experience the event within a given time range. Hopefully the color coding highlights how with hazard models, the residuals are always positive when the criterion variable is a 1 and always negative when the criterion is 0. Now we have a bit of a handle on the output from residuals(), let’s plot in bulk. residuals(fit11.10) %&gt;% data.frame() %&gt;% bind_cols(sex_pp) %&gt;% mutate(event = factor(event)) %&gt;% ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = event)) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_pointrange(fatten = 3/4, alpha = 1/2) + scale_color_viridis_d(option = &quot;A&quot;, end = .6) + ylab(&quot;residual&quot;) + theme(legend.position = &quot;top&quot;, panel.grid = element_blank()) This plot is our analogue to the top portion of Singer and Willett’s Figure 12.9. But whereas we plotted our residual summaries, they plotted the expected values of their deviance residuals. In contrast with the material on page 464, I am not going to discuss deviance residuals or the sums of the squared deviance residuals. Our brms workflow offers an alternative. Before we offer our alternative, we might focus on deviance residuals for just a moment: Deviance residuals are so named because, when squared, they represent an individual’s contribution to the deviance statistic for that time period. The sum of the squared deviance residuals across all the records in a person-period data set yields the deviance statistic for the specified model…. The absolute value of a deviance residual indicates how well the model fits that person’s data for that period. Large absolute values identify person-period records whose outcomes are poorly predicted. (p. 464) Back in Chapter 11, we computed the LOO for fit11.10. Let’s take a look at that. loo(fit11.10) ## ## Computed from 4000 by 822 log-likelihood matrix ## ## Estimate SE ## elpd_loo -322.8 17.6 ## p_loo 8.3 0.6 ## looic 645.6 35.1 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Notice the part of the output that read “All Pareto k estimates are good (k &lt; 0.5).” We can pull those Pareto k estimates like so. loo(fit11.10)$diagnostics %&gt;% data.frame() %&gt;% glimpse() ## Rows: 822 ## Columns: 2 ## $ pareto_k &lt;dbl&gt; 0.008061078, 0.085796089, 0.181442984, 0.138530329, -0.048113828, 0.003439661, -0.025045732… ## $ n_eff &lt;dbl&gt; 2018.803, 2404.460, 1584.098, 5110.548, 5025.786, 5330.718, 5194.354, 4981.467, 4868.271, 4… We formatted the output for convenience. Notice there are 822 rows–one for each case in the data. Almost like a computational byproduct, brms returned pareto_k and n_eff values when we computed the LOO estimates for the model. Our focus will be on the pareto_k column. Here are those pareto_k values in a plot. loo(fit11.10)$diagnostics %&gt;% data.frame() %&gt;% # attach the `id` values bind_cols(sex_pp) %&gt;% ggplot(aes(x = id, y = pareto_k)) + geom_point(alpha = 3/4) + geom_text(data = . %&gt;% filter(pareto_k &gt; .2), aes(x = id + 2, label = id), size = 3, hjust = 0) + theme(panel.grid = element_blank()) To learn the technical details about pareto_k, check out Vehtari, Gelman, and Gabry’s (2017) Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC or Vehtari, Simpson, Gelman, Yao, and Gabry’s (2021) Pareto smoothed importance sampling. In short, we can use pareto_k values to flag cases that were overly-influential on the model in a way that’s a little like Singer and Willett’s deviance residuals. As pointed out in the loo reference manual (Gabry, 2020), the makers of the loo package warn against pareto_k values when they get much larger than \\(0.5\\). We should be a little worried by values that exceed the \\(0.7\\) threshold and it’s very likely a problem when they get larger than \\(1\\). In this case, they’re all below \\(0.4\\) and all is good. To learn more about pareto_k values and what the loo package can do for you, check out Vehtari and Gabry’s (2020) vignette, Using the loo package. Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_3.0.4 patchwork_1.1.2 brms_2.19.0 Rcpp_1.0.10 lubridate_1.9.2 forcats_1.0.0 ## [7] stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 ## [13] ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] tensorA_0.36.2 rstudioapi_0.14 jsonlite_1.8.4 magrittr_2.0.3 TH.data_1.1-2 ## [6] estimability_1.4.1 farver_2.1.1 nloptr_2.0.3 rmarkdown_2.21 vctrs_0.6.2 ## [11] minqa_1.2.5 base64enc_0.1-3 htmltools_0.5.5 distributional_0.3.2 sass_0.4.6 ## [16] StanHeaders_2.26.25 bslib_0.4.2 htmlwidgets_1.6.2 plyr_1.8.8 sandwich_3.0-2 ## [21] emmeans_1.8.6 zoo_1.8-12 cachem_1.0.8 igraph_1.4.2 mime_0.12 ## [26] lifecycle_1.0.3 pkgconfig_2.0.3 colourpicker_1.2.0 Matrix_1.5-4 R6_2.5.1 ## [31] fastmap_1.1.1 shiny_1.7.4 digest_0.6.31 colorspace_2.1-0 ps_1.7.5 ## [36] crosstalk_1.2.0 projpred_2.5.0 labeling_0.4.2 fansi_1.0.4 timechange_0.2.0 ## [41] abind_1.4-5 mgcv_1.8-42 compiler_4.3.0 bit64_4.0.5 withr_2.5.0 ## [46] backports_1.4.1 inline_0.3.19 shinystan_2.6.0 gamm4_0.2-6 pkgbuild_1.4.0 ## [51] highr_0.10 MASS_7.3-58.4 gtools_3.9.4 loo_2.6.0 tools_4.3.0 ## [56] httpuv_1.6.11 threejs_0.3.3 glue_1.6.2 callr_3.7.3 nlme_3.1-162 ## [61] promises_1.2.0.1 grid_4.3.0 checkmate_2.2.0 reshape2_1.4.4 generics_0.1.3 ## [66] gtable_0.3.3 tzdb_0.4.0 hms_1.1.3 utf8_1.2.3 ggdist_3.3.0 ## [71] pillar_1.9.0 markdown_1.7 vroom_1.6.3 posterior_1.4.1 later_1.3.1 ## [76] splines_4.3.0 lattice_0.21-8 survival_3.5-5 bit_4.0.5 tidyselect_1.2.0 ## [81] miniUI_0.1.1.1 knitr_1.42 arrayhelpers_1.1-0 gridExtra_2.3 bookdown_0.34 ## [86] stats4_4.3.0 xfun_0.39 bridgesampling_1.1-2 matrixStats_0.63.0 DT_0.27 ## [91] rstan_2.21.8 stringi_1.7.12 boot_1.3-28.1 evaluate_0.21 codetools_0.2-19 ## [96] cli_3.6.1 RcppParallel_5.1.7 shinythemes_1.2.0 xtable_1.8-4 munsell_0.5.0 ## [101] processx_3.8.1 jquerylib_0.1.4 coda_0.19-4 svUnit_1.0.6 parallel_4.3.0 ## [106] rstantools_2.3.1 ellipsis_0.3.2 prettyunits_1.1.1 dygraphs_1.1.1.6 bayesplot_1.10.0 ## [111] Brobdingnag_1.2-9 lme4_1.1-33 viridisLite_0.4.2 mvtnorm_1.1-3 scales_1.2.1 ## [116] xts_0.13.1 crayon_1.5.2 rlang_1.1.1 multcomp_1.4-23 shinyjs_2.1.0 Footnote References Aalen, O. O. (1988). Heterogeneity in survival analysis. Statistics in Medicine, 7(11), 1121–1137. https://doi.org/10.1002/sim.4780071105 Beck, N. (1999). Modelling space and time: The event history approach. In E. Scarbrough &amp; E. Tanenbaum (Eds.), Research strategies in social science: A guide to new approaches. Oxford University Press. https://doi.org/10.1093/0198292376.001.0001 Beck, Nathaniel, Katz, J. N., &amp; Tucker, R. (1998). Taking time seriously: Time-series-cross-section analysis with a binary dependent variable. American Journal of Political Science, 42(4), 1260–1288. https://doi.org/10.2307/2991857 Gabry, J. (2020). loo reference manual, Version 2.4.1. https://CRAN.R-project.org/package=loo/loo.pdf Gamse, B. C., &amp; Conger, D. (1997). An evaluation of the Spencer post-doctoral dissertation program. Abt Associates. Graham, S. E. (1997). The exodus from mathematics: When and why? [PhD thesis]. Harvard Graduate School of Education. Heckman, J., &amp; Singer, B. S. (Eds.). (1984). Longitudinal analysis of labor market data. Cambridge University Press. https://doi.org/10.1017/CCOL0521304539 Keiley, M. K., &amp; Martin, N. C. (2002). Child abuse, neglect, and juvenile delinquency: How “new” statistical approaches can inform our understanding of “old” questions reanalysis of Widom, 1989 [Manuscript Submitted for Publication]. Mare, R. D. (1994). Discrete-time bivariate hazards with unobserved heterogeneity: A partially observed contingency table approach. Sociological Methodology, 341–383. https://doi.org/10.2307/270987 Scheike, T. H., &amp; Jensen, T. K. (1997). A discrete survival model with random effects: An application to time to pregnancy. Biometrics, 318–329. https://doi.org/10.2307/2533117 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Sueyoshi, G. T. (1995). A class of binary response models for grouped duration data. Journal of Applied Econometrics, 10(4), 411–431. https://doi.org/10.1002/jae.3950100406 Vaupel, J. W., Manton, K. G., &amp; Stallard, E. (1979). The impact of heterogeneity in individual frailty on the dynamics of mortality. Demography, 16(3), 439–454. https://doi.org/10.2307/2061224 Vaupel, J. W., &amp; Yashin, A. I. (1985). Heterogeneity’s ruses: Some surprising effects of selection on population dynamics. The American Statistician, 39(3), 176–185. https://doi.org/10.1080/00031305.1985.10479424 Vehtari, A., &amp; Gabry, J. (2020). Using the loo package (version \\(&gt;\\)= 2.0.0). https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4 Vehtari, A., Simpson, D., Gelman, A., Yao, Y., &amp; Gabry, J. (2021). Pareto smoothed importance sampling. https://arxiv.org/abs/1507.02646 Wheaton, B., Roszell, P., &amp; Hall, K. (1997). The impact of twenty childhood and adult traumatic stressors on the risk of psychiatric disorder. In I. H. Gotlib &amp; B. Wheaton (Eds.), Stress and adversity over the life course: Trajectories and turning points (pp. 50–72). Cambridge University Press. https://doi.org/10.1017/CBO9780511527623 In their reference section, Singer and Willett indicated this was a manuscript submitted for publication. To my knowledge, it was never published.↩︎ "],["describing-continuous-time-event-occurrence-data.html", "13 Describing Continuous-Time Event Occurrence Data 13.1 A framework for characterizing the distribution of continuous-time event data 13.2 Grouped methods for estimating continuous-time survivor and hazard functions 13.3 The Kaplan-Meier method of estimating the continuous-time survivor function 13.4 The cumulative hazard function 13.5 Kernel-smoothed estimates of the hazard function 13.6 Developing an intuition about continuous-time survivor, cumulative hazard, and kernel-smoothed hazard functions Session info", " 13 Describing Continuous-Time Event Occurrence Data In this chapter, we present strategies for describing continuous-time event data. Although survivor and hazard functions continue to form the cornerstone of our work, the challenge in the time scale from discrete to continuous demands that we revise our fundamental definitions and modify estimation strategies…. In the [second half of the chapter], we offer solutions to the core conundrum embedded in contentious-time event data: our inability to estimate the hazard function well. This is a concern as it leads some researchers to conclude that they should not even try to ascertain the pattern of risk over time. (Singer &amp; Willett, 2003, pp, 468–469, emphasis in the original) 13.1 A framework for characterizing the distribution of continuous-time event data Variables measured with greater precision contain more information than those measured with less precision… Finer distinctions, as long as they can be made reliably, lead to more subtle interpretations and more powerful analyses. Unfortunately, a switch from discrete- to continuous-time survival analysis is not as trivial as you might hope. In discrete time, the definition of the hazard function is intuitive, its values are easily estimated, and simple graphic displays can illuminate its behavior. In continuous time, although the survivor function is easily defined and estimated, the hazard function is not. As explained below, we must revise its definition and develop new methods for its estimation and exploration. (p. 469) 13.1.1 Salient features of continuous-time event occurrence data. Because continuous time is infinitely divisible, the distribution of event times displays two highly salient properties: The probability of observing any particular event time is infinitesimally small. In continuous time, the probability that an event will occur at any specific instant approaches 0. The probability may nor reach 0, but as time’s divisions become finer and finer, it becomes smaller and smaller. The probability that two or more individuals will share the same event time is also infinitesimally small. If the probability of event occurrence at each instant is infinitesimally small, the probability of cooccurrence (a tie) must be smaller still. (p. 470, emphasis in the original) Load the horn honking data from Diekmann, Jungbauer-Gans, Krassing, and Lorenz (1996). library(tidyverse) honking &lt;- read_csv(&quot;data/honking.csv&quot;) %&gt;% # make all names lower case rename_all(str_to_lower) %&gt;% mutate(censor_1 = abs(censor - 1)) glimpse(honking) ## Rows: 57 ## Columns: 4 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23… ## $ seconds &lt;dbl&gt; 2.88, 4.63, 2.36, 2.68, 2.50, 4.30, 1.86, 4.01, 1.41, 9.59, 4.44, 3.14, 2.83, 12.… ## $ censor &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ censor_1 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, … Here’s a quick way to arrange the seconds values and censor status of each case in a similar way to how they appear in Table 13.1. honking %&gt;% arrange(seconds) %&gt;% transmute(seconds = ifelse(censor == 0, seconds, str_c(seconds, &quot;*&quot;))) ## # A tibble: 57 × 1 ## seconds ## &lt;chr&gt; ## 1 1.41 ## 2 1.41* ## 3 1.51 ## 4 1.67 ## 5 1.68 ## 6 1.86 ## 7 2.12 ## 8 2.19 ## 9 2.36* ## 10 2.48 ## # ℹ 47 more rows For kicks, here’s a tile-plot version of Table 13.1. honking %&gt;% arrange(seconds) %&gt;% # `formatC()` allows us to retain the trailing zeros when converting the numbers to text mutate(text = formatC(seconds, digits = 2, format = &quot;f&quot;)) %&gt;% mutate(time = ifelse(censor == 0, text, str_c(text, &quot;*&quot;)), row = c(rep(1:6, times = 9), 1:3), col = rep(1:10, times = c(rep(6, times = 9), 3))) %&gt;% ggplot(aes(x = col, y = row)) + geom_tile(aes(fill = seconds)) + geom_text(aes(label = time, color = seconds &lt; 10)) + scale_fill_viridis_c(option = &quot;B&quot;, limits = c(0, NA)) + scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) + scale_y_reverse() + labs(subtitle = &quot;Table 13.1: Known and censored (*) event times for 57 motorists blocked by another\\nautomobile (reaction times are recorded to the nearest hundredth of a second)&quot;) + theme_void() + theme(legend.position = &quot;none&quot;) 13.1.2 The survivor function. “In continuous time, the survival probability for individual \\(i\\) at time \\(t_j\\) is the probability that his or her event time, \\(T_i\\) will exceed \\(t_j\\)” (p. 472). This follows the equation \\[S(t_{ij}) = \\Pr [T_i &gt; t_j].\\] Heads up: When Singer and Willett “do not distinguish individuals on the basis of predictors, [they] remove the subscript \\(i\\), letting \\(S(t_j)\\) represent the survivor function for a randomly selected member of the population” (p. 472). 13.1.3 The hazard function. The hazard function assesses the risk–at a particular moment–that an individual who has not yet done so will experience the target event. In discrete time, the moments are time periods, which allows us to express hazard as a conditional probability. In continuous time, the moments are the infinite numbers of infinitesimally small instants of time that exist within any finite time period, a change that requires us to alter our definition. (pp. 472–473, emphasis in the original) Singer and Willett the went on to demonstrate the notion of “infinitesimally small instants of time” by dividing a year into days, hours, minutes, and seconds. Here’s how we might use R to practice dividing up a year into smaller and smaller units. year &lt;- 1 days &lt;- 365 hours &lt;- 24 minutes &lt;- 60 seconds &lt;- 60 year * days ## [1] 365 year * days * hours ## [1] 8760 year * days * hours * minutes ## [1] 525600 year * days * hours * minutes * seconds ## [1] 31536000 Building, we define the continuous-time hazard function as \\[h(t_{ij}) = \\text{limit as } \\Delta t \\rightarrow 0 \\left \\{ \\frac{\\Pr[T_i \\text{ is in the interval } (t_j, t_j + \\Delta t) | T_i \\geq t_j]}{\\Delta t} \\right \\},\\] where \\([t_j, t_j + \\Delta t)\\) is the \\(j\\)th time interval and “the opening phrase ‘\\(\\text{limit as } \\Delta t \\rightarrow 0\\)’ indicates that we evaluate the conditional probability in brackets as the interval width modes closer and closer to 0” (p. 474). Because the definitions of hazard differ in continuous and discrete time, their interpretations differ as well. Most important, continuous-time hazard is not a probability. Instead, it is a rate, assessing the conditional probability of event occurrence per unit of time. No matter how tempted you might be to use the nomenclature of probability to describe rates in continuous time, please resist the urge. Rates and probabilities are not the same, and so the interpretive language is not interchangeable. (p. 474, emphasis in the original) Closing out this section, we read: An important difference between continuous-time hazard rates and discrete-time hazard probabilities is that rates are not bounded from above. Although neither can be negative, rates can easily exceed 1.0…. The possibility that continuous-time hazard rate can exceed 1 has serious consequences because it requires that we revise the statistical models that incorporate the effects of predictors. We cannot posit a model in terms of logit hazard (as in discrete time) because that transformation is defined only for values of hazard between 0 and 1. As a result, when we specify continuous-time hazard models in chapter 14, our specification will focus on the logarithm of hazard, a transformation that is defines for all values of hazard greater than 0. (p. 475, emphasis in the original) 13.2 Grouped methods for estimating continuous-time survivor and hazard functions In principle, in continuous time, we would like to estimate a value for the survivor and hazard functions at every possible instant when an event could occur. In practice, we can do so only if we are willing to adopt constraining parametric assumptions about the distribution of event times. To support this approach, statisticians have identified dozens of different distributions–Weibull, Gompertz, gamma, and log-logistic, to name a few–that event times might follow, and in some fields—industrial product testing, for example–parametric estimation is the dominant mode of analysis (see, e.g., Lawless, 1982). In many other fields, including most of the social, behavioral, and medical sciences, nonparametric methods are more popular. The fundamental advantage of nonparametric methods is that we need not make constraining assumptions about the distribution of event times. This flexibility is important because: (1) few researchers have a sound basis for preferring one distribution over another; and (2) adopting an incorrect assumption can lead to erroneous conclusions. With a nonparametric approach, you essentially trade the possibility of a minor increase in efficiency if a particular assumption holds for the guarantee of doing nearly as well for most data sets, regardless of its tenability. For decades, in a kind of mathematic irony, statisticians obtained nonparametric estimates of the continuous-time survivor and hazard functions by grouping event times into a small number of intervals, constructing a life table, and applying the discrete-time strategies of chapter 10 (with some minor revisions noted below). In this section we describe two of the most popular of these grouped strategies: the discrete-time method (section 13.2.1) and the actuarial method (section 13.2.2). (pp. 475–476, emphasis in the original) As we’ll see, brms supports parametric and nonparametric continuous-time survival models. In the sections and chapters to come, we will make extensive use of the Cox model, which is nonparametric. However, if you look through the Time-to-event models section of Bürkner’s (2021c) vignette, Parameterization of response distributions in brms, you’ll see brms supports survival models with the exponential, inverse-Gaussian, gamma, log-normal, and Weibull likelihoods. 13.2.1 Constructing a grouped life table. Grouped estimation strategies begin with a life table that partitions continuous time into a manageable number of contiguous intervals. When choosing a partition, you should seek one that is: (1) substantively meaningful; (2) coarse enough to yield stable estimates; and (3) fine enough to reveal discernible patterns. (p. 476) For the first step in making the life table of Table 13.2, we’ll make variables that partition the seconds column of the honking data into lower and upper bounds. honking &lt;- honking %&gt;% mutate(lb = case_when( seconds &lt; 2 ~ 1, seconds &lt; 3 ~ 2, seconds &lt; 4 ~ 3, seconds &lt; 5 ~ 4, seconds &lt; 6 ~ 5, seconds &lt; 7 ~ 6, seconds &lt; 8 ~ 7, seconds &gt;= 8 ~ 8 )) %&gt;% mutate(ub = if_else(lb == 8, 18, lb + 1)) %&gt;% mutate(time_interval = str_c(&quot;[&quot;, lb, &quot;, &quot;, ub, &quot;)&quot;)) honking %&gt;% head() ## # A tibble: 6 × 7 ## id seconds censor censor_1 lb ub time_interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2.88 0 1 2 3 [2, 3) ## 2 2 4.63 1 0 4 5 [4, 5) ## 3 3 2.36 1 0 2 3 [2, 3) ## 4 4 2.68 0 1 2 3 [2, 3) ## 5 5 2.5 0 1 2 3 [2, 3) ## 6 6 4.3 1 0 4 5 [4, 5) Now we’ll transform the data into a life-table format, which we’ll save as honking_aggregated. honking_aggregated &lt;- honking %&gt;% mutate(event = ifelse(censor == 0, &quot;n_events&quot;, &quot;n_censored&quot;)) %&gt;% count(lb, event) %&gt;% pivot_wider(names_from = event, values_from = n) %&gt;% mutate(ub = if_else(lb == 8, 18, lb + 1)) %&gt;% mutate(time_interval = str_c(&quot;[&quot;, lb, &quot;, &quot;, ub, &quot;)&quot;)) %&gt;% mutate(n_censored = ifelse(is.na(n_censored), 0, n_censored)) %&gt;% mutate(total = n_censored + n_events) %&gt;% mutate(n_at_risk = sum(total) - cumsum(lag(total, default = 0))) %&gt;% select(lb, ub, time_interval, n_at_risk, n_events, n_censored) %&gt;% mutate(`p(t)` = n_events / n_at_risk) honking_aggregated ## # A tibble: 8 × 7 ## lb ub time_interval n_at_risk n_events n_censored `p(t)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 [1, 2) 57 5 1 0.0877 ## 2 2 3 [2, 3) 51 14 3 0.275 ## 3 3 4 [3, 4) 34 9 2 0.265 ## 4 4 5 [4, 5) 23 6 4 0.261 ## 5 5 6 [5, 6) 13 2 2 0.154 ## 6 6 7 [6, 7) 9 2 2 0.222 ## 7 7 8 [7, 8) 5 1 0 0.2 ## 8 8 18 [8, 18) 4 3 1 0.75 13.2.2 The discrete-time method. Here we simply apply the discrete-time hazard model to our discretized continuous-time data. Before we fit the model, we’ll define a new term, \\(\\hat p(t_j)\\). Recall back to Section 10.2 where we defined the hazard function \\(\\hat h(t_{j})\\) as \\[\\hat h(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j}.\\] Now we’re working with continuous-time data (even if they’re momentarily discretized), we focus instead on \\(\\hat p(t_{j})\\). In words, \\(\\hat p(t_{j})\\) is the conditional probability that a member of the risk set at the beginning of the interval \\(j\\) will experience the target event during that interval. In discrete time we labeled this quantity “hazard,” but now we use the term “conditional probability” to distinguish it from a continuous time hazard rate. Our conditional probability follows the formula \\[\\hat p(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j},\\] where \\(n \\text{ events}_j\\) is the number of individuals who experienced the event in the \\(j^{th}\\) period and \\(n \\text{ at risk}_j\\) is the number of those at risk at the beginning of the interval \\(j\\). Time to fire up brms. library(brms) library(tidybayes) For our first model, we will use the binomial likelihood with the aggregated version of the honking data, honking_aggregated. The main time variable in those data is time_interval, the lower and upper bounds for which are identified in the lb and ub columns, respectively. In anticipation of the upcoming plots, we’ll use the ub variable for time. But to make fitting the model easier with the brm() function, we’ll first save a factor version of the variable. honking_aggregated &lt;- honking_aggregated %&gt;% mutate(ub_f = factor(ub)) In the last chapter, we used the normal(0, 4) prior, which was permissive in the log-odds metric. Here we’ll be more conservative and use normal(0, 1.5), which is weakly-regularizing on the log-odds metric, but flat in the probability metric. A plot might help show this. set.seed(13) tibble(`log odds` = rnorm(1e6, mean = 0, sd = 1.5)) %&gt;% mutate(probability = inv_logit_scaled(`log odds`)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = 0)) + stat_histinterval(normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;prior predictive distribution&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) Otherwise, this model is just like any of the other unconditional discrete-time models we’ve fit with brm(). fit13.1 &lt;- brm(data = honking_aggregated, family = binomial, n_events | trials(n_at_risk) ~ 0 + ub_f, prior(normal(0, 1.5), class = b), chains = 4, cores = 1, iter = 2000, warmup = 1000, seed = 13, file = &quot;fits/fit13.01&quot;) Check the parameter summary. print(fit13.1) ## Family: binomial ## Links: mu = logit ## Formula: n_events | trials(n_at_risk) ~ 0 + ub_f ## Data: honking_aggregated (Number of observations: 8) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## ub_f2 -2.22 0.42 -3.09 -1.46 1.00 5346 2965 ## ub_f3 -0.95 0.31 -1.57 -0.36 1.00 6285 2728 ## ub_f4 -0.99 0.38 -1.76 -0.29 1.00 6025 3127 ## ub_f5 -0.98 0.44 -1.88 -0.17 1.00 5497 2969 ## ub_f6 -1.48 0.65 -2.85 -0.28 1.00 5418 3237 ## ub_f7 -1.09 0.72 -2.56 0.24 1.00 6550 3093 ## ub_f8 -1.04 0.87 -2.76 0.60 1.00 6850 3053 ## ub_f18 0.79 0.92 -0.90 2.67 1.00 6543 2850 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As will become apparent in a bit, our normal(0, 1.5) prior was not inconsequential. Our aggregated data were composed of the event/censoring information of 57 cases, spread across 8 time periods. This left little information in the likelihood, particularly for the later time periods. As a consequence, the prior left clear marks in the posterior. As with the discrete-time model, we can formally define the survivor function for the continuous-time model as \\[\\hat S(t_j) = \\big(1 - \\hat p(t_1)\\big) \\big(1 - \\hat p(t_2)\\big)... \\big(1 - \\hat p(t_j)\\big).\\] It’ll take a little wrangling effort to transform the output from as_draws_df(fit13.1) into a useful form for plotting and summarizing \\(\\hat S(t_j)\\). We’ll save it as s. s &lt;- as_draws_df(fit13.1) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% mutate(b_ub_f0 = 0) %&gt;% select(b_ub_f0, everything()) %&gt;% set_names(c(1:8, 18)) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;time&quot;, values_to = &quot;p&quot;) %&gt;% mutate(time = time %&gt;% as.integer()) %&gt;% group_by(iter) %&gt;% mutate(survivor = cumprod(1 - p)) %&gt;% ungroup() Now we can make the first 6 columns of Table 13.2 by combining a subset of the honking_aggregated data with a summary of our s. bind_cols( # select the first 5 columns for Table 13.2 honking_aggregated %&gt;% select(time_interval:`p(t)`), # add the 6th column s %&gt;% filter(time &gt; 1) %&gt;% group_by(time) %&gt;% summarise(median = median(survivor), sd = sd(survivor)) %&gt;% mutate_if(is.double, round, digits = 4) %&gt;% transmute(`S(t)` = str_c(median, &quot; (&quot;, sd, &quot;)&quot;)) ) ## # A tibble: 8 × 6 ## time_interval n_at_risk n_events n_censored `p(t)` `S(t)` ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 [1, 2) 57 5 1 0.0877 0.8999 (0.0379) ## 2 [2, 3) 51 14 3 0.275 0.6449 (0.0626) ## 3 [3, 4) 34 9 2 0.265 0.4646 (0.0654) ## 4 [4, 5) 23 6 4 0.261 0.3324 (0.0625) ## 5 [5, 6) 13 2 2 0.154 0.264 (0.0596) ## 6 [6, 7) 9 2 2 0.222 0.1897 (0.0557) ## 7 [7, 8) 5 1 0 0.2 0.1327 (0.0516) ## 8 [8, 18) 4 3 1 0.75 0.0393 (0.0307) You’ll note that our posterior summary values in S(t) differ a little from those in the text. Remember, the likelihood was weak and we used a regularizing prior. If we had more cases spread across fewer discretized time periods, the likelihood would have done a better job updating the prior. Now let’s take a look at the posterior of our survivor function, \\(\\hat S(t_j)\\), in our version of the upper left panel of Figure 13.1. s %&gt;% ggplot(aes(x = time, y = survivor)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + stat_lineribbon(alpha = 1/2) + # add the ML-based survival estimates geom_line(data = honking_aggregated %&gt;% mutate(s = cumprod(1 - `p(t)`)), aes(x = ub, y = s), color = &quot;red&quot;) + scale_fill_grey(&quot;CI level&quot;, start = .7, end = .4) + scale_x_continuous(&quot;seconds after light turns green&quot;, limits = c(0, 20)) + ylab(expression(widehat(italic(S(t[j]))))) + coord_cartesian(ylim = c(0, 1)) + theme(panel.grid = element_blank()) For a little context, we superimposed the sample (ML) estimates of the survivor function in red. Based on the posterior median, our median lifetime appears to be between the time intervals of \\([2, 3)\\) and \\([3, 4)\\). Sticking with those medians, here’s the exact number using Miller’s (1981) interpolation approach from Section 10.2.2. s_medians &lt;- s %&gt;% mutate(lb = time - 1, ub = time) %&gt;% mutate(time_interval = str_c(&quot;[&quot;, lb, &quot;, &quot;, ub, &quot;)&quot;)) %&gt;% filter(ub %in% c(3, 4)) %&gt;% group_by(time_interval) %&gt;% summarise(median = median(survivor)) %&gt;% pull(median) 3 + (s_medians[1] - .5) / (s_medians[1] - s_medians[2]) * (4 - 3) ## [1] 3.803486 Here’s how we might convert the output of as_draws_df(fit13.1) into a useful format for our hazard function. h &lt;- as_draws_df(fit13.1) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% set_names(c(2:8, 18)) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;time&quot;, values_to = &quot;p&quot;) %&gt;% mutate(time = time %&gt;% as.integer()) h ## # A tibble: 32,000 × 3 ## iter time p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2 0.149 ## 2 1 3 0.188 ## 3 1 4 0.241 ## 4 1 5 0.219 ## 5 1 6 0.220 ## 6 1 7 0.270 ## 7 1 8 0.209 ## 8 1 18 0.493 ## 9 2 2 0.105 ## 10 2 3 0.347 ## # ℹ 31,990 more rows For continuous-time data, hazard is a rate, which is the limit of the conditional probability of event occurrence in a (vanishingly small) interval divided by the interval’s width. A logical estimator is thus the ratio of the conditional probability of event occurrence in an interval to the interval’s width. (p. 479) Thus, our new definition of hazard is \\[\\hat h(t_j) = \\frac{\\hat p(t_j)}{\\text{width}_j},\\] where \\(\\text{width}_j\\) denotes the width of the \\(j\\)th interval. The widths of most of our intervals were 1 (seconds). The final interval, \\([8, 18)\\), had a width of ten. Once we add that information to the h data, we can use the formula above to convert \\(\\hat p(t_j)\\) to \\(\\hat h(t_j)\\). h &lt;- h %&gt;% mutate(width = if_else(time &lt;= 8, 1, 10)) %&gt;% mutate(hazard = p / width) Now we can make the first 7 columns of Table 13.2 by adding an cleaned-up version of our h object to what we had before. bind_cols( # select the first 5 columns for Table 13.2 honking_aggregated %&gt;% select(time_interval:`p(t)`), # add the 6th column s %&gt;% filter(time &gt; 1) %&gt;% group_by(time) %&gt;% summarise(median = median(survivor), sd = sd(survivor)) %&gt;% mutate_if(is.double, round, digits = 4) %&gt;% transmute(`S(t)` = str_c(median, &quot; (&quot;, sd, &quot;)&quot;)), # add the 7th column h %&gt;% group_by(time) %&gt;% summarise(median = median(hazard), sd = sd(hazard)) %&gt;% mutate_if(is.double, round, digits = 4) %&gt;% transmute(`h(t)` = str_c(median, &quot; (&quot;, sd, &quot;)&quot;)) ) ## # A tibble: 8 × 7 ## time_interval n_at_risk n_events n_censored `p(t)` `S(t)` `h(t)` ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 [1, 2) 57 5 1 0.0877 0.8999 (0.0379) 0.1001 (0.0379) ## 2 [2, 3) 51 14 3 0.275 0.6449 (0.0626) 0.2796 (0.0621) ## 3 [3, 4) 34 9 2 0.265 0.4646 (0.0654) 0.272 (0.0735) ## 4 [4, 5) 23 6 4 0.261 0.3324 (0.0625) 0.2747 (0.0856) ## 5 [5, 6) 13 2 2 0.154 0.264 (0.0596) 0.1898 (0.0971) ## 6 [6, 7) 9 2 2 0.222 0.1897 (0.0557) 0.262 (0.128) ## 7 [7, 8) 5 1 0 0.2 0.1327 (0.0516) 0.268 (0.1575) ## 8 [8, 18) 4 3 1 0.75 0.0393 (0.0307) 0.0683 (0.0176) Perhaps even more so than with our estimates for the survivor function, our hazard estimates show the influence of our prior on the posterior. For example, note how our posterior standard deviations tend to be a bit smaller than the standard errors reported in the text. To my mind, plotting the marginal posteriors for the intervals of our hazard function really helps hit this home. h %&gt;% mutate(time = factor(time)) %&gt;% ggplot(aes(x = hazard, y = time)) + geom_vline(xintercept = .5, color = &quot;white&quot;) + stat_halfeye(.width = c(.5, .95), normalize = &quot;xy&quot;) + xlim(0, 1) + theme(panel.grid = element_blank()) As wide and sloppy as those distributions look, they’re more precise than the estimates returned by Maximum Likelihood (ML). To finish this section out with the lower left panel of Figure 13.1, here’s what our hazard function looks like. h %&gt;% ggplot(aes(x = time, y = hazard)) + stat_lineribbon(alpha = 1/2) + scale_fill_grey(&quot;CI level&quot;, start = .7, end = .4) + scale_x_continuous(&quot;seconds after light turns green&quot;, limits = c(0, 20)) + ylab(expression(widehat(italic(h(t[j]))))) + coord_cartesian(ylim = c(0, .35)) + theme(panel.grid = element_blank()) 13.2.3 The actuarial method. I’m not going to dive into a full explanation of the actuarial method. For that, read the book. However, the actuarial method presents a challenge for our brms paradigm. To appreciate the challenge, we’ll need a couple block quotes: For the survivor function, we ask: What does it mean to be “at risk of surviving” past the end of an interval? Because a censored individual is no longer “at risk of surviving” once censoring occurs, we redefine each interval’s risk set to account for the censoring we assume to occur equally throughout. This implies that half the censored individuals would no longer be at risk half-way through, so we redefine the number of individuals “at risk of surviving past interval \\(j\\)” to be: \\[n&#39; \\; at \\; risk_j = n \\; at \\; risk_j - \\frac{n \\; censored_j}{2}.\\] The actuarial estimate of the survivor function is obtained by substituting \\(n&#39; \\; at \\; risk_j\\) for \\(n \\; at \\; risk_j\\) in the discrete-time formulas just presented in section 13.2.2 (equations 13.3 and 13.4). (pp. 480–481, emphasis in the original) Further: To estimate the hazard function using the actuarial approach, we again redefine what it means to be “at risk.” Now, however, we ask about the “risk of event occurrence” during the interval, not the “risk of survival” past the interval. This change of definition suggests that each interval’s risk set should be diminished not just by censoring but also by event occurrence, because either eliminates the possibility of subsequent event occurrence. Because categorization continues to prevent us from knowing precisely when people leave the risk set, we assume that exits are scattered at random throughout the interval. This implies that half these individuals are no longer at risk of event occurrence halfway through, so we redefine the number of individuals “at risk of event occurrence” in interval \\(j\\) to be: \\[n&#39;&#39; \\; at \\; risk_j = n \\; at \\; risk_j - \\frac{n \\; censored_j}{2} - \\frac{n \\; events_j}{2}.\\] The actuarial estimator of the continuous-time hazard function is then obtained by substituting \\(n&#39;&#39; \\; at \\; risk_j\\) for \\(n \\; at \\; risk_j\\) in discrete-time formulas of section 13.2.2 (equations 13.3 and 13.5). (pp. 481–481, emphasis in the original) Here’s how we might implement the equations for \\(n&#39; \\; at \\; risk_j\\) and \\(n&#39;&#39; \\; at \\; risk_j\\) in our honking_aggregated data. honking_aggregated &lt;- honking_aggregated %&gt;% mutate(n_p_at_risk_a = n_at_risk - (n_censored / 2), n_pp_at_risk_a = n_at_risk - (n_censored / 2) - (n_events / 2)) honking_aggregated ## # A tibble: 8 × 10 ## lb ub time_interval n_at_risk n_events n_censored `p(t)` ub_f n_p_at_risk_a n_pp_at_risk_a ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 [1, 2) 57 5 1 0.0877 2 56.5 54 ## 2 2 3 [2, 3) 51 14 3 0.275 3 49.5 42.5 ## 3 3 4 [3, 4) 34 9 2 0.265 4 33 28.5 ## 4 4 5 [4, 5) 23 6 4 0.261 5 21 18 ## 5 5 6 [5, 6) 13 2 2 0.154 6 12 11 ## 6 6 7 [6, 7) 9 2 2 0.222 7 8 7 ## 7 7 8 [7, 8) 5 1 0 0.2 8 5 4.5 ## 8 8 18 [8, 18) 4 3 1 0.75 18 3.5 2 The essence of our problem is we’ve been using the binomial likelihood to fit discrete-time hazard functions with brms. In this paradigm, we feed in data composed of the number of successes and the corresponding number of trials to compute probability \\(p\\) of a success within a given trial. Although \\(p\\) is a continuous value ranging from 0 to 1, the binomial likelihood takes the numbers of successes and trials to be non-negative integers. If you try to feed our actuarial \\(n&#39;&#39; \\; at \\; risk_j\\) variable, n_at_risk_pp into the formula argument for brms::brm() (e.g., n_events | trials(n_at_risk_pp) ~ 0 + ub_f), brms will return the warning: Error: Number of trials must be positive integers. We can, however, follow along and compute the ML estimates by hand. honking_aggregated &lt;- honking_aggregated %&gt;% mutate(`S(t)_a` = cumprod(1 - n_events / n_p_at_risk_a), `p(t)_a` = n_events / n_pp_at_risk_a, width = if_else(lb == 8, 10, 1)) %&gt;% mutate(`h(t)_a` = `p(t)_a` / width) honking_aggregated ## # A tibble: 8 × 14 ## lb ub time_interval n_at_risk n_events n_censored `p(t)` ub_f n_p_at_risk_a n_pp_at_risk_a ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 [1, 2) 57 5 1 0.0877 2 56.5 54 ## 2 2 3 [2, 3) 51 14 3 0.275 3 49.5 42.5 ## 3 3 4 [3, 4) 34 9 2 0.265 4 33 28.5 ## 4 4 5 [4, 5) 23 6 4 0.261 5 21 18 ## 5 5 6 [5, 6) 13 2 2 0.154 6 12 11 ## 6 6 7 [6, 7) 9 2 2 0.222 7 8 7 ## 7 7 8 [7, 8) 5 1 0 0.2 8 5 4.5 ## 8 8 18 [8, 18) 4 3 1 0.75 18 3.5 2 ## # ℹ 4 more variables: `S(t)_a` &lt;dbl&gt;, `p(t)_a` &lt;dbl&gt;, width &lt;dbl&gt;, `h(t)_a` &lt;dbl&gt; Before we make our version of the right-hand side of Figure 13.1, we’ll need to augment the data a little. Then geom_step() will do most of the magic. p1 &lt;- # add `S(t)_a` values for `lb = c(0, 18)` honking_aggregated %&gt;% select(lb, `S(t)_a`) %&gt;% bind_rows(tibble(lb = c(0, 18), `S(t)_a` = c(1, honking_aggregated$`S(t)_a`[8]))) %&gt;% # reorder arrange(lb) %&gt;% # plot! ggplot(aes(x = lb, y = `S(t)_a`)) + geom_step() + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(expression(widehat(italic(S(t[j])))), limits = c(0, 1)) p2 &lt;- # add `h(t)_a` values for `lb = 18` honking_aggregated %&gt;% select(lb, `h(t)_a`) %&gt;% bind_rows(tibble(lb = 18, `h(t)_a` = honking_aggregated$`h(t)_a`[8])) %&gt;% arrange(lb) %&gt;% ggplot(aes(x = lb, y = `h(t)_a`)) + geom_step() + scale_x_continuous(&quot;seconds after light turns green&quot;, limits = c(0, 20)) + scale_y_continuous(expression(widehat(italic(h(t[j])))), limits = c(0, .35)) Combine the subplots and make our version of the right-hand side of Figure 13.1. library(patchwork) (p1 / p2) &amp; theme(panel.grid = element_blank()) I’m not going to bother with computing the ML standard errors for the actuarial survivor and hazard estimates. You can reference page 482 in the text for more on those. 13.3 The Kaplan-Meier method of estimating the continuous-time survivor function A fundamental problem with grouped estimation methods is that they artificially categorize what is now, by definition, a continuous variable…. Shouldn’t it be possible to use the observed data–the actual event times–to describe the distribution of event occurrences? This compelling idea underlies the Kaplan-Meier method, named for the statisticians who demonstrated (in 1958) that the intuitive approach–also known as the product-limit method–has maximum likelihood properties as well. Below, we explain how this approach works and why it is preferable. The Kaplan-Meier method is a simple extension of the discrete-time method with a fundamental change: instead of rounding event times to construct the intervals, capitalize on the raw event times and construct intervals so that each contains just one observed event time (as shown in table 13.3). Each Kaplan-Meier interval begins at one observed event time and ends just before the next. (p. 483, emphasis in the original) As discussed in the prose in the middle of page 483, here are the first three event times. honking %&gt;% filter(censor == 0) %&gt;% top_n(-3, seconds) ## # A tibble: 3 × 7 ## id seconds censor censor_1 lb ub time_interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 9 1.41 0 1 1 2 [1, 2) ## 2 40 1.51 0 1 1 2 [1, 2) ## 3 50 1.67 0 1 1 2 [1, 2) The Kaplan-Meier estimate of the survivor function is obtained by applying the discrete-time estimator of section 13.2.2 to the data of these intervals. [Most] statistical packages include a routine for computing and plotting the estimates. Numerically, the process is simple: first compute the conditional probability of event occurrence (column 7) and then successively multiply the complements of these probabilities together to obtain the Kaplan-Meier estimate of the survivor function (column 8). Because the Kaplan-Meier estimator of the survivor function is identical to the discrete-time estimator of chapter 10, its standard errors (column 9) are estimated using the same formula (pp. 483–485). To walk this out, we’ll first use the frequentist survival package. library(survival) Use the survival::survfit() function to fit the unconditional model with the Kaplan-Meier estimator. fit13.2 &lt;- survfit(data = honking, Surv(seconds, censor_1) ~ 1) The summary() returns a lot of output. summary(fit13.2) ## Call: survfit(formula = Surv(seconds, censor_1) ~ 1, data = honking) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1.41 57 1 0.9825 0.0174 0.94896 1.000 ## 1.51 55 1 0.9646 0.0246 0.91758 1.000 ## 1.67 54 1 0.9467 0.0299 0.88985 1.000 ## 1.68 53 1 0.9289 0.0343 0.86405 0.999 ## 1.86 52 1 0.9110 0.0380 0.83950 0.989 ## 2.12 51 1 0.8931 0.0412 0.81587 0.978 ## 2.19 50 1 0.8753 0.0441 0.79296 0.966 ## 2.48 48 1 0.8570 0.0468 0.77004 0.954 ## 2.50 47 1 0.8388 0.0492 0.74765 0.941 ## 2.53 46 1 0.8206 0.0514 0.72572 0.928 ## 2.54 45 1 0.8023 0.0534 0.70418 0.914 ## 2.56 44 1 0.7841 0.0552 0.68299 0.900 ## 2.62 43 1 0.7659 0.0569 0.66212 0.886 ## 2.68 42 1 0.7476 0.0584 0.64154 0.871 ## 2.83 39 1 0.7285 0.0599 0.61996 0.856 ## 2.88 38 1 0.7093 0.0614 0.59868 0.840 ## 2.89 37 1 0.6901 0.0626 0.57769 0.824 ## 2.92 36 1 0.6710 0.0637 0.55695 0.808 ## 2.98 35 1 0.6518 0.0647 0.53648 0.792 ## 3.14 33 1 0.6320 0.0657 0.51549 0.775 ## 3.17 32 1 0.6123 0.0666 0.49477 0.758 ## 3.21 31 1 0.5925 0.0673 0.47429 0.740 ## 3.22 30 1 0.5728 0.0679 0.45405 0.723 ## 3.24 29 1 0.5530 0.0684 0.43404 0.705 ## 3.56 27 1 0.5325 0.0688 0.41338 0.686 ## 3.57 26 1 0.5121 0.0692 0.39297 0.667 ## 3.58 25 1 0.4916 0.0694 0.37282 0.648 ## 3.78 24 1 0.4711 0.0694 0.35291 0.629 ## 4.10 22 1 0.4497 0.0695 0.33217 0.609 ## 4.18 21 1 0.4283 0.0694 0.31172 0.588 ## 4.44 19 1 0.4057 0.0693 0.29028 0.567 ## 4.51 18 1 0.3832 0.0690 0.26919 0.545 ## 4.52 17 1 0.3606 0.0686 0.24847 0.523 ## 4.96 14 1 0.3349 0.0683 0.22451 0.500 ## 5.39 12 1 0.3070 0.0681 0.19875 0.474 ## 5.73 11 1 0.2791 0.0674 0.17386 0.448 ## 6.03 9 1 0.2481 0.0666 0.14651 0.420 ## 6.30 7 1 0.2126 0.0659 0.11585 0.390 ## 7.20 5 1 0.1701 0.0650 0.08044 0.360 ## 9.59 4 1 0.1276 0.0611 0.04991 0.326 ## 12.29 3 1 0.0851 0.0535 0.02478 0.292 ## 13.18 2 1 0.0425 0.0403 0.00665 0.272 Taking a cue from the good folks at IDRE, saving the summary results as an object will make it easy to subset and augment that information into our version of Table 13.1. # save the summary as t t &lt;- summary(fit13.2) # subset, augment, and save as honking_km honking_km &lt;- tibble(seconds = t$time, n_risk = t$n.risk, n_events = t$n.event) %&gt;% mutate(`p(t)` = n_events / n_risk, n_censored = n_risk - n_events - lead(n_risk, default = 0), interval = 1:n(), interval_f = factor(1:n(), levels = 0:n()), start = seconds, end = lead(seconds, default = Inf)) %&gt;% select(interval:interval_f, seconds, start:end, n_risk:n_events, n_censored, `p(t)`) honking_km ## # A tibble: 42 × 9 ## interval interval_f seconds start end n_risk n_events n_censored `p(t)` ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.41 1.41 1.51 57 1 1 0.0175 ## 2 2 2 1.51 1.51 1.67 55 1 0 0.0182 ## 3 3 3 1.67 1.67 1.68 54 1 0 0.0185 ## 4 4 4 1.68 1.68 1.86 53 1 0 0.0189 ## 5 5 5 1.86 1.86 2.12 52 1 0 0.0192 ## 6 6 6 2.12 2.12 2.19 51 1 0 0.0196 ## 7 7 7 2.19 2.19 2.48 50 1 1 0.02 ## 8 8 8 2.48 2.48 2.5 48 1 0 0.0208 ## 9 9 9 2.5 2.5 2.53 47 1 0 0.0213 ## 10 10 10 2.53 2.53 2.54 46 1 0 0.0217 ## # ℹ 32 more rows We haven’t bothered adding the top interval == 0 row, but one could add that information with a little bind_rows() labor. Note how we slipped in a few extra columns (e.g., interval_f) because they’ll come in handy, later. We compute the Kaplan-Meier estimates for the survivor function by serially multiplying the compliments for the estimates of the conditional probability values, \\(\\hat p(t)\\). As in other examples, that’s just a little cumprod() code. honking_km &lt;- honking_km %&gt;% mutate(`S(t)` = cumprod(1 - `p(t)`)) Instead of doing that by hand, we could have just subset the surv vector within t. t$surv ## [1] 0.98245614 0.96459330 0.94673046 0.92886762 0.91100478 0.89314195 0.87527911 0.85704413 ## [9] 0.83880914 0.82057416 0.80233918 0.78410420 0.76586922 0.74763424 0.72846413 0.70929402 ## [17] 0.69012391 0.67095380 0.65178369 0.63203267 0.61228165 0.59253063 0.57277961 0.55302859 ## [25] 0.53254605 0.51206351 0.49158097 0.47109843 0.44968486 0.42827130 0.40573070 0.38319011 ## [33] 0.36064951 0.33488883 0.30698143 0.27907403 0.24806580 0.21262783 0.17010227 0.12757670 ## [41] 0.08505113 0.04252557 Here we subset the standard errors for \\(\\hat S(t)\\). honking_km &lt;- honking_km %&gt;% mutate(`se[S(t)]` = t$std.err) head(honking_km) ## # A tibble: 6 × 11 ## interval interval_f seconds start end n_risk n_events n_censored `p(t)` `S(t)` `se[S(t)]` ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.41 1.41 1.51 57 1 1 0.0175 0.982 0.0174 ## 2 2 2 1.51 1.51 1.67 55 1 0 0.0182 0.965 0.0246 ## 3 3 3 1.67 1.67 1.68 54 1 0 0.0185 0.947 0.0299 ## 4 4 4 1.68 1.68 1.86 53 1 0 0.0189 0.929 0.0343 ## 5 5 5 1.86 1.86 2.12 52 1 0 0.0192 0.911 0.0380 ## 6 6 6 2.12 2.12 2.19 51 1 0 0.0196 0.893 0.0412 We can plot the fitted \\(\\hat S(t)\\) values with `geom_step() to make our version of the top half of Figure 13.2. p1 &lt;- honking_km %&gt;% select(seconds, `S(t)`) %&gt;% bind_rows(tibble(seconds = 17.15, `S(t)` = 0.04252557)) %&gt;% ggplot(aes(x = seconds, y = `S(t)`)) + geom_step() + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 20)) Now add the actuarial and discrete-time estimates to make our version of the lower panel of Figure 13.2. arrow &lt;- tibble(x = c(5, 8.7, 2.7), y = c(.8, .4, .23), xend = c(2.6, 7, 3.9), yend = c(.875, .25, .33)) text &lt;- tibble(x = c(5, 8.7, 2.7), y = c(.77, .43, .2), label = c(&quot;Kaplan Meier&quot;, &quot;Discrete-time&quot;, &quot;Actuarial&quot;)) p2 &lt;- honking_km %&gt;% select(seconds, `S(t)`) %&gt;% bind_rows(tibble(seconds = 17.15, `S(t)` = 0.04252557)) %&gt;% ggplot(aes(x = seconds, y = `S(t)`)) + geom_step() + geom_step(data = honking_aggregated, aes(x = ub, y = `S(t)_a`), linetype = 3, direction = &quot;vh&quot;) + geom_line(data = honking_aggregated %&gt;% mutate(s = cumprod(1 - `p(t)`)), aes(x = ub, y = s), linetype = 2) + geom_segment(data = arrow, aes(x = x, xend = xend, y = y, yend = yend), linewidth = 1/3, arrow = arrow(length = unit(0.15,&quot;cm&quot;), type = &quot;closed&quot;)) + geom_text(data = text, aes(x = x, y = y, label = label)) + scale_x_continuous(&quot;seconds after light turns green&quot;, limits = c(0, 20)) Combine and plot. (p1 / p2) &amp; scale_y_continuous(expression(widehat(italic(S(t[j])))), limits = c(0, 1)) &amp; theme(panel.grid = element_blank()) Did you notice how in both subplots we used bind_rows() to add in an S(t) value for seconds = 17.15? In the text, we read: As for actuarial estimates, we plot Kaplan-Meier estimates as a step function that associates the estimated probability with the entire interval. If the largest event time is censored, as it is here (17.15), we extend the step for the last estimate out to that largest censored value. (pp. 485–486) Those bind_rows() lines are what extended “the step for the last estimate.” Since that last estimate for \\(\\hat S(t_j) = 0.04252557\\), we set S(t) = 0.04252557 in the plot data. We can get the median lifetime by executing print(fit13.2). print(fit13.2) ## Call: survfit(formula = Surv(seconds, censor_1) ~ 1, data = honking) ## ## n events median 0.95LCL 0.95UCL ## [1,] 57 42 3.58 3.17 4.96 Like in the text, it’s 3.58. Even though there is no Kaplan-Meier estimate for hazard, we can compute a Kaplan-Meier type hazard with the formula \\[\\hat h_\\text{KM} (t_j) = \\frac{\\hat p_\\text{KM} (t_j)}{\\text{width}_j}.\\] Here we compute both the \\(\\text{width}_j\\) and \\(\\hat h_\\text{KM} (t_j)\\) values by hand. honking_km &lt;- honking_km %&gt;% mutate(width = end - start) %&gt;% mutate(width = if_else(end == Inf, 17.15 - start, width)) %&gt;% # this might be wrong mutate(`h[km](t)` = `p(t)` / width) honking_km ## # A tibble: 42 × 13 ## interval interval_f seconds start end n_risk n_events n_censored `p(t)` `S(t)` `se[S(t)]` ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.41 1.41 1.51 57 1 1 0.0175 0.982 0.0174 ## 2 2 2 1.51 1.51 1.67 55 1 0 0.0182 0.965 0.0246 ## 3 3 3 1.67 1.67 1.68 54 1 0 0.0185 0.947 0.0299 ## 4 4 4 1.68 1.68 1.86 53 1 0 0.0189 0.929 0.0343 ## 5 5 5 1.86 1.86 2.12 52 1 0 0.0192 0.911 0.0380 ## 6 6 6 2.12 2.12 2.19 51 1 0 0.0196 0.893 0.0412 ## 7 7 7 2.19 2.19 2.48 50 1 1 0.02 0.875 0.0441 ## 8 8 8 2.48 2.48 2.5 48 1 0 0.0208 0.857 0.0468 ## 9 9 9 2.5 2.5 2.53 47 1 0 0.0213 0.839 0.0492 ## 10 10 10 2.53 2.53 2.54 46 1 0 0.0217 0.821 0.0514 ## # ℹ 32 more rows ## # ℹ 2 more variables: width &lt;dbl&gt;, `h[km](t)` &lt;dbl&gt; Singer and Willett remarked that “because the interval width varies widely (and is itself a function of the distribution of event times), the resulting estimates vary from one interval to the next. Their values are usually so erratic that pattern identification is nearly impossible” (p. 487). That sounds fun. Let’s explore them in a plot! honking_km %&gt;% ggplot(aes(x = start, y = `h[km](t)`)) + geom_path() + geom_point() + scale_x_continuous(&quot;seconds after light turns green&quot;, limits = c(0, 15)) + scale_y_continuous(expression(hat(italic(h))[KM](italic(t[j])))) + theme(panel.grid = element_blank()) Erratic, indeed. 13.3.1 Fitting a Bayesian Kaplan-Meier model It’s worth repeating one of the quotes from earlier: The Kaplan-Meier method is a simple extension of the discrete-time method with a fundamental change: instead of rounding event times to construct the intervals, capitalize on the raw event times and construct intervals so that each contains just one observed event time (as shown in table 13.3). Each Kaplan-Meier interval begins at one observed event time and ends just before the next. (p. 483, emphasis in the original) Whether we’re working with one event occurrence at a time or binning them in intervals, the basic product of the model is a conditional probability. The binomial likelihood served us well when we binned the event occurrences into largish time intervals, and it will work just the same when we work them in serial fashion. The biggest obstacle is properly setting up the data. After some experimentation, the easiest way to format the data properly is to just use the summary results from survfit(data = honking, Surv(seconds, censor_1) ~ 1), which we saved as honking_km. Take another look. glimpse(honking_km) ## Rows: 42 ## Columns: 13 ## $ interval &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, … ## $ interval_f &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, … ## $ seconds &lt;dbl&gt; 1.41, 1.51, 1.67, 1.68, 1.86, 2.12, 2.19, 2.48, 2.50, 2.53, 2.54, 2.56, 2.62, 2… ## $ start &lt;dbl&gt; 1.41, 1.51, 1.67, 1.68, 1.86, 2.12, 2.19, 2.48, 2.50, 2.53, 2.54, 2.56, 2.62, 2… ## $ end &lt;dbl&gt; 1.51, 1.67, 1.68, 1.86, 2.12, 2.19, 2.48, 2.50, 2.53, 2.54, 2.56, 2.62, 2.68, 2… ## $ n_risk &lt;dbl&gt; 57, 55, 54, 53, 52, 51, 50, 48, 47, 46, 45, 44, 43, 42, 39, 38, 37, 36, 35, 33,… ## $ n_events &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ n_censored &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0… ## $ `p(t)` &lt;dbl&gt; 0.01754386, 0.01818182, 0.01851852, 0.01886792, 0.01923077, 0.01960784, 0.02000… ## $ `S(t)` &lt;dbl&gt; 0.9824561, 0.9645933, 0.9467305, 0.9288676, 0.9110048, 0.8931419, 0.8752791, 0.… ## $ `se[S(t)]` &lt;dbl&gt; 0.01738929, 0.02459209, 0.02992911, 0.03428307, 0.03799347, 0.04123439, 0.04410… ## $ width &lt;dbl&gt; 0.10, 0.16, 0.01, 0.18, 0.26, 0.07, 0.29, 0.02, 0.03, 0.01, 0.02, 0.06, 0.06, 0… ## $ `h[km](t)` &lt;dbl&gt; 0.17543860, 0.11363636, 1.85185185, 0.10482180, 0.07396450, 0.28011204, 0.06896… In the n_events column we have the number of cases that experienced the event in a given moment in continuous time. In the n_risk column we have the number of possible cases that could have experienced the event at that moment. In the interval_f column we’ve saved each moment as a factor, conveniently named \\(1, 2,..., 42\\). Thus we can fit a simple Bayesian Kaplan-Meier model with brms::brm() by specifying formula = n_events | trials(n_risk) ~ 0 + interval_f. Consider the implications for our priors. Because we’re treating each instance in time as a factor, that means the number of cases experiencing the event in one of those factors will always be 1 or some other small number in the unusual case of a tie. But the number in the denominator, n_risk, will tend to be relatively large, which means the probabilities will tend to be small. The weakly regularizing prior approach centered on zero might not make sense in this context. In earlier models, we used normal(0, 4) and normal(0, 1.5). Here’s what those look like when we convert them to the probability metric. set.seed(13) tibble(sd = c(4, 1.5)) %&gt;% mutate(prior = factor(str_c(&quot;normal(0, &quot;, sd, &quot;)&quot;), levels = str_c(&quot;normal(0, &quot;, c(1, 1.5, 4), &quot;)&quot;)), log_odds = map(sd, rnorm, n = 1e5, mean = 0)) %&gt;% unnest(log_odds) %&gt;% mutate(p = inv_logit_scaled(log_odds)) %&gt;% ggplot(aes(x = p, y = prior)) + stat_histinterval(.width = c(.5, .95), normalize = &quot;xy&quot;) + labs(x = expression(italic(p)), y = &quot;prior (log-odds scale)&quot;) + coord_cartesian(ylim = c(1.5, 2.5)) + theme(panel.grid = element_blank()) The normal(0, 4) prior does not reflect our expectation that the probabilities will tend to be small. Interestingly, normal(0, 4) pushes a lot of the prior mass to the edges. What we want is a prior that pushed the mass to the left. Here are some options: crossing(mean = -4:-1, sd = 1:4) %&gt;% mutate(log_odds = map2(mean, sd, rnorm, n = 1e5)) %&gt;% unnest(log_odds) %&gt;% mutate(p = inv_logit_scaled(log_odds), mean = factor(str_c(&quot;mean = &quot;, mean), levels = str_c(&quot;mean = &quot;, -4:-1)), sd = str_c(&quot;sd = &quot;, sd)) %&gt;% ggplot(aes(x = p, y = 0)) + stat_histinterval(.width = c(.5, .95), normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(italic(p))) + theme(panel.grid = element_blank()) + facet_grid(sd ~ mean) As you decrease the prior mean, the mass in the probability metric heads to zero. Increasing or shrinking the prior standard deviation accelerates or attenuates that leftward concentration. To my way of thinking, we want a prior that, while concentrating the mass toward zero, still offers a good spread toward the middle. Let’s try normal(-4, 3). fit13.3 &lt;- brm(data = honking_km, family = binomial, n_events | trials(n_risk) ~ 0 + interval_f, prior(normal(-4, 3), class = b), chains = 4, cores = 1, iter = 2000, warmup = 1000, seed = 13, file = &quot;fits/fit13.03&quot;) Check the summary. print(fit13.3) ## Family: binomial ## Links: mu = logit ## Formula: n_events | trials(n_risk) ~ 0 + interval_f ## Data: honking_km (Number of observations: 42) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## interval_f1 -4.44 1.11 -7.04 -2.69 1.00 4651 2269 ## interval_f2 -4.38 1.14 -7.05 -2.60 1.00 4689 2095 ## interval_f3 -4.36 1.06 -6.89 -2.69 1.00 4901 2254 ## interval_f4 -4.37 1.10 -6.99 -2.60 1.00 4794 2450 ## interval_f5 -4.36 1.10 -6.84 -2.60 1.00 5305 2612 ## interval_f6 -4.35 1.08 -6.85 -2.57 1.00 4501 2058 ## interval_f7 -4.33 1.13 -6.83 -2.54 1.00 4643 2119 ## interval_f8 -4.29 1.11 -6.81 -2.55 1.00 3664 2194 ## interval_f9 -4.27 1.13 -6.84 -2.46 1.00 5032 2071 ## interval_f10 -4.24 1.09 -6.77 -2.49 1.00 5016 2570 ## interval_f11 -4.24 1.12 -6.87 -2.48 1.00 5200 2416 ## interval_f12 -4.21 1.11 -6.74 -2.39 1.00 4661 2543 ## interval_f13 -4.17 1.10 -6.64 -2.46 1.00 4406 2370 ## interval_f14 -4.15 1.11 -6.72 -2.43 1.00 4496 2204 ## interval_f15 -4.10 1.13 -6.78 -2.30 1.00 4647 2480 ## interval_f16 -4.05 1.12 -6.60 -2.25 1.00 5277 2682 ## interval_f17 -4.04 1.14 -6.78 -2.25 1.00 5079 2419 ## interval_f18 -4.02 1.09 -6.47 -2.22 1.00 5461 2182 ## interval_f19 -3.98 1.07 -6.41 -2.26 1.00 4621 2071 ## interval_f20 -3.96 1.11 -6.52 -2.16 1.00 5156 2405 ## interval_f21 -3.91 1.12 -6.49 -2.15 1.00 5830 2750 ## interval_f22 -3.92 1.17 -6.59 -2.05 1.00 4747 2477 ## interval_f23 -3.84 1.13 -6.44 -2.03 1.00 5307 2642 ## interval_f24 -3.84 1.17 -6.45 -2.00 1.00 5376 2604 ## interval_f25 -3.75 1.12 -6.29 -1.97 1.00 5781 2390 ## interval_f26 -3.75 1.17 -6.41 -1.91 1.00 5049 2221 ## interval_f27 -3.68 1.13 -6.34 -1.86 1.00 5229 2307 ## interval_f28 -3.64 1.11 -6.26 -1.86 1.00 4821 2600 ## interval_f29 -3.59 1.19 -6.31 -1.76 1.00 4501 1990 ## interval_f30 -3.54 1.19 -6.23 -1.68 1.00 4724 2340 ## interval_f31 -3.45 1.18 -6.17 -1.56 1.00 5584 2490 ## interval_f32 -3.38 1.16 -6.10 -1.51 1.00 4384 2208 ## interval_f33 -3.30 1.13 -5.89 -1.52 1.00 5179 2489 ## interval_f34 -3.15 1.17 -5.91 -1.30 1.00 4945 2160 ## interval_f35 -3.02 1.21 -5.87 -1.05 1.00 4791 2176 ## interval_f36 -2.95 1.27 -5.93 -0.86 1.00 5333 2108 ## interval_f37 -2.76 1.24 -5.59 -0.76 1.00 5395 2432 ## interval_f38 -2.51 1.31 -5.50 -0.42 1.00 5710 2377 ## interval_f39 -2.14 1.33 -5.16 0.10 1.00 5369 2374 ## interval_f40 -1.89 1.36 -4.82 0.42 1.00 4516 2210 ## interval_f41 -1.59 1.44 -4.71 0.91 1.00 5522 2466 ## interval_f42 -1.02 1.52 -4.25 1.70 1.01 5791 2443 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our parameter diagnostics look excellent. It might be helpful to inspect their posteriors in a plot. Here we show them in both the log-odds and probability metrics. draws &lt;- as_draws_df(fit13.3) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% set_names(1:42) draws %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;, values_to = &quot;log_odds&quot;) %&gt;% mutate(parameter = factor(parameter, levels = 1:42), probability = inv_logit_scaled(log_odds)) %&gt;% pivot_longer(probability:log_odds) %&gt;% ggplot(aes(x = value, y = parameter)) + stat_halfeye(.width = .95, normalize = &quot;xy&quot;, size = 1/2) + labs(x = expression(hat(italic(p))[Bayes](italic(t[j]))), y = expression(italic(j))) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_x&quot;) Let’s wrangle our draws object a little to put it in a more useful format. draws &lt;- draws %&gt;% mutate_all(inv_logit_scaled) %&gt;% mutate(`0` = 0) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;interval&quot;, values_to = &quot;p&quot;) %&gt;% mutate(interval = interval %&gt;% as.double()) %&gt;% arrange(interval) %&gt;% group_by(iter) %&gt;% mutate(survivor = cumprod(1 - p)) %&gt;% ungroup() glimpse(draws) ## Rows: 172,000 ## Columns: 4 ## $ iter &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23… ## $ interval &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ p &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ survivor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … We might want to compare our Bayesian \\(\\hat p(t_j)\\) estimates with their Maximum Likelihood counterparts. Since the Bayesian marginal posteriors are rather asymmetrical, we’ll summarize \\(\\hat p_\\text{Bayes} (t_j)\\) with means, medians, and modes. draws %&gt;% mutate(interval = factor(interval, levels = 0:42)) %&gt;% group_by(interval) %&gt;% summarise(mean = mean(survivor), median = median(survivor), mode = Mode(survivor)) %&gt;% pivot_longer(-interval, names_to = &quot;posterior estimate&quot;) %&gt;% ggplot(aes(x = interval, y = value)) + geom_point(aes(color = `posterior estimate`), size = 3, shape = 1) + geom_point(data = honking_km %&gt;% mutate(interval = factor(interval, levels = 0:42)), aes(y = `S(t)`)) + scale_color_viridis_d(option = &quot;D&quot;, begin = .1, end = .8) + ylab(expression(hat(italic(p))(italic(t[j])))) + theme(legend.background = element_blank(), legend.key = element_rect(fill = &quot;grey92&quot;), legend.position = c(.85, .8), panel.grid = element_blank()) The black dots were the ML estimates and the colored circles were the Bayesian counterparts. Overall, it looks like they matched up pretty well! Building on those sensibilities, here’s an alternative version of the top panel from Figure 13.2, this time comparing the frequentist \\(\\hat S (t_j)\\) with our Bayesian counterpart. draws %&gt;% group_by(interval) %&gt;% summarise(mean = mean(survivor), median = median(survivor), mode = Mode(survivor)) %&gt;% pivot_longer(-interval, names_to = &quot;posterior estimate&quot;) %&gt;% left_join(honking_km %&gt;% distinct(interval, seconds), by = &quot;interval&quot;) %&gt;% mutate(seconds = if_else(is.na(seconds), 0, seconds)) %&gt;% ggplot(aes(x = seconds, y = value)) + geom_step(aes(color = `posterior estimate`), size = 1) + geom_step(data = honking_km, aes(y = `S(t)`)) + scale_color_viridis_d(option = &quot;D&quot;, begin = .1, end = .8) + scale_x_continuous(&quot;time&quot;, limits = c(0, 20)) + ylab(expression(widehat(italic(S(t[j]))))) + theme(legend.background = element_blank(), legend.key = element_rect(fill = &quot;grey92&quot;), legend.position = c(.85, .8), panel.grid = element_blank()) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Again, we showed the ML \\(\\hat S (t_j)\\) in black and the Bayesian counterpart as summarized by three alternative measures of central tendency in color. Overall, the results were very similar across methods. What this plot makes clear is that it’s the last few estimates for that where our Bayesian estimates diverge from ML. If you compare this plot with the previous one, it appears that the nature of the divergence is our Bayesian estimates are shrunk a bit toward \\(p = .5\\), though the modes shrank less than the medians and means. Also recall how uncertain our posteriors were for the last few intervals. This is because the likelihoods for those intervals were incredibly weak. Take a glance at the last few rows in the data. tail(honking_km) ## # A tibble: 6 × 13 ## interval interval_f seconds start end n_risk n_events n_censored `p(t)` `S(t)` `se[S(t)]` width ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 37 37 6.03 6.03 6.3 9 1 1 0.111 0.248 0.0666 0.270 ## 2 38 38 6.3 6.3 7.2 7 1 1 0.143 0.213 0.0659 0.9 ## 3 39 39 7.2 7.2 9.59 5 1 0 0.2 0.170 0.0650 2.39 ## 4 40 40 9.59 9.59 12.3 4 1 0 0.25 0.128 0.0611 2.7 ## 5 41 41 12.3 12.3 13.2 3 1 0 0.333 0.0851 0.0535 0.890 ## 6 42 42 13.2 13.2 Inf 2 1 1 0.5 0.0425 0.0403 3.97 ## # ℹ 1 more variable: `h[km](t)` &lt;dbl&gt; Based on the n_risk column, the number of trials ranged from \\(n = 9\\) at interval == 37 to \\(n = 2\\) fpr interval == 42. With so little information informing the likelihood, you largely get back the prior. Moving forward, here’s another version of the survivor function of Figure 13.2, but this time using posterior intervals to highlight uncertainty in the estimates. # augment draws draws &lt;- draws %&gt;% bind_rows( draws %&gt;% filter(interval == 42) %&gt;% mutate(interval = 43) ) draws %&gt;% left_join( bind_rows( distinct(honking_km, interval, seconds), tibble(interval = 43, seconds = 17.15)), by = &quot;interval&quot;) %&gt;% mutate(seconds = if_else(is.na(seconds), 0, seconds)) %&gt;% ggplot(aes(x = seconds, y = survivor)) + stat_lineribbon(step = &quot;hv&quot;, size = 3/4, .width = c(.5, .95)) + annotate(geom = &quot;text&quot;, x = 5.3, y = .54, hjust = 0, size = 3.5, label = &quot;This time the black line is the Bayesian posterior median,\\nwhich is the `stat_lineribbon()` default.&quot;) + geom_segment(x = 5.3, xend = 4.2, y = .55, yend = .475, size = 1/5, arrow = arrow(length = unit(0.15,&quot;cm&quot;), type = &quot;closed&quot;)) + scale_fill_grey(&quot;CI&quot;, start = .8, end = .6, labels = c(&quot;95%&quot;, &quot;50%&quot;)) + scale_x_continuous(&quot;time&quot;, limits = c(0, 20)) + ylab(expression(widehat(italic(S(t[j]))))) + theme(legend.background = element_blank(), legend.key = element_rect(fill = &quot;grey92&quot;), legend.position = c(.925, .85), panel.grid = element_blank()) Just for giggles, here’s how you might depict our \\(\\hat S_\\text{Bayes} (t_j)\\) with more of a 3D approach. draws %&gt;% left_join( bind_rows( distinct(honking_km, interval, seconds), tibble(interval = 43, seconds = 17.15)), by = &quot;interval&quot;) %&gt;% mutate(seconds = if_else(is.na(seconds), 0, seconds)) %&gt;% ggplot(aes(x = seconds, y = survivor)) + stat_lineribbon(.width = seq(from = .01, to = .99, by = .01), step = &quot;hv&quot;, size = 0, show.legend = F) + scale_fill_grey(start = .89, end = 0) + scale_x_continuous(&quot;time&quot;, limits = c(0, 20)) + ylab(expression(widehat(italic(S(t[j]))))) + theme(panel.grid = element_blank()) 13.4 The cumulative hazard function Kaplan-Meier type estimates of hazard are simply too erratic to be meaningful. This is where the cumulative hazard function comes in. Denoted \\(H (t_{ij})\\), the cumulative hazard function assesses, at each point in time, the total amount of accumulated risk that individual \\(i\\) has faced from the beginning of time until the present. (p. 488, emphasis in the original) The cumulative hazard function follows the equation \\[H (t_{ij}) = \\underset{\\text{between } t_0 \\text{ and } t_j}{\\text{cumulation}} [h(t_{ij})],\\] “where the phrase ‘cumulation between \\(t_0\\) and \\(t_j\\)’ indicates that cumulative hazard totals the infinite number of specific values of \\(h(t_{ij})\\) that exist between \\(t_0\\) and \\(t_j\\)” (p. 488). 13.4.1 Understanding the meaning of cumulative hazard. Although the absolute values of the cumulative hazard function aren’t particularly illuminating, the overall shape is. Figure 13.3 gives several examples. We don’t have the data or the exact specifications for the functions expressed in Figure 13.3. But if you’re okay with a little imprecision, we can make a few good guesses. Before diving in, it’ll help simplify our subplot code if we make two custom geoms. We’ll call them geom_h() and geom_H(). geom_h &lt;- function(subtitle, ...) { list( geom_line(...), scale_x_continuous(NULL, breaks = NULL), scale_y_continuous(expression(italic(h)(italic(t[ij]))), breaks = 0:5 * 0.02), labs(subtitle = subtitle), coord_cartesian(ylim = c(0, .1)) ) } geom_H &lt;- function(y_ul, ...) { list( geom_line(...), ylab(expression(italic(H)(italic(t[ij])))), coord_cartesian(ylim = c(0, y_ul)) ) } Now we have our custom geoms, here’s the code to make Figure 13.3. # a: constant hazard d &lt;- tibble(time = seq(from = 0, to = 100, by = 1)) %&gt;% mutate(h = 0.05) %&gt;% mutate(H = cumsum(h)) p1 &lt;- d %&gt;% ggplot(aes(x = time, y = h)) + geom_h(subtitle = &quot;A: Constant hazard&quot;) p2 &lt;- d %&gt;% ggplot(aes(x = time, y = H)) + geom_H(y_ul = 6) # b: increasing hazard d &lt;- tibble(time = seq(from = 0, to = 100, by = 1)) %&gt;% mutate(h = 0.001 * time) %&gt;% mutate(H = cumsum(h)) p3 &lt;- d %&gt;% ggplot(aes(x = time, y = h)) + geom_h(subtitle = &quot;B: Increasing hazard&quot;) p4 &lt;- d %&gt;% ggplot(aes(x = time, y = H)) + geom_H(y_ul = 5) # decreasing hazard d &lt;- tibble(time = seq(from = .2, to = 100, by = .1)) %&gt;% # note out use of the gamma distribution (see ) mutate(h = dgamma(time, shape = .02, rate = .001)) %&gt;% mutate(H = cumsum(h)) p5 &lt;- d %&gt;% ggplot(aes(x = time, y = h)) + geom_h(subtitle = &quot;C: Decreasing hazard&quot;) p6 &lt;- d %&gt;% ggplot(aes(x = time, y = H)) + geom_H(y_ul = 1.2) # increasing &amp; decreasing hazard d &lt;- tibble(time = seq(from = 1, to = 100, by = 1)) %&gt;% # note our use of the Fréchet distribution mutate(h = dfrechet(time, loc = 0, scale = 250, shape = .5) * 25) %&gt;% mutate(H = cumsum(h)) p7 &lt;- d %&gt;% ggplot(aes(x = time, y = h)) + geom_h(subtitle = &quot;D: Increasing &amp;\\n decreasing hazard&quot;) p8 &lt;- d %&gt;% ggplot(aes(x = time, y = H)) + geom_H(y_ul = 5) # combine with patchwork and plot! ((p1 / p2) | (p3 / p4) | (p5 / p6) | (p7 / p8)) &amp; theme(panel.grid = element_blank()) Did you notice our use of the gamma and Fréchet distributions? Both are supported for continuous-time survival models in brms (see Bürkner’s vignette, Parameterization of response distributions in brms). 13.4.2 Estimating the cumulative hazard function. The two methods to estimate the cumulative hazard function are the Nelson-Aalen method and the negative log survivor function method. The Nelson-Aalen method used Kaplan-Meier-type hazard estimates as follows: \\[\\hat H_\\text{NA} (t_j) = \\hat h_\\text{KM} (t_1) \\text{width}_1 + \\hat h_\\text{KM} (t_2) \\text{width}_2 + \\dots + \\hat h_\\text{KM} (t_j) \\text{width}_j,\\] where \\(\\hat h_\\text{KM} (t_j) \\text{width}_j\\) is the total hazard during the \\(j\\)th interval. The negative log survivor function method makes use of the estimates of the Kaplan-Meier survivor function with the formula \\[\\hat H_{- \\text{LS}} (t_j) = - \\log \\hat S_\\text{KM} (t_{ij}),\\] where \\(\\hat S_\\text{KM} (t_{ij})\\), recall, is the cumulative survivor function for the Kaplan-Meier estimator. Here we put both formulas to use and make our version of Figure 13.4. # for annotation text &lt;- tibble(seconds = c(13.15, 13.4), y = c(3.275, 2.65), label = c(&quot;Negative~log~survivor*&#39;,&#39;*~hat(italic(H))[-LS](italic(t[j]))&quot;, &quot;Nelson-Aalen*&#39;,&#39;*~hat(italic(H))[N][A](italic(t[j]))&quot;)) # top plot p1 &lt;- honking_km %&gt;% mutate(width = if_else(width == Inf, 17.15 - start, width)) %&gt;% mutate(`H[na](t)` = cumsum(`h[km](t)` * width), `H[ls](t)` = - log(`S(t)`)) %&gt;% select(seconds, `H[na](t)`, `H[ls](t)`) %&gt;% bind_rows(tibble(seconds = 17.15, `H[na](t)` = 2.78499694, `H[ls](t)` = 3.15764982)) %&gt;% # add a line index mutate(line = rep(letters[1:4], times = c(12, 21, 7, 3))) %&gt;% ggplot(aes(x = seconds)) + geom_step(aes(y = `H[ls](t)`), color = &quot;grey50&quot;) + geom_step(aes(y = `H[na](t)`), linetype = 2) + geom_text(data = text, aes(y = y, label = label), hjust = 0, size = 3, parse = T) + scale_x_continuous(&quot;time&quot;, limits = c(0, 20)) + scale_y_continuous(expression(widehat(italic(H)(italic(t[j])))), breaks = seq(from = 0, to = 3.5, by = .5), limits = c(0, 3.5)) + theme(panel.grid = element_blank()) # bottom plot p2 &lt;- p1 + stat_smooth(data = . %&gt;% filter(line != &quot;d&quot;), aes(y = (`H[ls](t)` + `H[na](t)`) / 2, color = line, fill = line), method = &quot;lm&quot;, show.legend = F) + scale_fill_viridis_d(option = &quot;A&quot;, begin = .2, end = .8) + scale_color_viridis_d(option = &quot;A&quot;, begin = .2, end = .8) # combine (p1 + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 20))) / p2 Visualizing the cumulative hazard function from our Bayesian fit13.3 is a minor extension to the approach we used for the survivor function, above. As an example, here’s our plot for \\(\\hat H_{- \\text{LS}} (t_j)\\). The biggest change in the code is the last line in the second mutate() statement, hazard = -log(survivor). draws %&gt;% left_join( bind_rows( distinct(honking_km, interval, seconds), tibble(interval = 43, seconds = 17.15)), by = &quot;interval&quot;) %&gt;% mutate(seconds = if_else(is.na(seconds), 0, seconds), # convert S to H hazard = -log(survivor)) %&gt;% ggplot(aes(x = seconds, y = hazard)) + stat_lineribbon(step = &quot;hv&quot;, size = 3/4, .width = c(.5, .8, .95)) + scale_fill_grey(&quot;CI&quot;, start = .85, end = .6, labels = c(&quot;95%&quot;, &quot;80%&quot;, &quot;50%&quot;)) + scale_x_continuous(&quot;time&quot;, limits = c(0, 20)) + ylab(expression(widehat(italic(H)[-LS](italic(t[j]))))) + theme(legend.background = element_blank(), legend.key = element_rect(fill = &quot;grey92&quot;), legend.position = c(.925, .73), panel.grid = element_blank()) 13.5 Kernel-smoothed estimates of the hazard function The idea behind kernel smoothing is simple. At each of many distinct points in time, estimate a function’s average value by aggregating together all the point estimates available within the focal time’s temporal vicinity. Conceptually, kernel-smoothed estimates are a type of moving average. They do not identify precise values of hazard at each point in time but rather approximate values based on the estimates nearby. Even though each smoothed value only approximates the underlying true value, a plot over time can help reveal the underlying function’s shape. Kernel smoothing requires a set of point estimates to smooth. For the hazard function, one way of obtaining these point estimates is by computing successive differences in the estimated cumulative hazard function from each observed even time until the next. Each difference acts as a pseudo-slope, a measure of the local rate of change in cumulative hazard during that period. Either Nelson-Aalen estimates or negative log survivor function estimates of cumulative hazard can be used. (pp. 495–496) Singer and Willett showed examples of this smoothing in Figure 13.5, in which they applied a smoothing algorithm to the \\(H_{- \\text{LS}} (t_j)\\) estimates. The good folks at IDRE have already worked out the code to reproduce Singer and Willette’s smoothing algorithm. The IDRE folks called their custom function smooth(). my_smooth &lt;- function(width, time, survive) { n &lt;- length(time) lo &lt;- time[1] + width hi &lt;- time[n] - width npt &lt;- 50 inc &lt;- (hi - lo) / npt s &lt;- t(lo + t(c(1:npt)) * inc) slag &lt;- c(1, survive[1:n - 1]) h &lt;- 1 - survive / slag x1 &lt;- as.vector(rep(1, npt)) %*% (t(time)) x2 &lt;- s %*% as.vector(rep(1, n)) x &lt;- (x1 - x2) / width k &lt;- .75 * (1 - x * x) * (abs(x) &lt;= 1) lambda &lt;- (k %*% h) / width smoothed &lt;- data.frame(x = s, y = lambda) return(smoothed) } We’ve renamed the function my_smooth() to avoid overwriting the already existing smooth() function. The code for my_smooth() is a mild update from the one in the link above in that it returns its results in a data frame. Let’s give it a quick whirl. my_smooth(width = 1, time = honking_km$seconds, survive = honking_km$`S(t)`) %&gt;% glimpse() ## Rows: 50 ## Columns: 2 ## $ x &lt;dbl&gt; 2.6054, 2.8008, 2.9962, 3.1916, 3.3870, 3.5824, 3.7778, 3.9732, 4.1686, 4.3640, 4.5594, … ## $ y &lt;dbl&gt; 0.32686176, 0.36745825, 0.38958042, 0.38516865, 0.37093732, 0.34810198, 0.33956166, 0.31… The time and survive arguments take seconds and S(t) values, respectively. The width argument determines over how many values we would like to smooth over on a given point. For example, width = 1 would, for a given point on the time axis, aggregate over all values \\(\\pm1\\) that point. Larger width values result in more aggressive smoothing. Also notice my_smooth() returned a data frame containing 50 rows for two columns, x and y. Those columns are the coordinates for the TIME and smoothed hazard, respectively. Here we put my_smooth() to work and make our version of Figure 13.5. tibble(width = 1:3) %&gt;% mutate(xy = map(width, ~my_smooth(width = ., time = honking_km$seconds, survive = honking_km$`S(t)`)), width = str_c(&quot;width = &quot;, width)) %&gt;% unnest(xy) %&gt;% ggplot(aes(x = x, y = y)) + geom_line() + scale_x_continuous(&quot;seconds after light turns green&quot;, limits = c(0, 20)) + ylab(&quot;smoothed hazard&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ width, nrow = 3) 13.6 Developing an intuition about continuous-time survivor, cumulative hazard, and kernel-smoothed hazard functions Buckle up and load the relapse data from Cooney and colleagues (1991); the US Supreme Court tenure data from Zorn and van Winkle (2000); the first depressive episode data from Sorenson, Rutter, and Aneshensel (1991); and the health-workers employment data from Singer and colleagues (1998). alcohol_relapse &lt;- read_csv(&quot;data/alcohol_relapse.csv&quot;) %&gt;% rename_all(str_to_lower) judges &lt;- read_csv(&quot;data/judges.csv&quot;) first_depression &lt;- read_csv(&quot;data/firstdepression.csv&quot;) health_workers &lt;- read_csv(&quot;data/healthworkers.csv&quot;) glimpse(alcohol_relapse) ## Rows: 89 ## Columns: 3 ## $ weeks &lt;dbl&gt; 0.7142857, 0.7142857, 1.1428571, 1.4285714, 1.7142857, 1.7142857, 2.1428571, 2.7142… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … glimpse(judges) ## Rows: 109 ## Columns: 7 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … ## $ tenure &lt;dbl&gt; 1, 6, 6, 9, 21, 9, 1, 13, 4, 15, 31, 4, 34, 30, 16, 19, 23, 34, 20, 2, 32, 14, 32, … ## $ dead &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ retire &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ leave &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ age &lt;dbl&gt; 50, 57, 44, 47, 57, 38, 59, 47, 51, 55, 36, 44, 45, 32, 49, 42, 58, 32, 55, 49, 44,… ## $ year &lt;dbl&gt; 1789, 1789, 1789, 1789, 1789, 1790, 1791, 1793, 1796, 1796, 1798, 1799, 1801, 1804,… glimpse(first_depression) ## Rows: 2,974 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … ## $ age &lt;dbl&gt; 4, 6, 8, 8, 9, 9, 10, 10, 11, 12, 12, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 1… ## $ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… glimpse(health_workers) ## Rows: 2,074 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, … ## $ weeks &lt;dbl&gt; 0.14, 0.42, 1.00, 1.28, 1.28, 1.71, 1.85, 1.85, 1.85, 2.00, 2.14, 2.14, 2.14, 2.28,… ## $ censor &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,… For our first go, just fit the models with survfit(). fit13.4 &lt;- survfit(data = alcohol_relapse, Surv(weeks, abs(censor - 1)) ~ 1) fit13.5 &lt;- survfit(data = judges, Surv(tenure, leave) ~ 1) fit13.6 &lt;- survfit(data = first_depression, Surv(age, abs(censor - 1)) ~ 1) fit13.7 &lt;- survfit(data = health_workers, Surv(weeks, abs(censor - 1)) ~ 1) With the model from Section 13.3, we organized the Kaplan-Meier output in a tibble following the outline of Table 13.3 (p. 484). That layout was useful for plotting the frequentist results and for fitting the Bayesian version of the model with brms. Since we’re juggling four models, let’s make a convenience function to do that with a single line of code. Call it km_tibble(). km_tibble &lt;- function(surv_fit) { t &lt;- summary(surv_fit) length &lt;- length(t$time) d &lt;- tibble(time = c(0, t$time), n_risk = c(t$n.risk[1], t$n.risk), n_events = c(0, t$n.event)) %&gt;% mutate(p = n_events / n_risk, n_censored = n_risk - n_events - lead(n_risk, default = 0), interval = 0:length, interval_f = factor(0:length, levels = 0:c(length + 1))) %&gt;% select(interval:interval_f, time, n_risk:n_events, n_censored, p) %&gt;% mutate(S = cumprod(1 - p)) %&gt;% mutate(H = ifelse(interval == 0, NA, ifelse(S == 0, NA, -log(S))), p = ifelse(interval == 0, NA, p)) d &lt;- bind_rows( d, d %&gt;% slice(n()) %&gt;% mutate(interval = length + 1, interval_f = factor(length + 1, levels = 0:c(length + 1)), time = surv_fit$time %&gt;% max())) return(d) } Here’s an example of our custom km_tibble() function works based on our earlier model of the honking data, fit13.2. km_tibble(fit13.2) ## # A tibble: 44 × 9 ## interval interval_f time n_risk n_events n_censored p S H ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 57 0 0 NA 1 NA ## 2 1 1 1.41 57 1 1 0.0175 0.982 0.0177 ## 3 2 2 1.51 55 1 0 0.0182 0.965 0.0360 ## 4 3 3 1.67 54 1 0 0.0185 0.947 0.0547 ## 5 4 4 1.68 53 1 0 0.0189 0.929 0.0738 ## 6 5 5 1.86 52 1 0 0.0192 0.911 0.0932 ## 7 6 6 2.12 51 1 0 0.0196 0.893 0.113 ## 8 7 7 2.19 50 1 1 0.02 0.875 0.133 ## 9 8 8 2.48 48 1 0 0.0208 0.857 0.154 ## 10 9 9 2.5 47 1 0 0.0213 0.839 0.176 ## # ℹ 34 more rows Now apply km_tibble() to our four new fits. km &lt;- tibble(ml_fit = str_c(&quot;fit13.&quot;, 4:7)) %&gt;% mutate(d = map(ml_fit, ~ get(.) %&gt;% km_tibble())) %&gt;% unnest(d) If we want the settings in our \\(x\\)- and \\(y\\)-axes to differ across subplots, good old facet_wrap() and facet_grid() aren’t going to cut it for our version of Figure 13.6. To avoid needless repetition in the settings across subplot code, we’ll make a few custom geoms. geom_S &lt;- function(x_ul, y_lab = NULL, ...) { list( geom_hline(yintercept = .5, color = &quot;white&quot;), geom_step(...), scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)), scale_y_continuous(y_lab, breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;), limits = c(0, 1)) ) } geom_H &lt;- function(x_ul, y_lab = NULL, ...) { list( geom_step(...), scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)), scale_y_continuous(y_lab, limits = c(0, NA)) ) } geom_h &lt;- function(x_lab, x_ul, y_lab = NULL, ...) { list( geom_line(...), scale_x_continuous(x_lab, limits = c(0, x_ul)), scale_y_continuous(y_lab, limits = c(0, NA)) ) } Use geom_S() to make and save the top row, the \\(\\widehat{S (t_j)}\\) plots. p1 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.4&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 110, y_lab = expression(widehat(italic(S(t[j]))))) + labs(subtitle = &quot;Cooney et al (1991)&quot;) p2 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.5&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 36, y_lab = NULL) + labs(subtitle = &quot;Zorn &amp; Van Winkle (2000)&quot;) p3 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.6&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 102, y_lab = NULL) + labs(subtitle = &quot;Sorenson et al (1991)&quot;) p4 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.7&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 150, y_lab = NULL) + labs(subtitle = &quot;Singer et al (1998)&quot;) Use geom_H() to make and save the middle row, the \\(\\widehat{H (t_j)}\\) plots. p5 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.4&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 110, y_lab = expression(widehat(italic(H(t[j]))))) p6 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.5&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 36, y_lab = NULL) p7 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.6&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 102, y_lab = NULL) p8 &lt;- km %&gt;% filter(ml_fit == &quot;fit13.7&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 150, y_lab = NULL) Use geom_h() to make and save the bottom row, the \\(\\widehat{h (t_j)}\\) plots. p9 &lt;- my_smooth(width = 12, time = fit13.4$time, survive = fit13.4$surv) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;weeks after discharge&quot;, x_ul = 110, y_lab = expression(widehat(italic(h(t[j]))))) p10 &lt;- my_smooth(width = 5, time = fit13.5$time, survive = fit13.5$surv) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;years on court&quot;, x_ul = 36, y_lab = NULL) p11 &lt;- my_smooth(width = 7, time = fit13.6$time, survive = fit13.6$surv) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;age (in years)&quot;, x_ul = 102, y_lab = NULL) p12 &lt;- my_smooth(width = 12, time = fit13.7$time, survive = fit13.7$surv) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;weeks since hired&quot;, x_ul = 150, y_lab = NULL) Finally, combine the subplots and behold our version of Figure 13.6! ((p1 / p5 / p9) | (p2 / p6 / p10) | (p3 / p7 / p11) | (p4 / p8 / p12)) &amp; theme(panel.grid = element_blank()) 13.6.1 Bonus: Bayesians can compare continuous-time survivor, cumulative hazard, and kernel-smoothed hazard functions, too. Now let’s repeat that process as Bayesians. Here we fit those last four models with brm(). Note the data statements. Filtering by ml_fit allowed us to select the correct subset of data saved in km. The two filtering statements by interval allowed us to focus on the actual data instead of including the two rows we added for plotting conveniences. Otherwise the brm() code is just like what we used before. fit13.8 &lt;- brm(data = filter(km, ml_fit == &quot;fit13.4&quot; &amp; interval &gt; 0 &amp; interval &lt; 61), family = binomial, n_events | trials(n_risk) ~ 0 + interval_f, prior(normal(-4, 3), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 13, file = &quot;fits/fit13.08&quot;) fit13.9 &lt;- brm(data = filter(km, ml_fit == &quot;fit13.5&quot; &amp; interval &gt; 0 &amp; interval &lt; 34), family = binomial, n_events | trials(n_risk) ~ 0 + interval_f, prior(normal(-4, 3), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 13, file = &quot;fits/fit13.09&quot;) fit13.10 &lt;- brm(data = filter(km, ml_fit == &quot;fit13.6&quot; &amp; interval &gt; 0 &amp; interval &lt; 44), family = binomial, n_events | trials(n_risk) ~ 0 + interval_f, prior(normal(-4, 3), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 13, file = &quot;fits/fit13.10&quot;) fit13.11 &lt;- brm(data = filter(km, ml_fit == &quot;fit13.7&quot; &amp; interval &gt; 0 &amp; interval &lt; 306), family = binomial, n_events | trials(n_risk) ~ 0 + interval_f, prior(normal(-4, 3), class = b), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 13, file = &quot;fits/fit13.11&quot;) For the sake of space, I’m not going to show all the summary output. If you’re following along, I still recommend you give them a look. Spoiler alert: the parameter diagnostics look great. print(fit13.8) print(fit13.9) print(fit13.10) print(fit13.11) Since we’re working with four brm() fits, it might make sense to bundle the steps and keep the results in one place. Here we make something of a super function. With wrangle_draws(), we’ll extract the posterior draws from each model; add a couple interval columns; convert the results to the \\(\\widehat{p (t_j)}\\), \\(\\widehat{S (t_j)}\\), and \\(\\widehat{H (t_j)}\\) metrics; and join the results to the data stored in km. If the steps seem overwhelming, just flip back to the ends of Sections 13.3 and 13.4. This is a small extension of the data wrangling steps we took to make the \\(\\widehat{S (t_j)}\\) and \\(\\widehat{H (t_j)}\\) plots for our brms model fit13.3. wrangle_draws &lt;- function(brms, survfit) { # extract the draws draws &lt;- get(brms) %&gt;% as_draws_df() %&gt;% select(starts_with(&quot;b_&quot;)) # how many columns? n_col &lt;- ncol(draws) # transform to the p metric, add a 0 interval, make it long, and add S draws &lt;- draws %&gt;% set_names(1:n_col) %&gt;% mutate_all(inv_logit_scaled) %&gt;% mutate(`0` = 0) %&gt;% mutate(draw = 1:n()) %&gt;% pivot_longer(-draw, names_to = &quot;interval&quot;, values_to = &quot;p&quot;) %&gt;% mutate(interval = interval %&gt;% as.double()) %&gt;% arrange(interval) %&gt;% group_by(draw) %&gt;% mutate(S = cumprod(1 - p)) %&gt;% ungroup() %&gt;% mutate(H = -log(S)) # add the final interval, join the data, and return() bind_rows(draws, draws %&gt;% filter(interval == n_col) %&gt;% mutate(interval = n_col + 1)) %&gt;% left_join(km %&gt;% filter(ml_fit == survfit) %&gt;% select(interval:n_censored), by = &quot;interval&quot;) %&gt;% return() } Our wrangle_draws() function takes two arguments, brms and survfit, which indicate the desired brms model and the corresponding index within km that contains the associated survival data. Let’s put it to work. draws &lt;- tibble(brms = str_c(&quot;fit13.&quot;, 8:11), survfit = str_c(&quot;fit13.&quot;, 4:7)) %&gt;% mutate(draw = map2(brms, survfit, wrangle_draws)) %&gt;% unnest(draw) ## Warning: There were 4 warnings in `mutate()`. ## The first warning was: ## ℹ In argument: `draw = map2(brms, survfit, wrangle_draws)`. ## Caused by warning: ## ! Dropping &#39;draws_df&#39; class as required metadata was removed. ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 3 remaining warnings. draws ## # A tibble: 1,796,000 × 12 ## brms survfit draw interval p S H interval_f time n_risk n_events n_censored ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fit13.8 fit13.4 1 0 0 1 0 0 0 89 0 0 ## 2 fit13.8 fit13.4 2 0 0 1 0 0 0 89 0 0 ## 3 fit13.8 fit13.4 3 0 0 1 0 0 0 89 0 0 ## 4 fit13.8 fit13.4 4 0 0 1 0 0 0 89 0 0 ## 5 fit13.8 fit13.4 5 0 0 1 0 0 0 89 0 0 ## 6 fit13.8 fit13.4 6 0 0 1 0 0 0 89 0 0 ## 7 fit13.8 fit13.4 7 0 0 1 0 0 0 89 0 0 ## 8 fit13.8 fit13.4 8 0 0 1 0 0 0 89 0 0 ## 9 fit13.8 fit13.4 9 0 0 1 0 0 0 89 0 0 ## 10 fit13.8 fit13.4 10 0 0 1 0 0 0 89 0 0 ## # ℹ 1,795,990 more rows Like before, we have 12 subplots to make and we can reduce redundancies in the code by working with custom geoms. To accommodate our Bayesian fits, we’ll redefine geom_S() and geom_H() to depict the step functions with tidybayes::stat_lineribbon(). Happily, our geom_h() is good to go as is. geom_S &lt;- function(x_ul, y_lab = NULL, ...) { list( geom_hline(yintercept = .5, color = &quot;white&quot;), stat_lineribbon(step = &quot;hv&quot;, size = 1/2, .width = c(.5, .95), show.legend = F, ...), scale_fill_grey(start = .8, end = .6), scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)), scale_y_continuous(y_lab, breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;), limits = c(0, 1)) ) } geom_H &lt;- function(x_ul, y_lab = NULL, ...) { list( stat_lineribbon(step = &quot;hv&quot;, size = 1/2, .width = c(.5, .95), show.legend = F, ...), scale_fill_grey(start = .8, end = .6), scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)), scale_y_continuous(y_lab, limits = c(0, NA)) ) } Make and save the subplots. # use `geom_S()` to make and save the top row p1 &lt;- draws %&gt;% filter(brms == &quot;fit13.8&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 110, y_lab = expression(widehat(italic(S(t[j]))))) + labs(subtitle = &quot;Cooney et al (1991)&quot;) p2 &lt;- draws %&gt;% filter(brms == &quot;fit13.9&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 36, y_lab = NULL) + labs(subtitle = &quot;Zorn &amp; Van Winkle (2000)&quot;) p3 &lt;- draws %&gt;% filter(brms == &quot;fit13.10&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 102, y_lab = NULL) + labs(subtitle = &quot;Sorenson et al (1991)&quot;) p4 &lt;- draws %&gt;% filter(brms == &quot;fit13.11&quot;) %&gt;% ggplot(aes(x = time, y = S)) + geom_S(x_ul = 150, y_lab = NULL) + labs(subtitle = &quot;Singer et al (1998)&quot;) # use `geom_H()` to make and save the middle row p5 &lt;- draws %&gt;% filter(brms == &quot;fit13.8&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 110, y_lab = expression(widehat(italic(H(t[j]))))) p6 &lt;- draws %&gt;% filter(brms == &quot;fit13.9&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 36, y_lab = NULL) p7 &lt;- draws %&gt;% filter(brms == &quot;fit13.10&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 102, y_lab = NULL) p8 &lt;- draws %&gt;% filter(brms == &quot;fit13.11&quot;) %&gt;% ggplot(aes(x = time, y = H)) + geom_H(x_ul = 150, y_lab = NULL) # use `geom_h()` to make and save the bottom row p9 &lt;- draws %&gt;% filter(brms == &quot;fit13.8&quot;) %&gt;% group_by(interval, time) %&gt;% summarize(median = median(S)) %&gt;% nest(data = everything()) %&gt;% mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% unnest(smooth) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;weeks after discharge&quot;, x_ul = 110, y_lab = expression(widehat(italic(h(t[j]))))) p10 &lt;- draws %&gt;% filter(brms == &quot;fit13.9&quot;) %&gt;% group_by(interval, time) %&gt;% summarize(median = median(S)) %&gt;% nest(data = everything()) %&gt;% mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% unnest(smooth) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;years on court&quot;, x_ul = 36, y_lab = NULL) p11 &lt;- draws %&gt;% filter(brms == &quot;fit13.10&quot;) %&gt;% group_by(interval, time) %&gt;% summarize(median = median(S)) %&gt;% nest(data = everything()) %&gt;% mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% unnest(smooth) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;age (in years)&quot;, x_ul = 102, y_lab = NULL) p12 &lt;- draws %&gt;% filter(brms == &quot;fit13.11&quot;) %&gt;% group_by(interval, time) %&gt;% summarize(median = median(S)) %&gt;% nest(data = everything()) %&gt;% mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% unnest(smooth) %&gt;% ggplot(aes(x = x, y = y)) + geom_h(x_lab = &quot;weeks since hired&quot;, x_ul = 150, y_lab = NULL) Finally, combine the subplots and behold our Bayesian alternative version of Figure 13.6! ((p1 / p5 / p9) | (p2 / p6 / p10) | (p3 / p7 / p11) | (p4 / p8 / p12)) &amp; theme(panel.grid = element_blank()) Session info sessionInfo() ## R version 4.3.0 (2023-04-21) ## Platform: x86_64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Chicago ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] survival_3.5-5 patchwork_1.1.2 tidybayes_3.0.4 brms_2.19.0 Rcpp_1.0.10 lubridate_1.9.2 ## [7] forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [13] tibble_3.2.1 ggplot2_3.4.2 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] tensorA_0.36.2 rstudioapi_0.14 jsonlite_1.8.4 magrittr_2.0.3 ## [5] TH.data_1.1-2 estimability_1.4.1 farver_2.1.1 nloptr_2.0.3 ## [9] rmarkdown_2.21 vctrs_0.6.2 minqa_1.2.5 base64enc_0.1-3 ## [13] htmltools_0.5.5 distributional_0.3.2 sass_0.4.6 StanHeaders_2.26.25 ## [17] bslib_0.4.2 htmlwidgets_1.6.2 plyr_1.8.8 sandwich_3.0-2 ## [21] emmeans_1.8.6 zoo_1.8-12 cachem_1.0.8 igraph_1.4.2 ## [25] mime_0.12 lifecycle_1.0.3 pkgconfig_2.0.3 colourpicker_1.2.0 ## [29] Matrix_1.5-4 R6_2.5.1 fastmap_1.1.1 shiny_1.7.4 ## [33] digest_0.6.31 colorspace_2.1-0 ps_1.7.5 crosstalk_1.2.0 ## [37] projpred_2.5.0 labeling_0.4.2 fansi_1.0.4 timechange_0.2.0 ## [41] abind_1.4-5 mgcv_1.8-42 compiler_4.3.0 bit64_4.0.5 ## [45] withr_2.5.0 backports_1.4.1 inline_0.3.19 shinystan_2.6.0 ## [49] gamm4_0.2-6 pkgbuild_1.4.0 highr_0.10 MASS_7.3-58.4 ## [53] gtools_3.9.4 loo_2.6.0 tools_4.3.0 httpuv_1.6.11 ## [57] threejs_0.3.3 glue_1.6.2 callr_3.7.3 nlme_3.1-162 ## [61] promises_1.2.0.1 grid_4.3.0 checkmate_2.2.0 reshape2_1.4.4 ## [65] generics_0.1.3 gtable_0.3.3 tzdb_0.4.0 hms_1.1.3 ## [69] utf8_1.2.3 ggdist_3.3.0 pillar_1.9.0 markdown_1.7 ## [73] vroom_1.6.3 posterior_1.4.1 later_1.3.1 splines_4.3.0 ## [77] lattice_0.21-8 bit_4.0.5 tidyselect_1.2.0 miniUI_0.1.1.1 ## [81] knitr_1.42 arrayhelpers_1.1-0 gridExtra_2.3 bookdown_0.34 ## [85] stats4_4.3.0 xfun_0.39 bridgesampling_1.1-2 matrixStats_0.63.0 ## [89] DT_0.27 rstan_2.21.8 stringi_1.7.12 boot_1.3-28.1 ## [93] evaluate_0.21 codetools_0.2-19 cli_3.6.1 RcppParallel_5.1.7 ## [97] shinythemes_1.2.0 xtable_1.8-4 munsell_0.5.0 processx_3.8.1 ## [101] jquerylib_0.1.4 coda_0.19-4 svUnit_1.0.6 parallel_4.3.0 ## [105] rstantools_2.3.1 ellipsis_0.3.2 prettyunits_1.1.1 dygraphs_1.1.1.6 ## [109] bayesplot_1.10.0 Brobdingnag_1.2-9 lme4_1.1-33 viridisLite_0.4.2 ## [113] mvtnorm_1.1-3 scales_1.2.1 xts_0.13.1 crayon_1.5.2 ## [117] rlang_1.1.1 multcomp_1.4-23 shinyjs_2.1.0 References Bürkner, P.-C. (2021c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Cooney, N. L., Kadden, R. M., Litt, M. D., &amp; Getter, H. (1991). Matching alcoholics to coping skills or interactional therapies: Two-year follow-up results. Journal of Consulting and Clinical Psychology, 59(4), 598. https://doi.org/10.1037/0022-006X.59.4.598 Diekmann, A., Jungbauer-Gans, M., Krassnig, H., &amp; Lorenz, S. (1996). Social status and aggression: A field study analyzed by survival analysis. The Journal of Social Psychology, 136(6), 761–768. https://doi.org/10.1080/00224545.1996.9712252 Kaplan, E. L., &amp; Meier, P. (1958). Nonparametric estimation from incomplete observations. Journal of the American Statistical Association, 53(282), 457–481. https://doi.org/10.1080/01621459.1958.10501452 Lawless, J. F. (1982). Statistical models and methods for lifetime data. John Wiley &amp; Sons. Miller, R. G. (1981). Survival analysis. John Wiley &amp; Sons. Singer, J. D., Davidson, S. M., Graham, S., &amp; Davidson, H. S. (1998). Physician retention in community and migrant health centers: Who stays and for how long? Medical Care, 1198–1213. http://www.jstor.org/stable/3766886 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Sorenson, S. B., Rutter, C. M., &amp; Aneshensel, C. S. (1991). Depression in the community: An investigation into age of onset. Journal of Consulting and Clinical Psychology, 59(4), 541. https://doi.org/10.1037/0022-006X.59.4.541 Zorn, C. J., &amp; Van Winkle, S. R. (2000). A competing risks model of Supreme Court vacancies, 1789. Political Behavior, 22(2), 145–166. https://doi.org/10.1023/A:1006667601289 "],["references.html", "References", " References Aalen, O. O. (1988). Heterogeneity in survival analysis. Statistics in Medicine, 7(11), 1121–1137. https://doi.org/10.1002/sim.4780071105 Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Bates, D., Maechler, M., Bolker, B., &amp; Steven Walker. (2022). lme4: Linear mixed-effects models using Eigen’ and S4. https://CRAN.R-project.org/package=lme4 Beck, N. (1999). Modelling space and time: The event history approach. In E. Scarbrough &amp; E. Tanenbaum (Eds.), Research strategies in social science: A guide to new approaches. Oxford University Press. https://doi.org/10.1093/0198292376.001.0001 Beck, Nathaniel, Katz, J. N., &amp; Tucker, R. (1998). Taking time seriously: Time-series-cross-section analysis with a binary dependent variable. American Journal of Political Science, 42(4), 1260–1288. https://doi.org/10.2307/2991857 Brennan, R. L. (2001). Generalizability Theory. Springer-Verlag. https://doi.org/10.1007/978-1-4757-3456-0 Brilleman, S. (2019). Estimating survival (time-to-event) models with rstanarm. https://github.com/stan-dev/rstanarm/blob/feature/frailty-models/vignettes/surv.Rmd Brilleman, S. L., Elci, E. M., Novik, J. B., &amp; Wolfe, R. (2020). Bayesian survival analysis using the rstanarm R package. https://arxiv.org/abs/2002.09633 Brown, D. R., &amp; Gary, L. E. (1985). Predictors of depressive symptoms among unemployed Black adults. Journal of Sociology and Social Welfare, 12, 736. https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=1721&amp;amp=&amp;context=jssw&amp;amp=&amp;sei-redir=1&amp;referer=https%253A%252F%252Fscholar.google.com%252Fscholar%253Fq%253D%252522CES-D%252522%252Bunemployment%2526hl%253Den%2526as_sdt%253D0%25252C44%2526as_ylo%253D1977%2526as_yhi%253D2000#search=%22CES-D%20unemployment%22 Bryk, A. S., &amp; Raudenbush, S. W. (1987). Application of hierarchical linear models to assessing change. Psychological Bulletin, 101(1), 147. https://doi.org/10.1037/0033-2909.101.1.147 Bürkner, P.-C. (2020). Bayesian item response modeling in R with brms and Stan. arXiv:1905.09501 [Stat]. http://arxiv.org/abs/1905.09501 Bürkner, P.-C. (2021a). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Bürkner, P.-C. (2021b). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html Bürkner, P.-C. (2021c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C. (2022a). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017 Bürkner, P.-C. (2021d). brms reference manual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf Bürkner, P.-C. (2022b). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms Bürkner, P.-C., Gabry, J., Kay, M., &amp; Vehtari, A. (2022). posterior: Tools for working with posterior distributions. https://CRAN.R-project.org/package=posterior Capaldi, D. M., Crosby, L., &amp; Stoolmiller, M. (1996). Predicting the timing of first sexual intercourse for at-risk adolescent males. Child Development, 67(2), 344–359. https://doi.org/10.2307/1131818 Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., &amp; Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01 Chung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., &amp; Liu, J. (2013). A nondegenerate penalized likelihood estimator for variance parameters in multilevel models. Psychometrika, 78(4), 685–709. https://doi.org/10.1007/s11336-013-9328-2 Cooney, N. L., Kadden, R. M., Litt, M. D., &amp; Getter, H. (1991). Matching alcoholics to coping skills or interactional therapies: Two-year follow-up results. Journal of Consulting and Clinical Psychology, 59(4), 598. https://doi.org/10.1037/0022-006X.59.4.598 Cox, David R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society: Series B (Methodological), 34(2), 187–202. https://doi.org/10.1111/j.2517-6161.1972.tb00899.x Cox, David Roxbee, &amp; Oakes, D. (1984). Analysis of survival data (Vol. 21). CRC Press. https://www.routledge.com/Analysis-of-Survival-Data/Cox-Oakes/p/book/9780412244902 Cranford, J. A., Shrout, P. E., Iida, M., Rafaeli, E., Yip, T., &amp; Bolger, N. (2006). A procedure for evaluating sensitivity to within-person change: Can mood measures in diary studies detect change reliably? Personality and Social Psychology Bulletin, 32(7), 917–929. https://doi.org/10.1177/0146167206287721 Cronbach, L. J., Gleser, G. C., Nanda, H., &amp; Rajaratnam, N. (1972). The dependability of behavioral measurements: Theory of generalizability for scores and profiles. John Wiley &amp; Sons. https://www.amazon.com/Dependability-Behavioral-Measurements-Generalizability-Profiles/dp/0471188506 Diekmann, A., Jungbauer-Gans, M., Krassnig, H., &amp; Lorenz, S. (1996). Social status and aggression: A field study analyzed by survival analysis. The Journal of Social Psychology, 136(6), 761–768. https://doi.org/10.1080/00224545.1996.9712252 Enders, C. K. (2010). Applied missing data analysis. Guilford Press. http://www.appliedmissingdata.com/ Flinn, C. J., &amp; Heckman, J. J. (1982). New methods for analyzing individual event histories. Sociological Methodology, 13, 99–140. https://doi.org/10.2307/270719 Frank, A. R., &amp; Keith, T. Z. (1984). Academic abilities of persons entering and remaining in special education. Exceptional Children, 51(1), 76–77. https://eric.ed.gov/?id=EJ306852 Gabry, J. (2020). loo reference manual, Version 2.4.1. https://CRAN.R-project.org/package=loo/loo.pdf Gabry, J., &amp; Mahr, T. (2022). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot Gabry, J., &amp; Modrák, M. (2020). Visual MCMC diagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Gamse, B. C., &amp; Conger, D. (1997). An evaluation of the Spencer post-doctoral dissertation program. Abt Associates. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/ Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100 Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942 Gelman, A., Hill, J., &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879 Gilks, W. R., Richardson, S., &amp; Spiegelhalter, D. (1995). Markov chain Monte Carlo in practice. Chapman and Hall/CRC. https://www.routledge.com/Markov-Chain-Monte-Carlo-in-Practice/Gilks-Richardson-Spiegelhalter/p/book/9780412055515 Ginexi, E. M., Howe, G. W., &amp; Caplan, R. D. (2000). Depression and control beliefs in relation to reemployment: What are the directions of effect? Journal of Occupational Health Psychology, 5(3), 323–336. https://doi.org/10.1037/1076-8998.5.3.323 Graham, S. E. (1997). The exodus from mathematics: When and why? [PhD thesis]. Harvard Graduate School of Education. Greenwood, M. (1926). The natural duration of cancer. Reports on Public Health and Medical Subjects, 33, 1–26. Head, R., &amp; Pike, D. (1975). A review of response surface methodology from a biometric point of view. Biometrics, 31, 803–851. Heckman, J., &amp; Singer, B. S. (Eds.). (1984). Longitudinal analysis of labor market data. Cambridge University Press. https://doi.org/10.1017/CCOL0521304539 Hu, X. J., &amp; Lawless, J. F. (1996). Estimation from truncated lifetime data with supplementary information on covariates and censoring times. Biometrika, 83(4), 747–761. https://doi.org/10.1093/biomet/83.4.747 Jaeger, B. C., Edwards, L. J., Das, K., &amp; Sen, P. K. (2017). An R2 statistic for fixed effects in the generalized linear mixed model. Journal of Applied Statistics, 44(6), 1086–1105. https://doi.org/10.1080/02664763.2016.1193725 Kaplan, E. L., &amp; Meier, P. (1958). Nonparametric estimation from incomplete observations. Journal of the American Statistical Association, 53(282), 457–481. https://doi.org/10.1080/01621459.1958.10501452 Kay, M. (2021). ggdist: Visualizations of distributions and uncertainty [Manual]. https://CRAN.R-project.org/package=ggdist Kay, M. (2023). tidybayes: Tidy data and ’geoms’ for Bayesian models. https://CRAN.R-project.org/package=tidybayes Keiley, Margaret Kraatz, Bates, J. E., Dodge, K. A., &amp; Pettit, G. S. (2000). A cross-domain growth analysis: Externalizing and internalizing behaviors during 8 years of childhood. Journal of Abnormal Child Psychology, 28(2), 161–179. https://doi.org/10.1023/A:1005122814723 Keiley, M. K., &amp; Martin, N. C. (2002). Child abuse, neglect, and juvenile delinquency: How “new” statistical approaches can inform our understanding of “old” questions reanalysis of Widom, 1989 [Manuscript Submitted for Publication]. Kreft, I. G. G., &amp; de Leeuw, J. (1990). Comparing four different statistical packages for hierarchical linear regression: GENMOD, HLM, ML2, and VARCL. CSE Dissemination Office, UCLA Graduate School of Education, 405 Hilgard Avenue, Los Angeles, CA 90024-1521. https://files.eric.ed.gov/fulltext/ED340731.pdf Kreft, I. G., &amp; de Leeuw, J. (1998). Introducing multilevel modeling. SAGE Publications, Inc. https://doi.org/https://dx.doi.org/10.4135/9781849209366 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kruschke, J. K., &amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin &amp; Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4 Kuhn, M., Jackson, S., &amp; Cimentada, J. (2020). corrr: Correlations in R [Manual]. https://CRAN.R-project.org/package=corrr Kurz, A. S. (2021). Statistical rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.2.0). https://bookdown.org/content/4857/ Kurz, A. S. (2020). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Lambert, B. (2018). A student’s guide to Bayesian statistics. SAGE Publications, Inc. https://ben-lambert.com/a-students-guide-to-bayesian-statistics/ Lawless, J. F. (1982). Statistical models and methods for lifetime data. John Wiley &amp; Sons. Li, H., &amp; Lahiri, P. (2010). An adjusted maximum likelihood method for solving small area estimation problems. Journal of Multivariate Analysis, 101(4), 882–892. https://doi.org/10.1016/j.jmva.2009.10.009 Little, R. J. (1995). Modeling the drop-out mechanism in repeated-measures studies. Journal of the American Statistical Association, 90(431), 1112–1121. https://doi.org/10.1080/01621459.1995.10476615 Little, R. J. A., &amp; Rubin, D., B. (1987). Statistical analysis with missing data. Wiley. Little, R. J., &amp; Rubin, D. B. (2019). Statistical analysis with missing data (third, Vol. 793). John Wiley &amp; Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798 LoPilato, A. C., Carter, N. T., &amp; Wang, M. (2015). Updating generalizability theory in management research: Bayesian estimation of variance components. Journal of Management, 41(2), 692–717. https://doi.org/10.1177/0149206314554215 Mallinckrod, C. H., Lane, P. W., Schnell, D., Peng, Y., &amp; Mancuso, J. P. (2008). Recommendations for the primary analysis of continuous endpoints in longitudinal clinical trials. Drug Information Journal, 42(4), 303–319. https://doi.org/10.1177/009286150804200402 Mare, R. D. (1994). Discrete-time bivariate hazards with unobserved heterogeneity: A partially observed contingency table approach. Sociological Methodology, 341–383. https://doi.org/10.2307/270987 McElreath, R. (2020a). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2020b). rethinking R package. https://xcelab.net/rm/software/ McNeish, D., Stapleton, L. M., &amp; Silverman, R. D. (2017). On the unnecessary ubiquity of hierarchical linear modeling. Psychological Methods, 22(1), 114. https://doi.org/10.1037/met0000078 Miller, R. G. (1981). Survival analysis. John Wiley &amp; Sons. Morris, C., &amp; Tang, R. (2011). Estimating random effects via adjustment for density maximization. Statistical Science, 26(2), 271–287. https://doi.org/10.1214/10-STS349 Newsom, J. T. (2015). Longitudinal structural equation modeling: A comprehensive introduction. Routledge. http://www.longitudinalsem.com/ Nezlek, J. B. (2007). A multilevel framework for understanding relationships among traits, states, situations and behaviours. European Journal of Personality, 21(6), 789–810. https://doi.org/10.1002/per.640 Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114 Pearl, J., Glymour, M., &amp; Jewell, N. P. (2016). Causal Inference in Statistics - A Primer (1st Edition). Wiley. https://www.wiley.com/en-us/Causal+Inference+in+Statistics%3A+A+Primer-p-9781119186847 Pedersen, T. L. (2022). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork Peng, R. D. (2019). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/ Pinheiro, J., Bates, D., &amp; R-core. (2021). nlme: Linear and nonlinear mixed effects models [Manual]. https://CRAN.R-project.org/package=nlme Plummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. Proceedings of the 3rd International Workshop on Distributed Statistical Computing, 124, 1–10. http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf Plummer, M. (2012). JAGS Version 3.3.0 user manual. http://www.stat.cmu.edu/~brian/463-663/week10/articles,%20manuals/jags_user_manual.pdf R Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Radloff, L. S. (1977). The CES-D Scale: A self-report depression scale for research in the general population. Applied Psychological Measurement, 1(3), 385–401. https://doi.org/10.1177/014662167700100306 Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (Second Edition). SAGE Publications, Inc. https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230 Raudenbush, S. W., &amp; Chan, W.-S. (2016). Growth curve analysis in accelerated longitudinal designs. Journal of Research in Crime and Delinquency, 29(4), 387–411. https://doi.org/10.1177/0022427892029004001 Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych Rights, Jason D., &amp; Cole, D. A. (2018). Effect size measures for multilevel models in clinical child and adolescent research: New R-squared methods and recommendations. Journal of Clinical Child &amp; Adolescent Psychology, 47(6), 863–873. https://doi.org/10.1080/15374416.2018.1528550 Rights, Jason D., &amp; Sterba, S. K. (2020). New recommendations on the use of R-squared differences in multilevel model comparisons. Multivariate Behavioral Research, 55(4), 568–599. https://doi.org/10.1080/00273171.2019.1660605 Ripley, B. (2022). MASS: Support functions and datasets for venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS Robinson, D., Hayes, A., &amp; Couch, S. (2022). broom: Convert statistical objects into tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom Rogosa, D. R., &amp; Willett, J. B. (1985). Understanding correlates of change by modeling individual differences in growth. Psychometrika, 50(2), 203–228. https://doi.org/10.1007/BF02294247 Rogosa, D., Brandt, D., &amp; Zimowski, M. (1982). A growth curve approach to the measurement of change. Psychological Bulletin, 92(3), 726–748. https://doi.org/10.1037/0033-2909.92.3.726 Rupert G. Miller, Jr. (1997). Beyond ANOVA: Basics of applied statistics. Chapman and Hall/CRC. https://www.routledge.com/Beyond-ANOVA-Basics-of-Applied-Statistics/Jr/p/book/9780412070112 Sandberg, D. E., Meyer-Bahlburg, H. F. L., &amp; Yager, T. J. (1991). The Child Behavior Checklist nonclinical standardization samples: Should they be utilized as norms? Journal of the American Academy of Child &amp; Adolescent Psychiatry, 30(1), 124–134. https://doi.org/10.1097/00004583-199101000-00019 Schafer, J. L. (1997). Analysis of incomplete multivariate data. CRC press. https://www.routledge.com/Analysis-of-Incomplete-Multivariate-Data/Schafer/p/book/9780412040610 Scheike, T. H., &amp; Jensen, T. K. (1997). A discrete survival model with random effects: An application to time to pregnancy. Biometrics, 318–329. https://doi.org/10.2307/2533117 Schloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Larmarange, J. (2021). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally Shrout, P. E., &amp; Lane, S. P. (2012). Psychometrics. In M. R. Mehl &amp; T. S. Conner (Eds.), Handbook of research methods for studying daily life (pp. 302–320). The Guilford Press. https://www.guilford.com/books/Handbook-of-Research-Methods-for-Studying-Daily-Life/Mehl-Conner/9781462513055 Singer, J. D. (1992). Are special educators’ career paths special? Results from a 13-year longitudinal study. Exceptional Children, 59(3), 262–279. https://doi.org/10.1177/001440299305900309 Singer, J. D., Davidson, S. M., Graham, S., &amp; Davidson, H. S. (1998). Physician retention in community and migrant health centers: Who stays and for how long? Medical Care, 1198–1213. http://www.jstor.org/stable/3766886 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Snijders, T. A. B., &amp; Bosker, R. J. (1994). Modeled variance in two-level models. Sociological Methods &amp; Research, 22(3), 342–363. https://doi.org/10.1177/0049124194022003004 Sorenson, S. B., Rutter, C. M., &amp; Aneshensel, C. S. (1991). Depression in the community: An investigation into age of onset. Journal of Consulting and Clinical Psychology, 59(4), 541. https://doi.org/10.1037/0022-006X.59.4.541 Spiegelhalter, D. J., Best, N. G., Carlin, B. P., &amp; Linde, A. V. D. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4), 583–639. https://doi.org/10.1111/1467-9868.00353 Stan Development Team. (2021a). Stan reference manual, Version 2.27. https://mc-stan.org/docs/2_27/reference-manual/ Stan Development Team. (2021b). Stan user’s guide, Version 2.26. https://mc-stan.org/docs/2_26/stan-users-guide/index.html Steegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Sueyoshi, G. T. (1995). A class of binary response models for grouped duration data. Journal of Applied Econometrics, 10(4), 411–431. https://doi.org/10.1002/jae.3950100406 Therneau, Terry M. (2021a). A package for survival analysis in R. https://CRAN.R-project.org/package=survival/vignettes/survival.pdf Therneau, Terry M. (2021b). survival reference manual, Version 3.2-10. https://CRAN.R-project.org/package=survival/survival.pdf Therneau, Terry M. (2021c). survival: Survival analysis [Manual]. https://github.com/therneau/survival Therneau, Terry M., &amp; Grambsch, P. M. (2000). Modeling survival data: Extending the Cox model. Springer. https://link.springer.com/book/10.1007/978-1-4757-3294-8 Tomarken, A., Shelton, R., Elkins, L., &amp; Anderson, T. (1997). Sleep deprivation and anti-depressant medication: Unique effects on positive and negative affect. American Psychological Society Meeting, Washington, DC. Turnbull, B. W. (1974). Nonparametric estimation of a survivorship function with doubly censored data. Journal of the American Statistical Association, 69(345), 169–173. https://doi.org/10.1080/01621459.1974.10480146 Turnbull, B. W. (1976). The empirical distribution function with arbitrarily grouped, censored and truncated data. Journal of the Royal Statistical Society: Series B (Methodological), 38(3), 290–295. https://doi.org/10.1111/j.2517-6161.1976.tb01597.x van Buuren, S. (2018). Flexible imputation of missing data (Second Edition). CRC Press. https://stefvanbuuren.name/fimd/ Vaupel, J. W., Manton, K. G., &amp; Stallard, E. (1979). The impact of heterogeneity in individual frailty on the dynamics of mortality. Demography, 16(3), 439–454. https://doi.org/10.2307/2061224 Vaupel, J. W., &amp; Yashin, A. I. (1985). Heterogeneity’s ruses: Some surprising effects of selection on population dynamics. The American Statistician, 39(3), 176–185. https://doi.org/10.1080/00031305.1985.10479424 Vehtari, A., &amp; Gabry, J. (2020). Using the loo package (version \\(&gt;\\)= 2.0.0). https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp; Gelman, A. (2022). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/ Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved \\(\\widehat{R}\\) for assessing convergence of MCMC. arXiv Preprint arXiv:1903.08008. https://arxiv.org/abs/1903.08008? Vehtari, A., Simpson, D., Gelman, A., Yao, Y., &amp; Gabry, J. (2021). Pareto smoothed importance sampling. https://arxiv.org/abs/1507.02646 Venables, W. N., &amp; Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4 Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html Wheaton, B., Roszell, P., &amp; Hall, K. (1997). The impact of twenty childhood and adult traumatic stressors on the risk of psychiatric disorder. In I. H. Gotlib &amp; B. Wheaton (Eds.), Stress and adversity over the life course: Trajectories and turning points (pp. 50–72). Cambridge University Press. https://doi.org/10.1017/CBO9780511527623 Wickham, H. (2022). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Willett, J. B. (1989). Some results on reliability for the longitudinal measurement of change: Implications for the design of studies of individual growth. Educational and Psychological Measurement, 49(3), 587–602. https://doi.org/10.1177/001316448904900309 Willett, J. B. (1988). Chapter 9: Questions and answers in the measurement of change. Review of Research in Education, 15, 345–422. https://doi.org/10.2307/1167368 Williams, D. R., Rouder, J., &amp; Rast, P. (2019). Beneath the surface: Unearthing within-Person variability and mean relations with Bayesian mixed models. https://doi.org/10.31234/osf.io/gwatq Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091 Zorn, C. J., &amp; Van Winkle, S. R. (2000). A competing risks model of Supreme Court vacancies, 1789. Political Behavior, 22(2), 145–166. https://doi.org/10.1023/A:1006667601289 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
