[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied longitudinal data analysis in brms and the tidyverse",
    "section": "",
    "text": "What and why\nThis project is based on Singer and Willett’s classic (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence. You can download the data used in the text at http://www.bristol.ac.uk/cmm/learning/support/singer-willett.html and find a wealth of ideas on how to fit the models in the text at https://stats.idre.ucla.edu/other/examples/alda/. My contributions show how to fit these models and others like them within a Bayesian framework. I make extensive use of Paul Bürkner’s brms package (Bürkner, 2017, 2018, 2022), which makes it easy to fit Bayesian regression models in R (R Core Team, 2022) using Hamiltonian Monte Carlo (HMC) via the Stan probabilistic programming language (Carpenter et al., 2017). Much of the data wrangling and plotting code is done with packages connected to the tidyverse (Wickham et al., 2019; Wickham, 2022).",
    "crumbs": [
      "What and why"
    ]
  },
  {
    "objectID": "index.html#caution-work-in-progress",
    "href": "index.html#caution-work-in-progress",
    "title": "Applied longitudinal data analysis in brms and the tidyverse",
    "section": "Caution: Work in progress",
    "text": "Caution: Work in progress\nThis release contains drafts of Chapters 1 through 7 and 9 through 13. Chapters 1 through 7 provide the motivation and foundational principles for fitting longitudinal multilevel models. Chapters 9 through 13 motivation and foundational principles for fitting discrete-time survival analyses.\nIn addition to fleshing out more of the chapters, I plan to add more goodies like introductions to multivariate longitudinal models and mixed-effect location and scale models. But there is no time-table for this project. To keep up with the latest changes, check in at the GitHub repository, https://github.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse, or follow my announcements on twitter (here) or bluesky (here).",
    "crumbs": [
      "What and why"
    ]
  },
  {
    "objectID": "index.html#r-setup",
    "href": "index.html#r-setup",
    "title": "Applied longitudinal data analysis in brms and the tidyverse",
    "section": "R setup",
    "text": "R setup\nTo get the full benefit from this ebook, you’ll need some software. Happily, everything will be free (provided you have access to a decent personal computer and an good internet connection).\nFirst, you’ll need to install R, which you can learn about at https://cran.r-project.org/.\nThough not necessary, your R experience might be more enjoyable if done through the free RStudio interface, which you can learn about at https://rstudio.com/products/rstudio/.\nOnce you have installed R, execute the following to install the bulk of the add-on packages. This will probably take a few minutes to finish. Go make yourself a coffee.\n\npackages &lt;- c(\n  \"bayesplot\", \"brms\", \"broom\", \"devtools\", \"flextable\", \"GGally\", \"ggmcmc\", \n  \"ggrepel\", \"gtools\", \"loo\", \"patchwork\", \"psych\", \"Rcpp\", \"remotes\", \"rstan\", \n  \"StanHeaders\", \"survival\", \"tidybayes\", \"tidyverse\")\n\ninstall.packages(packages, dependencies = T)\n\nA couple of the other packages are not officially available via the Comprehensive R Archive Network (CRAN; https://cran.r-project.org/). You can download them directly from GitHub by executing the following.\n\ndevtools::install_github(\"stan-dev/cmdstanr\")\ndevtools::install_github(\"rmcelreath/rethinking\")\n\nIt’s possible you’ll have problems installing some of these packages. Here are some likely suspects and where you can find help:\n\nfor difficulties installing brms, go to https://github.com/paul-buerkner/brms#how-do-i-install-brms or search around in the brms section of the Stan forums;\nfor difficulties installing cmdstanr, go to https://mc-stan.org/cmdstanr/articles/cmdstanr.html;\nfor difficulties installing rethinking, go to https://github.com/rmcelreath/rethinking#quick-installation; and\nfor difficulties installing rstan, go to https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started.",
    "crumbs": [
      "What and why"
    ]
  },
  {
    "objectID": "index.html#license-and-citation",
    "href": "index.html#license-and-citation",
    "title": "Applied longitudinal data analysis in brms and the tidyverse",
    "section": "License and citation",
    "text": "License and citation\nThis book is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details, here. In short, you can use my work. Just please give me the appropriate credit the same way you would for any other scholarly resource. Here’s the citation information:\n\n@book{kurzAppliedLongitudinalDataAnalysis2026,\n  title = {Applied longitudinal data analysis in brms and the tidyverse},\n  author = {Kurz, A. Solomon},\n  year = {2026},\n  month = {1},\n  edition = {version 0.0.4},\n  url = {https://solomon.quarto.pub/alda/}\n}",
    "crumbs": [
      "What and why"
    ]
  },
  {
    "objectID": "index.html#comments",
    "href": "index.html#comments",
    "title": "Applied longitudinal data analysis in brms and the tidyverse",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2022). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01\n\n\nR Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nWickham, H. (2022). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "What and why"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  A Framework for Investigating Change over Time",
    "section": "",
    "text": "1.1 When might you study change over time?\nPerhaps a better question is: When wouldn’t you?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Framework for Investigating Change over Time</span>"
    ]
  },
  {
    "objectID": "01.html#distinguishing-between-two-types-of-questions-about-change",
    "href": "01.html#distinguishing-between-two-types-of-questions-about-change",
    "title": "1  A Framework for Investigating Change over Time",
    "section": "1.2 Distinguishing between two types of questions about change",
    "text": "1.2 Distinguishing between two types of questions about change\nOn page 8, Singer and Willett proposed there are two fundamental questions for longitudinal data analysis:\n\n“How does the outcome change over time?” and\n“Can we predict differences in these changes?”\n\nWithin the hierarchical framework, we often speak about two levels of change. We address within-individual change at level-1.\n\nThe goal of a level-1 analysis is to describe the shape of each person’s individual growth trajectory.\nIn the second stage of an analysis of change, known as level-2, we ask about interindividual differences in change… The goal of a level-2 analysis is to detect heterogeneity in change across individuals and to determine the relationship between predictors and the shape of each person’s individual growth trajectory. (p. 8, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Framework for Investigating Change over Time</span>"
    ]
  },
  {
    "objectID": "01.html#three-important-features-of-a-study-of-change",
    "href": "01.html#three-important-features-of-a-study-of-change",
    "title": "1  A Framework for Investigating Change over Time",
    "section": "1.3 Three important features of a study of change",
    "text": "1.3 Three important features of a study of change\n\nThree or more waves of data\nAn outcome whose values change systematically over time\nA sensible metric for clocking time\n\n\n1.3.1 Multiple waves of data\nSinger and Willett criticized two-waves data on two grounds.\n\nFirst, it cannot tell us about the shape of each person’s individual growth trajectory, the focus of our level-1 question. Did all the change occur immediately after the first assessment? Was progress steady or delayed? Second, it cannot distinguish true change from measurement error. If measurement error renders pretest scores too low and posttest scores too high, you might conclude erroneously that scores increase over time when a longer temporal view would suggest the opposite. In statistical terms, two-waves studies cannot describe individual trajectories of change and they confound true change with measurement error (see Rogosa et al., 1982). (p. 10, emphasis in the original)\n\nI am not a fan of this ‘true change/measurement error’ way of speaking and would rather speak in terms of systemic and [seemingly] un-systemic changes among means and variances. Otherwise put, I’d rather speak in terms of trait and state. Two waves of data do not allow us to disentangle systemic mean differences from stable means and substantial variances for those means. Two waves of data do not allow us to disentangle changes in traits from stable traits but important differences in states. For an introduction to this way of thinking, check out Nezlek’s (2007) A multilevel framework for understanding relationships among traits, states, situations and behaviors.\n\n\n1.3.2 A sensible metric for time\n\nChoice of a time metric affects several interrelated decisions about the number and spacing of data collection waves….\nOur overarching point is that there is no single answer to the seemingly simple question about the most sensible metric for time. You should adopt whatever scale makes most sense for your outcomes and your research question….\nOur point is simple: choose a metric for the that reflect the cadence you expect to be most useful for your outcome. (p. 11)\n\nData collection waves can be evenly spaced or not. E.g., if you anticipate a time period of rapid nonlinear change, it might be helpful to increase the density of assessments during that period. Everyone does not have to have the same assessment schedule. If all are assessed on the same schedule, we describe the data as time-structured. When assessment schedules vary across participants, the data are termed time-unstructured. The data are balanced if all participants have the same number of waves. Issues like attrition and so on lead to unbalanced data. Though they may have some pedagogical use, I have not found these terms useful in practice.\n\n\n1.3.3 A continuous outcome that changes systematically over time\nTo my eye, the most interesting part of this section is the discussion of measurement validity over time. E.g.,\n\nwhen we say the metric in which the outcome is measured must be preserved across time, we mean that the outcome scores must be equatable over time–a given value of the outcome on any occasion must represent the same “amount” of the outcome on every occasion. Outcome equatability is easiest to ensure when you use the identical instrument for measurement repeatedly over time. (p. 13)\n\nThis isn’t as simple as is sounds. Though it’s beyond the scope of this project, you might learn more about this from a study of the longitudinal measurement invariance literature. To dive in, see the first couple chapters in Newsom’s (2015) text, Longitudinal structural equation modeling: A comprehensive introduction.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Framework for Investigating Change over Time</span>"
    ]
  },
  {
    "objectID": "01.html#session-info",
    "href": "01.html#session-info",
    "title": "1  A Framework for Investigating Change over Time",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5         tools_4.5.1       htmltools_0.5.9   rstudioapi_0.17.1\n [8] rmarkdown_2.30    knitr_1.51        jsonlite_2.0.0    xfun_0.55         digest_0.6.39     rlang_1.1.7       evaluate_1.0.5",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Framework for Investigating Change over Time</span>"
    ]
  },
  {
    "objectID": "01.html#comments",
    "href": "01.html#comments",
    "title": "1  A Framework for Investigating Change over Time",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nNewsom, J. T. (2015). Longitudinal structural equation modeling: A comprehensive introduction. Routledge. http://www.longitudinalsem.com/\n\n\nNezlek, J. B. (2007). A multilevel framework for understanding relationships among traits, states, situations and behaviours. European Journal of Personality, 21(6), 789–810. https://doi.org/10.1002/per.640\n\n\nRogosa, D., Brandt, D., & Zimowski, M. (1982). A growth curve approach to the measurement of change. Psychological Bulletin, 92(3), 726–748. https://doi.org/10.1037/0033-2909.92.3.726\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nWillett, J. B. (1989). Some results on reliability for the longitudinal measurement of change: Implications for the design of studies of individual growth. Educational and Psychological Measurement, 49(3), 587–602. https://doi.org/10.1177/001316448904900309",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Framework for Investigating Change over Time</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "",
    "text": "2.1 Creating a longitudinal data set\nThese are also sometimes referred to as the wide and long data formats, respectively.\nAs you will see, we will use two primary functions from the tidyverse to convert data from one format to another.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "02.html#creating-a-longitudinal-data-set",
    "href": "02.html#creating-a-longitudinal-data-set",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "",
    "text": "In longitudinal work, data-set organization is less straightforward because you can use two very different arrangements:\n\n\nA person-level data set, in which each person has one record and multiple variables contain the data from each measurement occasion\n\n\n\n\nA person-period data set, in which each person has multiple records—one for each measurement occasion (p. 17, emphasis in the original)\n\n\n\n\n\n\n2.1.1 The person-level data set\nHere we load the person-level data from this UCLA web site. These are the NLY data (see Raudenbush & Chan, 2016) shown in the top of Figure 2.1.\n\nlibrary(tidyverse)\n\ntolerance &lt;- read_csv(\"https://stats.idre.ucla.edu/wp-content/uploads/2016/02/tolerance1.txt\", col_names = T)\n\nhead(tolerance, n = 16)\n\n# A tibble: 16 × 8\n      id tol11 tol12 tol13 tol14 tol15  male exposure\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     9  2.23  1.79  1.9   2.12  2.66     0     1.54\n 2    45  1.12  1.45  1.45  1.45  1.99     1     1.16\n 3   268  1.45  1.34  1.99  1.79  1.34     1     0.9 \n 4   314  1.22  1.22  1.55  1.12  1.12     0     0.81\n 5   442  1.45  1.99  1.45  1.67  1.9      0     1.13\n 6   514  1.34  1.67  2.23  2.12  2.44     1     0.9 \n 7   569  1.79  1.9   1.9   1.99  1.99     0     1.99\n 8   624  1.12  1.12  1.22  1.12  1.22     1     0.98\n 9   723  1.22  1.34  1.12  1     1.12     0     0.81\n10   918  1     1     1.22  1.99  1.22     0     1.21\n11   949  1.99  1.55  1.12  1.45  1.55     1     0.93\n12   978  1.22  1.34  2.12  3.46  3.32     1     1.59\n13  1105  1.34  1.9   1.99  1.9   2.12     1     1.38\n14  1542  1.22  1.22  1.99  1.79  2.12     0     1.44\n15  1552  1     1.12  2.23  1.55  1.55     0     1.04\n16  1653  1.11  1.11  1.34  1.55  2.12     0     1.25\n\n\nWith person-level data, each participant has a single row. In these data, participants are indexed by their id number. To see how many participants are in these data, just count() the rows.\n\ntolerance %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    16\n\n\nThe nrow() function will work, too.\n\ntolerance %&gt;% \n  nrow()\n\n[1] 16\n\n\nWith the base R cor() function, you can get the Pearson’s correlation matrix shown in Table 2.1.\n\ncor(tolerance[ , 2:6]) %&gt;%\n  round(digits = 2)\n\n      tol11 tol12 tol13 tol14 tol15\ntol11  1.00  0.66  0.06  0.14  0.26\ntol12  0.66  1.00  0.25  0.21  0.39\ntol13  0.06  0.25  1.00  0.59  0.57\ntol14  0.14  0.21  0.59  1.00  0.83\ntol15  0.26  0.39  0.57  0.83  1.00\n\n\nWe used the round() function to limit the number of decimal places in the output. Leave it off and you’ll see cor() returns up to seven decimal places instead. It can be hard to see the patters within a matrix of numerals. It might be easier in a plot.\n\ncor(tolerance[ , 2:6]) %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(\"row\") %&gt;% \n  pivot_longer(-row,\n               names_to = \"column\",\n               values_to = \"correlation\") %&gt;% \n  mutate(row = factor(row) %&gt;% fct_rev(.)) %&gt;% \n  \n  ggplot(aes(x = column, y = row)) + \n  geom_raster(aes(fill = correlation)) + \n  geom_text(aes(label = round(correlation, digits = 2)),\n            size = 3.5) +\n  scale_fill_gradient(low = \"white\", high = \"red4\", limits = c(0, 1)) +\n  scale_x_discrete(NULL, position = \"top\", expand = c(0, 0)) +\n  scale_y_discrete(NULL, expand = c(0, 0)) +\n  theme(axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nIf all you wanted was the lower diagonal, you could use the lowerCor() function from the psych package (Revelle, 2022).\n\npsych::lowerCor(tolerance[ , 2:6])\n\n      tol11 tol12 tol13 tol14 tol15\ntol11 1.00                         \ntol12 0.66  1.00                   \ntol13 0.06  0.25  1.00             \ntol14 0.14  0.21  0.59  1.00       \ntol15 0.26  0.39  0.57  0.83  1.00 \n\n\nFor more ways to compute, organize, and visualize correlations within the tidyverse paradigm, check out the corrr package (Kuhn et al., 2020).\n\n\n2.1.2 The person-period data set\nHere are the person-period data (i.e., those shown in the bottom of Figure 2.1).\n\ntolerance_pp &lt;- read_csv(\"https://stats.idre.ucla.edu/wp-content/uploads/2016/02/tolerance1_pp.txt\",\n                         col_names = T)\n\ntolerance_pp %&gt;%\n  slice(c(1:9, 76:80))\n\n# A tibble: 14 × 6\n      id   age tolerance  male exposure  time\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     9    11      2.23     0     1.54     0\n 2     9    12      1.79     0     1.54     1\n 3     9    13      1.9      0     1.54     2\n 4     9    14      2.12     0     1.54     3\n 5     9    15      2.66     0     1.54     4\n 6    45    11      1.12     1     1.16     0\n 7    45    12      1.45     1     1.16     1\n 8    45    13      1.45     1     1.16     2\n 9    45    14      1.45     1     1.16     3\n10  1653    11      1.11     0     1.25     0\n11  1653    12      1.11     0     1.25     1\n12  1653    13      1.34     0     1.25     2\n13  1653    14      1.55     0     1.25     3\n14  1653    15      2.12     0     1.25     4\n\n\nWith data like these, the simple use of count() or nrow() won’t help us discover how many participants there are in the tolerance_pp data. One quick way is to count() the number of distinct() id values.\n\ntolerance_pp %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    16\n\n\nA fundamental skill is knowing how to convert longitudinal data in one format to the other. If you’re using packages within the tidyverse, the pivot_longer() function will get you from the person-level format to the person-period format.\n\ntolerance %&gt;%\n  # this is the main event\n  pivot_longer(-c(id, male, exposure),\n               names_to = \"age\", \n               values_to = \"tolerance\") %&gt;% \n  # here we remove the `tol` prefix from the `age` values and then save the numbers as integers\n  mutate(age = str_remove(age, \"tol\") %&gt;% as.integer()) %&gt;% \n  # these last two lines just make the results look more like those in the last code chunk\n  arrange(id, age) %&gt;%\n  slice(c(1:9, 76:80))\n\n# A tibble: 14 × 5\n      id  male exposure   age tolerance\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n 1     9     0     1.54    11      2.23\n 2     9     0     1.54    12      1.79\n 3     9     0     1.54    13      1.9 \n 4     9     0     1.54    14      2.12\n 5     9     0     1.54    15      2.66\n 6    45     1     1.16    11      1.12\n 7    45     1     1.16    12      1.45\n 8    45     1     1.16    13      1.45\n 9    45     1     1.16    14      1.45\n10  1653     0     1.25    11      1.11\n11  1653     0     1.25    12      1.11\n12  1653     0     1.25    13      1.34\n13  1653     0     1.25    14      1.55\n14  1653     0     1.25    15      2.12\n\n\nYou can learn more about the pivot_longer() function here and here.\nAs hinted at in the above hyperlinks, the opposite of the pivot_longer() function is pivot_wider(). We can use pivot_wider() to convert the person-period tolerance_pp data to the same format as the person-level tolerance data.\n\ntolerance_pp %&gt;% \n  # we'll want to add that `tol` prefix back to the `age` values\n  mutate(age = str_c(\"tol\", age)) %&gt;% \n  # this variable is just in the way. we'll drop it\n  select(-time) %&gt;%\n  # here's the main action\n  pivot_wider(names_from = age, values_from = tolerance)\n\n# A tibble: 16 × 8\n      id  male exposure tol11 tol12 tol13 tol14 tol15\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     9     0     1.54  2.23  1.79  1.9   2.12  2.66\n 2    45     1     1.16  1.12  1.45  1.45  1.45  1.99\n 3   268     1     0.9   1.45  1.34  1.99  1.79  1.34\n 4   314     0     0.81  1.22  1.22  1.55  1.12  1.12\n 5   442     0     1.13  1.45  1.99  1.45  1.67  1.9 \n 6   514     1     0.9   1.34  1.67  2.23  2.12  2.44\n 7   569     0     1.99  1.79  1.9   1.9   1.99  1.99\n 8   624     1     0.98  1.12  1.12  1.22  1.12  1.22\n 9   723     0     0.81  1.22  1.34  1.12  1     1.12\n10   918     0     1.21  1     1     1.22  1.99  1.22\n11   949     1     0.93  1.99  1.55  1.12  1.45  1.55\n12   978     1     1.59  1.22  1.34  2.12  3.46  3.32\n13  1105     1     1.38  1.34  1.9   1.99  1.9   2.12\n14  1542     0     1.44  1.22  1.22  1.99  1.79  2.12\n15  1552     0     1.04  1     1.12  2.23  1.55  1.55\n16  1653     0     1.25  1.11  1.11  1.34  1.55  2.12",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "02.html#descriptive-analysis-of-individual-change-over-time",
    "href": "02.html#descriptive-analysis-of-individual-change-over-time",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "2.2 Descriptive analysis of individual change over time",
    "text": "2.2 Descriptive analysis of individual change over time\nThe following “descriptive analyses [are intended to] reveal the nature and idiosyncrasies of each person’s temporal pattern of growth, addressing the question: How does each person change over time” (p. 23)?\n\n2.2.1 Empirical growth plots\nEmpirical growth plots show individual-level sequence in a variable of interest over time. We’ll put age on the \\(x\\)-axis, tolerance on the y-axis, and make our variant of Figure 2.2 with geom_point(). It’s the facet_wrap() part of the code that splits the plot up by id.\n\ntolerance_pp %&gt;%\n  ggplot(aes(x = age, y = tolerance)) +\n  geom_point() +\n  coord_cartesian(ylim = c(1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id)\n\n\n\n\n\n\n\n\nBy default, ggplot2 sets the scales of the \\(x\\)- and \\(y\\)-axes to the same values across subpanels. If you’d like to free that constraint, play around with the scales argument within facet_wrap().\n\n\n2.2.2 Using a trajectory to summarize each person’s empirical growth record\nIf we wanted to connect the dots, we might just add a geom_line() line.\n\ntolerance_pp %&gt;%\n  ggplot(aes(x = age, y = tolerance)) +\n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id)\n\n\n\n\n\n\n\n\nHowever, Singer and Willett recommend two other approaches:\n\nnonparametric smoothing\nparametric functions\n\n\n2.2.2.1 Smoothing the empirical growth trajectory nonparametrically\nFor our version of Figure 2.3, we’ll use a loess smoother. When using the stat_smooth() function in ggplot2, you can control how smooth or wiggly the line is with the span argument.\n\ntolerance_pp %&gt;%\n  ggplot(aes(x = age, y = tolerance)) +\n  geom_point() +\n  stat_smooth(method = \"loess\", se = F, span = 0.9) +\n  coord_cartesian(ylim = c(1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id)\n\n\n\n\n\n\n\n\n\n\n2.2.2.2 Smoothing the empirical growth trajectory using OLS single-level Bayesian regression\nAlthough “fitting person-specific regression models, one individual at a time, is hardly the most efficient use of longitudinal data” (p. 28), we may as well play along with the text. It’ll have pedagogical utility. You’ll see.\nFor this section, we’ll take a cue from Hadley Wickham and use group_by() and nest() to make a tibble composed of tibbles (i.e., a nested tibble).\n\nby_id &lt;- tolerance_pp %&gt;%\n  group_by(id) %&gt;%\n  nest()\n\nYou can get a sense of what we did with head().\n\nby_id %&gt;% head()\n\n# A tibble: 6 × 2\n# Groups:   id [6]\n     id data            \n  &lt;dbl&gt; &lt;list&gt;          \n1     9 &lt;tibble [5 × 5]&gt;\n2    45 &lt;tibble [5 × 5]&gt;\n3   268 &lt;tibble [5 × 5]&gt;\n4   314 &lt;tibble [5 × 5]&gt;\n5   442 &lt;tibble [5 × 5]&gt;\n6   514 &lt;tibble [5 × 5]&gt;\n\n\nAs indexed by id, each participant now has their own data set stored in the data column. To get a better sense, we’ll use our double-bracket subsetting skills to open up the first data set, the one for id == 9. If you’re not familiar with this skill, you can learn more from Chapter 9 of Roger Peng’s great (2019) online book, R programming for data science, or Jenny Bryan’s fun and useful talk, Behind every great plot there’s a great deal of wrangling.\n\nby_id$data[[1]]\n\n# A tibble: 5 × 5\n    age tolerance  male exposure  time\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1    11      2.23     0     1.54     0\n2    12      1.79     0     1.54     1\n3    13      1.9      0     1.54     2\n4    14      2.12     0     1.54     3\n5    15      2.66     0     1.54     4\n\n\nOur by_id data object has many data sets stored in a higher-level data set. The code we used is verbose, but that’s what made it human-readable. Now we have our nested tibble, we can make a function that will fit the simple linear model tolerance ~ 1 + time to each id-level data set. Why use time as the predictor? you ask. On page 29 in the text, Singer and Willett clarified they fit their individual models with \\((\\text{age} - 11)\\) in order to have the model intercepts centered at 11 years old rather than 0. If we wanted to, we could make an \\((\\text{age} - 11)\\) variable like so.\n\nby_id$data[[1]] %&gt;% \n  mutate(age_minus_11 = age - 11)\n\n# A tibble: 5 × 6\n    age tolerance  male exposure  time age_minus_11\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1    11      2.23     0     1.54     0            0\n2    12      1.79     0     1.54     1            1\n3    13      1.9      0     1.54     2            2\n4    14      2.12     0     1.54     3            3\n5    15      2.66     0     1.54     4            4\n\n\nDid you notice how our age_minus_11 variable is the same as the time variable already in the data set? Yep, that’s why we’ll be using time in the model. In our data, \\((\\text{age} - 11)\\) is encoded as time.\nSinger and Willett used OLS to fit their exploratory models. We could do that to with the lm() function and we will do a little of that in this project. But let’s get frisky and fit the models as Bayesians, instead. Our primary statistical package for fitting Bayesian models will be Paul Bürkner’s brms. Let’s open it up.\n\nlibrary(brms)\n\nSince this is our first Bayesian model, we should start slow. The primary model-fitting function in brms is brm(). The function is astonishingly general and includes numerous arguments, most of which have sensible defaults. The primary two arguments are data and formula. I’m guessing they’re self-explanatory. I’m not going to go into detail on the three arguments at the bottom of the code. We’ll go over them later. For simple models like these, I would have omitted them entirely, but given the sparsity of the data (i.e., 5 data points per model), I wanted to make sure we gave the algorithm a good chance to arrive at reasonable estimates.\n\nfit2.1 &lt;- brm(\n  data = by_id$data[[1]],\n  formula = tolerance ~ 1 + time,\n  prior = prior(normal(0, 2), class = b),\n  iter = 4000, chains = 4, cores = 4,\n  seed = 2,\n  file = \"fits/fit02.01\")\n\nWe just fit a single-level Bayesian regression model for our first participant. We saved the results as an object named fit2.1. We can return a useful summary of fit2.1 with either print() or summary(). Since it’s less typing, we’ll use print().\n\nprint(fit2.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: tolerance ~ 1 + time \n   Data: by_id$data[[1]] (Number of observations: 5) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.90      0.56     0.70     3.09 1.00     3912     2327\ntime          0.12      0.23    -0.34     0.60 1.00     3919     2322\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.60      0.45     0.21     1.85 1.00     1460     2085\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe ‘Intercept’ and ‘time’ coefficients are the primary regression parameters. Also notice ‘sigma’, which is our variant of the residual standard error you might get from an OLS output (e.g., from base R lm()). Since we’re Bayesians, the output summaries do not contain \\(p\\)-values. But we do get posterior standard deviations (i.e., the ‘Est.Error’ column) and the upper- and lower-levels of the percentile-based 95% intervals.\nYou probably heard somewhere that Bayesian statistics require priors. We can see what those were by pulling them out of our fit2.1 object.\n\nfit2.1$prior\n\n                  prior     class coef group resp dpar nlpar lb ub tag       source\n           normal(0, 2)         b                                              user\n           normal(0, 2)         b time                                 (vectorized)\n student_t(3, 2.1, 2.5) Intercept                                           default\n   student_t(3, 0, 2.5)     sigma                             0             default\n\n\nThe prior in the top line, normal(0, 2), is for all parameters of class = b. We actually specified this in our brm() code, above, with the code snip: prior = prior(normal(0, 2), class = b). At this stage in the project, my initial impulse was to leave this line blank and save the discussion of how to set priors by hand for later. However, the difficulty is that the first several models we’re fitting are all of \\(n = 5\\). Bayesian statistics handle small-\\(n\\) models just fine. However, when your \\(n\\) gets small, the algorithms we use to implement our Bayesian models benefit from priors that are at least modestly informative. As it turns out, the brms default priors are flat for parameters of class = b. They offer no information beyond that contained in the likelihood. To stave off algorithm problems with our extremely-small-\\(n\\) data subsets, we used normal(0, 2) instead. In our model, the only parameter of class = b is the regression slope for time. On the scale of the data, normal(0, 2) is a vary-permissive prior for our time slope.\nIn addition to our time slope parameter, our model contained an intercept and a residual variance. From the fit2.1$prior output, we can see those were student_t(3, 2.1, 2.5) and student_t(3, 0, 2.5), respectively. brms default priors are designed to be weakly informative. Given the data and the model, these priors have a minimal influence on the results. We’ll focus more on priors later in the project. For now just recognize that even if you don’t specify your priors, you can’t escape using some priors when using brm(). This is a good thing.\nOkay, so that was the model for just one participant. We want to do that for all 16. Instead of repeating that code 15 times, we can work in bulk. With brms, you can reuse a model with the update() function. Here’s how to do that with the data from our second participant.\n\nfit2.2 &lt;- update(\n  fit2.1, \n  newdata = by_id$data[[2]],\n  control = list(adapt_delta = 0.9),\n  file = \"fits/fit02.02\")\n\nPeek at the results.\n\nprint(fit2.2)\n\n Family: gaussian \n  Links: mu = identity \nFormula: tolerance ~ 1 + time \n   Data: by_id$data[[2]] (Number of observations: 5) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.15      0.33     0.51     1.82 1.00     3658     2470\ntime          0.17      0.14    -0.09     0.43 1.00     3388     2474\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.34      0.31     0.11     1.13 1.00     1435     1668\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nDifferent participants yield different model results.\nLooking ahead a bit, we’ll need to know how to get the \\(R^2\\) for a single-level Gaussian model. With brms, you do that with the bayes_R2() function.\n\nbayes_R2(fit2.2)\n\n    Estimate Est.Error       Q2.5     Q97.5\nR2 0.6261212 0.2472922 0.01118864 0.8148438\n\n\nThough the default spits out summary statistics, you can get the full posterior distribution for the \\(R^2\\) by specifying summary = F.\n\nbayes_R2(fit2.2, summary = F) %&gt;% \n  str()\n\n num [1:8000, 1] 0.519 0.779 0.742 0.59 0.554 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr \"R2\"\n\n\nOur code returned a numeric vector. If you’d like to plot the results with ggplot2, you’ll need to convert the vector to a data frame.\n\nbayes_R2(fit2.2, summary = F) %&gt;% \n  data.frame() %&gt;% \n  \n  ggplot(aes(x = R2)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(expression(italic(R)[Bayesian]^2), limits = c(0, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nYou’ll note how non-Gaussian the Bayesian \\(R^2\\) can be. Also, with the combination of default minimally-informative priors and only 5 data points, there’ massive uncertainty in the shape. As such, the value of central tendency will vary widely based on which statistic you use.\n\nbayes_R2(fit2.2, summary = F) %&gt;% \n  data.frame() %&gt;% \n  summarise(mean   = mean(R2),\n            median = median(R2),\n            mode   = tidybayes::Mode(R2))\n\n       mean    median      mode\n1 0.6261212 0.7504917 0.8149813\n\n\nBy default, bayes_R2() returns the mean. You can get the median with the robust = TRUE argument. To pull the mode, you’ll need to use summary = F and feed the results into a mode function, like tidybayes::Mode().\nI should also point out the brms package did not get these \\(R^2\\) values by traditional method used in, say, OLS estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out Gelman, Goodrich, Gabry, and Vehtari’s (2019) paper, [R-squared for Bayesian regression models]https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1549100).\nWith a little tricky programming, we can use the purrr::map() function to serially fit this model to each of our participant-level data sets. We’ll save the results as fits.\n\nfits &lt;- by_id %&gt;%\n  mutate(model = map(data, ~update(fit2.1, newdata = ., seed = 2)))\n\nLet’s walk through what we did. The map() function takes two primary arguments, .x and .f, respectively. We set .x = data, which meant we wanted to iterate over the contents in our data vector. Recall that each row of data itself contained an entire data set–one for each of the 16 participants. It’s with the second argument .f that we indicated what we wanted to do with our rows of data. We set that to .f = ~update(fit2.1, newdata = ., seed = 2). With the ~ syntax, we entered in a formula, which was update(fit2.1, newdata = ., seed = 2). Just like we did with fit2.2, above, we reused the model formula and other technical specs from fit2.1. Now notice the middle part of the formula, newdata = .. That little . refers to the element we specified in the .x argument. What this combination means is that for each of the 16 rows of our nested by_id tibble, we plugged in the id-specific data set into update(fit, newdata[[i]]) where i is simply meant as a row index. The new column, model, contains the output of each of the 16 iterations.\n\nprint(fits)\n\n# A tibble: 16 × 3\n# Groups:   id [16]\n      id data             model    \n   &lt;dbl&gt; &lt;list&gt;           &lt;list&gt;   \n 1     9 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 2    45 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 3   268 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 4   314 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 5   442 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 6   514 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 7   569 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 8   624 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n 9   723 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n10   918 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n11   949 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n12   978 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n13  1105 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n14  1542 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n15  1552 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n16  1653 &lt;tibble [5 × 5]&gt; &lt;brmsfit&gt;\n\n\nNext, we’ll want to extract the necessary summary information from our fits to remake our version of Table 2.2. There’s a lot of info in that table, so let’s take it step by step. First, we’ll extract the posterior means (i.e., “Estimate”) and standard deviations (i.e., “se”) for the initial status and rate of change of each model. We’ll also do the same for sigma (i.e., the square of the “Residual variance”).\n\nmean_structure &lt;- fits %&gt;% \n  mutate(coefs = map(model, ~ posterior_summary(.)[1:2, 1:2] %&gt;% \n                       data.frame() %&gt;% \n                       rownames_to_column(\"coefficients\"))) %&gt;% \n  unnest(coefs) %&gt;% \n  select(-data, -model) %&gt;% \n  unite(temp, Estimate, Est.Error) %&gt;% \n  pivot_wider(names_from = coefficients,\n              values_from = temp) %&gt;% \n  separate(b_Intercept, into = c(\"init_stat_est\", \"init_stat_sd\"), sep = \"_\") %&gt;% \n  separate(b_time, into = c(\"rate_change_est\", \"rate_change_sd\"), sep = \"_\") %&gt;% \n  mutate_if(is.character, ~ as.double(.) %&gt;% round(digits = 2)) %&gt;% \n  ungroup()\n\nhead(mean_structure)\n\n# A tibble: 6 × 5\n     id init_stat_est init_stat_sd rate_change_est rate_change_sd\n  &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1     9          1.9          0.56            0.12           0.23\n2    45          1.15         0.34            0.18           0.14\n3   268          1.55         0.57            0.03           0.23\n4   314          1.31         0.36           -0.03           0.14\n5   442          1.57         0.47            0.06           0.19\n6   514          1.43         0.35            0.26           0.14\n\n\nIt’s simpler to extract the residual variance. Recall that because brms gives that in the standard deviation metric (i.e., \\(\\sigma\\)), you need to square it to return it in a variance metric (i.e., \\(\\sigma^2\\)).\n\nresidual_variance &lt;- fits %&gt;% \n  mutate(residual_variance = map_dbl(model, ~ posterior_summary(.)[3, 1])^2) %&gt;% \n  mutate_if(is.double, round, digits = 2) %&gt;% \n  select(id, residual_variance)\n\nhead(residual_variance)\n\n# A tibble: 6 × 2\n# Groups:   id [6]\n     id residual_variance\n  &lt;dbl&gt;             &lt;dbl&gt;\n1     9              0.36\n2    45              0.11\n3   268              0.38\n4   314              0.14\n5   442              0.25\n6   514              0.12\n\n\nWe’ll extract our Bayesian \\(R^2\\) summaries, next. Given how nonnormal these are, we’ll use the posterior median rather than the mean. We get that by using the robust = T argument within the bayes_R2() function.\n\nr2 &lt;- fits %&gt;% \n  mutate(r2 = map_dbl(model, ~ bayes_R2(., robust = T)[1])) %&gt;% \n  mutate_if(is.double, round, digits = 2) %&gt;% \n  select(id, r2)\n\nhead(r2)\n\n# A tibble: 6 × 2\n# Groups:   id [6]\n     id    r2\n  &lt;dbl&gt; &lt;dbl&gt;\n1     9  0.34\n2    45  0.75\n3   268  0.19\n4   314  0.22\n5   442  0.24\n6   514  0.86\n\n\nHere we combine all the components with a series of left_join() statements and present it in a flextable-type table.\n\ntable &lt;- fits %&gt;% \n  unnest(data) %&gt;% \n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  select(id, male, exposure) %&gt;% \n  left_join(mean_structure,    by = \"id\") %&gt;% \n  left_join(residual_variance, by = \"id\") %&gt;% \n  left_join(r2,                by = \"id\") %&gt;% \n  rename(residual_var = residual_variance) %&gt;% \n  select(id, init_stat_est:r2, everything()) %&gt;% \n  ungroup()\n\ntable %&gt;% \n  flextable::flextable()\n\nidinit_stat_estinit_stat_sdrate_change_estrate_change_sdresidual_varr2maleexposure91.900.560.120.230.360.3401.54451.150.340.180.140.110.7511.162681.550.570.030.230.380.1910.903141.310.36-0.030.140.140.2200.814421.570.470.060.190.250.2401.135141.430.350.260.140.120.8610.905691.820.090.050.030.010.8501.996241.120.150.020.060.010.3710.987231.270.22-0.050.090.050.4500.819181.040.590.140.240.470.3201.219491.740.53-0.100.220.330.2910.939781.030.630.620.260.510.8711.591,1051.550.360.160.150.140.6711.381,5421.220.450.230.180.200.7501.441,5521.210.700.150.300.680.2801.041,6530.960.350.250.140.120.8401.25\n\n\nWe can make the four stem-and-leaf plots of Figure 2.4 with serial combinations of pull() and stem().\n\n# fitted initial status\ntable %&gt;% \n  pull(init_stat_est) %&gt;% \n  stem(scale = 2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   9 | 6\n  10 | 34\n  11 | 25\n  12 | 127\n  13 | 1\n  14 | 3\n  15 | 557\n  16 | \n  17 | 4\n  18 | 2\n  19 | 0\n\n# fitted rate of change\ntable %&gt;% \n  pull(rate_change_est) %&gt;% \n  stem(scale = 2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -1 | 0\n  -0 | 53\n   0 | 2356\n   1 | 24568\n   2 | 356\n   3 | \n   4 | \n   5 | \n   6 | 2\n\n# residual variance\ntable %&gt;% \n  pull(residual_var) %&gt;% \n  stem(scale = 2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  0 | 115\n  1 | 12244\n  2 | 05\n  3 | 368\n  4 | 7\n  5 | 1\n  6 | 8\n\n# r2 statistic\ntable %&gt;% \n  pull(r2) %&gt;% \n  stem(scale = 2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  1 | 9\n  2 | 2489\n  3 | 247\n  4 | 5\n  5 | \n  6 | 7\n  7 | 55\n  8 | 4567\n\n\nTo make Figure 2.5, we’ll combine information from the original data and the ‘Estimates’ (i.e., posterior means) from our Bayesian models we’ve encoded in mean_structure.\n\nby_id %&gt;% \n  unnest(data) %&gt;% \n  \n  ggplot(aes(x = time, y = tolerance, group = id)) +\n  geom_point() +\n  geom_abline(data = mean_structure,\n              aes(intercept = init_stat_est,\n                  slope = rate_change_est, group = id),\n              color = \"blue\") +\n  scale_x_continuous(breaks = 0:4, labels = 0:4 + 11) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "02.html#exploring-differences-in-change-across-people",
    "href": "02.html#exploring-differences-in-change-across-people",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "2.3 Exploring differences in change across people",
    "text": "2.3 Exploring differences in change across people\n“Having summarized how each individual changes over time, we now examine similarities and differences in these changes across people” (p. 33).\n\n2.3.1 Examining the entire set of smooth trajectories\nThe key to making our version of the left-hand side of Figure 2.6 is two stat_smooth() lines. The first one will produce the overall smooth. The second one, the one including the aes(group = id) argument, will give the id-specific smooths.\n\ntolerance_pp %&gt;%\n  ggplot(aes(x = age, y = tolerance)) +\n  stat_smooth(method = \"loess\", se = F, span = 0.9, size = 2) +\n  stat_smooth(aes(group = id),\n              method = \"loess\", se = F, span = 0.9, size = 1/4) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo get the linear OLS trajectories, just switch method = \"loess\" to method = \"lm\".\n\ntolerance_pp %&gt;%\n  ggplot(aes(x = age, y = tolerance)) +\n  stat_smooth(method = \"lm\", se = F, span = 0.9, size = 2) +\n  stat_smooth(aes(group = id),\n              method = \"lm\", se = F, span = 0.9, size = 1/4) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBut we wanted to be Bayesians. We already have the id-specific trajectories. All we need now is one based on all the data.\n\nfit2.3 &lt;- update(\n  fit2.1, \n  newdata = tolerance_pp,\n  file = \"fits/fit02.03\")\n\nHere’s the model summary.\n\nsummary(fit2.3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: tolerance ~ 1 + time \n   Data: tolerance_pp (Number of observations: 80) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.36      0.09     1.18     1.54 1.00     7617     5339\ntime          0.13      0.04     0.06     0.20 1.00     7805     5825\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.47      0.04     0.40     0.55 1.00     7905     6368\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBefore, we used posterior_summary() to isolate the posterior means and \\(SD\\)s. We can also use the fixef() function for that.\n\nfixef(fit2.3)\n\n           Estimate  Est.Error       Q2.5     Q97.5\nIntercept 1.3581349 0.09174200 1.17909618 1.5388820\ntime      0.1308509 0.03696973 0.05924085 0.2023267\n\n\nWith a little subsetting, we can extract just the means from each.\n\nfixef(fit2.3)[1, 1]\n\n[1] 1.358135\n\nfixef(fit2.3)[2, 1]\n\n[1] 0.1308509\n\n\nFor this plot, we’ll work more directly with the model formulas to plot the trajectories. We can use init_stat_est and rate_change_est from the mean_structure object as stand-ins for \\(\\beta_{0i}\\) and \\(\\beta_{1i}\\) from our model equation,\n\\[\\text{tolerance}_{ij} = \\beta_{0i} + \\beta_{1i} \\cdot \\text{time}_{ij} + \\epsilon_{ij},\\]\nwhere \\(i\\) indexes children and \\(j\\) indexes time points. All we need to do is plug in the appropriate values for time and we’ll have the fitted tolerance values for each level of id. After a little wrangling, the data will be in good shape for plotting.\n\ntol_fitted &lt;- mean_structure %&gt;% \n  mutate(`11` = init_stat_est + rate_change_est * 0,\n         `15` = init_stat_est + rate_change_est * 4) %&gt;% \n  select(id, `11`, `15`) %&gt;% \n  pivot_longer(-id, \n               names_to = \"age\", \n               values_to = \"tolerance\") %&gt;% \n  mutate(age = as.integer(age))\n\nhead(tol_fitted)\n\n# A tibble: 6 × 3\n     id   age tolerance\n  &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1     9    11      1.9 \n2     9    15      2.38\n3    45    11      1.15\n4    45    15      1.87\n5   268    11      1.55\n6   268    15      1.67\n\n\nWe’ll plot the id-level trajectories with those values and geom_line(). To get the overall trajectory, we’ll get tricky with fixef(fit2.3) and geom_abline().\n\ntol_fitted %&gt;% \n  ggplot(aes(x = age, y = tolerance, group = id)) +\n  geom_line(color = \"blue\", linewidth = 1/4) +\n  geom_abline(intercept = fixef(fit2.3)[1, 1] + fixef(fit2.3)[2, 1] * -11,\n              slope     = fixef(fit2.3)[2, 1],\n              color = \"blue\", linewidth = 2) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\n\n\n2.3.2 Using the results of model fitting to frame questions about change\nIf you’re new to the multilevel model, the ideas in this section are foundational.\n\nTo learn about the observed average pattern of change, we examine the sample averages of the fitted intercepts and slopes; these tell us about the average initial status and the average annual rate of change in the sample as a whole. To learn about the observed individual differences in change, we examine the sample variances and standard deviations of the intercepts and slopes; these tell us about the observed variability in initial status. And to learn about the observed relationship between initial status and the rate of change, we can examine the sample covariance or correlation between intercepts and slopes.\nFormal answers to these questions require the multilevel model for change of chapter 3. But we can presage this work by conducting simple descriptive analyses of the estimated intercepts and slopes. (p. 36, emphasis in the original)\n\nHere are the means and standard deviations presented in Table 2.3.\n\nmean_structure %&gt;% \n  pivot_longer(ends_with(\"est\")) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            sd   = sd(value)) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 2 × 3\n  name             mean    sd\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 init_stat_est    1.37  0.29\n2 rate_change_est  0.13  0.17\n\n\nHere’s how to get the Pearson’s correlation coefficient.\n\nmean_structure %&gt;% \n  select(init_stat_est, rate_change_est) %&gt;% \n  cor() %&gt;% \n  round(digits = 2)\n\n                init_stat_est rate_change_est\ninit_stat_est            1.00           -0.45\nrate_change_est         -0.45            1.00\n\n\n\n\n2.3.3 Exploring the relationship between change and time-invariant predictors\n“Evaluating the impact of predictors helps you uncover systematic patterns in the individual change trajectories corresponding to interindividual variation in personal characteristics” (p. 37).\n\n2.3.3.1 Graphically examining groups of smoothed individual growth trajectories\nIf we’d like Bayesian estimates differing by male, we’ll need to fit an interaction model.\n\nfit2.4 &lt;- update(\n  fit2.1, \n  newdata = tolerance_pp,\n  tolerance ~ 1 + time + male + time:male,\n  file = \"fits/fit02.04\")\n\nCheck the model summary.\n\nprint(fit2.4)\n\n Family: gaussian \n  Links: mu = identity \nFormula: tolerance ~ time + male + time:male \n   Data: tolerance_pp (Number of observations: 80) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.36      0.12     1.12     1.59 1.00     4013     4991\ntime          0.10      0.05     0.01     0.20 1.00     3938     5234\nmale          0.01      0.18    -0.34     0.37 1.00     3654     4641\ntime:male     0.06      0.07    -0.08     0.21 1.00     3242     4378\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.47      0.04     0.40     0.55 1.00     6365     5182\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere’s how to use fixef() and the model equation to get fitted values for tolerance based on specific values for time and male.\n\ntol_fitted_male &lt;- tibble(\n  male = rep(0:1, each = 2),\n  age  = rep(c(11, 15), times = 2)) %&gt;% \n  mutate(time = age - 11) %&gt;% \n  mutate(tolerance = fixef(fit2.4)[1, 1] + \n           fixef(fit2.4)[2, 1] * time + \n           fixef(fit2.4)[3, 1] * male + \n           fixef(fit2.4)[4, 1] * time * male)\n\ntol_fitted_male\n\n# A tibble: 4 × 4\n   male   age  time tolerance\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1     0    11     0      1.36\n2     0    15     4      1.77\n3     1    11     0      1.36\n4     1    15     4      2.03\n\n\nNow we’re ready to make our Bayesian version of the top panels of Figure 2.7.\n\ntol_fitted %&gt;% \n  # we need to add `male` values to `tol_fitted`\n  left_join(tolerance_pp %&gt;% select(id, male),\n            by = \"id\") %&gt;% \n  \n  ggplot(aes(x = age, y = tolerance, color = factor(male))) +\n  geom_line(aes(group = id),\n            linewidth = 1/4) +\n  geom_line(data = tol_fitted_male,\n            linewidth = 2) +\n  scale_color_viridis_d(end = 0.75) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ male)\n\n\n\n\n\n\n\n\nBefore we can do the same thing with exposure, we’ll need to dichotomize it by its median. A simple way is with a conditional statement within the if_else() function.\n\ntolerance_pp &lt;- tolerance_pp %&gt;% \n  mutate(exposure_01 = if_else(exposure &gt; median(exposure), 1, 0))\n\nNow fit the second interaction model.\n\nfit2.5 &lt;- update(\n  fit2.4, \n  newdata = tolerance_pp,\n  tolerance ~ 1 + time + exposure_01 + time:exposure_01,\n  file = \"fits/fit02.05\")\n\nHere’s the summary.\n\nprint(fit2.5)\n\n Family: gaussian \n  Links: mu = identity \nFormula: tolerance ~ time + exposure_01 + time:exposure_01 \n   Data: tolerance_pp (Number of observations: 80) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            1.39      0.12     1.17     1.63 1.00     3947     5198\ntime                 0.04      0.05    -0.05     0.14 1.00     3484     4418\nexposure_01         -0.07      0.17    -0.40     0.26 1.00     3299     4563\ntime:exposure_01     0.18      0.07     0.04     0.31 1.00     3052     4303\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.43      0.04     0.37     0.51 1.00     6192     5346\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow use fixef() and the model equation to get fitted values for tolerance based on specific values for time and exposure_01.\n\ntol_fitted_exposure &lt;- crossing(\n  exposure_01 = 0:1,\n  age         = c(11, 15)) %&gt;% \n  mutate(time = age - 11) %&gt;% \n  mutate(tolerance = fixef(fit2.5)[1, 1] + \n           fixef(fit2.5)[2, 1] * time + \n           fixef(fit2.5)[3, 1] * exposure_01 + \n           fixef(fit2.5)[4, 1] * time * exposure_01,\n         exposure = if_else(exposure_01 == 1, \"high exposure\", \"low exposure\") %&gt;% \n           factor(., levels = c(\"low exposure\", \"high exposure\")))\n\ntol_fitted_exposure\n\n# A tibble: 4 × 5\n  exposure_01   age  time tolerance exposure     \n        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;        \n1           0    11     0      1.39 low exposure \n2           0    15     4      1.56 low exposure \n3           1    11     0      1.32 high exposure\n4           1    15     4      2.20 high exposure\n\n\nDid you notice in the last lines in the second mutate() how we made a version of exposure that is a factor? That will come in handy for labeling and ordering the subplots. Now make our Bayesian version of the bottom panels of Figure 2.7.\n\ntol_fitted %&gt;% \n  # we need to add `exposure_01` values to `tol_fitted`\n  left_join(tolerance_pp %&gt;% select(id, exposure_01),\n            by = \"id\") %&gt;% \n  mutate(exposure = if_else(exposure_01 == 1, \"high exposure\", \"low exposure\") %&gt;% \n           factor(., levels = c(\"low exposure\", \"high exposure\"))) %&gt;% \n  \n  ggplot(aes(x = age, y = tolerance, color = exposure)) +\n  geom_line(aes(group = id),\n            linewidth = 1/4) +\n  geom_line(data = tol_fitted_exposure,\n            linewidth = 2) +\n  scale_color_viridis_d(option = \"A\", end = 0.75) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ exposure)\n\n\n\n\n\n\n\n\n\n\n2.3.3.2 The relationship between OLS-Estimated single-level Bayesian trajectories and substantive predictors\n“To investigate whether fitted trajectories vary systematically with predictors, we can treat the estimated intercepts and slopes as outcomes and explore the relationship between them and predictors” (p. 39). Here are the left panels of Figure 2.8.\n\np1 &lt;- mean_structure %&gt;% \n  pivot_longer(ends_with(\"est\")) %&gt;% \n  mutate(name = factor(name, labels = c(\"Fitted inital status\", \"Fitted rate of change\"))) %&gt;% \n  # we need to add `male` values to `tol_fitted`\n  left_join(tolerance_pp %&gt;% select(id, male),\n            by = \"id\") %&gt;% \n  \n  ggplot(aes(x = factor(male), y = value, color = name)) +\n  geom_point(alpha = 1/2) +\n  scale_color_viridis_d(option = \"B\", begin = 0.2, end = 0.7) +\n  labs(x = \"male\",\n       y = NULL) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ name, scale = \"free_y\", ncol = 1)\n\np1\n\n\n\n\n\n\n\n\nHere are the right panels.\n\np2 &lt;- mean_structure %&gt;% \n  pivot_longer(ends_with(\"est\")) %&gt;% \n  mutate(name = factor(name, labels = c(\"Fitted inital status\", \"Fitted rate of change\"))) %&gt;% \n  # we need to add `male` values to `tol_fitted`\n  left_join(tolerance_pp %&gt;% select(id, exposure),\n            by = \"id\") %&gt;% \n  \n  ggplot(aes(x = exposure, y = value, color = name)) +\n  geom_point(alpha = 1/2) +\n  scale_color_viridis_d(option = \"B\", begin = 0.2, end = 0.7) +\n  scale_x_continuous(breaks = 0:2,\n                     limits = c(0, 2.4)) +\n  labs(y = NULL) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ name, scale = \"free_y\", ncol = 1)\n\np2\n\n\n\n\n\n\n\n\nDid you notice how we saved those last two plots as p1 and p2? We can use syntax from the patchwork package (Pedersen, 2022) to combine them into one compound plot.\n\nlibrary(patchwork)\n\np1 + p2 + scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\nAs interesting as these plots are, do remember that “the need for ad hoc correlations has been effectively replaced by the widespread availability of computer software for fitting the multilevel model for change directly” (pp. 41–42). As you’ll see, Bürkner’s brms package is one of the foremost in that regard.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "02.html#improving-the-precision-and-reliability-of-ols-single-level-bayesian-estimated-rates-of-change-lessons-for-research-design",
    "href": "02.html#improving-the-precision-and-reliability-of-ols-single-level-bayesian-estimated-rates-of-change-lessons-for-research-design",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "2.4 Improving the precision and reliability of OLS single-level-Bayesian-estimated rates of change: Lessons for research design",
    "text": "2.4 Improving the precision and reliability of OLS single-level-Bayesian-estimated rates of change: Lessons for research design\n\nStatisticians assess the precision of a parameter estimate in terms of its sampling variation, a measure of the variability that would be found across infinite resamplings from the same population. The most common measure of sampling variability is an estimate’s standard error, the square root of its estimated sampling variance. Precision and standard error have an inverse relationship; the smaller the standard error, the more precise the estimate. (p. 41, emphasis in the original)\n\nSo here’s the deal: When Singer and Willett wrote “Statisticians assess…” a more complete expression would have been ‘Frequentist statisticians assess…’ Bayesian statistics are not based on asymptotic theory. They do not presume an idealized infinite distribution of replications. Rather, Bayesian statistics use Bayes theorem to estimate the probability of the parameters given the data. That probability has a distribution. Analogous to frequentist statistics, we often summarize that distribution (i.e., the posterior distribution) in terms of central tendency (e.g., posterior mean, posterior median, posterior mode) and spread. Spread? you say. We typically express spread in one or both of two ways. One typical expression of spread is the 95% intervals. In the Bayesian world, these are often called credible or probability intervals. The other typical expression of spread is the posterior standard deviation. In brms, this of typically summarized in the ‘Est.error’ column of the output of functions like print() and posterior_summary() and so on. The posterior standard deviation is analogous to the frequentist standard error. Philosophically and mechanically, they are not the same. But in practice, they are often quite similar.\nLater we read:\n\nUnlike precision which describes how well an individual slope estimate measures that person’s true rate of change, reliability describes how much the rate of change varies across people. Precision has meaning for the individual; reliability has meaning for the group. (p. 42)\n\nI have to protest. True, if we were working within a Classical Test Theory paradigm, this would be correct. But this places reliability with the context of group-based cross-sectional design. Though this is a popular design, it is not the whole story (i.e., see this book!). For introductions to more expansive and person-specific notions of reliability, check out Lee Cronbach’s Generalizability Theory (Brennan, 2001; Cronbach et al., 1972; also Cranford et al., 2006; LoPilato et al., 2015; Shrout & Lane, 2012).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "02.html#session-info",
    "href": "02.html#session-info",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] patchwork_1.3.2 brms_2.23.0     Rcpp_1.1.0      lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0   dplyr_1.1.4    \n [8] purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n  [1] mnormt_2.1.1            gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7            \n  [6] magrittr_2.0.4          multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          mgcv_1.9-3             \n [11] loo_2.9.0.9000          systemfonts_1.3.1       vctrs_0.6.5             reshape2_1.4.5          pkgconfig_2.0.3        \n [16] arrayhelpers_1.1-0      crayon_1.5.3            fastmap_1.2.0           backports_1.5.0         labeling_0.4.3         \n [21] utf8_1.2.6              rmarkdown_2.30          tzdb_0.5.0              ragg_1.5.0              bit_4.6.0              \n [26] xfun_0.55               jsonlite_2.0.0          tidybayes_3.0.7         uuid_1.2-1              psych_2.5.6            \n [31] parallel_4.5.1          R6_2.6.1                stringi_1.8.7           RColorBrewer_1.1-3      StanHeaders_2.36.0.9000\n [36] estimability_1.5.1      rstan_2.36.0.9000       knitr_1.51              zoo_1.8-14              bayesplot_1.15.0.9000  \n [41] Matrix_1.7-3            splines_4.5.1           timechange_0.3.0        tidyselect_1.2.1        rstudioapi_0.17.1      \n [46] abind_1.4-8             codetools_0.2-20        curl_7.0.0              pkgbuild_1.4.8          lattice_0.22-7         \n [51] plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1    S7_0.2.1                flextable_0.9.10       \n [56] askpass_1.2.1           posterior_1.6.1.9000    coda_0.19-4.1           evaluate_1.0.5          survival_3.8-3         \n [61] RcppParallel_5.1.11-1   zip_2.3.3               xml2_1.4.0              ggdist_3.3.3            pillar_1.11.1          \n [66] tensorA_0.36.2.1        checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0    generics_0.1.4         \n [71] vroom_1.6.6             hms_1.1.4               rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4           \n [76] glue_1.8.0              gdtools_0.4.4           emmeans_1.11.2-8        tools_4.5.1             data.table_1.17.8      \n [81] mvtnorm_1.3-3           grid_4.5.1              QuickJSR_1.8.1          nlme_3.1-168            cli_3.6.5              \n [86] textshaping_1.0.4       officer_0.7.2           fontBitstreamVera_0.1.1 svUnit_1.0.8            viridisLite_0.4.2      \n [91] Brobdingnag_1.2-9       V8_8.0.1                gtable_0.3.6            digest_0.6.39           fontquiver_0.2.1       \n [96] TH.data_1.1-4           htmlwidgets_1.6.4       farver_2.1.2            htmltools_0.5.9         lifecycle_1.0.5        \n[101] fontLiberation_0.1.0    openssl_2.3.4           bit64_4.6.0-1           MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "02.html#comments",
    "href": "02.html#comments",
    "title": "2  Exploring Longitudinal Data on Change",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBrennan, R. L. (2001). Generalizability Theory. Springer-Verlag. https://doi.org/10.1007/978-1-4757-3456-0\n\n\nCranford, J. A., Shrout, P. E., Iida, M., Rafaeli, E., Yip, T., & Bolger, N. (2006). A procedure for evaluating sensitivity to within-person change: Can mood measures in diary studies detect change reliably? Personality and Social Psychology Bulletin, 32(7), 917–929. https://doi.org/10.1177/0146167206287721\n\n\nCronbach, L. J., Gleser, G. C., Nanda, H., & Rajaratnam, N. (1972). The dependability of behavioral measurements: Theory of generalizability for scores and profiles. John Wiley & Sons. https://www.amazon.com/Dependability-Behavioral-Measurements-Generalizability-Profiles/dp/0471188506\n\n\nGelman, A., Goodrich, B., Gabry, J., & Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100\n\n\nKuhn, M., Jackson, S., & Cimentada, J. (2020). corrr: Correlations in R [Manual]. https://CRAN.R-project.org/package=corrr\n\n\nLoPilato, A. C., Carter, N. T., & Wang, M. (2015). Updating generalizability theory in management research: Bayesian estimation of variance components. Journal of Management, 41(2), 692–717. https://doi.org/10.1177/0149206314554215\n\n\nPedersen, T. L. (2022). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork\n\n\nPeng, R. D. (2019). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n\n\nRaudenbush, S. W., & Chan, W.-S. (2016). Growth curve analysis in accelerated longitudinal designs. Journal of Research in Crime and Delinquency, 29(4), 387–411. https://doi.org/10.1177/0022427892029004001\n\n\nRevelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych\n\n\nShrout, P. E., & Lane, S. P. (2012). Psychometrics. In M. R. Mehl & T. S. Conner (Eds.), Handbook of research methods for studying daily life (pp. 302–320). The Guilford Press. https://www.guilford.com/books/Handbook-of-Research-Methods-for-Studying-Daily-Life/Mehl-Conner/9781462513055\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Longitudinal Data on Change</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "",
    "text": "3.1 What is the purpose of the multilevel model for change?\nUnfortunately, we do not have access to the full data set Singer and Willett used in this chapter. For details, go here. However, I was able to use the data provided in Table 3.1 and the model results in Table 3.3 to simulate data with similar characteristics as the original. To see how I did it, look at the section at the end of the chapter.\nAnyway, here are the data in Table 3.1.\nlibrary(tidyverse)\n\nearly_int &lt;- tibble(\n  id      = rep(c(68, 70:72, 902, 904, 906, 908), each = 3),\n  age     = rep(c(1, 1.5, 2), times = 8),\n  cog     = c(103, 119, 96, 106, 107, 96, 112, 86, 73, 100, 93, 87, \n              119, 93, 99, 112, 98, 79, 89, 66, 81, 117, 90, 76),\n  program = rep(1:0, each = 12))\n\nprint(early_int)\n\n# A tibble: 24 × 4\n      id   age   cog program\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1    68   1     103       1\n 2    68   1.5   119       1\n 3    68   2      96       1\n 4    70   1     106       1\n 5    70   1.5   107       1\n 6    70   2      96       1\n 7    71   1     112       1\n 8    71   1.5    86       1\n 9    71   2      73       1\n10    72   1     100       1\n# ℹ 14 more rows\nLater on, we also fit models using \\(age - 1\\). Here we’ll compute that and save it as age_c.\nearly_int &lt;- early_int %&gt;% \n  mutate(age_c = age - 1)\n\nhead(early_int)\n\n# A tibble: 6 × 5\n     id   age   cog program age_c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n1    68   1     103       1   0  \n2    68   1.5   119       1   0.5\n3    68   2      96       1   1  \n4    70   1     106       1   0  \n5    70   1.5   107       1   0.5\n6    70   2      96       1   1\nHere we’ll load our simulation of the full \\(n = 103\\) data set.\nload(\"data/early_int_sim.rda\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#the-level-1-submodel-for-individual-change",
    "href": "03.html#the-level-1-submodel-for-individual-change",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "3.2 The level-1 submodel for individual change",
    "text": "3.2 The level-1 submodel for individual change\nThis part of the model is also called the individual growth model. Remember how in last chapter we fit a series of participant-specific models? That’s the essence of this part of the model.\nHere’s our version of Figure 3.1. Note that here we’re being lazy and just using OLS estimates.\n\nearly_int %&gt;% \n  ggplot(aes(x = age, y = cog)) +\n  stat_smooth(method = \"lm\", se = F) +\n  geom_point() +\n  scale_x_continuous(breaks = c(1, 1.5, 2)) +\n  ylim(50, 150) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id, ncol = 4)\n\n\n\n\n\n\n\n\nBased on these data, we postulate our level-1 submodel to be\n\\[\n\\text{cog}_{ij} = [ \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1) ] + [\\epsilon_{ij}].\n\\]\n\n3.2.1 The structural part of the level-1 submodel\nAs far as I can tell, the data for Figure 3.2 are something like this.\n\nd &lt;- tibble(id  = \"i\",\n            age = c(1, 1.5, 2),\n            cog = c(95, 100, 135))\n\nd\n\n# A tibble: 3 × 3\n  id      age   cog\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 i       1      95\n2 i       1.5   100\n3 i       2     135\n\n\nTo add in the horizontal dashed lines in Figure 3.2, we’ll need to fit a model. Let’s be lazy and use OLS. Don’t worry, we’ll use Bayes in a bit.\n\nfit3.1 &lt;- lm(\n  data = d,\n  cog ~ age)\n\nsummary(fit3.1)\n\n\nCall:\nlm(formula = cog ~ age, data = d)\n\nResiduals:\n  1   2   3 \n  5 -10   5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    50.00      26.93   1.857    0.314\nage            40.00      17.32   2.309    0.260\n\nResidual standard error: 12.25 on 1 degrees of freedom\nMultiple R-squared:  0.8421,    Adjusted R-squared:  0.6842 \nF-statistic: 5.333 on 1 and 1 DF,  p-value: 0.2601\n\n\nWe can use the fitted() function to compute the model-implied fitted values for cog based on the age values in the data. We’ll then save those in a sensibly-named vector which we’ll attach to the rest of the data.\n\nf &lt;- fitted(fit3.1) %&gt;% \n  data.frame() %&gt;% \n  set_names(\"fitted\") %&gt;% \n  bind_cols(d)\n\nprint(f)\n\n  fitted id age cog\n1     90  i 1.0  95\n2    110  i 1.5 100\n3    130  i 2.0 135\n\n\nTo make all the dashed lines and arrows in the figure, we’ll want a few specialty tibbles.\n\npath &lt;- tibble(age = c(1, 2, 2),\n               cog = c(90, 90, 130))\n\ntext &lt;- tibble(\n  age   = c(1.2, 1.65, 2.15, 1.125, 2.075),\n  cog   = c(105, 101, 137, 75, 110),\n  label = c(\"epsilon[italic(i)][1]\", \"epsilon[italic(i)][2]\", \n            \"epsilon[italic(i)][3]\", \"pi[0][italic(i)]\", \"pi[1][italic(i)]\"))\n\narrow &lt;- tibble(\n  age  = c(1.15, 1.6, 2.1, 1.1),\n  xend = c(1.01, 1.51, 2.01, 1.01),\n  cog  = c(103, 101, 137, 78),\n  yend = c(92.5, 105, 132.5, 89))\n\n# we're finally ready to plot!\nf %&gt;% \n  ggplot(aes(x = age, y = cog)) +\n  geom_point() +\n  # the main fitted trajectory\n  geom_line(aes(y = fitted)) +\n  # the thick dashed line bending upward at age == 2\n  geom_path(data = path,\n            linewidth = 1/2, linetype = 2) +\n  # the thin dashed vertical lines extending from the data dots to the fitted line\n  geom_segment(aes(xend = age, y = cog, yend = fitted),\n               linetype = 3, linewidth = 1/4) +\n  # the arrows\n  geom_segment(data = arrow,\n               aes(xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.1, \"cm\")), linewidth = 1/4) +\n  # the statistical notation\n  geom_text(data = text,\n            aes(label = label),\n            size = c(4, 4, 4, 5, 5), parse = T) +\n  # \"1 year\"\n  annotate(geom = \"text\",\n           x = 1.5, y = 86, label = \"1 year\") +\n  scale_x_continuous(breaks = c(1, 1.5, 2)) +\n  ylim(50, 150) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nIn-specifying a level-1 submodel that attempts to describe everyone (all the \\(i\\)’s) in the population, we implicitly assume that all the true individual change trajectories have a common algebraic form. But we do not assume that everyone has the same exact trajectory. Because each person has his or her own individual growth parameters (intercepts and slopes), different people can have their own distinct change trajectories. (pp. 53–54)\n\nIn this way, the multilevel model’s level-1 submodel is much like an interaction/moderation model with interaction terms for each level of \\(i\\).\n\n\n3.2.2 The stochastic part of the level-1 submodel\nThe last term in our level-1 equation from above was \\([\\epsilon_{ij}]\\). This is the residual variance left in the criterion after accounting for the predictor(s) in the model. It is a mixture of systemic variation that could be accounted for by adding covariates to the model as well as measurement error. The typical assumption is\n\\[\n\\epsilon_{ij} \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2).\n\\]\nWe should point out that another way to express this is\n\\[\\begin{align*}\n\\text{cog} & \\sim \\operatorname{Normal} (\\mu_{ij}, \\sigma_\\epsilon^2) \\\\\n\\mu_{ij}   & = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 1).\n\\end{align*}\\]\nThis won’t be a huge deal in the context of the models Singer and Willett presented in the initial chapters of the text but expressing the models this way can help one think in terms of likelihood functions. That’s a major advantage when you start working with data which are natural to model using other distributions (e.g., count data and the Poisson, binary data and the binomial). For more on this approach, check out either edition of McElreath’s (2015, 2020) Statistical Rethinking and my (2026b, 2026a) companion ebook(here and here) translating his work into brms and tidyverse code. Also, thinking in terms of likelihoods will pay off starting around Chapter 10 when we start fitting discrete-time survival models.\n\n\n3.2.3 Relating the level-1 submodel to the OLS exploratory methods of chapter 2\nTo get the top panel in Figure 3.3, we’ll use stat_smooth() to get the OLS trajectories.\n\nearly_int_sim %&gt;% \n  ggplot(aes(x = age, y = cog)) +\n  stat_smooth(aes(group = id),\n              method = \"lm\", se = F, size = 1/6) +\n  stat_smooth(method = \"lm\", se = F, size = 2) +\n  scale_x_continuous(breaks = c(1, 1.5, 2)) +\n  ylim(50, 150) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNote that now we’re working with our early_int_sim data, the one where we added the data of 95 simulated individuals to the real data of 8 id levels from Table 3.1. As such, our results will deviate a bit from those in the text.\nBut anyways, here we go on to fit 103 individual OLS models, one for each of the id levels. Don’t worry; we’ll depart from this madness shortly.\n\nby_id &lt;- early_int_sim %&gt;% \n  mutate(age_c = age - 1) %&gt;% \n  group_by(id) %&gt;% \n  nest() %&gt;% \n  mutate(model = map(data, ~lm(data = ., cog ~ age_c)))\n\nhead(by_id)\n\n# A tibble: 6 × 3\n# Groups:   id [6]\n     id data             model \n  &lt;dbl&gt; &lt;list&gt;           &lt;list&gt;\n1     1 &lt;tibble [3 × 4]&gt; &lt;lm&gt;  \n2     2 &lt;tibble [3 × 4]&gt; &lt;lm&gt;  \n3     3 &lt;tibble [3 × 4]&gt; &lt;lm&gt;  \n4     4 &lt;tibble [3 × 4]&gt; &lt;lm&gt;  \n5     5 &lt;tibble [3 × 4]&gt; &lt;lm&gt;  \n6     6 &lt;tibble [3 × 4]&gt; &lt;lm&gt;  \n\n\nNow we’ll use the great helper functions from the broom package (Robinson et al., 2022), tidy() and glance(), to store the coefficient information and model fit information, respectively, in a tidy data format (see here and also here).\n\n# install.packages(\"broom\")\nlibrary(broom)\n\nby_id &lt;- by_id %&gt;%\n  mutate(tidy   = map(model, tidy),\n         glance = map(model, glance))\n\nHere’s what our by_id object now looks like:\n\nby_id %&gt;%\n  head()\n\n# A tibble: 6 × 5\n# Groups:   id [6]\n     id data             model  tidy             glance           \n  &lt;dbl&gt; &lt;list&gt;           &lt;list&gt; &lt;list&gt;           &lt;list&gt;           \n1     1 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt;\n2     2 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt;\n3     3 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt;\n4     4 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt;\n5     5 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt;\n6     6 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 12]&gt;\n\n\nIf you want to extract the intercepts from the tidy column, you might execute code like this.\n\nunnest(by_id, tidy) %&gt;%\n  filter(term == \"(Intercept)\")\n\n# A tibble: 103 × 9\n# Groups:   id [103]\n      id data             model  term        estimate std.error statistic  p.value glance           \n   &lt;dbl&gt; &lt;list&gt;           &lt;list&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;           \n 1     1 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     117   2.31e-14   5.07e15 1.26e-16 &lt;tibble [1 × 12]&gt;\n 2     2 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     110.  5.22e+ 0   2.11e 1 3.01e- 2 &lt;tibble [1 × 12]&gt;\n 3     3 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     117.  1.08e+ 1   1.08e 1 5.87e- 2 &lt;tibble [1 × 12]&gt;\n 4     4 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     136.  5.59e+ 0   2.42e 1 2.62e- 2 &lt;tibble [1 × 12]&gt;\n 5     5 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     108.  3.35e+ 0   3.21e 1 1.99e- 2 &lt;tibble [1 × 12]&gt;\n 6     6 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     110.  2.61e+ 0   4.21e 1 1.51e- 2 &lt;tibble [1 × 12]&gt;\n 7     7 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     124.  7.45e- 1   1.66e 2 3.84e- 3 &lt;tibble [1 × 12]&gt;\n 8     8 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)      97   6.71e+ 0   1.45e 1 4.40e- 2 &lt;tibble [1 × 12]&gt;\n 9     9 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     121.  7.08e+ 0   1.71e 1 3.73e- 2 &lt;tibble [1 × 12]&gt;\n10    10 &lt;tibble [3 × 4]&gt; &lt;lm&gt;   (Intercept)     109.  3.73e- 1   2.92e 2 2.18e- 3 &lt;tibble [1 × 12]&gt;\n# ℹ 93 more rows\n\n\nThis first line took the model coefficients and their respective statistics (e.g., standard errors) and unnested them (i.e., took them out of the list of data frames and converted the data to a longer structure). The second line filtered out any coefficients that were not the intercept. In this case, there are just two coefficients, the intercept and the slope for age_c.\nWith that, we can make the leftmost stem and leaf plot from Figure 3.3.\n\nunnest(by_id, tidy) %&gt;%\n  filter(term == \"(Intercept)\") %&gt;% \n  pull(estimate) %&gt;% \n  stem()\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   8 | 3\n   8 | 5\n   9 | 000124\n   9 | 5666677779999\n  10 | 001111223334444\n  10 | 555677888889\n  11 | 00000223444\n  11 | 55667777788\n  12 | 001112222344\n  12 | 556666889\n  13 | 011223444\n  13 | 56\n  14 | \n  14 | 8\n\n\nHere’s the stem and leaf plot in the middle.\n\nunnest(by_id, tidy) %&gt;%\n  filter(term == \"age_c\") %&gt;% \n  pull(estimate) %&gt;% \n  stem()\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -4 | 7\n  -4 | 111\n  -3 | 9886666655\n  -3 | 322111000\n  -2 | 9877776666555\n  -2 | 44443321111110000\n  -1 | 999999888665\n  -1 | 433322211100000\n  -0 | 988877776\n  -0 | 33211\n   0 | 124\n   0 | 57\n   1 | 1113\n\n\nIf you want the residual variances (i.e., \\(\\sigma_\\epsilon^2\\)), you’d unnest() the glance column. They’ll be listed in the sigma column.\n\nunnest(by_id, glance) %&gt;% \n  pull(sigma) %&gt;% \n  stem(scale = 1)\n\n\n  The decimal point is at the |\n\n   0 | 0044444488888\n   1 | 22\n   2 | 000000444499999\n   3 | 3377777\n   4 | 11111599999\n   5 | 33377777\n   6 | 111111559\n   7 | 3333888\n   8 | 26\n   9 | 0004488\n  10 | 22\n  11 | 0088\n  12 | 777\n  13 | 115\n  14 | 3\n  15 | 59\n  16 | 3\n  17 | 16\n  18 | 0\n  19 | \n  20 | \n  21 | 266",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#the-level-2-submodel-for-systematic-interindividual-differences-in-change",
    "href": "03.html#the-level-2-submodel-for-systematic-interindividual-differences-in-change",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "3.3 The level-2 submodel for systematic interindividual differences in change",
    "text": "3.3 The level-2 submodel for systematic interindividual differences in change\nHere are the top panels of Figure 3.4.\n\nearly_int_sim &lt;- early_int_sim %&gt;% \n  mutate(label = str_c(\"program = \", program)) \n\nearly_int_sim %&gt;% \n  ggplot(aes(x = age, y = cog, color = label)) +\n  stat_smooth(aes(group = id),\n              method = \"lm\", se = F, size = 1/6) +\n  stat_smooth(method = \"lm\", se = F, size = 2) +\n  scale_color_viridis_d(option = \"B\", begin = 0.33, end = 0.67) +\n  scale_x_continuous(breaks = c(1, 1.5, 2)) +\n  ylim(50, 150) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ label)\n\n\n\n\n\n\n\n\nGiven the simplicity of the shapes, the bottom panels of Figure 3.4 will take a bit of preparatory work. First, we’ll need to wrangle the data a bit to get the necessary points. If we were working with a Bayesian model fit with brms, we’d use the fitted() function. But since we’re working with models fit with base R’s OLS estimator, lm(), we’ll use predict(), which accommodates a newdata argument. That’ll be crucial because in order to get the shapes correct, we’ll need to evaluate the minimum and maximum values across the fitted lines across a densely-packed sequence of age_c values.\n\n# how may `age_c` values do we need?\nn &lt;- 30\n\n# define the specific `age_c` values\nnd &lt;- tibble(age_c = seq(from = 0, to = 1, length.out = n))\n\n# wrangle\np &lt;- by_id %&gt;%\n  mutate(fitted  = map(model, ~predict(., newdata = nd))) %&gt;% \n  unnest(fitted) %&gt;% \n  mutate(age     = seq(from = 1, to = 2, length.out = n),\n         program = ifelse(id &lt; 900, 1, 0)) %&gt;% \n  group_by(program, age) %&gt;% \n  summarise(min = min(fitted),\n            max = max(fitted)) %&gt;%\n  mutate(label = str_c(\"program = \", program))\n\n# what did we do?\nhead(p)\n\n# A tibble: 6 × 5\n# Groups:   program [1]\n  program   age   min   max label      \n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1       0  1     82.7  133. program = 0\n2       0  1.03  82.4  132. program = 0\n3       0  1.07  82.1  131. program = 0\n4       0  1.10  81.8  131. program = 0\n5       0  1.14  81.6  130. program = 0\n6       0  1.17  81.3  129. program = 0\n\n\nBefore we plot, we’ll need a couple tibbles for the annotation.\n\ntext &lt;- tibble(\n  age   = 1.01,\n  cog   = c(101.5, 110),\n  label = c(\"program = 0\", \"program = 1\"),\n  text  = c(\"Average population trajectory,\", \"Average population trajectory,\"),\n  angle = c(345.7, 349))\n\nmath &lt;- tibble(\n  age   = 1.01,\n  cog   = c(94.5, 103),\n  label = c(\"program = 0\", \"program = 1\"),\n  text  = c(\"gamma[0][0] + gamma[10](italic(age) - 1)\", \"(gamma[0][0] + gamma[10]) + (gamma[10] + gamma[11]) (italic(age) - 1)\"),\n  angle = c(345.7, 349))\n\nFinally, we’re ready for the bottom panels of Figure 3.4.\n\np %&gt;%\n  ggplot(aes(x = age)) +\n  geom_ribbon(aes(ymin = min, ymax = max, fill = label),\n              alpha = 1/3) +\n  stat_smooth(data = early_int_sim,\n              aes(y = cog, color = label),\n              method = \"lm\", se = F, size = 1) +\n  geom_text(data = text,\n            aes(y = cog, label = text, angle = angle),\n            hjust = 0) +\n  geom_text(data = math,\n            aes(y = cog, label = text, angle = angle),\n            hjust = 0, parse = T) +\n  scale_fill_viridis_d(option = \"B\", begin = 0.33, end = 0.67) +\n  scale_color_viridis_d(option = \"B\", begin = 0.33, end = 0.67) +\n  scale_x_continuous(breaks = c(1, 1.5, 2)) +\n  scale_y_continuous(\"cog\", limits = c(50, 150)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ label)\n\n\n\n\n\n\n\n\nReturning to the text (p. 58), Singer and Willett asked: “What kind [of] population model might have given rise to these patterns?” Their answer is the level-2 model should have 4 specific features:\n\n“Its outcomes must be the individual growth parameters.”\n“The level-2 submodel must be written in separate parts, one for each level-1 growth parameter.”\n“Each part must specify a relationship between an individual growth parameter and the predictor.”\n“Each model must allow individuals who share common predictor values to vary in their individual change trajectories.”\n\nGiven the current model, the level-2 submodel of change may be expressed as\n\\[\\begin{align*}\n\\pi_{0i} & = \\gamma_{00} + \\gamma_{01} \\text{program} + \\zeta_{0i} \\\\\n\\pi_{1i} & = \\gamma_{10} + \\gamma_{11} \\text{program} + \\zeta_{1i}.\n\\end{align*}\\]\nWe’ll discuss the details about the \\(\\zeta\\) terms in a bit.\n\n3.3.1 Structural components of the level-2 submodel\n\nThe structural parts of the level-2 submodel contain four level-2 parameters–\\(\\gamma_{00}\\), \\(\\gamma_{01}\\), \\(\\gamma_{10}\\), and \\(\\gamma_{11}\\)–known collectively as the fixed effects. The fixed effects capture systematic interindividual differences in change trajectory according to values of the level-2 predictors. (p. 60, emphasis in the original)\n\n\n\n3.3.2 Stochastic components of the level-2 submodel\n\nEach part of the level-2 submodel contains a residual that allows the value of each person’s growth parameters to be scattered around the relevant population averages. These residuals, \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) in [the] equation [above], represent those portions of the level-2 outcomes–the individual growth parameters–that remain unexplained by the level-2 predictor(s). (p. 61)\n\nWe often summarize the \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) deviations as \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\), respectively. And importantly, these variance parameters have a covariance \\(\\sigma_{01}\\). However, and this next part is quite important, brms users should know that unlike the convention in many frequentist software packages (Bates et al., 2015; e.g., lme4, Bates et al., 2022) and in the text, brms parameterizes these in the standard-deviation metric. That is, in brms, these are expressed as \\(\\sigma_0\\) and \\(\\sigma_1\\). Similarly, the \\(\\sigma_{01}\\) presented in brms output is in a correlation metric, rather than a covariance. There are technical reasons for this are outside of the scope of the present situation (see Bürkner, 2017). The consequences is that we’ll make frequent use of squares and square roots in this project when comparing our brms::brm() results to those in the text.\nAs on page 63 of the text, the typical way to express the multivariate distribution of the \\(\\zeta\\) parameters would be\n\\[\\begin{align*}\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &\n\\sim \\operatorname{N}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},  \n\\begin{bmatrix} \\sigma_0^2 & \\sigma_{01}\\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix}\n\\end{pmatrix},\n\\end{align*}\\]\nwhere the bracketed matrix on the right part of the equation is the variance/covariance matrix. If we summarize the vector of \\(\\zeta\\) terms as \\(u\\) and so on, we can re-express the above equation as\n\\[\nu \\sim \\operatorname N (\\mathbf 0, \\mathbf \\Sigma),\n\\]\nwhere \\(\\mathbf 0\\) is the vector of 0 means and \\(\\mathbf \\Sigma\\) is the variance/covariance matrix. In Stan, and thus brms, we typically decompose \\(\\mathbf \\Sigma\\) as\n\\[\\begin{align*}\n\\mathbf \\Sigma & = \\mathbf D \\mathbf \\Omega \\mathbf D, \\text{where} \\\\\n\\mathbf D      & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\text{and} \\\\\n\\mathbf \\Omega & = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}.\n\\end{align*}\\]\nThus \\(\\mathbf D\\) is the diagonal matrix of standard deviations and \\(\\mathbf \\Omega\\) is the correlation matrix.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#fitting-the-multilevel-model-for-change-to-data",
    "href": "03.html#fitting-the-multilevel-model-for-change-to-data",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "3.4 Fitting the multilevel model for change to data",
    "text": "3.4 Fitting the multilevel model for change to data\nSinger and Willett discussed how in the 90s, we saw a bloom of software for fitting multilevel models. Very notably, they mentioned BUGS (Gilks et al., 1995) which stands for ‘Bayesian inference using Gibbs sampling’ and was a major advance in Bayesian software. As we learn in Kruschke (2015):\n\nIn 1997, BUGS had a Windows-only version called WinBUGS, and later it was reimplemented in OpenBUGS which also runs best on Windows operating systems. JAGS (Plummer, 2003, 2012) retained many of the design features of BUGS, but with different samplers under the hood and better usability across different computer-operating systems (Windows, MacOS, and other forms of Linux/Unix). (pp. 193–194)\n\nThere’s also Stan. From their homepage, https://mc-stan.org, we read “Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation.” Stan is free and open-source and you can find links to various documentation resources, such as the current Stan user’s guide (Stan Development Team, 2021b) and Stan reference manual (Stan Development Team, 2021a), at https://mc-stan.org/users/documentation/. Unlike BUGS and JAGS, Stan samples from the posterior via Hamiltonian Monte Carlo, which tends to scale particularly well for complex multilevel models. However, in this project we won’t be working with Stan directly. Rather, we’ll interface with it indirectly through brms. To my knowledge, brms is the most flexible and user-friendly interface for Stan within the R ecosystem.\nTalking about the various software options, Singer and Willett wrote:\n\nAll have their strengths, and we use many of them in our research and in this book. At their core, each program does the same job; it fits the multilevel model for change to data and provides parameter estimates, measures of precision, diagnostics, and so on. There is also some evidence that all the different packages produce the same, or similar, answers to a given problem (Kreft & de Leeuw, 1990). So, in one sense, it does not matter which program you choose. (p. 64)\n\nBut importantly, in the next paragraph the authors clarified their text focused on “one particular method of estimation–maximum likelihood” (p. 64, emphasis in the original). This is quite important because, whereas we might expect various maximum-likelihood-based packages to yield the same or similar results, this will not necessarily hold when working with Bayesian software which, in addition to point estimates and expressions of uncertainty, yields an entire posterior distribution as a consequence of Bayes’ theorem,\n\\[\np(\\theta \\mid d) = \\frac{p(d \\mid \\theta)\\ p(\\theta)}{p(d)},\n\\]\nwhere \\(p(\\theta \\mid d)\\) is the posterior distribution, \\(p(d \\mid \\theta)\\) is the likelihood (i.e., the star of maximum likelihood), \\(p(\\theta)\\) are the priors, and \\(p(d)\\) is the normalizing constant, the probability of the data which transforms the numerator to a probability metric.\nGiven that multilevel models are a fairly advanced topic, it is not my goal in this project to offer a tutorial on the foundations of applied Bayesian statistics. I’m taking it for granted you are familiar with the basics. But if you’re very ambitious and this is new or if you’re just rusty, I recommend you back up and lean the ropes with Richard McElreath’s excellent introductory text, Statistical Rethinking. He has great freely-available lectures that augment the text and I also have ebooks (here and here) translating both editions of his text into brms and tidyverse-style code.\n\n3.4.1 The advantages of maximum likelihood Bayesian estimation\nI just don’t think I have the strength to evangelize Bayes, at the moment. McElreath covered that a little bit in Statistical Rethinking, but he mainly took it for granted. Kruschke has been more of a Bayesian evangelist, examples of which are his 2015 text or his (2018) coauthored paper with Torrin Liddell, The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. My assumption is if you’re reading this, you’re already interested.\n\n\n3.4.2 Using maximum likelihood modern Bayesian methods to fit a multilevel model\nJust as with maximum likelihood, you have to specify a likelihood function with Bayes, too. The likelihood, \\(p(d \\mid \\theta)\\), is half of the numerator of Bayes’ theorem and its meaning in Bayes is that same as with maximum likelihood estimation. The likelihood “describes the probability of observing the sample data as a function of the model’s unknown parameters” (p. 66). But unlike with maximum likelihood, we multiply the likelihood with the prior(s) and normalize the results so they’re in a probability metric.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#examining-estimated-fixed-effects",
    "href": "03.html#examining-estimated-fixed-effects",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "3.5 Examining estimated fixed effects",
    "text": "3.5 Examining estimated fixed effects\nSinger and Willett discussed hypothesis testing in this section. The Bayesian paradigm can be used for hypothesis testing. For an introduction to this approach, check out Chapters 11 and 12 of Kruschke’s (2015) text. This will not be our approach in this project. My perspective on Bayesian modeling is more influenced by McElreath’s text and by Andrew Gelman’s various works. I like fitting models, inspecting their parameters, interpreting them from an effect-size perspective, and considering posterior predictions. You’ll see plenty of examples of this approach in the examples to come.\n\n3.5.1 Interpreting estimated fixed effects\nSinger and Willett presented the maximum likelihood results of our multilevel model in Table 3.3. Before we present ours, we’ll need to fit our corresponding Bayesian model. Let’s fire up brms.\n\nlibrary(brms)\n\nJust like we did with the single-level models in the last chapter, we’ll fit our Bayesian multilevel models with the brm() function.\n\nfit3.2 &lt;- brm(\n  data = early_int_sim,\n  family = gaussian,\n  formula = cog ~ 0 + Intercept + age_c + program + age_c:program + (1 + age_c | id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  control = list(adapt_delta = 0.99),\n  seed = 3,\n  file = \"fits/fit03.02\")\n\nCompared to last chapter, we’ve added a few arguments. Notice the second line, family = gaussian. Remember all that likelihood talk from the last few sections? Well, with family = gaussian we’ve indicated we want to use the Gaussian likelihood function. As this is the brms default, we didn’t actually need to type out the argument. But we’ll follow this convention for the remainder of the text for two reasons. First, I hope it’s pedagogically useful to remind you of what likelihood you’re working with. Second, I think it’s generally a good idea to explicate your likelihood. In the context of the initial chapters of this text, this might seem unnecessary. We’ll constantly be using the Gaussian. But that’s largely a pedagogical decision made by the authors. There are lots of every-day applications for multilevel models with other likelihood functions, such as those suited for discrete data. And when the day comes you’ll need to fit a multilevel logistic regression model, you’ll need to use a different setting in the family argument. Plus, we will have some practice using other likelihood functions in the survival chapters later in the text.\nHere’s the big new thing: our formula line specified a multilevel model! Check out the (1 + age_c | id) syntax on the right. This syntax is designed to be similar to the that of the widely-used frequentist lme4 package. In the brms reference manual (Bürkner, 2021a), we lean this syntax follows the generic form (gterms | group) where “the optional gterms part may contain effects that are assumed to vary across grouping variables specified in group. We call them ‘group-level’ or ‘varying’ effects, or (adopting frequentist vocabulary) ‘random’ effects, although the latter name is misleading in a Bayesian context” (p. 35). And like with base R’s lm() function or with lme4, the 1 portion is a stand-in for the intercept. Thus, with 1 + age_c, we indicated we wanted the intercept and age_c slope to vary across groups. On the right side of the |, we defined our grouping variable as id.\nAnother important part of the formula syntax concerns the intercept for the fixed effects. See the 0 + Intercept part? Here’s the deal: If we were using default behavior, we’d have coded either 1 + ... or just left that part out entirely. Both would have estimated the fixed intercept according to brms default behavior. But that’s the issue. By default, brms::brm() presumes your predictors are mean centered. This is critical because the default priors set by brms::brm() are also set based on this assumption. As it turns out, neither our age_c nor program variables are centered that way. program is a dummy variable and age_c is centered on 1, not the mean. Now since the brm() default priors are rather broad and uninformative, This probably wouldn’t have made much of a difference, here. However, we may as well address this issue now and avoid bad practices. So, with our 0 + Intercept solution, we told brm() to suppress the default intercept and replace it with our smartly-named Intercept parameter. This is our fixed effect for the population intercept and, importantly, brms() will assign default priors to it based on the data themselves without assumptions about centering.\nI’d like to acknowledge at this point that if brms and/or the multilevel model are new to you, this can be disorienting and confusing. I’m sorry. The world is unfair and Singer and Willett didn’t write their text with brms in mind. Had they done so, they’d have used mean-centered predictors for the first few models to put off this technical issue until later chapters. Yet here we are. So if you’re feeling frustrated, that mainly just means you’re paying attention. Good job! Forge on, friends. It’ll get better.\nThe line starting with iter largely explicates brms::brm() default settings. The only change from the defaults is cores = 4, which allow you to sample from all four chains simultaneously.\nThe control line opens up a can of worms I just don’t want to address at this point in the text. It’s a technical setting that helped us do a better job sampling form the posterior. We’ll have more opportunities to talk about it later. For now, know that the default setting for adapt_delta is 0.8. The value ranges from 0 to 1.\nThe last line of interest is seed = 3. Markov chain Monte Carlo methods use pseudo-random number generators to sample from the posterior. To make the results of a pseudo-random process reproducible, you set the seed. There’s nothing special about setting it to 3. I just did so because that’s the chapter we’re on. Play around with other values and see what happens.\nOkay, that’s a lot of boring technical talk. Let’s use print() to see what we did!\n\nprint(fit3.2)\n\n Family: gaussian \n  Links: mu = identity \nFormula: cog ~ 0 + Intercept + age_c + program + age_c:program + (1 + age_c | id) \n   Data: early_int_sim (Number of observations: 309) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 103) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            9.61      1.18     7.50    12.07 1.01      645     1850\nsd(age_c)                3.80      2.37     0.18     8.60 1.02      288      861\ncor(Intercept,age_c)    -0.46      0.37    -0.95     0.62 1.01     1683     1325\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       106.72      1.87   103.06   110.47 1.00      846     1437\nage_c           -20.60      1.98   -24.49   -16.84 1.00     1936     2757\nprogram           8.98      2.48     4.21    13.97 1.00      886     1594\nage_c:program     3.30      2.62    -1.75     8.31 1.00     1792     2358\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.59      0.50     7.62     9.64 1.01      754     1246\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we’re in multilevel-model land, we have three main sections in the output. Let’s start in the middle. The ‘Population-Level Effects:’ section contains our brms analogues to the “Fixed Effects” portion of Singer and Willett’s Table 3.3. These are our \\(\\gamma\\) parameter summaries. Like we briefly covered in the last chapter, brms does not give \\(z\\)- or \\(p\\)-values. But we do get high-quality percentile-based Bayesian 95% intervals. Perhaps somewhat frustratingly, our ‘Estimate’ values are a little different from those in the text. In this instance, this is more due to us not having access to the original data than differences between Bayesian and maximum likelihood estimation. They’ll be closer in many other examples.\nThe bottom section, ‘Family Specific Parameters:’, is our analogue to the first line in Table 3.3’s “Variance Components” section. What Singer and Willett referred to as \\(\\epsilon_{ij}\\), the brms package calls sigma. But importantly, do recall that Stan and brms parameterize variance components in the standard-deviation metric. You’d have to square our sigma to put it in a similar metric to the estimate in the text. This will be the case throughout this project. But why call this section ‘Family Specific Parameters’? Well, not all likelihoods have a \\(\\sigma\\) parameter. In the Poisson likelihood, for example, the mean and variance scale together as one parameter called \\(\\lambda\\). Since brms is designed to handle a whole slew of likelihood functions (see the Parameterization of response distributions in brms vignette, Bürkner, 2021b) it behooved Bürkner to give this section a generic name.\nNow we’re ready to draw our attention to the topmost section. The ‘Group-Level Effects:’ are our brms variants of the Level 2 section of Singer and Willett’s “Variance Components” section in Table 3.3. Our sd(Intercept) corresponds to their \\(\\sigma_0^2\\) and our sd(age_c) corresponds to their \\(\\sigma_1^2\\). But recall, ours are in a standard-deviation metric. The estimates in the book are expressed as variances. Finally, our cor(Intercept,age_c) parameter is a correlation among the varying-effects, whereas Singer and Willett’s \\(\\sigma_{01}\\) is a covariance.\nIn addition to print(), a handy way to pull the fixed effects from a brm() model is with the fixef() function.\n\nfixef(fit3.2)\n\n                Estimate Est.Error       Q2.5     Q97.5\nIntercept     106.722531  1.874381 103.055303 110.46952\nage_c         -20.598997  1.976751 -24.485154 -16.83980\nprogram         8.981118  2.481598   4.214897  13.97079\nage_c:program   3.301134  2.617679  -1.754186   8.31457\n\n\nYou can subset its components with [] syntax. E.g., here we’ll pull the posterior mean for the overall intercept and round to two decimal places.\n\nfixef(fit3.2)[1, 1] %&gt;% \n  round(digits = 2)\n\n[1] 106.72\n\n\nThus, we can write our version of the equations atop page 70 as\n\n\\(\\hat{\\pi}_{0i} =\\) 106.72 \\(+\\) 8.98\\(\\text{program}_i\\) and\n\\(\\hat{\\pi}_{1i} =\\) -20.6 \\(+\\) 3.3\\(\\text{program}_i\\).\n\nAgain, our results differ largely because they’re based on simulated data rather than the real data in the text. We’ll be able to work with the original data in the later chapters. Anyway, here are the results of those two equations.\n\nfixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1]\n\n[1] 115.7036\n\nfixef(fit3.2)[2, 1] + fixef(fit3.2)[4, 1]\n\n[1] -17.29786\n\n\nHere’s how to get our estimates corresponding to the values at the bottom of page 70.\n\n# when `program` is 0\nfixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] * 0 \n\n[1] 106.7225\n\nfixef(fit3.2)[2, 1] * 1 + fixef(fit3.2)[4, 1] * 1 * 0\n\n[1] -20.599\n\n# when `program` is 1\nfixef(fit3.2)[1, 1] + fixef(fit3.2)[3, 1] * 1 \n\n[1] 115.7036\n\nfixef(fit3.2)[2, 1] * 1 + fixef(fit3.2)[4, 1] * 1 * 1\n\n[1] -17.29786\n\n\nTo make our version of Figure 3.5, we’ll pump the necessary age_c and program values into the full formula of the fixed effects.\n\n# specify the values for our covariates `age_c` and `program`\ncrossing(age_c   = 0:1,\n         program = 0:1) %&gt;% \n  # push those values through the fixed effects\n  mutate(cog = fixef(fit3.2)[1, 1] + fixef(fit3.2)[2, 1] * age_c + fixef(fit3.2)[3, 1] * program + fixef(fit3.2)[4, 1] * age_c * program,\n         # wrangle a bit\n         age     = age_c + 1,\n         size    = ifelse(program == 1, 1/5, 3),\n         program = factor(program, levels = c(\"0\", \"1\"))) %&gt;% \n\n  # plot!\n  ggplot(aes(x = age, y = cog, group = program)) +\n  geom_line(aes(size = program)) +\n  scale_size_manual(values = c(1, 1/2)) +\n  scale_x_continuous(breaks = c(1, 1.5, 2)) +\n  ylim(50, 150) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n3.5.2 Single parameter tests for the fixed effects\n\nAs in regular regression, you can conduct a hypothesis test on each fixed effect (each \\(\\gamma\\)) using a single parameter text. Although you can equate the parameter value to any pre-specified value in your hypothesis text, most commonly you examine the null hypothesis that, controlling for all other predictors in the model, the population value of the parameter is 0, \\(H_0: \\gamma = 0\\), against the two-sided alternative that it is not, \\(H_1: \\gamma \\neq 0\\). (p. 71)\n\nYou can do this with brms with the hypothesis() function. I’m not a fan of this method and am not going to showcase it in this ebook If you insist on the NHST paradigm, you’ll have to go that alone.\nWithin the Bayesian paradigm, we have an entire posterior distribution. So let’s just look at that. Recall that in the print() and fixef() outputs, we get the parameter estimates for our \\(\\gamma\\)’s summarized in terms of the posterior mean (i.e., ‘Estimate’), the posterior standard deviation (i.e., ‘Est.error’), and the percentile-based 95% credible intervals (i.e., ‘Q2.5’ and ‘Q97.5’). But we can get much richer output with the as_draws_df() function.\n\ndraws &lt;- as_draws_df(fit3.2)\n\nHere’s a look at the first 10 columns.\n\ndraws[, 1:10] %&gt;%\n  glimpse()\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nRows: 4,000\nColumns: 10\n$ b_Intercept              &lt;dbl&gt; 108.0618, 107.2084, 109.1522, 106.4010, 107.3905, 107.2919, 106.0169, 108.7411, 104.9963, 104.7…\n$ b_age_c                  &lt;dbl&gt; -23.50480, -18.62689, -25.27896, -21.78287, -20.97538, -21.27200, -18.34308, -22.61039, -19.641…\n$ b_program                &lt;dbl&gt; 7.237718, 5.465535, 5.877555, 8.384501, 7.767299, 9.631066, 9.390404, 9.625814, 14.050277, 9.44…\n$ `b_age_c:program`        &lt;dbl&gt; 5.57015707, 4.59582937, 8.03606790, 2.85711876, 1.38155175, 1.14612760, 3.25427893, 2.80812054,…\n$ sd_id__Intercept         &lt;dbl&gt; 9.576004, 9.140204, 7.546825, 8.363722, 8.200802, 8.482164, 9.351426, 8.111708, 6.697821, 7.986…\n$ sd_id__age_c             &lt;dbl&gt; 1.848329916, 3.318119224, 0.274335138, 1.024330616, 0.754860893, 1.415988154, 0.601034200, 0.36…\n$ cor_id__Intercept__age_c &lt;dbl&gt; -0.36934536, -0.60400186, -0.39142979, 0.75791077, 0.62513115, -0.04495901, -0.70390683, -0.727…\n$ sigma                    &lt;dbl&gt; 8.524191, 8.746213, 8.705385, 8.643440, 8.958219, 8.622695, 8.441646, 8.001595, 8.679356, 9.423…\n$ `r_id[1,Intercept]`      &lt;dbl&gt; 5.8731365, 9.5448881, 1.7200079, 10.0495574, 11.5061132, 13.6526215, 4.9137664, 5.4256201, -0.8…\n$ `r_id[2,Intercept]`      &lt;dbl&gt; 8.5201226, 2.7280508, -1.8207925, 6.6334510, 3.5402767, 1.7699966, -1.6385070, 1.7369407, -4.32…\n\n\nNote the warning message. The as_draws_df() function returns a special kind of data frame, which includes 3 metadata columns, which are typically hidden from view. Sometimes we want those metadata columns, sometimes we don’t. You’ll see. Anyway, here are the dimensions of our draws data frame.\n\ndraws %&gt;% \n  dim()\n\n[1] 4000  219\n\n\nWe saved our results as draws, which is a data frame with 4,000 rows (i.e., 1,000 post-warmup posterior draws times 4 chains) and columns depicting the model parameters, as well as the metadata values. With brms, the \\(\\gamma\\) parameters (i.e., the fixed effects or population parameters) get b_ prefixes in the as_draws_df() output. So we can isolate them like so.\n\ndraws %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  head()\n\n# A tibble: 6 × 4\n  b_Intercept b_age_c b_program `b_age_c:program`\n        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1        108.   -23.5      7.24              5.57\n2        107.   -18.6      5.47              4.60\n3        109.   -25.3      5.88              8.04\n4        106.   -21.8      8.38              2.86\n5        107.   -21.0      7.77              1.38\n6        107.   -21.3      9.63              1.15\n\n\nJust a little more data wrangling will put draws in a format suitable for plotting.\n\ndraws %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density(color = \"transparent\", fill = \"grey25\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nSure, you could fixate on zero if you wanted to. But of more interest is the overall shape of each parameter’s posterior distribution. Look at the central tendency and spread for each. Look at where each is in the parameter space. To my mind, that story is so much richer than fixating on zero.\nWe’ll have more to say along these lines in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#examining-estimated-variance-components",
    "href": "03.html#examining-estimated-variance-components",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "3.6 Examining estimated variance components",
    "text": "3.6 Examining estimated variance components\n\nEstimated variance and covariance components are trickier to interpret as their numeric values have little absolute meaning and there are no graphic aids to fall back on. Interpretation for a single fitted model is especially difficult as you lack benchmarks for evaluating the components’ magnitudes. This increases the utility of hypothesis testing, for at least the tests provide some benchmark (against the null value of 0) for comparison. (p. 72)\n\nNo, no, no! I do protest. No!\nAs I hope to demonstrate, our Bayesian brms paradigm offers rich and informative alternatives to the glib picture Singer and Willett painted back in 2003. Nowadays, we have full Bayesian estimation with Stan. Rejoice, friends. Rejoice.\n\n3.6.1 Interpreting the estimated variance components\nTo extract just the variance components of a brm() model, use the VarCorr() function.\n\nVarCorr(fit3.2)\n\n$id\n$id$sd\n          Estimate Est.Error      Q2.5     Q97.5\nIntercept 9.606573  1.176684 7.4998925 12.072630\nage_c     3.795158  2.366547 0.1823513  8.595911\n\n$id$cor\n, , Intercept\n\n            Estimate Est.Error       Q2.5     Q97.5\nIntercept  1.0000000 0.0000000  1.0000000 1.0000000\nage_c     -0.4615782 0.3736345 -0.9520766 0.6173991\n\n, , age_c\n\n            Estimate Est.Error       Q2.5     Q97.5\nIntercept -0.4615782 0.3736345 -0.9520766 0.6173991\nage_c      1.0000000 0.0000000  1.0000000 1.0000000\n\n\n$id$cov\n, , Intercept\n\n           Estimate Est.Error      Q2.5      Q97.5\nIntercept  93.67049  23.18788  56.24839 145.748403\nage_c     -20.51951  18.70078 -63.58490   5.023044\n\n, , age_c\n\n           Estimate Est.Error         Q2.5     Q97.5\nIntercept -20.51951  18.70078 -63.58490466  5.023044\nage_c      20.00237  21.03167   0.03325202 73.889680\n\n\n\n$residual__\n$residual__$sd\n Estimate Est.Error     Q2.5    Q97.5\n 8.594205 0.5049914 7.615114 9.637543\n\n\nIn case that output is confusing, VarCorr() returned a 2-element list of lists. We can use the [[]] subsetting syntax to isolate the first list of lists.\n\nVarCorr(fit3.2)[[1]] %&gt;% str()\n\nList of 3\n $ sd : num [1:2, 1:4] 9.61 3.8 1.18 2.37 7.5 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"Intercept\" \"age_c\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n $ cor: num [1:2, 1:4, 1:2] 1 -0.462 0 0.374 1 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"Intercept\" \"age_c\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n  .. ..$ : chr [1:2] \"Intercept\" \"age_c\"\n $ cov: num [1:2, 1:4, 1:2] 93.7 -20.5 23.2 18.7 56.2 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:2] \"Intercept\" \"age_c\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n  .. ..$ : chr [1:2] \"Intercept\" \"age_c\"\n\n\nIf you just want the \\(\\zeta\\)’s, subset the first list of the first list.\n\nVarCorr(fit3.2)[[1]][[1]]\n\n          Estimate Est.Error      Q2.5     Q97.5\nIntercept 9.606573  1.176684 7.4998925 12.072630\nage_c     3.795158  2.366547 0.1823513  8.595911\n\n\nHere’s how to get their correlation matrix.\n\nVarCorr(fit3.2)[[1]][[2]]\n\n, , Intercept\n\n            Estimate Est.Error       Q2.5     Q97.5\nIntercept  1.0000000 0.0000000  1.0000000 1.0000000\nage_c     -0.4615782 0.3736345 -0.9520766 0.6173991\n\n, , age_c\n\n            Estimate Est.Error       Q2.5     Q97.5\nIntercept -0.4615782 0.3736345 -0.9520766 0.6173991\nage_c      1.0000000 0.0000000  1.0000000 1.0000000\n\n\nAnd perhaps of great interest, here’s how to get their variance/covariance matrix.\n\nVarCorr(fit3.2)[[1]][[3]]\n\n, , Intercept\n\n           Estimate Est.Error      Q2.5      Q97.5\nIntercept  93.67049  23.18788  56.24839 145.748403\nage_c     -20.51951  18.70078 -63.58490   5.023044\n\n, , age_c\n\n           Estimate Est.Error         Q2.5     Q97.5\nIntercept -20.51951  18.70078 -63.58490466  5.023044\nage_c      20.00237  21.03167   0.03325202 73.889680\n\n\nYou can also use the appropriate algebraic operations to transform some of the columns in the as_draws_df() output into the variance metric used in the text. Here we’ll do so for the elements in the variance/covariance matrix and \\(\\sigma_\\epsilon^2\\), too.\n\nas_draws_df(fit3.2) %&gt;% \n  mutate(`sigma[0]^2`       = sd_id__Intercept^2,\n         `sigma[1]^2`       = sd_id__age_c^2,\n         `sigma[0][1]`      = sd_id__Intercept * cor_id__Intercept__age_c * sd_id__age_c,\n         `sigma[epsilon]^2` = sigma^2) %&gt;% \n  pivot_longer(starts_with(\"sigma[\"),\n               values_to = \"posterior\") %&gt;% \n  \n  ggplot(aes(x = posterior)) +\n  geom_density(color = \"transparent\", fill = \"grey33\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.text = element_text(size = 12)) +\n  facet_wrap(~ name, scales = \"free\", labeller = label_parsed) \n\n\n\n\n\n\n\n\nAs it turns out, the multilevel variance components are often markedly non-Gaussian. This is important for the next section.\n\n\n3.6.2 Single parameter tests for the variance components\n\nStatisticians disagree as to the nature, form, and effectiveness of these tests. Rupert G. Miller (1997), Raudenbush & Bryk (2002), and others have long questioned their utility because of their sensitivity to departures from normality. Longford (1999) describes their sensitivity to sample size and imbalance (unequal numbers of observations per person) and argues that they are so misleading that they should be abandoned completely. (p. 73)\n\nThis reminds me of parts from Gelman and Hill’s (2006) text on multilevel models. In Section 12.7 on the topic of model building and statistical significance, they wrote:\n\nIt is not appropriate to use statistical significance as a criterion for including particular group indicators in a multilevel model….\n\n[They went on to discuss a particular example from the text, regarding radon levels in housed in various counties.]\n\nHowever, we should include all 85 counties in the model, and nothing is lost by doing so. The purpose of the multilevel model is not to use whether radon levels in county 1 are statistically significantly different from those in county 2, or from the Minnesota average. Rather, we seek the best possible estimate in each county, with appropriate accounting for uncertainty. Rather that make some significance threshold, we allow all the intercepts to vary and recognize that we may not have much precision in many of the individual groups….\nThe same principle holds for the models discussed in the following chapters, which include varying slopes, non-nested levels, discrete data, and other complexities. Once we have included a source of variation, we do not use statistical significance to pick and choose indicators to include or exclude from the model.\nIn practice, our biggest constraints–the main reasons we do not use extremely elaborate models in which all coefficients can vary with respect to all grouping factors–are fitting and understanding complex models. The lmer() function works well when it works, but it can break down for models with many groping factors. (p. 272, emphasis in the original)\n\nFor context, lmer() is the primary function in the frequentist lme4 package. After pointing out difficulties with lmer(), they went on to point out how the Bayesian Bugs software can often overcome limitations in frequentist packages. We now have the benefit of Stan and brms. My general recommendation is if your theory suggests there should be group-level variability and you’ve collected the necessary data to fit that model, fit the full model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#bonus-how-did-you-simulate-that-data",
    "href": "03.html#bonus-how-did-you-simulate-that-data",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "3.7 Bonus: How did you simulate that data?",
    "text": "3.7 Bonus: How did you simulate that data?\nWhat makes our task difficult is the multilevel model we’d like to simulate our data for has both varying intercepts and slopes. And worst yet, those varying intercepts and slopes have a correlation structure. Also of note, Singer and Willett presented their summary statistics in the form of a variance/covariance matrix in Table 3.3.\nAs it turns out, the mvnorm() function from the MASS package (Ripley, 2022; Venables & Ripley, 2002) will allow us to simulate multivariate normal data from a given mean structure and variance/covariance matrix. So our first step in simulating our data is to simulate the \\(103 – 8 = 95\\) \\(\\zeta\\) values. We’ll name the results z.\n\n# how many people are we simulating?\nn &lt;- 103 - 8\n\n# what's the variance/covariance matrix?\nsigma &lt;- matrix(c(124.64, -36.41,\n                  -36.41, 12.29),\n                ncol = 2)\n\n# what's our mean structure?\nmu &lt;- c(0, 0)\n\n# set the seed and simulate!\nset.seed(3)\nz &lt;- MASS::mvrnorm(n = n, mu = mu, Sigma = sigma) %&gt;% \n  data.frame() %&gt;% \n  set_names(\"zeta_0\", \"zeta_1\")\n\nhead(z)\n\n      zeta_0     zeta_1\n1 10.7586672 -3.0908765\n2  3.4258938 -0.4186497\n3 -3.0770183  0.2140130\n4 12.5303603 -4.9043416\n5 -2.1114641  0.8936950\n6 -0.5521597 -0.6310265\n\n\nFor our next step, we’ll define our \\(\\gamma\\) parameters. These are also taken from Table 3.3.\n\ng &lt;- tibble(id       = 1:n,\n            gamma_00 = 107.84,\n            gamma_01 = 6.85,\n            gamma_10 = -21.13,\n            gamma_11 = 5.27)\n\nhead(g)\n\n# A tibble: 6 × 5\n     id gamma_00 gamma_01 gamma_10 gamma_11\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1     108.     6.85    -21.1     5.27\n2     2     108.     6.85    -21.1     5.27\n3     3     108.     6.85    -21.1     5.27\n4     4     108.     6.85    -21.1     5.27\n5     5     108.     6.85    -21.1     5.27\n6     6     108.     6.85    -21.1     5.27\n\n\nNote how they’re the same for each row. That’s the essence of the meaning of a fixed effect.\nAnyway, this next block is a big one. After we combine g and z, we add in the appropriate program and age_c values. You can figure out those from pages 46 and 47. We then insert our final model parameter, \\(\\epsilon\\), and combine the \\(\\gamma\\)’s and \\(\\zeta\\)’s to make our two \\(\\pi\\) parameters (see page 60). Once that’s all in place, we’re ready to use the model formula to calculate the expected cog values from the \\(\\pi\\)’s, age_c, and \\(\\epsilon\\).\n\n# set the seed for the second `mutate()` line\nset.seed(3)\n\nearly_int_sim &lt;- bind_cols(g, z) %&gt;% \n  mutate(program = rep(1:0, times = c(54, 41))) %&gt;% \n  expand(nesting(id, gamma_00, gamma_01, gamma_10, gamma_11, zeta_0, zeta_1, program),\n         age_c = c(0, 0.5, 1)) %&gt;% \n  mutate(epsilon = rnorm(n(), mean = 0, sd = sqrt(74.24))) %&gt;% \n  mutate(pi_0 = gamma_00 + gamma_01 * program + zeta_0,\n         pi_1 = gamma_10 + gamma_11 * program + zeta_1) %&gt;% \n  mutate(cog = pi_0 + pi_1 * age_c + epsilon)\n\nhead(early_int_sim)\n\n# A tibble: 6 × 13\n     id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c epsilon  pi_0  pi_1   cog\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0    -8.29   125. -19.0  117.\n2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5  -2.52   125. -19.0  113.\n3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1     2.23   125. -19.0  109.\n4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0    -9.93   118. -16.3  108.\n5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5   1.69   118. -16.3  112.\n6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1     0.260  118. -16.3  102.\n\n\nBut before we do, we’ll want to wrangle a little. We need an age column. If you look closely at Table 3.3, you’ll see all the cog values are integers. So we’ll round ours to match. Finally, we’ll want to renumber our id values to match up better with those in Table 3.3.\n\nearly_int_sim &lt;- early_int_sim %&gt;% \n  mutate(age = age_c + 1,\n         cog = round(cog, digits = 0),\n         id  = ifelse(id &gt; 54, id + 900, id))\n\nhead(early_int_sim)\n\n# A tibble: 6 × 14\n     id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c epsilon  pi_0  pi_1   cog   age\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0    -8.29   125. -19.0   117   1  \n2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5  -2.52   125. -19.0   113   1.5\n3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1     2.23   125. -19.0   109   2  \n4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0    -9.93   118. -16.3   108   1  \n5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5   1.69   118. -16.3   112   1.5\n6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1     0.260  118. -16.3   102   2  \n\n\nFinally, now we just need to prune the columns with the model parameters, rearrange the order of the columns we’d like to keep, and join these data with those from Table 3.3.\n\nearly_int_sim &lt;- early_int_sim %&gt;% \n  select(id, age, cog, program, age_c) %&gt;% \n  full_join(early_int,\n            by = c(\"id\", \"age\", \"cog\", \"program\", \"age_c\")) %&gt;% \n  arrange(id, age)\n\nglimpse(early_int_sim)\n\nRows: 309\nColumns: 5\n$ id      &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12,…\n$ age     &lt;dbl&gt; 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.…\n$ cog     &lt;dbl&gt; 117, 113, 109, 108, 112, 102, 112, 113, 85, 138, 110, 97, 106, 107, 99, 111, 98, 92, 124, 108, 94, 94, 93, 74, 1…\n$ program &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ age_c   &lt;dbl&gt; 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.…\n\n\nHere we save our results in an external file for use later.\n\nsave(early_int_sim,\n     file = \"data/early_int_sim.rda\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#session-info",
    "href": "03.html#session-info",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] brms_2.23.0     Rcpp_1.1.0      broom_1.0.10    lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0   dplyr_1.1.4    \n [8] purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000          S7_0.2.1               \n [6] fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39           timechange_0.3.0       \n[11] estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3          magrittr_2.0.4         \n[16] posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7             tools_4.5.1             utf8_1.2.6             \n[21] knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1    htmlwidgets_1.6.4       curl_7.0.0             \n[26] pkgbuild_1.4.8          plyr_1.8.9              RColorBrewer_1.1-3      abind_1.4-8             multcomp_1.4-29        \n[31] withr_3.0.2             grid_4.5.1              stats4_4.5.1            xtable_1.8-4            inline_0.3.21          \n[36] emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65             cli_3.6.5               mvtnorm_1.3-3          \n[41] rmarkdown_2.30          generics_0.1.4          RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5         \n[46] tzdb_0.5.0              rstan_2.36.0.9000       splines_4.5.1           bayesplot_1.15.0.9000   parallel_4.5.1         \n[51] matrixStats_1.5.0       vctrs_0.6.5             V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1         \n[56] jsonlite_2.0.0          hms_1.1.4               glue_1.8.0              codetools_0.2-20        distributional_0.5.0   \n[61] stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1           htmltools_0.5.9        \n[66] Brobdingnag_1.2-9       R6_2.6.1                evaluate_1.0.5          lattice_0.22-7          backports_1.5.0        \n[71] rstantools_2.5.0.9000   gridExtra_2.3           coda_0.19-4.1           nlme_3.1-168            checkmate_2.3.3        \n[76] mgcv_1.9-3              xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "03.html#comments",
    "href": "03.html#comments",
    "title": "3  Introducing the Multilevel Model for Change",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates, D., Maechler, M., Bolker, B., & Steven Walker. (2022). lme4: Linear mixed-effects models using Eigen’ and S4. https://CRAN.R-project.org/package=lme4\n\n\nBryk, A. S., & Raudenbush, S. W. (1987). Application of hierarchical linear models to assessing change. Psychological Bulletin, 101(1), 147. https://doi.org/10.1037/0033-2909.101.1.147\n\n\nBürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2021a). brms reference manual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2021b). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942\n\n\nGilks, W. R., Richardson, S., & Spiegelhalter, D. (1995). Markov chain Monte Carlo in practice. Chapman and Hall/CRC. https://www.routledge.com/Markov-Chain-Monte-Carlo-in-Practice/Gilks-Richardson-Spiegelhalter/p/book/9780412055515\n\n\nKreft, I. G. G., & de Leeuw, J. (1990). Comparing four different statistical packages for hierarchical linear regression: GENMOD, HLM, ML2, and VARCL. CSE Dissemination Office, UCLA Graduate School of Education, 405 Hilgard Avenue, Los Angeles, CA 90024-1521. https://files.eric.ed.gov/fulltext/ED340731.pdf\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKruschke, J. K., & Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4\n\n\nKurz, A. S. (2026a). Statistical rethinking 2 with brms and the tidyverse (version 0.5.0). https://solomon.quarto.pub/sr2/\n\n\nKurz, A. S. (2026b). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0). https://solomon.quarto.pub/sr/\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nPlummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. Proceedings of the 3rd International Workshop on Distributed Statistical Computing, 124, 1–10. http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf\n\n\nPlummer, M. (2012). JAGS Version 3.3.0 user manual. http://www.stat.cmu.edu/~brian/463-663/week10/articles,%20manuals/jags_user_manual.pdf\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (Second Edition). SAGE Publications, Inc. https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230\n\n\nRipley, B. (2022). MASS: Support functions and datasets for venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS\n\n\nRobinson, D., Hayes, A., & Couch, S. (2022). broom: Convert statistical objects into tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom\n\n\nRogosa, D. R., & Willett, J. B. (1985). Understanding correlates of change by modeling individual differences in growth. Psychometrika, 50(2), 203–228. https://doi.org/10.1007/BF02294247\n\n\nRupert G. Miller, Jr. (1997). Beyond ANOVA: Basics of applied statistics. Chapman and Hall/CRC. https://www.routledge.com/Beyond-ANOVA-Basics-of-Applied-Statistics/Jr/p/book/9780412070112\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nStan Development Team. (2021a). Stan reference manual, Version 2.27. https://mc-stan.org/docs/2_27/reference-manual/\n\n\nStan Development Team. (2021b). Stan user’s guide, Version 2.26. https://mc-stan.org/docs/2_26/stan-users-guide/index.html\n\n\nVenables, W. N., & Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "",
    "text": "4.1 Example: Changes in adolescent alcohol use\n“We now delve deeper into the specification, estimation, and interpretation of the multilevel model for change” (Singer & Willett, 2003, p. 75).\nLoad the data.\nlibrary(tidyverse)\n\nalcohol1_pp &lt;- read_csv(\"data/alcohol1_pp.csv\")\n\nhead(alcohol1_pp)\n\n# A tibble: 6 × 9\n     id   age   coa  male age_14 alcuse  peer  cpeer  ccoa\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1    14     1     0      0   1.73 1.26   0.247 0.549\n2     1    15     1     0      1   2    1.26   0.247 0.549\n3     1    16     1     0      2   2    1.26   0.247 0.549\n4     2    14     1     1      0   0    0.894 -0.124 0.549\n5     2    15     1     1      1   0    0.894 -0.124 0.549\n6     2    16     1     1      2   1    0.894 -0.124 0.549\nDo note we already have an \\((\\text{age} - 14)\\) variable in the data, age_14.\nHere’s our version of Figure 4.1, using stat_smooth() to get the exploratory OLS trajectories.\nalcohol1_pp %&gt;%\n  filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;%\n  \n  ggplot(aes(x = age, y = alcuse)) +\n  stat_smooth(method = \"lm\", se = F) +\n  geom_point() +\n  coord_cartesian(xlim = c(13, 17),\n                  ylim = c(-1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id, ncol = 4)\nBy this figure, Singer and Willett suggested the simple linear level-1 submodel following the form\n\\[\\begin{align}\n\\text{alcuse}_{ij} & = \\pi_{0i} + \\pi_{1i} (\\text{age}_{ij} - 14) + \\epsilon_{ij}\\\\\n\\epsilon_{ij}      & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon^2),\n\\end{align}\\]\nwhere \\(\\pi_{0i}\\) is the initial status of participant \\(i\\), \\(\\pi_{1i}\\) is participant \\(i\\)’s rate of change, and \\(\\epsilon_{ij}\\) is the variation in participant \\(i\\)’s data not accounted for in the model.\nSinger and Willett made their Figure 4.2 “with a random sample of 32 of the adolescents” (p. 78). If we just wanted a random sample of rows, the sample_n() function would do the job. But since we’re working with long data, we’ll need some group_by() + nest() mojo. I got the trick from Jenny Bryan’s vignette, Sample from groups, n varies by group. Setting the seed makes the results from sample_n() reproducible. Here are the top panels.\nset.seed(4)\n\nalcohol1_pp %&gt;% \n  group_by(id) %&gt;% \n  nest() %&gt;% \n  sample_n(size = 32, replace = T) %&gt;% \n  unnest(data) %&gt;%\n  mutate(coa = ifelse(coa == 0, \"coa = 0\", \"coa = 1\")) %&gt;%\n\n  ggplot(aes(x = age, y = alcuse, group = id)) +\n  stat_smooth(method = \"lm\", se = F, linewidth = 1/4) +\n  coord_cartesian(xlim = c(13, 17),\n                  ylim = c(-1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ coa)\nWe have similar data wrangling needs for the bottom panels.\nset.seed(4)\n\nalcohol1_pp %&gt;% \n  group_by(id) %&gt;% \n  nest() %&gt;% \n  ungroup() %&gt;% \n  sample_n(size = 32, replace = T) %&gt;% \n  unnest(data) %&gt;%\n  mutate(hp = ifelse(peer &lt; mean(peer), \"low peer\", \"high peer\")) %&gt;%\n  mutate(hp = factor(hp, levels = c(\"low peer\", \"high peer\"))) %&gt;%\n\n  ggplot(aes(x = age, y = alcuse, group = id)) +\n  stat_smooth(method = \"lm\", se = F, linewidth = 1/4) +\n  coord_cartesian(xlim = c(13, 17),\n                  ylim = c(-1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ hp)\nBased on the exploratory analyses, Singer and Willett posited the initial level-2 submodel might take the form\n\\[\\begin{align}\n\\pi_{0i} & = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\zeta_{0i}\\\\\n\\pi_{1i} & = \\gamma_{10} + \\gamma_{11} \\text{coa}_i + \\zeta_{1i} \\\\\n\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\operatorname{Normal} \\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} \\sigma_0^2 & \\sigma_{01}\\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix}\n\\end{pmatrix},\n\\end{align}\\]\nwhere \\(\\gamma_{00}\\) and \\(\\gamma_{10}\\) are the level-2 intercepts, the population averages when \\(\\text{coa} = 0\\), \\(\\gamma_{10}\\) and \\(\\gamma_{11}\\) are the level-2 slopes expressing the difference when \\(\\text{coa} = 1\\) and \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\) are the unexplained variation across the \\(\\text{id}\\)-level intercepts and slopes. Since we’ll be fitting the model with brms::brm(), the \\(\\Sigma\\) matrix will be parameterized in terms of standard deviations and their correlation. So we might re-express the model as\n\\[\\begin{align}\n\\pi_{0i} & = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\zeta_{0i}\\\\\n\\pi_{1i} & = \\gamma_{10} + \\gamma_{11} \\text{coa}_i + \\zeta_{1i} \\\\\n\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\operatorname{Normal} \\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} \\sigma_0 & \\rho_{01}\\\\ \\rho_{01} & \\sigma_1 \\end{bmatrix}\n\\end{pmatrix}.\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#the-composite-specification-of-the-multilevel-model-for-change",
    "href": "04.html#the-composite-specification-of-the-multilevel-model-for-change",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.2 The composite specification of the multilevel model for change",
    "text": "4.2 The composite specification of the multilevel model for change\nWith a little algebra, we can combine the level-1 and level-2 submodels into the composite multilevel model for change, which follows the form\n\\[\\begin{align}\n\\text{alcuse}_{ij} & = \\big [ \\gamma_{00} + \\gamma_{10} \\text{age\\_14}_{ij} + \\gamma_{01} \\text{coa}_i + \\gamma_{11} (\\text{coa}_i \\times \\text{age\\_14}_{ij}) \\big ] \\\\\n& \\;\\;\\;\\;\\; + [ \\zeta_{0i} + \\zeta_{1i} \\text{age\\_14}_{ij} + \\epsilon_{ij} ] \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\\n\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} \\sigma_0^2 & \\sigma_{01} \\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix}\n\\end{pmatrix},\n\\end{align}\\]\nwhere the brackets in the first line partition the structural model (i.e., the model for \\(\\mu\\)) and the stochastic components (i.e., the \\(\\sigma\\) terms). We should note that this is the format that most closely mirrors what we use in the formula argument in brms::brm(). As long as age is not centered on the mean, our brms syntax would be: formula = alcuse ~ 0 + Intercept + age_c + coa + age_c:coa + (1 + age_c | id).\n\n4.2.1 The structural component of the composite model\n\nAlthough their interpretation is identical, the \\(\\gamma\\)s in the composite model describe patterns of change in a different way. Rather than postulating first how ALCUSE is related to TIME and the individual growth parameters, and second how the individual growth parameters are related to COA, the composite specification in equation 4.3 postulates that ALCUSE depends simultaneously on: (1) the level-1 predictor, TIME; (2) the level-2 predictor, COA; and (3) the cross-level interaction, COA by TIME. From this perspective, the composite model’s structural portion strongly resembles a regular regression model with predictors, TIME and COA, appearing as main effects (associated with \\(\\gamma_{10}\\) and \\(\\gamma_{01}\\), respectively) and in a cross-level interaction (associated with \\(\\gamma_{11}\\)). (p. 82, emphasis in the original)\n\n\n\n4.2.2 The stochastic component of the composite model\n\nA distinctive feature of the composite multilevel model is its composite residual, the three terms in the second set of brackets on the right of equation 4.3 that combine together the level-1 residual and the two level-2 residuals:\n\\[\\text{Composite residual: } [ \\zeta_{0i} +  \\zeta_{1i} \\text{age\\_14}_{ij} + \\epsilon_{ij} ].\\] The composite residual is not a simple sum. Instead, the second level-2 residual, \\(\\zeta_{1i}\\), is multiplied by the level-1 predictor, \\([\\text{age\\_14}_{ij}]\\), before joining its siblings. Despite its unusual construction, the interpretation of the composite residual is straightforward: it describes the difference between the observed and expected value of \\([\\text{alcuse}]\\) for individual \\(i\\) on occasion \\(j\\).\nThe mathematical form of the composite residual reveals two important properties about the occasion-specific residuals not readily apparent in the level-1/level-2 specification: they can be both autocorrelated and heteroscedastic within person. (p. 84, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#sec-Methods-of-estimation-revisited",
    "href": "04.html#sec-Methods-of-estimation-revisited",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.3 Methods of estimation, revisited",
    "text": "4.3 Methods of estimation, revisited\nIn this section, the authors introduced generalized least squares (GLS) estimation and iterative generalized least squares (IGLS) estimation and then distinguished between full and restricted maximum likelihood estimation. Since our goal is to fit these models as Bayesians, we won’t be using or discussing any of these in this project. There are, of course, different ways to approach Bayesian estimation. Though we’re using Hamiltonian Monte Carlo, we could use other algorithms, such as the Gibbs sampler. However, all that is outside of the scope of this project.\nI suppose the only thing to add is that whereas GLS estimates come from minimizing a weighted function of the residuals and maximum likelihood estimates come from maximizing the log-likelihood function, the results of our Bayesian analyses (i.e., the posterior distribution) come from the consequences of Bayes’ theorem,\n\\[\np(\\theta \\mid d) = \\frac{p(d \\mid \\theta)\\ p(\\theta)}{p(d)}.\n\\]\nIf you really want to dive into the details of this, I suggest referencing a proper introductory Bayesian textbook, such as McElreath (2015, 2020), Kruschke (2015), or Gelman et al. (2013). I haven’t had time to check it out, but I’ve heard Labmert’s (2018) text is good, too. And for details specific to Stan, and thus brms, you might check out the documentation resources at https://mc-stan.org/users/documentation/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#first-steps-fitting-two-unconditional-multilevel-models-for-change",
    "href": "04.html#first-steps-fitting-two-unconditional-multilevel-models-for-change",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.4 First steps: Fitting two unconditional multilevel models for change",
    "text": "4.4 First steps: Fitting two unconditional multilevel models for change\nSinger and Willett recommended that before you fit your full theoretical multilevel model of change–the one with all the interesting covariates–you should fit two simpler preliminary models. The first is the unconditional means model. The second is the unconditional growth model.\nI agree. In addition to the reasons they cover in the text, this is just good pragmatic data analysis. Start simple and build up to the more complicated models only after you’re confident you understand what’s going on with the simpler ones. And if you’re new to them, you’ll discover this is especially so with Bayesian methods.\n\n4.4.1 The unconditional means model\nThe likelihood for the unconditional means model follows the formula\n\\[\n\\begin{align}\n\\text{alcuse}_{ij} & =  \\gamma_{00} +  \\zeta_{0i} + \\epsilon_{ij} \\\\\n\\epsilon_{ij}      & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon^2) \\\\\n\\zeta_{0i}         & \\sim \\operatorname{Normal}(0, \\sigma_0^2).\n\\end{align}\n\\]\nLet’s open brms.\n\nlibrary(brms)\n\nUp till this point, we haven’t focused on priors. It would have been reasonable to wonder if we’d been using them at all. Yes, we have. Even if you don’t specify priors in the brm() function, it’ll compute default weakly-informative priors for you. You might be wondering, What might these default priors look like? The get_prior() function let us take a look.\n\nget_prior(data = alcohol1_pp, \n          family = gaussian,\n          alcuse ~ 1 + (1 | id))\n\n                prior     class      coef group resp dpar nlpar lb ub tag       source\n student_t(3, 1, 2.5) Intercept                                                default\n student_t(3, 0, 2.5)        sd                                  0             default\n student_t(3, 0, 2.5)        sd              id                  0        (vectorized)\n student_t(3, 0, 2.5)        sd Intercept    id                  0        (vectorized)\n student_t(3, 0, 2.5)     sigma                                  0             default\n\n\nFor this model, all three priors are based on Student’s \\(t\\)-distribution. In case you’re rusty, the normal distribution is just a special case of Student’s \\(t\\)-distribution. Whereas the normal is defined by two parameters (\\(\\mu\\) and \\(\\sigma\\)), the \\(t\\) distribution is defined by \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\). In frequentist circles, \\(\\nu\\) is often called the degrees of freedom. More generally, it’s also referred to as a normality parameter. We’ll examine the prior more closely in a bit.\nFor now, let’s practice setting our priors by manually specifying them within brm(). You do with the prior argument. There are actually several ways to do this. To explore all the options, check out the set_prior section of the brms reference manual (Bürkner, 2021). I typically define my individual priors with the prior() function. When there are more than one priors to define, I typically bind them together within c(...).\nOther than the addition of our fancy prior statement, the rest of the settings within brm() are much like those in prior chapters. Let’s fit the model.\n\nfit4.1 &lt;- brm(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 1 + (1 | id),\n  prior = c(prior(student_t(3, 1, 2.5), class = Intercept),\n            prior(student_t(3, 0, 2.5), class = sd),\n            prior(student_t(3, 0, 2.5), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.01\")\n\nHere are the results.\n\nprint(fit4.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: alcuse ~ 1 + (1 | id) \n   Data: alcohol1_pp (Number of observations: 246) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 82) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.77      0.08     0.62     0.94 1.00     1435     1909\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.92      0.10     0.73     1.11 1.00     2339     2661\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.76      0.04     0.68     0.84 1.00     3111     3184\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompare the results to those listed under “Model A” in Table 4.1. It’s important to keep in mind that brms returns ‘sigma’ and ‘sd(Intercept)’ in the standard deviation metric rather than the variance metric. “But I want them in the variance metric like in the text!”, you say. Okay fine. The best way to do the transformations is after saving the results from as_draws_df().\n\ndraws &lt;- as_draws_df(fit4.1)\n\n# first 12 columns\nglimpse(draws[, 1:12])\n\nRows: 4,000\nColumns: 12\n$ b_Intercept         &lt;dbl&gt; 0.9260752, 0.8988015, 0.7792639, 0.9267970, 1.0072838, 1.0562188, 1.0780959, 0.9407926, 0.8587646, 1…\n$ sd_id__Intercept    &lt;dbl&gt; 0.6292301, 0.8677994, 0.7717634, 0.7467758, 0.8470288, 0.7486265, 0.8325376, 0.8884310, 0.7528115, 0…\n$ sigma               &lt;dbl&gt; 0.7599271, 0.8144932, 0.7414686, 0.7462817, 0.7440307, 0.7267132, 0.7605753, 0.7747775, 0.7484712, 0…\n$ Intercept           &lt;dbl&gt; 0.9260752, 0.8988015, 0.7792639, 0.9267970, 1.0072838, 1.0562188, 1.0780959, 0.9407926, 0.8587646, 1…\n$ `r_id[1,Intercept]` &lt;dbl&gt; 0.56563626, 1.05765351, 0.32009632, 1.01302426, 1.12657098, 0.73703012, 0.61667144, 0.70950858, 0.57…\n$ `r_id[2,Intercept]` &lt;dbl&gt; 0.14035689, -1.04326905, 0.20125180, -1.04240101, -0.96516047, -0.38975280, -0.32878521, -0.64058597…\n$ `r_id[3,Intercept]` &lt;dbl&gt; 0.71379397, 1.08655936, 0.46016312, 1.20593498, 1.02992133, 0.53444476, 0.85255220, 0.72442600, 1.22…\n$ `r_id[4,Intercept]` &lt;dbl&gt; 0.65146382, -0.18945701, 0.36695755, 0.12656534, 0.44032227, -0.52046316, -0.32618081, 0.54323995, -…\n$ `r_id[5,Intercept]` &lt;dbl&gt; -0.57232307, -0.71108942, -1.13825886, -0.28109246, 0.03749920, -0.08587860, -0.85565527, -0.3308069…\n$ `r_id[6,Intercept]` &lt;dbl&gt; 1.2239650, 1.9661097, 1.6771725, 1.4380018, 1.5282743, 1.9311362, 1.4623721, 1.2316318, 1.3831150, 1…\n$ `r_id[7,Intercept]` &lt;dbl&gt; 0.4836527, 0.8445632, 0.1372376, 0.9000158, 1.1413053, 0.7295728, 0.4994844, 0.6001993, 0.8965447, 0…\n$ `r_id[8,Intercept]` &lt;dbl&gt; -0.3678699, -0.8601715, -0.3067060, -1.0642781, -1.0739491, -0.6637780, -0.4680221, -1.0934336, -0.6…\n\n\nSince all we’re interested in are the variance components, we’ll select() out the relevant columns from draws, compute the squared versions, and save the results in a mini data frame, v.\n\nv &lt;- draws %&gt;% \n  select(sigma, sd_id__Intercept) %&gt;% \n  mutate(sigma_2_epsilon = sigma^2,\n         sigma_2_0       = sd_id__Intercept^2)\n\nhead(v)\n\n# A tibble: 6 × 4\n  sigma sd_id__Intercept sigma_2_epsilon sigma_2_0\n  &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 0.760            0.629           0.577     0.396\n2 0.814            0.868           0.663     0.753\n3 0.741            0.772           0.550     0.596\n4 0.746            0.747           0.557     0.558\n5 0.744            0.847           0.554     0.717\n6 0.727            0.749           0.528     0.560\n\n\nWe can view their distributions like this.\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_vline(xintercept = c(0.25, 0.5, 0.75, 1), color = \"white\") +\n  geom_density(size = 0, fill = \"black\") +\n  scale_x_continuous(NULL, limits = c(0, 1.25),\n                     breaks = seq(from = 0, to = 1.25, by = 0.25)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\n\n\n\n\n\nIn case it’s hard to follow what just happened, the estimates in the brms-default standard-deviation metric are the two panels on the top. Those on the bottom are in the Singer-and-Willett style variance metric. Like we discussed toward the end of last chapter, the variance parameters won’t often be Gaussian. In my experience, they’re typically skewed to the right. There’s nothing wrong with that. This is a recurrent pattern among distributions that are constrained to be zero and above.\nIf you’re interested, you can summarize those posteriors like so.\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean   = mean(value),\n            median = median(value),\n            sd     = sd(value),\n            ll     = quantile(value, prob = 0.025),\n            ul     = quantile(value, prob = 0.975)) %&gt;% \n  # this last bit just rounds the output\n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 4 × 6\n  name              mean median    sd    ll    ul\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 sd_id__Intercept 0.766  0.76  0.083 0.617 0.942\n2 sigma            0.756  0.754 0.042 0.678 0.844\n3 sigma_2_0        0.594  0.577 0.13  0.381 0.887\n4 sigma_2_epsilon  0.573  0.569 0.065 0.46  0.713\n\n\nFor this model, our posterior medians are closer to the estimates in the text (Table 4.1) than the means. However, our posterior standard deviations are pretty close to the standard errors in the text.\nOne of the advantages of our Bayesian method is that when we compute something like the intraclass correlation coefficient \\(\\rho\\), we get an entire distribution for the parameter rather than a measly point estimates. This is always the case with Bayes. The algebraic transformations of the posterior distribution are themselves distributions. Before we compute \\(\\rho\\), do pay close attention to the formula,\n\\[\n\\rho = \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_\\epsilon^2}.\n\\]\nEven though our brms output yields the variance parameters in the standard-deviation metric, the formula for \\(\\rho\\) demands we use variances. That’s nothing a little squaring can’t fix. Here’s what our \\(\\rho\\) looks like.\n\nv %&gt;%\n  transmute(rho = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% \n  \n  ggplot(aes(x = rho)) +\n  geom_density(size = 0, fill = \"black\") +\n  scale_x_continuous(expression(rho), limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThough the posterior for \\(\\rho\\) is indeed centered around .5, look at how wide and uncertain that distribution is. The bulk of the posterior mass takes up almost half of the parameter space. If you wanted the summary statistics, you might do what we did for the variance parameters, above.\n\nv %&gt;%\n  transmute(rho = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% \n  summarise(mean   = mean(rho),\n            median = median(rho),\n            sd     = sd(rho),\n            ll     = quantile(rho, prob = 0.025),\n            ul     = quantile(rho, prob = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 1 × 5\n   mean median    sd    ll    ul\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.505  0.506 0.063  0.38 0.624\n\n\nConcerning \\(\\rho\\), Singer and Willett pointed out\n\nit summarizes the size of the residual autocorrelation in the composite unconditional means mode….\nEach person has a different composite residual on each occasion of measurement. But notice the difference in the subscripts of the pieces of the composite residual: while the level-1 residual, \\(\\epsilon_{ij}\\) has two subscripts (\\(i\\) and \\(j\\)), the level-2 residual, \\(\\zeta_{0i}\\), has only one (\\(i\\)). Each person can have a different \\(\\epsilon_{ij}\\) on each occasion, but has only one \\(\\zeta_{0i}\\) across every occasion. The repeated presence of \\(\\zeta_{0i}\\) in individual \\(i\\)’s composite residual links his or her composite residuals across occasions. The error autocorrelation coefficient quantifies the magnitude of this linkage; in the unconditional means model, the error autocorrelation coefficient is the intraclass correlation coefficient. Thus, we estimate that, for each person, the average correlation between any pair of composite residuals–between occasions 1 and 2, or 2 and 3, or 1 and 3–is [.5]. (pp. 96–97, emphasis in the original)\n\nBecause of the differences in how they’re estimated with and presented by brm(), we focused right on the variance components. But before we move on to the next section, we should back up a bit. On page 93, Singer and Willett discussed their estimate for \\(\\gamma_{00}\\). Here’s ours.\n\nfixef(fit4.1)\n\n           Estimate  Est.Error     Q2.5    Q97.5\nIntercept 0.9237214 0.09789998 0.734189 1.114758\n\n\nThey talked about how squaring that value puts it back to the natural metric the data were originally collected in. [Recall that as discussed earlier in the text the alcuse variable was square-root transformed because of excessive skew.] If you want a quick and dirty look, you can square our results, too.\n\nfixef(fit4.1)^2 \n\n           Estimate   Est.Error      Q2.5    Q97.5\nIntercept 0.8532612 0.009584406 0.5390335 1.242685\n\n\nHowever, I do not recommend this method. Though it did okay at transforming the posterior mean (i.e., Estimate), it’s not a great way to get the summary statistics correct. To do that, you’ll need to work with the posterior samples themselves. Remember how we saved them as draws? Let’s refresh ourselves and look at the first few columns.\n\ndraws %&gt;% \n  select(b_Intercept:sigma) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  b_Intercept sd_id__Intercept sigma\n        &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1       0.926            0.629 0.760\n2       0.899            0.868 0.814\n3       0.779            0.772 0.741\n4       0.927            0.747 0.746\n5       1.01             0.847 0.744\n6       1.06             0.749 0.727\n\n\nSee that b_Intercept column there? That contains our posterior draws from \\(\\gamma_{00}\\). If you want proper summary statistics from the transformed estimate, get them after transforming that column.\n\ndraws %&gt;% \n  transmute(gamma_00_squared = b_Intercept^2) %&gt;% \n  summarise(mean   = mean(gamma_00_squared),\n            median = median(gamma_00_squared),\n            sd     = sd(gamma_00_squared),\n            ll     = quantile(gamma_00_squared, prob = 0.025),\n            ul     = quantile(gamma_00_squared, prob = 0.975)) %&gt;%\n  mutate_if(is.double, round, digits = 3) %&gt;% \n  pivot_longer(everything())\n\n# A tibble: 5 × 2\n  name   value\n  &lt;chr&gt;  &lt;dbl&gt;\n1 mean   0.863\n2 median 0.855\n3 sd     0.181\n4 ll     0.539\n5 ul     1.24 \n\n\nAnd one last bit before we move on to the next section. Remember how we discovered what the brm() default priors were for our model with the handy get_prior() function? Let’s refresh ourselves on how that worked.\n\nget_prior(data = alcohol1_pp, \n          family = gaussian,\n          alcuse ~ 1 + (1 | id))\n\n                prior     class      coef group resp dpar nlpar lb ub tag       source\n student_t(3, 1, 2.5) Intercept                                                default\n student_t(3, 0, 2.5)        sd                                  0             default\n student_t(3, 0, 2.5)        sd              id                  0        (vectorized)\n student_t(3, 0, 2.5)        sd Intercept    id                  0        (vectorized)\n student_t(3, 0, 2.5)     sigma                                  0             default\n\n\nWe inserted the data and the model and get_prior() returned the default priors. Especially for new Bayesians, or even for experienced Bayesians working with unfamiliar models, it can be handy to plot your priors to get a sense of them.\nBase R has an array of functions based on the \\(t\\) distribution (e.g., rt(), dt()). These functions are limited in that while they allow users to select the desired \\(\\nu\\) values (i.e., degrees of freedom), they fix \\(\\mu = 0\\) and \\(\\sigma = 1\\). If you want to stick with the base R functions, you can find tricky ways around this. To avoid overwhelming anyone new to Bayes or the multilevel model or R or some exasperating combination, let’s just make things simpler and use a couple convenience functions from the ggdist package (Kay, 2021).\nWe’ll start with the default intercept prior, \\(t(\\nu = 3, \\mu = 1, \\sigma = 2.5)\\). Here’s the density in the range \\([-20, 20]\\).\n\nlibrary(ggdist)\n\nprior(student_t(3, 1, 2.5)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.95, p_limits = c(0.001, 0.999)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = expression(paste(\"prior for \", gamma[0][0])),\n       x = \"parameter space\") +\n  theme(panel.grid = element_blank()) +\n  coord_cartesian(xlim = c(-20, 20))\n\n\n\n\n\n\n\n\nThough it’s centered on 1, the inner 95% of the density is well between -10 and 10. Given the model estimate ended up about 0.9, it looks like that was a pretty broad and minimally-informative prior. However, the prior isn’t flat and it does help guard against wasting time and HMC iterations sampling from ridiculous regions of the parameter space such as -10,000 or +500,000,000. No adolescent is drinking that much (or that little–how does one drink a negative value?).\nHere’s the shape of the variance priors.\n\nprior(student_t(3, 0, 2.5), lb = 0) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.95, p_limits = c(0.001, 0.999)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = expression(paste(\"prior for both \", sigma[0], \" and \", sigma[epsilon])),\n       x = \"parameter space\") +\n  coord_cartesian(xlim = c(0, 20)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nRecall that by brms default, the variance parameters have a lower-limit of 0. So specifying a Student’s \\(t\\) or other Gaussian-like prior on them ends up cutting the distribution off at 0. Given that our estimates were both below 1, it appears that these priors were minimally informative. But again, they did help prevent brm() from sampling from negative values or from obscenely-large values.\nThese priors look kinda silly, you might say. Anyone with a little common sense can do better. Well, sure. Probably. Maybe. But keep in mind we’re still getting the layout of the land. And plus, this was a pretty simple model. Selecting high-quality priors gets tricky as the models get more complicated. In other chapters, we’ll explore other ways to specify priors for our multilevel models. But to keep things simple for now, let’s keep practicing inspecting and using the defaults with get_prior() and so on.\n\n\n4.4.2 The unconditional growth model\nUsing the composite formula, our next model, the unconditional growth model, follows the form\n\\[\n\\begin{align}\n\\text{alcuse}_{ij} & = \\gamma_{00} + \\gamma_{10} \\text{age\\_14}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{age\\_14}_{ij} + \\epsilon_{ij} \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon^2) \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} \\sigma_0^2 & \\sigma_{01} \\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix}\n\\end{pmatrix}.\n\\end{align}\n\\]\nWith it, we now have a full composite stochastic model. Let’s query the brms::brm() default priors when we apply this model to our data.\n\nget_prior(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id))\n\n                prior class      coef group resp dpar nlpar lb ub tag       source\n               (flat)     b                                                default\n               (flat)     b    age_14                                 (vectorized)\n               (flat)     b Intercept                                 (vectorized)\n               lkj(1)   cor                                                default\n               lkj(1)   cor              id                           (vectorized)\n student_t(3, 0, 2.5)    sd                                  0             default\n student_t(3, 0, 2.5)    sd              id                  0        (vectorized)\n student_t(3, 0, 2.5)    sd    age_14    id                  0        (vectorized)\n student_t(3, 0, 2.5)    sd Intercept    id                  0        (vectorized)\n student_t(3, 0, 2.5) sigma                                  0             default\n\n\nSeveral things of note: First, notice how we continue to use the student_t(3, 0, 2.5) for all three of our standard-deviation-metric variance parameters. Since we’re now estimating \\(\\sigma_0\\) and \\(\\sigma_1\\), which themselves have a correlation, \\(\\rho_{01}\\), we have a prior of class = cor. I’m going to put off what is meant by the name lkj, but for the moment just realize that this prior is essentially noninformative within this context.\nThere’s a major odd development with this output. Notice how the prior column is (flat) for the rows for our two coefficients of class b. And if you’re a little confused, recall that because our predictor age_14 is not mean-centered, we’ve used the 0 + Intercept syntax, which switches the model intercept parameter to the class of b. From the set_prior section of the reference manual for brms version 2.12.0, we read: “The default prior for population-level effects (including monotonic and category specific effects) is an improper flat prior over the reals” (p. 179). At present, these priors are uniform across the entire parameter space. They’re not just weak, their entirely noninformative. That is, the likelihood dominates the posterior for those parameters.\nHere’s how to fit the model with these priors.\n\nfit4.2 &lt;- brm(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id),\n  prior = c(prior(student_t(3, 0, 2.5), class = sd),\n            prior(student_t(3, 0, 2.5), class = sigma),\n            prior(lkj(1), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  control = list(adapt_delta = 0.9),\n  file = \"fits/fit04.02\")\n\nHow did we do?\n\nprint(fit4.2, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: alcuse ~ 0 + Intercept + age_14 + (1 + age_14 | id) \n   Data: alcohol1_pp (Number of observations: 246) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 82) \n                      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.790     0.104    0.596    1.005 1.003      804     1329\nsd(age_14)               0.360     0.097    0.121    0.528 1.014      241      255\ncor(Intercept,age_14)   -0.108     0.276   -0.506    0.643 1.007      434      316\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.649     0.108    0.437    0.855 1.001     1744     2522\nage_14       0.272     0.064    0.146    0.397 1.000     3580     2964\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.607     0.053    0.514    0.720 1.008      396      587\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf your compare our results with those in the “Model B” column in Table 4.1, you’ll see our summary results match well with those in the text. Our \\(\\gamma\\)’s (i.e., ‘Population-Level Effects:’) are near identical. The leftmost panel in Figure 4.3 shows the prototypical trajectory, based on the \\(\\gamma\\)s. A quick way to get that within our brms framework is with the conditional_effects() function. Here’s the default output.\n\nconditional_effects(fit4.2)\n\n\n\n\n\n\n\n\nStaying with conditional_effects() allows users some flexibility for customizing the plot(s). For example, the default behavior is to depict the trajectory in terms of its 95% intervals and posterior median. If you’d prefer the 80% intervals and the posterior mean, customize it like so.\n\nconditional_effects(fit4.2,\n                    robust = F,\n                    prob = 0.8)\n\n\n\n\n\n\n\n\nWe’ll explore more options with brms::conditional_effects() with Model C. For now, let’s turn our focus on the stochastic elements in the model. Here we extract the posterior samples and do the conversions to see how they compare with Singer and Willett’s.\n\ndraws &lt;- as_draws_df(fit4.2)\n\nv &lt;- draws %&gt;% \n  transmute(sigma_2_epsilon = sigma^2,\n            sigma_2_0       = sd_id__Intercept^2,\n            sigma_2_1       = sd_id__age_14^2,\n            sigma_01        = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14)\n\nhead(v)\n\n# A tibble: 6 × 4\n  sigma_2_epsilon sigma_2_0 sigma_2_1 sigma_01\n            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1           0.282     0.834     0.289   -0.220\n2           0.336     0.772     0.263   -0.114\n3           0.364     0.725     0.234   -0.173\n4           0.269     0.889     0.250   -0.147\n5           0.343     0.558     0.291   -0.124\n6           0.342     1.04      0.178   -0.187\n\n\nThis time, our v object only contains the stochastic components in the variance metric. Let’s plot.\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density(size = 0, fill = \"black\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nFor each, their posterior mass is centered near the point estimates Singer and Willet reported in the text. Here are the summary statistics.\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean   = mean(value),\n            median = median(value),\n            sd     = sd(value),\n            ll     = quantile(value, prob = 0.025),\n            ul     = quantile(value, prob = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 4 × 6\n  name              mean median    sd     ll    ul\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 sigma_01        -0.049 -0.043 0.076 -0.217 0.08 \n2 sigma_2_0        0.635  0.618 0.167  0.355 1.01 \n3 sigma_2_1        0.139  0.137 0.065  0.015 0.279\n4 sigma_2_epsilon  0.371  0.363 0.066  0.264 0.519\n\n\nHappily, they’re quite comparable to those in the text.\nWe’ve been pulling the posterior samples for all parameters with as_draws_df() and subsetting to a few variables of interest, such as the variance parameters. But it our primary interest is just the iterations for the variance parameters, we can extract them in a more focused way with the VarCorr() function. Here’s how we’d do so for fit4.2.\n\nVarCorr(fit4.2, summary = F) %&gt;% \n  str()\n\nList of 2\n $ id        :List of 3\n  ..$ sd : num [1:4000, 1:2] 0.913 0.879 0.852 0.943 0.747 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ draw    : chr [1:4000] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ variable: chr [1:2] \"Intercept\" \"age_14\"\n  .. ..- attr(*, \"nchains\")= int 4\n  ..$ cor: num [1:4000, 1:2, 1:2] 1 1 1 1 1 1 1 1 1 1 ...\n  .. ..- attr(*, \"dimnames\")=List of 3\n  .. .. ..$ : NULL\n  .. .. ..$ : chr [1:2] \"Intercept\" \"age_14\"\n  .. .. ..$ : chr [1:2] \"Intercept\" \"age_14\"\n  ..$ cov: num [1:4000, 1:2, 1:2] 0.834 0.772 0.725 0.889 0.558 ...\n  .. ..- attr(*, \"dimnames\")=List of 3\n  .. .. ..$ : NULL\n  .. .. ..$ : chr [1:2] \"Intercept\" \"age_14\"\n  .. .. ..$ : chr [1:2] \"Intercept\" \"age_14\"\n $ residual__:List of 1\n  ..$ sd: num [1:4000, 1] 0.531 0.579 0.603 0.518 0.586 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ draw    : chr [1:4000] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ variable: chr \"\"\n  .. ..- attr(*, \"nchains\")= int 4\n\n\nThat last part, the contents of the second higher-level list indexed by $ residual, contains the contents for \\(\\sigma_\\epsilon\\). On page 100 in the text, Singer and Willett compared \\(\\sigma_\\epsilon^2\\) from the first model to that from the second. We might do that like so.\n\ncbind(VarCorr(fit4.1, summary = F)[[2]][[1]],\n      VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% \n  data.frame() %&gt;% \n  mutate_all(~.^2) %&gt;% \n  set_names(str_c(\"fit4.\", 1:2)) %&gt;% \n  mutate(`fit4.1 - fit4.2` = fit4.1 - fit4.2) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name, levels = c(\"fit4.1\", \"fit4.2\", \"fit4.1 - fit4.2\"))) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_vline(xintercept = 0.5, color = \"white\") +\n  geom_density(fill = \"grey25\", color = \"transparent\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(sigma[epsilon]^2)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\", ncol = 3)\n\n\n\n\n\n\n\n\nTo compute a formal summary of the decline in \\(\\sigma_\\epsilon^2\\) after adding time to the model, we might summarize like before.\n\ncbind(VarCorr(fit4.1, summary = F)[[2]][[1]],\n      VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% \n  data.frame() %&gt;% \n  mutate_all(~ .^2) %&gt;% \n  set_names(str_c(\"fit4.\", 1:2)) %&gt;% \n  mutate(proportion_decline = (fit4.1 - fit4.2) / fit4.1) %&gt;% \n  summarise(mean   = mean(proportion_decline),\n            median = median(proportion_decline),\n            sd     = sd(proportion_decline),\n            ll     = quantile(proportion_decline, prob = 0.025),\n            ul     = quantile(proportion_decline, prob = 0.975)) %&gt;%\n  mutate_if(is.double, round, digits = 3)\n\n   mean median    sd    ll   ul\n1 0.345   0.36 0.139 0.024 0.57\n\n\nIn case it wasn’t clear, when we presented fit4.1 – fit4.2 in the density plot, that was a simple difference score. However, we computed proportion_decline above by dividing that difference score by fit4.1; that’s what put the difference in a proportion metric. Anyway, Singer and Willett’s method led them to summarize the decline as .40. Our method was a more conservative .34-ish. And very happily, our method allows us to describe the proportion decline with summary statistics for the full posterior, such as with the \\(\\textit{SD}\\) and the 95% intervals.\n\ndraws %&gt;% \n  ggplot(aes(x = cor_id__Intercept__age_14)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  geom_density(fill = \"grey25\", color = \"transparent\") +\n  scale_x_continuous(expression(rho[0][1]), limits = c(-1, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe estimate Singer and Willett hand-computed in the text, -.22, is near the mean of our posterior distribution for \\(\\rho_{01}\\). However, our distribution provides a full expression of the uncertainty in the parameter. As are many other values within the parameter space, zero is indeed a credible value for \\(\\rho_{01}\\).\nOn page 101, we get the generic formula for computing the residual variance for a given occasion \\(j\\),\n\\[\n\\sigma_{\\text{Residual}_j}^2 = \\sigma_0^2 + \\sigma_1^2 \\text{time}_j + 2 \\sigma_{01} \\text{time}_j + \\sigma_\\epsilon^2.\n\\]\nIf we were just interested in applying it to one of our age values, say 14, we might apply the formula to the posterior like this.\n\ndraws %&gt;% \n  transmute(sigma_2_residual_j = sd_id__Intercept^2 + \n              sd_id__age_14^2 * 0 + \n              2 * sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * 0 + \n              sigma^2) %&gt;% \n  head()\n\n# A tibble: 6 × 1\n  sigma_2_residual_j\n               &lt;dbl&gt;\n1              1.12 \n2              1.11 \n3              1.09 \n4              1.16 \n5              0.901\n6              1.38 \n\n\nBut given we’d like to do so over several values of age, it might be better to wrap the equation in a custom function. Let’s call it make_s2rj().\n\nmake_s2rj &lt;- function(x) {\n  draws %&gt;% \n    transmute(sigma_2_residual_j = sd_id__Intercept^2 + sd_id__age_14^2 * x + 2 * sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * x + sigma^2) %&gt;% \n    pull()\n}\n\nNow we can put our custom make_s2rj() function to work within the purrr::map() paradigm. We’ll plot the results.\n\ntibble(age = 14:16) %&gt;% \n  mutate(age_c = age - 14) %&gt;% \n  mutate(s2rj = map(age_c, make_s2rj)) %&gt;% \n  unnest(s2rj) %&gt;% \n  mutate(label = str_c(\"age = \", age)) %&gt;% \n  \n  ggplot(aes(x = s2rj)) +\n  geom_density(fill = \"grey25\", color = \"transparent\") +\n  # just for reference \n  geom_vline(xintercept = 1, color = \"grey92\", linetype = 2) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Behold the shape of longitudinal heteroscedasticity.\",\n       x = expression(sigma[italic(Residual[j])]^2)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ label, scales = \"free_y\", ncol = 1)\n\n\n\n\n\n\n\n\nWe see a subtle increase over time, particularly from age = 15 to age = 16. Yep, that’s heteroscedasticity. It is indeed “beyond the bland homoscedasticity we assume of residuals in cross-sectional data” (p. 101).\nWe might also be interested in computing the autocorrelation between the composite residuals on occasions \\(j\\) and \\(j'\\), which follows the formula\n\\[\n\\rho_{\\text{Residual}_j, \\text{Residual}_{j'}} = \\frac{\\sigma_0^2 + \\sigma_{01} (\\text{time}_j + \\text{time}_{j'}) + \\sigma_1^2 \\text{time}_j \\text{time}_{j'}} {\\sqrt{\\sigma_{\\text{Residual}_j}^2 \\sigma_{\\text{Residual}_{j'}}^2 }}.\n\\]\nWe only want to do that by hand once. Let’s make a custom function following the formula.\n\n make_rho_rj_rjp &lt;- function(j, jp) {\n  \n  # define the elements in the denominator  \n  s2rj_j  &lt;- make_s2rj(j)\n  s2rj_jp &lt;- make_s2rj(jp)\n  \n  # compute\n  draws %&gt;% \n    transmute(r = (sd_id__Intercept^2 + \n                     sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14 * (j + jp) + \n                     sd_id__age_14^2 * j * jp) /\n                sqrt(s2rj_j * s2rj_jp)) %&gt;% \n    pull()\n}\n\nIf you only cared about measures of central tendency, such as the posterior median, you could use the function like this.\n\nmake_rho_rj_rjp(0, 1) %&gt;% median()\n\n[1] 0.5688327\n\nmake_rho_rj_rjp(1, 2) %&gt;% median()\n\n[1] 0.7217349\n\nmake_rho_rj_rjp(0, 2) %&gt;% median()\n\n[1] 0.5158972\n\n\nHere are the full posteriors.\n\ntibble(occasion = 1:3) %&gt;% \n  mutate(age_c = occasion - 1,\n         j     = c(1, 2, 1) - 1,\n         jp    = c(2, 3, 3) - 1) %&gt;% \n  mutate(r = map2(j, jp, make_rho_rj_rjp)) %&gt;% \n  unnest(r) %&gt;% \n  mutate(label = str_c(\"occasions \", j + 1, \" and \", jp + 1)) %&gt;% \n  \n  ggplot(aes(x = r)) +\n  # just for reference\n  geom_vline(xintercept = c(0.5, 0.75), color = \"white\") +\n  geom_density(fill = \"grey25\", color = \"transparent\") +\n  scale_x_continuous(expression(rho[Residual[italic(j)]][Residual[italic(j*minute)]]), limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Behold the shapes of our autocorrelations!\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ label, scales = \"free_y\", ncol = 1)\n\n\n\n\n\n\n\n\n\n\n4.4.3 Quantifying the proportion of outcome variation “explained”\nBecause of the way the multilevel model partitions off variance into different sources (e.g., \\(\\sigma_0^2\\), \\(\\sigma_1^2\\), and \\(\\sigma_\\epsilon^2\\) in the unconditional growth model), the conventional \\(R^2\\) is not applicable for evaluating models in the traditional OLS sense of percent of variance explained. Several pseudo \\(R^2\\) statistics are frequently used instead. Be warned, “statisticians have yet to agree on appropriate summaries (Kreft & de Leeuw, 1998; Snijders & Bosker, 1994)” (p. 102). See also Jaeger et al. (2017), Jason D. Rights & Cole (2018), and Jason D. Rights & Sterba (2020). To my eye, none of the solutions presented in this section are magic bullets.\n\n4.4.3.1 An overall summary of total outcome variability explained\n\nIn multiple regression, one simple way of computing a summary \\(R^2\\) statistic is to square the sample correlation between observed and predicted values of the outcome. The same approach can be used in the multilevel model for change. All you need to do is: (1) compute the predicted outcome value for each person on each occasion of measurement; and (2) square the sample correlation between observed and predicted values. The resultant pseudo-\\(R^2\\) statistic assesses the proportion of total outcome variation “explained” by the multilevel model’s specific contribution of predictors. (p. 102, emphasis added)\n\nSinger and Willett called this \\(R_{y, \\hat y}^2\\). They then walked through an example with their Model B (fit4.2), the unconditional growth model. Within our brms paradigm, we typically use the fitted() function to return predicted outcome values for cases within the data. The default option for the fitted() function is to return these predictions after accounting for the level-2 clustering. As we will see, Singer and Willett’s \\(R_{y, \\hat y}^2\\) statistic only accounts for predictors (i.e., age_14, in this case), not clustering variables (i.e., id, in this case). To follow Singer and Willett’s specification, we need to set re_formula = NA, which will instruct fitted() to return the expected values without reference to the level-2 clustering. Here’s a look at the first six rows of that output.\n\nfitted(fit4.2, re_formula = NA) %&gt;% \n  head()\n\n      Estimate  Est.Error      Q2.5     Q97.5\n[1,] 0.6486449 0.10802024 0.4374269 0.8552847\n[2,] 0.9205147 0.09986409 0.7215163 1.1174546\n[3,] 1.1923845 0.12800296 0.9349210 1.4442575\n[4,] 0.6486449 0.10802024 0.4374269 0.8552847\n[5,] 0.9205147 0.09986409 0.7215163 1.1174546\n[6,] 1.1923845 0.12800296 0.9349210 1.4442575\n\n\nWithin our Bayesian/brms paradigm, out expected values come with expressions of uncertainty in terms of the posterior standard deviation and percentile-based 95% intervals. If we followed Singer and Willett’s method in the text, we’d only work with the posterior means as presented within the Estimate column. But since we’re Bayesians, we should attempt to work with the model uncertainty. One approach is to set summary = F.\n\nf &lt;- fitted(fit4.2,\n            summary = F,\n            re_formula = NA) %&gt;%\n  data.frame() %&gt;% \n  set_names(1:ncol(.)) %&gt;% \n  rownames_to_column(\"draw\")\n\ndim(f)\n\n[1] 4000  247\n\n\nWith those settings, fitted() returned a \\(4,000 \\times 246\\) numeric array. The 4,000 rows corresponded to the 4,000 post-warmup HMC draws. Each of the 246 columns corresponded to one of the 246 rows in the original alcohol1_pp data. To make the output more useful, we converted it to a data frame, named the columns by the row numbers corresponding to the original alcohol1_pp data, and converted the row names to an draw column.\nIn the next code block, we’ll convert f to the long format and use left_join() to join it with the relevant subset of the alcohol1_pp data.\n\nf &lt;- f %&gt;% \n  pivot_longer(-draw,\n               names_to = \"row\",\n               values_to = \"fitted\") %&gt;% \n  mutate(row = row %&gt;% as.integer()) %&gt;% \n  left_join(\n    alcohol1_pp %&gt;% \n      mutate(row = 1:n()) %&gt;% \n      select(row, alcuse),\n    by = \"row\") \n\nf\n\n# A tibble: 984,000 × 4\n   draw    row fitted alcuse\n   &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1         1  0.758   1.73\n 2 1         2  0.945   2   \n 3 1         3  1.13    2   \n 4 1         4  0.758   0   \n 5 1         5  0.945   0   \n 6 1         6  1.13    1   \n 7 1         7  0.758   1   \n 8 1         8  0.945   2   \n 9 1         9  1.13    3.32\n10 1        10  0.758   0   \n# ℹ 983,990 more rows\n\n\nIf we collapse the distinction across the 4,000 HMC draws, here is the squared correlation between fitted and alcuse.\n\nf %&gt;% \n  summarise(r  = cor(fitted, alcuse),\n            r2 = cor(fitted, alcuse)^2)\n\n# A tibble: 1 × 2\n      r     r2\n  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.186 0.0345\n\n\nThis is close to the \\(R_{y, \\hat y}^2 = .043\\) Singer and Willett reported in the text. It might seem unsatisfying how this seemingly ignores model uncertainty by collapsing across HMC draws. Here’s a look at what happens is we compute the \\(R_{y, \\hat y}^2\\) separately for each iteration.\n\nf %&gt;% \n  mutate(draw = draw %&gt;% as.double()) %&gt;% \n  group_by(draw) %&gt;% \n  summarise(r  = cor(fitted, alcuse),\n            r2 = cor(fitted, alcuse)^2)\n\n# A tibble: 4,000 × 3\n    draw     r     r2\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 0.208 0.0434\n 2     2 0.208 0.0434\n 3     3 0.208 0.0434\n 4     4 0.208 0.0434\n 5     5 0.208 0.0434\n 6     6 0.208 0.0434\n 7     7 0.208 0.0434\n 8     8 0.208 0.0434\n 9     9 0.208 0.0434\n10    10 0.208 0.0434\n# ℹ 3,990 more rows\n\n\nNow for every level of draw, \\(R_{y, \\hat y}^2 = .0434\\), which matches up nicely with the text. But it seems odd that the value should be the same for each of the 4,000 HMC draws. Sadly, my efforts to debug my workflow have been unsuccessful. If you see a flaw in this method, please share on GitHub.\nJust for kicks, here’s a more compact alternative to our fitted() + left_join() approach that more closely resembles the work flow Singer and Willett showed on pages 102 and 103.\n\ntibble(age_14 = 0:2) %&gt;% \n  mutate(fitted = map(age_14, ~ draws$b_Intercept + draws$b_age_14 * .)) %&gt;% \n  full_join(alcohol1_pp %&gt;% select(id, age_14, alcuse),\n            by = \"age_14\") %&gt;% \n  mutate(row = 1:n()) %&gt;% \n  unnest(fitted) %&gt;% \n  mutate(draw = rep(1:4000, times = alcohol1_pp %&gt;% nrow())) %&gt;% \n  group_by(draw) %&gt;% \n  summarise(r  = cor(fitted, alcuse),\n            r2 = cor(fitted, alcuse)^2)\n\n# A tibble: 4,000 × 3\n    draw     r     r2\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 0.208 0.0434\n 2     2 0.208 0.0434\n 3     3 0.208 0.0434\n 4     4 0.208 0.0434\n 5     5 0.208 0.0434\n 6     6 0.208 0.0434\n 7     7 0.208 0.0434\n 8     8 0.208 0.0434\n 9     9 0.208 0.0434\n10    10 0.208 0.0434\n# ℹ 3,990 more rows\n\n\nEither way, our results agree with those in the text: about “4.3% of the total variability in ALCUSE is associated with linear time” (p. 103, emphasis in the original).\n\n\n4.4.3.2 Pseudo-\\(R^2\\) statistics computed from the variance components\n\nResidual variation–that portion of the outcome variation unexplained by a model’s predictors–provides another criterion for comparison. When you fit a series of models, you hope that added predictors further explain unexplained outcome variation, causing residual variation to decline. The magnitude of this decline quantifies the improvement in fit. A large decline suggests that the predictors make a big difference; a small, or zero, decline suggests that they do not. To assess these declines on a common scale, we compute the proportional reduction in residual variance as we add predictors.\nEach unconditional model yields residual variances that serve as yardsticks for comparison. The unconditional means model provides a baseline estimate of \\(\\sigma_\\epsilon^2\\); the unconditional growth model provides baseline estimates of \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\). Each leads to its own pseudo-\\(R^2\\) statistic. (p. 103, emphasis in the original)\n\nThis provides three more pseudo-\\(R^2\\) statistics: \\(R_\\epsilon^2\\), \\(R_0^2\\), and \\(R_1^2\\). The formula for the first is\n\\[\nR_\\epsilon^2 = \\frac{\\sigma_\\epsilon^2 (\\text{unconditional means model}) - \\sigma_\\epsilon^2 (\\text{unconditional growth model})}{\\sigma_\\epsilon^2 (\\text{unconditional means model})}.\n\\]\nWe’ve actually already computed this one, above, under the name where we referred to it as the decline in \\(\\sigma_\\epsilon^2\\) after adding time to the model. Here it is again.\n\ncbind(VarCorr(fit4.1, summary = F)[[2]][[1]],\n      VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% \n  data.frame() %&gt;% \n  mutate_all(~ .^2) %&gt;% \n  set_names(str_c(\"fit4.\", 1:2)) %&gt;% \n  mutate(r_2_epsilon = (fit4.1 - fit4.2) / fit4.1) %&gt;% \n  summarise(mean   = mean(r_2_epsilon),\n            median = median(r_2_epsilon),\n            sd     = sd(r_2_epsilon),\n            ll     = quantile(r_2_epsilon, prob = 0.025),\n            ul     = quantile(r_2_epsilon, prob = 0.975)) %&gt;%\n  mutate_if(is.double, round, digits = 3)\n\n   mean median    sd    ll   ul\n1 0.345   0.36 0.139 0.024 0.57\n\n\nHere’s a look at the full distribution for our \\(\\sigma_\\epsilon^2\\).\n\ncbind(VarCorr(fit4.1, summary = F)[[2]][[1]],\n      VarCorr(fit4.2, summary = F)[[2]][[1]]) %&gt;% \n  data.frame() %&gt;% \n  mutate_all(~ .^2) %&gt;% \n  set_names(str_c(\"fit4.\", 1:2)) %&gt;% \n  mutate(r_2_epsilon = (fit4.1 - fit4.2) / fit4.1) %&gt;%\n  \n  ggplot(aes(x = r_2_epsilon)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  geom_density(fill = \"grey25\", color = \"transparent\") +\n  scale_x_continuous(expression(Pseudo~italic(R)[epsilon]^2), limits = c(-1, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWhen we use the full posteriors of our two \\(\\epsilon_\\epsilon^2\\) parameters, we end up with a slightly smaller statistic than the one in the text. So our conclusion is about 35% of the intraindividual variance is accounted for by time.\nIf we consider additional models with predictors for the \\(\\zeta\\)s, we can examine similar pseudo \\(R^2\\) statistics following the generic form\n\\[\nR_\\zeta^2 = \\frac{\\sigma_\\zeta^2 (\\text{unconditional growth model}) - \\sigma_\\zeta^2 (\\text{subsequent model})}{\\sigma_\\zeta^2 (\\text{unconditional growth model})},\n\\]\nwhere \\(\\zeta\\) could refer to \\(\\zeta_{0i}\\), \\(\\zeta_{1i}\\), and so on. If you look back up at the shape of the full posterior of \\(R_\\epsilon^2\\), you’ll notice part of the left tail crosses zero. “Unlike traditional \\(R^2\\) statistics, which will always be positive (or zero), some of these statistics can be negative” (p. 104)! If you compute them, interpret pseudo-\\(R^2\\) statistics with a grain of salt.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#practical-data-analytic-strategies-for-model-building",
    "href": "04.html#practical-data-analytic-strategies-for-model-building",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.5 Practical data analytic strategies for model building",
    "text": "4.5 Practical data analytic strategies for model building\n\nA sound statistical model includes all necessary predictors and no unnecessary ones. But how do you separate the wheat from the chaff? We suggest you rely on a combination of substantive theory, research questions, and statistical evidence. Never let a computer select predictors mechanically. (pp. 104–105, emphasis in the original)\n\n\n4.5.1 A taxonomy of statistical models\n\nWe suggest that you base decisions to enter, retain, and remove predictors on a combination of logic, theory, and prior research, supplemented by judicious [parameter evaluation] and comparison of model fit. At the outset, you might examine the effect of each predictor individually. You might then focus on predictors of primary interest (while including others whose effects you want to control). As in regular regression, you can add predictors singly or in groups and you can address issues of functional form using interactions and transformations. As you develop the taxonomy, you will progress toward a “final model” whose interpretation addresses your research questions. We place quotes around this term to emphasize that we believe no statistical model is ever final; it is simply a placeholder until a better model is found. (p. 105, emphasis in the original)\n\n\n\n4.5.2 Interpreting fitted models\n\nYou need not interpret every model you fit, especially those designed to guide interim decision making. When writing up findings for presentation and publication, we suggest that you identify a manageable subset of models that, taken together, tells a persuasive story parsimoniously. At a minimum, this includes the unconditional means model, the unconditional growth model, and a “final model”. You may also want to present intermediate models that either provide important building blocks or tell interesting stories in their own right. (p. 106)\n\nIn the dawn of the post-replication crisis era, it’s astonishing to reread and transcribe this section and the one above. I like a lot of what the authors had to say. Much of it seems like good pragmatic advice. But if they were to rewrite these sections again, I wonder what changes they’d make. Would they recommend researchers preregister their primary hypothesis, variables of interest, and perhaps their model building strategy (Nosek et al., 2018)? Would they be interested in a multiverse analysis (Steegen et al., 2016)? Would they still recommend sharing only a subset of one’s analyses in the era of sharing platforms like GitHub and the Open Science Framework? Would they weigh in on developments in causal inference (Pearl et al., 2016)?\n\n4.5.2.1 Model C: The uncontrolled effects of COA\nThe default priors for Model C are the same as for the unconditional growth model. All we’ve done is add parameters of class = b. As these default to improper flat priors, we have nothing to add to the prior argument to include them. Feel free to check with get_prior(). For the sake of practice, this model follows the form\n\\[\n\\begin{align}\n\\text{alcuse}_{ij} & = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\gamma_{10} \\text{age\\_14}_{ij} + \\gamma_{11} \\text{coa}_i \\times \\text{age\\_14}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{age\\_14}_{ij} + \\epsilon_{ij} \\\\\n\\epsilon_{ij} & \\sim \\text{Normal} (0, \\sigma_\\epsilon^2) \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\text{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} \\sigma_0^2 & \\sigma_{01} \\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix}\n\\end{pmatrix}.\n\\end{align}\n\\]\nFit the model.\n\nfit4.3 &lt;- brm(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 0 + Intercept + age_14 + coa + age_14:coa + (1 + age_14 | id),\n  prior = c(prior(student_t(3, 0, 2.5), class = sd),\n            prior(student_t(3, 0, 2.5), class = sigma),\n            prior(lkj(1), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.03\")\n\nCheck the summary.\n\nprint(fit4.3, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: alcuse ~ 0 + Intercept + age_14 + coa + age_14:coa + (1 + age_14 | id) \n   Data: alcohol1_pp (Number of observations: 246) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 82) \n                      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.697     0.099    0.512    0.897 1.008      909     1347\nsd(age_14)               0.368     0.095    0.148    0.530 1.015      382      470\ncor(Intercept,age_14)   -0.099     0.276   -0.488    0.627 1.012      590      686\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept     0.318     0.132    0.067    0.579 1.002     2094     2538\nage_14        0.294     0.086    0.127    0.467 1.000     2803     2837\ncoa           0.744     0.200    0.350    1.132 1.004     1967     1979\nage_14:coa   -0.051     0.129   -0.314    0.203 0.999     2798     2829\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.606     0.052    0.515    0.717 1.014      486      810\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur \\(\\gamma\\)’s are quite similar to those presented in the text. Our \\(\\sigma_\\epsilon\\) for this model is about the same as with fit4.2. Let’s practice with conditional_effects() to plot the consequences of this model.\n\nconditional_effects(fit4.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time we got back three plots. The first two were of the lower-order parameters \\(\\gamma_{10}\\) and \\(\\gamma_{01}\\). Note how the plot for coa treated it as a continuous variable. This is because the variable was saved as an integer in the original data set.\n\nfit4.3$data %&gt;% \n  glimpse()\n\nRows: 246\nColumns: 5\n$ alcuse    &lt;dbl&gt; 1.732051, 2.000000, 2.000000, 0.000000, 0.000000, 1.000000, 1.000000, 2.000000, 3.316625, 0.000000, 2.000000, …\n$ Intercept &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age_14    &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,…\n$ coa       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id        &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 12, 1…\n\n\nCoding it as an integer further complicated things for the third plot returned by conditional_effects(), the one for the interaction of age_14 and coa, \\(\\gamma_{11}\\).\nSince coa is binary, the natural way to express its interaction with age_14 would be with age_14 on the \\(x\\)-axis and two separate trajectories, one for each value of coa. That’s what Singer and Willett very sensibly did with the middle panel of Figure 4.3. However, the conditional_effects()function defaults to expressing interactions such that the first variable in the term–in this case,age_14–is on the \\(x\\)-axis and the second variable in the term–coa, treated as an integer–is depicted in three lines corresponding its mean and its mean \\(\\pm\\) one standard deviation. This is great for continuous variables, but incoherent for categorical ones. The fix is to adjust the data and refit the model.\n\nfit4.4 &lt;- update(\n  fit4.3,\n  newdata = alcohol1_pp %&gt;% mutate(coa = factor(coa)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.04\")\n\nWe might compare the updated model with its predecessor. To get a focused look, we can use the posterior_summary() function with a little subsetting.\n\nposterior_summary(fit4.3)[1:4, ] %&gt;% round(digits = 3)\n\n             Estimate Est.Error   Q2.5 Q97.5\nb_Intercept     0.318     0.132  0.067 0.579\nb_age_14        0.294     0.086  0.127 0.467\nb_coa           0.744     0.200  0.350 1.132\nb_age_14:coa   -0.051     0.129 -0.314 0.203\n\nposterior_summary(fit4.4)[1:4, ] %&gt;% round(digits = 3)\n\n              Estimate Est.Error   Q2.5 Q97.5\nb_Intercept      0.318     0.132  0.067 0.579\nb_age_14         0.294     0.086  0.127 0.467\nb_coa1           0.744     0.200  0.350 1.132\nb_age_14:coa1   -0.051     0.129 -0.314 0.203\n\n\nThe results are about the same. The payoff comes when we try again with conditional_effects().\n\nconditional_effects(fit4.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch better. Now the plot for \\(\\gamma_{01}\\) treats coa as binary and our plot for the interaction between age_14 and coa is much close to the one in Figure 4.3. Since we’re already on a conditional_effects() tangent, we may as well go further. When working with models like fit4.3 where you have multiple fixed effects, sometimes you only want the plots for a subset of those effects. For example, if our main goal is to do a good job tastefully reproducing the middle plot in Figure 4.3, we only need the interaction plot. In such a case, use the effects argument.\n\nconditional_effects(fit4.4, effects = \"age_14:coa\")\n\n\n\n\n\n\n\n\nEarlier we discussed how conditional_effects() lets users adjust some of the output. But if you want an extensive overhaul, it’s better to save the output of conditional_effects() as an object and manipulate that object with the plot() function.\n\nce &lt;- conditional_effects(fit4.4, effects = \"age_14:coa\")\n\nstr(ce)\n\nList of 1\n $ age_14:coa:'data.frame': 200 obs. of  12 variables:\n  ..$ age_14    : num [1:200] 0 0 0.0202 0.0202 0.0404 ...\n  ..$ coa       : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 2 1 2 1 2 ...\n  ..$ alcuse    : num [1:200] 0.922 0.922 0.922 0.922 0.922 ...\n  ..$ Intercept : num [1:200] 1 1 1 1 1 1 1 1 1 1 ...\n  ..$ id        : logi [1:200] NA NA NA NA NA NA ...\n  ..$ cond__    : Factor w/ 1 level \"1\": 1 1 1 1 1 1 1 1 1 1 ...\n  ..$ effect1__ : num [1:200] 0 0 0.0202 0.0202 0.0404 ...\n  ..$ effect2__ : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 2 1 2 1 2 ...\n  ..$ estimate__: num [1:200] 0.318 1.061 0.324 1.066 0.329 ...\n  ..$ se__      : num [1:200] 0.13 0.147 0.13 0.146 0.129 ...\n  ..$ lower__   : num [1:200] 0.0672 0.7737 0.0743 0.7796 0.0811 ...\n  ..$ upper__   : num [1:200] 0.579 1.346 0.583 1.35 0.587 ...\n  ..- attr(*, \"effects\")= chr [1:2] \"age_14\" \"coa\"\n  ..- attr(*, \"response\")= chr \"alcuse\"\n  ..- attr(*, \"surface\")= logi FALSE\n  ..- attr(*, \"categorical\")= logi FALSE\n  ..- attr(*, \"ordinal\")= logi FALSE\n  ..- attr(*, \"points\")='data.frame':   246 obs. of  6 variables:\n  .. ..$ age_14   : num [1:246] 0 1 2 0 1 2 0 1 2 0 ...\n  .. ..$ coa      : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n  .. ..$ resp__   : num [1:246] 1.73 2 2 0 0 ...\n  .. ..$ cond__   : Factor w/ 1 level \"1\": 1 1 1 1 1 1 1 1 1 1 ...\n  .. ..$ effect1__: num [1:246] 0 1 2 0 1 2 0 1 2 0 ...\n  .. ..$ effect2__: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n - attr(*, \"class\")= chr \"brms_conditional_effects\"\n\n\nOur ce is an object of class “’brms_conditional_effects” which contains a list of a single data frame. Had we omitted our effects argument, above, we’d have a list of 3 instead. Anyway, these data frames contain the necessary information to produce the plot. The advantage of saving ce this way is we can now insert it into the plot() function. The simple output is the same as before.\n\nplot(ce)\n\n\n\n\n\n\n\n\nThe plot() function will allow us to do other things, like add in the original data or omit the white grid lines.\n\nce %&gt;% \n  plot(points = T,\n       point_args = list(size = 1/4, alpha = 1/4, width = 0.05, height = 0.05, color = \"black\"),\n       theme = theme(panel.grid = element_blank()))\n\n\n\n\n\n\n\n\nAnd for even more control, you can tack on typical ggplot2 functions. But when you want to do so, make sure to set the plot = FALSE argument and then subset after the right parenthesis of the plot() function.\n\nplot(ce, \n     theme = theme(legend.position = \"none\",\n                   panel.grid = element_blank()),\n     plot = FALSE)[[1]] +\n  annotate(geom = \"text\",\n           x = 2.1, y = c(0.95, 1.55),\n           label = str_c(\"coa = \", 0:1),\n           hjust = 0, size = 3.5) +\n  scale_fill_brewer(type = \"qual\") +\n  scale_color_brewer(type = \"qual\") +\n  scale_x_continuous(\"age\", limits = c(-1, 3), labels = 13:17) +\n  scale_y_continuous(limits = c(0, 2), breaks = 0:2)\n\n\n\n\n\n\n\n\nBut anyway, let’s get back on track and talk about the variance components. Singer and Willett contrasted \\(\\sigma_\\epsilon^2\\) from Model B to the new one from Model C. We might use VarCorr() to do the same.\n\nVarCorr(fit4.2)[[2]]\n\n$sd\n  Estimate  Est.Error      Q2.5    Q97.5\n 0.6066275 0.05331062 0.5138619 0.720457\n\nVarCorr(fit4.3)[[2]]\n\n$sd\n  Estimate  Est.Error      Q2.5     Q97.5\n 0.6059488 0.05215441 0.5146807 0.7172405\n\n\nWe could have also extracted that information by subsetting posterior_summary().\n\nposterior_summary(fit4.2)[\"sigma\", ]\n\n  Estimate  Est.Error       Q2.5      Q97.5 \n0.60662753 0.05331062 0.51386186 0.72045701 \n\nposterior_summary(fit4.3)[\"sigma\", ]\n\n  Estimate  Est.Error       Q2.5      Q97.5 \n0.60594880 0.05215441 0.51468065 0.71724052 \n\n\nAnyway, to get these in a variance metric, just square their posterior samples and summarize.\nOur next task is to formally compare fit4.2 and fit4.3 in terms of declines in \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\).\n\nbind_cols(\n  as_draws_df(fit4.2) %&gt;% \n    transmute(fit2_sigma_2_0 = sd_id__Intercept^2,\n              fit2_sigma_2_1 = sd_id__age_14^2),\n  as_draws_df(fit4.3) %&gt;% \n    transmute(fit3_sigma_2_0 = sd_id__Intercept^2,\n              fit3_sigma_2_1 = sd_id__age_14^2)\n) %&gt;% \n  mutate(`decline~'in'~sigma[0]^2` = (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0,\n         `decline~'in'~sigma[1]^2` = (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) %&gt;% \n  pivot_longer(contains(\"decline\")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  geom_density(fill = \"grey25\", color = \"transparent\") +\n  scale_x_continuous(NULL, limits = c(-5, 2)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, labeller = label_parsed, ncol = 1)\n\n\n\n\n\n\n\n\nHere are the percents of variance declined from fit4.2 to fit4.3.\n\nbind_cols(\n  as_draws_df(fit4.2) %&gt;% \n    transmute(fit2_sigma_2_0 = sd_id__Intercept^2,\n              fit2_sigma_2_1 = sd_id__age_14^2),\n  as_draws_df(fit4.3) %&gt;% \n    transmute(fit3_sigma_2_0 = sd_id__Intercept^2,\n              fit3_sigma_2_1 = sd_id__age_14^2)\n) %&gt;% \n  mutate(`decline~'in'~sigma[0]^2` = (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0,\n         `decline~'in'~sigma[1]^2` = (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1) %&gt;% \n  pivot_longer(contains(\"decline\")) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean   = mean(value),\n            median = median(value),\n            sd     = sd(value),\n            ll     = quantile(value, prob = 0.025),\n            ul     = quantile(value, prob = 0.975))\n\n# A tibble: 2 × 6\n  name                       mean  median       sd     ll    ul\n  &lt;chr&gt;                     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 decline~'in'~sigma[0]^2   0.161  0.221     0.343 -0.649 0.644\n2 decline~'in'~sigma[1]^2 -56.9   -0.0318 3464.    -8.23  0.825\n\n\nIn this case, we end up with massive uncertainty when working with the full posteriors. This is particularly the case with the difference in \\(\\sigma_1^2\\), which is left skewed for days. Here are the results when we only use point estimates.\n\nbind_cols(\n  as_draws_df(fit4.2) %&gt;% \n    transmute(fit2_sigma_2_0 = sd_id__Intercept^2,\n              fit2_sigma_2_1 = sd_id__age_14^2),\n  as_draws_df(fit4.3) %&gt;% \n    transmute(fit3_sigma_2_0 = sd_id__Intercept^2,\n              fit3_sigma_2_1 = sd_id__age_14^2)\n) %&gt;% \n  summarise_all(median) %&gt;% \n  transmute(`% decline in sigma_2_0` = 100 * (fit2_sigma_2_0 - fit3_sigma_2_0) / fit2_sigma_2_0,\n            `% decline in sigma_2_1` = 100 * (fit2_sigma_2_1 - fit3_sigma_2_1) / fit2_sigma_2_1)\n\n# A tibble: 1 × 2\n  `% decline in sigma_2_0` `% decline in sigma_2_1`\n                     &lt;dbl&gt;                    &lt;dbl&gt;\n1                     21.4                    -5.06\n\n\n“These variance components are now called partial or conditional variances because they quantify the interindividual differences in change that remain unexplained by the model’s predictors” (p. 108, emphasis in the original).\n\n\n4.5.2.2 Model D: The controlled effects of COA\nThis model follows the form\n\\[\n\\begin{align}\n\\text{alcuse}_{ij} & = \\gamma_{00} + \\gamma_{01} \\text{coa}_i + \\gamma_{02} \\text{peer}_i + \\gamma_{10} \\text{age\\_14}_{ij} \\\\\n& \\;\\;\\; + \\gamma_{11} \\text{coa}_i \\times \\text{age\\_14}_{ij} + \\gamma_{12} \\text{peer}_i \\times \\text{age\\_14}_{ij} \\\\\n& \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{age\\_14}_{ij} + \\epsilon_{ij} \\\\\n\\epsilon_{ij} & \\sim \\text{Normal} (0, \\sigma_\\epsilon^2) \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\sim \\text{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} \\sigma_0^2 & \\sigma_{01} \\\\ \\sigma_{01} & \\sigma_1^2 \\end{bmatrix}\n\\end{pmatrix}.\n\\end{align}\n\\]\nFit that joint.\n\nfit4.5 &lt;- brm(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:coa + age_14:peer + (1 + age_14 | id),\n  prior = c(prior(student_t(3, 0, 2.5), class = sd),\n            prior(student_t(3, 0, 2.5), class = sigma),\n            prior(lkj(1), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.05\")\n\n\nprint(fit4.5, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:coa + age_14:peer + (1 + age_14 | id) \n   Data: alcohol1_pp (Number of observations: 246) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 82) \n                      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.484     0.101    0.282    0.677 1.005      620     1183\nsd(age_14)               0.374     0.081    0.211    0.531 1.011      389      611\ncor(Intercept,age_14)    0.089     0.336   -0.428    0.849 1.017      289      399\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept     -0.311     0.152   -0.613   -0.011 1.001     2165     2545\nage_14         0.429     0.121    0.188    0.659 1.001     1929     2603\ncoa            0.575     0.166    0.248    0.898 1.000     2331     2428\npeer           0.694     0.115    0.468    0.922 1.001     2148     2639\nage_14:coa    -0.012     0.132   -0.272    0.243 1.000     2960     2787\nage_14:peer   -0.152     0.089   -0.322    0.028 1.000     1895     2601\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.604     0.047    0.516    0.700 1.005      498     1748\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAll our \\(\\gamma\\) estimates are similar to those presented in Table 4.1. Let’s compute the variances and the covariance, \\(\\sigma_{01}^2\\). Here are the plots.\n\n v &lt;- as_draws_df(fit4.5) %&gt;% \n  transmute(sigma_2_epsilon = sigma^2,\n            sigma_2_0       = sd_id__Intercept^2,\n            sigma_2_1       = sd_id__age_14^2,\n            sigma_01        = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14)\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_density(size = 0, fill = \"black\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nAnd now we compute the summary statistics.\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean   = mean(value),\n            median = median(value),\n            sd     = sd(value),\n            ll     = quantile(value, prob = 0.025),\n            ul     = quantile(value, prob = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 4 × 6\n  name             mean median    sd     ll    ul\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 sigma_01        0      0.006 0.056 -0.125 0.091\n2 sigma_2_0       0.244  0.236 0.098  0.079 0.459\n3 sigma_2_1       0.146  0.141 0.061  0.044 0.282\n4 sigma_2_epsilon 0.367  0.362 0.058  0.267 0.49 \n\n\nLike the \\(\\gamma\\)’s, our variance components are all similar to those in the text.\n\nbind_cols(\n  as_draws_df(fit4.2) %&gt;% \n    transmute(fit4.2_sigma_2_epsilon = sigma^2,\n              fit4.2_sigma_2_0 = sd_id__Intercept^2,\n              fit4.2_sigma_2_1 = sd_id__age_14^2),\n  as_draws_df(fit4.5) %&gt;% \n    transmute(fit4.5_sigma_2_epsilon = sigma^2,\n              fit4.5_sigma_2_0 = sd_id__Intercept^2,\n              fit4.5_sigma_2_1 = sd_id__age_14^2)\n) %&gt;% \n  summarise_all(median) %&gt;% \n  mutate(`% decline in sigma_2_epsilon` = 100 * (fit4.2_sigma_2_epsilon - fit4.5_sigma_2_epsilon) / fit4.2_sigma_2_epsilon,\n         `% decline in sigma_2_0` = 100 * (fit4.2_sigma_2_0 - fit4.5_sigma_2_0) / fit4.2_sigma_2_0,\n         `% decline in sigma_2_1` = 100 * (fit4.2_sigma_2_1 - fit4.5_sigma_2_1) / fit4.2_sigma_2_1) %&gt;% \n  pivot_longer(contains(\"%\")) %&gt;% \n  select(name, value)\n\n# A tibble: 3 × 2\n  name                          value\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 % decline in sigma_2_epsilon  0.251\n2 % decline in sigma_2_0       61.8  \n3 % decline in sigma_2_1       -3.36 \n\n\nThe percentages in which our variance components declined relative to the unconditional growth model are of similar orders of magnitude as those presented in the text.\n\n\n4.5.2.3 Model E: A tentative “final model” for the controlled effects of coa\nThis model is just like the last, but with the simple omission of the \\(\\gamma_{12}\\) parameter.\n\nfit4.6 &lt;- brm(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id),\n  prior = c(prior(student_t(3, 0, 2.5), class = sd),\n            prior(student_t(3, 0, 2.5), class = sigma),\n            prior(lkj(1), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.06\")\n\n\nprint(fit4.6, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id) \n   Data: alcohol1_pp (Number of observations: 246) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 82) \n                      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.478     0.105    0.269    0.679 1.002      772     1234\nsd(age_14)               0.360     0.081    0.204    0.516 1.002      395      910\ncor(Intercept,age_14)    0.140     0.349   -0.406    0.889 1.004      385      669\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept     -0.310     0.152   -0.612   -0.007 1.000     1855     2386\nage_14         0.426     0.109    0.209    0.635 1.003     2101     2090\ncoa            0.570     0.150    0.282    0.861 1.001     2458     2688\npeer           0.692     0.115    0.462    0.923 1.000     1865     2058\nage_14:peer   -0.150     0.087   -0.319    0.026 1.001     1990     2253\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.608     0.048    0.520    0.705 1.002      543     1561\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe \\(\\gamma\\)’s all look well behaved. Here are the variance component summaries.\n\n v &lt;- as_draws_df(fit4.6) %&gt;% \n  transmute(sigma_2_epsilon = sigma^2,\n            sigma_2_0       = sd_id__Intercept^2,\n            sigma_2_1       = sd_id__age_14^2,\n            sigma_01        = sd_id__Intercept * cor_id__Intercept__age_14 * sd_id__age_14)\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean   = mean(value),\n            median = median(value),\n            sd     = sd(value),\n            ll     = quantile(value, prob = 0.025),\n            ul     = quantile(value, prob = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 4 × 6\n  name             mean median    sd     ll    ul\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 sigma_01        0.008  0.014 0.054 -0.113 0.094\n2 sigma_2_0       0.24   0.229 0.1    0.073 0.461\n3 sigma_2_1       0.136  0.13  0.059  0.042 0.267\n4 sigma_2_epsilon 0.372  0.369 0.059  0.27  0.497\n\n\n\n\n\n4.5.3 Displaying prototypical change trajectories\nOn page 111, Singer and Willett computed the various levels of the \\(\\pi\\) coefficients when coa == 0 or coa == 1. To follow along, we’ll want to work directly with the posterior draws from fit4.3.\n\ndraws &lt;- as_draws_df(fit4.3) \n\ndraws %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  head()\n\n# A tibble: 6 × 4\n  b_Intercept b_age_14 b_coa `b_age_14:coa`\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1       0.501    0.155 0.477         0.125 \n2       0.265    0.434 0.929        -0.171 \n3       0.452    0.184 0.682        -0.0405\n4       0.626    0.286 0.734        -0.113 \n5       0.154    0.319 1.06         -0.184 \n6       0.217    0.238 0.778         0.0450\n\n\nHere we apply the formulas to the posterior draws and then summarize with posterior means.\n\ndraws %&gt;%\n  select(starts_with(\"b_\")) %&gt;% \n  transmute(pi_0_coa0 = b_Intercept + b_coa          * 0,\n            pi_1_coa0 = b_age_14    + `b_age_14:coa` * 0,\n            pi_0_coa1 = b_Intercept + b_coa          * 1,\n            pi_1_coa1 = b_age_14    + `b_age_14:coa` * 1) %&gt;%\n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;%\n  summarise(posterior_mean = mean(value) %&gt;% round(digits = 3))\n\n# A tibble: 4 × 2\n  name      posterior_mean\n  &lt;chr&gt;              &lt;dbl&gt;\n1 pi_0_coa0          0.318\n2 pi_0_coa1          1.06 \n3 pi_1_coa0          0.294\n4 pi_1_coa1          0.243\n\n\nWe already plotted these trajectories and their 95% intervals a few sections up. If we want to work with the full composite model to predict \\(Y_{ij}\\) (i.e., alcuse) directly, we multiply the b_coa, b_age_14, and b_age_14:coa vectors by the appropriate values of coa and peer. For example, here’s what you’d code if you wanted the initial alcuse status for when coa == 1.\n\ndraws %&gt;%\n  select(starts_with(\"b_\")) %&gt;% \n  mutate(y = b_Intercept + b_coa * 1 + b_age_14 * 0 + `b_age_14:coa` * 0 * 1) %&gt;% \n  head()\n\n# A tibble: 6 × 5\n  b_Intercept b_age_14 b_coa `b_age_14:coa`     y\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1       0.501    0.155 0.477         0.125  0.978\n2       0.265    0.434 0.929        -0.171  1.19 \n3       0.452    0.184 0.682        -0.0405 1.13 \n4       0.626    0.286 0.734        -0.113  1.36 \n5       0.154    0.319 1.06         -0.184  1.21 \n6       0.217    0.238 0.778         0.0450 0.995\n\n\nIf you were to take the mean of that new y column, you’d discover it’s the same as the mean of our pi_0_coa1, above.\n\ndraws %&gt;%\n  select(starts_with(\"b_\")) %&gt;% \n  mutate(y = b_Intercept + b_coa * 1 + b_age_14 * 0 + `b_age_14:coa` * 0 * 1) %&gt;% \n  summarise(pi_0_coa1 = mean(y))\n\n# A tibble: 1 × 1\n  pi_0_coa1\n      &lt;dbl&gt;\n1      1.06\n\n\nSinger and Willett suggested four strategies to help researchers pick the prototypical values of the predictors to focus on:\n\nSubstantively interesting values (e.g., typical ages, values corresponding to transition points)\nA range of percentiles (e.g., 25th, 50th, and 75th)\nJust the sample mean\nThe sample mean \\(\\pm\\) something like 1 standard deviation\n\nThey next discuss the right panel of Figure 4.3. We could continue to work directly with the as_draws_df() to make our version of that figure. But it you want to accompany the posterior mean trajectories with their 95% intervals, and I hope you do, the as_draws_df() method will get tedious. Happily, brms offers users and alternative with the fitted() function. Since the right panel is somewhat complicated, it’ll behoove us to practice with the simpler left panel, first.\nIn fit4.2 (i.e., Model C), age_14 is the only predictor. Here we’ll specify the values along the range in the original data, ranging from 0 to 2. However, we end up specifying a bunch of values within that range in addition to the two endpoints. This is because the 95% intervals typically have a bow tie shape. To depict that shape well, we need more than a couple values. We save those values as a tibble called nd (i.e., new data). We make use of them within fitted with the newdata = nd argument.\nSince we’re only interested in the general trajectory, the consequence of the \\(\\gamma\\)’s, we end up coding re_formula = NA. In so doing, we ask fitted() to ignore the group-level effects. In this example, that means we are ignoring the id-level deviations from the overall trajectories. If you’re confused by that that means, don’t worry. That part of the model should become more clear as we go along in the text.\nSince fitted() returns an array, we then convert the results into a data frame for use within the tidyverse framework. For plotting, it’s handy to bind those results together with the nd, the predictor values we used to compute the fitted values with. In the final wrangling step, we use our age_14 values to compute the age values.\n\nnd &lt;- tibble(age_14 = seq(from = 0, to = 2, length.out = 30))\n\nf &lt;- fitted(fit4.2, \n            newdata = nd,\n            re_formula = NA) %&gt;%\n  data.frame() %&gt;%\n  bind_cols(nd) %&gt;% \n  mutate(age = age_14 + 14)\n\nhead(f)\n\n   Estimate Est.Error      Q2.5     Q97.5     age_14      age\n1 0.6486449 0.1080202 0.4374269 0.8552847 0.00000000 14.00000\n2 0.6673945 0.1062598 0.4588409 0.8694024 0.06896552 14.06897\n3 0.6861442 0.1046542 0.4815716 0.8862499 0.13793103 14.13793\n4 0.7048938 0.1032104 0.5020449 0.9004389 0.20689655 14.20690\n5 0.7236434 0.1019356 0.5225452 0.9160866 0.27586207 14.27586\n6 0.7423931 0.1008359 0.5418554 0.9329727 0.34482759 14.34483\n\n\nSince we only had one predictor, age_14, for which we specified 30 specific values, we ended up with 30 rows in our output. By default, fitted() summarized the fitted values with posterior means (Estimate), standard deviations (Est.Error), and percentile-based 95% intervals (Q2.5 and Q97.5). The other columns are the values we bound to them. Here’s how we might use these to make our fitted() version of the leftmost panel of Figure 4.3.\n\nf %&gt;%\n  ggplot(aes(x = age)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey75\", alpha = 3/4) +\n  geom_line(aes(y = Estimate)) +\n  scale_y_continuous(\"alcuse\", breaks = 0:2, limits = c(0, 2)) +\n  coord_cartesian(xlim = c(13, 17)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWith fit4.6 (i.e., Model E), we now have three predictors. We’d like to see the full range across age_14 for four combinations of coa and peer values. To my mind, the easiest way to get those values right is with a little crossing() and expand().\n\nnd &lt;- crossing(coa  = 0:1,\n               peer = c(0.655, 1.381)) %&gt;% \n  expand_grid(age_14 = seq(from = 0, to = 2, length.out = 30))\n\nhead(nd, n = 10)\n\n# A tibble: 10 × 3\n     coa  peer age_14\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     0 0.655 0     \n 2     0 0.655 0.0690\n 3     0 0.655 0.138 \n 4     0 0.655 0.207 \n 5     0 0.655 0.276 \n 6     0 0.655 0.345 \n 7     0 0.655 0.414 \n 8     0 0.655 0.483 \n 9     0 0.655 0.552 \n10     0 0.655 0.621 \n\n\nNow we use fitted() much like before.\n\nf &lt;- fitted(fit4.6, \n            newdata = nd,\n            re_formula = NA) %&gt;%\n  data.frame() %&gt;%\n  bind_cols(nd) %&gt;%\n  # a little wrangling will make plotting much easier\n  mutate(age  = age_14 + 14,\n         coa  = ifelse(coa == 0, \"coa = 0\", \"coa = 1\"),\n         peer = factor(peer))\n\nglimpse(f)\n\nRows: 120\nColumns: 8\n$ Estimate  &lt;dbl&gt; 0.1434126, 0.1660213, 0.1886299, 0.2112385, 0.2338471, 0.2564557, 0.2790643, 0.3016729, 0.3242815, 0.3468901, …\n$ Est.Error &lt;dbl&gt; 0.1116056, 0.1099213, 0.1084337, 0.1071511, 0.1060808, 0.1052293, 0.1046021, 0.1042030, 0.1040348, 0.1040986, …\n$ Q2.5      &lt;dbl&gt; -0.078921058, -0.053915222, -0.029177931, -0.005559805, 0.020911593, 0.044099037, 0.068646061, 0.093990729, 0.…\n$ Q97.5     &lt;dbl&gt; 0.3649108, 0.3848853, 0.4041290, 0.4213087, 0.4410728, 0.4662987, 0.4864363, 0.5055074, 0.5282342, 0.5505795, …\n$ coa       &lt;chr&gt; \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", \"coa = 0\", …\n$ peer      &lt;fct&gt; 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655, 0.655…\n$ age_14    &lt;dbl&gt; 0.00000000, 0.06896552, 0.13793103, 0.20689655, 0.27586207, 0.34482759, 0.41379310, 0.48275862, 0.55172414, 0.…\n$ age       &lt;dbl&gt; 14.00000, 14.06897, 14.13793, 14.20690, 14.27586, 14.34483, 14.41379, 14.48276, 14.55172, 14.62069, 14.68966, …\n\n\nFor our version of the right panel of Figure 4.3, most of the action is in ggplot(), geom_ribbon(), geom_line(), and facet_wrap(). All the rest is cosmetic.\n\nf %&gt;%\n  ggplot(aes(x = age, color = peer, fill = peer)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              size = 0, alpha = 1/4) +\n  geom_line(aes(y = Estimate, size = peer)) +\n  scale_size_manual(values = c(1/2, 1)) +\n  scale_fill_manual(values = c(\"blue3\", \"red3\")) +\n  scale_color_manual(values = c(\"blue3\", \"red3\")) +\n  scale_y_continuous(\"alcuse\", breaks = 0:2) +\n  labs(subtitle = \"High peer values are in red; low ones are in blue.\") +\n  coord_cartesian(xlim = c(13, 17)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ coa)\n\n\n\n\n\n\n\n\nIn my opinion, it works better to split the plot into two when you include the 95% intervals.\n\n\n4.5.4 Recentering predictors to improve interpretation\n\nThe easiest strategy for recentering a time-invariant predictor is to subtract its sample mean from each observed value. When we center a predictor on its sample mean, the level-2 fitted intercepts represent the average fitted values of initial status (or rate of change). We can also recenter a time-invariant predictor by subtracting another meaningful value… Recentering works best when the centering constant is substantively meaningful. (pp. 113–114)\n\nAs we’ll see later, centering can also make it easier to select meaningful priors on the model intercept. If you look at our alcohol1_pp data, you’ll see we already have centered versions of our time-invariant predictors. They’re the last two columns, cpeer and ccoa.\n\nalcohol1_pp %&gt;% \n  glimpse()\n\nRows: 246\nColumns: 9\n$ id     &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12, …\n$ age    &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 1…\n$ coa    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ male   &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age_14 &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1,…\n$ alcuse &lt;dbl&gt; 1.732051, 2.000000, 2.000000, 0.000000, 0.000000, 1.000000, 1.000000, 2.000000, 3.316625, 0.000000, 2.000000, 1.7…\n$ peer   &lt;dbl&gt; 1.2649111, 1.2649111, 1.2649111, 0.8944272, 0.8944272, 0.8944272, 0.8944272, 0.8944272, 0.8944272, 1.7888544, 1.7…\n$ cpeer  &lt;dbl&gt; 0.2469111, 0.2469111, 0.2469111, -0.1235728, -0.1235728, -0.1235728, -0.1235728, -0.1235728, -0.1235728, 0.770854…\n$ ccoa   &lt;dbl&gt; 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0.549, 0…\n\n\nIf you wanted to center them by hand, you’d just execute something like this.\n\nalcohol1_pp %&gt;% \n  mutate(peer_c = peer - mean(peer))\n\n# A tibble: 246 × 10\n      id   age   coa  male age_14 alcuse  peer  cpeer  ccoa peer_c\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    14     1     0      0   1.73 1.26   0.247 0.549  0.247\n 2     1    15     1     0      1   2    1.26   0.247 0.549  0.247\n 3     1    16     1     0      2   2    1.26   0.247 0.549  0.247\n 4     2    14     1     1      0   0    0.894 -0.124 0.549 -0.123\n 5     2    15     1     1      1   0    0.894 -0.124 0.549 -0.123\n 6     2    16     1     1      2   1    0.894 -0.124 0.549 -0.123\n 7     3    14     1     1      0   1    0.894 -0.124 0.549 -0.123\n 8     3    15     1     1      1   2    0.894 -0.124 0.549 -0.123\n 9     3    16     1     1      2   3.32 0.894 -0.124 0.549 -0.123\n10     4    14     1     1      0   0    1.79   0.771 0.549  0.771\n# ℹ 236 more rows\n\n\nDid you notice how our peer_c values, above, deviated slightly from those in cpeer? That’s because peer_c was based on the exact sample mean. Those in cpeer are based on the sample mean as provided in the text, 1.018, which is introduces rounding error. For the sake of simplicity, we’ll go with centered variables matching up with the text.\nHere we’ll hastily fit the models with help from the update() function.\n\nfit4.7 &lt;- update(\n  fit4.6,\n  newdata = alcohol1_pp,\n  alcuse ~ 0 + Intercept + age_14 + coa + cpeer + age_14:cpeer + (1 + age_14 | id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.07\")\n\nfit4.8 &lt;- update(\n  fit4.6,\n  newdata = alcohol1_pp,\n  alcuse ~ 0 + Intercept + age_14 + ccoa + peer + age_14:peer + (1 + age_14 | id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.08\")\n\nHere we reproduce the \\(\\gamma\\)s from fit4.6 and compare the to the updates from fit4.7 and fit4.8.\n\nfixef(fit4.6) %&gt;% round(digits = 3)\n\n            Estimate Est.Error   Q2.5  Q97.5\nIntercept     -0.310     0.152 -0.612 -0.007\nage_14         0.426     0.109  0.209  0.635\ncoa            0.570     0.150  0.282  0.861\npeer           0.692     0.115  0.462  0.923\nage_14:peer   -0.150     0.087 -0.319  0.026\n\nfixef(fit4.7) %&gt;% round(digits = 3)\n\n             Estimate Est.Error   Q2.5 Q97.5\nIntercept       0.390     0.108  0.183 0.601\nage_14          0.272     0.064  0.148 0.397\ncoa             0.576     0.151  0.280 0.878\ncpeer           0.692     0.112  0.463 0.905\nage_14:cpeer   -0.151     0.086 -0.317 0.020\n\nfixef(fit4.8) %&gt;% round(digits = 3)\n\n            Estimate Est.Error   Q2.5 Q97.5\nIntercept     -0.054     0.140 -0.322 0.227\nage_14         0.423     0.109  0.205 0.635\nccoa           0.579     0.149  0.286 0.870\npeer           0.692     0.112  0.467 0.909\nage_14:peer   -0.150     0.086 -0.316 0.022",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#comparing-models-using-deviance-statistics",
    "href": "04.html#comparing-models-using-deviance-statistics",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.6 Comparing models using deviance statistics",
    "text": "4.6 Comparing models using deviance statistics\nAs you will see, we will also make use of deviance within our Bayesian Stan-based paradigm. But we’ll do so a little differently than what Singer and Willett presented.\n\n4.6.1 The deviance statistic\nAs it turns out, we Bayesians use the log-likelihood (LL), too. Recall how the numerator in the right-hand side of Bayes’ Theorem was \\(p(\\text{data} \\mid \\theta) p(\\theta)\\)? That first part, \\(p(\\text{data} \\mid \\theta)\\), is the likelihood. In words, the likelihood is the probability of the data given the parameters. We generally work with the log of the likelihood rather than the likelihood itself because it’s easier to work with statistically.\nWhen you’re working with brms, you can extract the LL with the log_lik() function. Here’s an example with fit4.1, our unconditional means model.\n\nlog_lik(fit4.1) %&gt;% \n  str()\n\n num [1:4000, 1:246] -0.694 -0.752 -0.984 -0.665 -0.769 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nYou may have noticed we didn’t just get a single value back. Rather, we got an array of 4,000 rows and 246 columns. The reason we got 4,000 rows is because that’s how many post-warmup iterations we drew from the posterior. I.e., we set brm(..., iter = 2000, warmup = 1000, chains = 4). With respect to the 246 columns, that’s how many rows there are in the alcohol1_pp data. So for each case in the data, we get an entire posterior distribution of LL values.\nWith the multilevel model, we can define deviance for a given model as its LL times -2,\n\\[\n\\text{Deviance} = -2 LL_\\text{current model}.\n\\]\nHere that is in code for fit4.1.\n\nll &lt;- log_lik(fit4.1) %&gt;%\n  data.frame() %&gt;% \n  mutate(ll = rowSums(.)) %&gt;% \n  mutate(deviance = -2 * ll) %&gt;% \n  select(ll, deviance, everything())\n\ndim(ll)\n\n[1] 4000  248\n\n\nBecause we used HMC, deviance is a distribution rather than a single number. Here’s what it looks like for fit4.1.\n\nll %&gt;% \n  ggplot(aes(x = deviance)) +\n  geom_density(fill = \"grey25\", size = 0) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nMuch like the frequentists, we Bayesian generally prefer models with smaller deviance distributions.\nThe reasons frequentists multiply the LL by -2 is because after doing so, the difference in deviance values between two models follows a \\(\\chi^2\\) distribution and the old \\(\\chi^2\\)-difference test is widely-used in frequentist statistics. Bayesians often just go ahead and use the -2 multiplication, too. It’s largely out of tradition. But as we’ll see, some contemporary Bayesians are challenging that tradition.\n\n\n4.6.2 When and how can you compare deviance statistics?\nAs for the frequentists, deviance values/distributions in the Bayesian context are only meaningful in the relative sense. You cannot directly interpret a single models deviance distribution by the magnitude or sign of its central tendency. But you can compare two or more models by the relative locations of their deviance distributions. When doing so, they must have been computed using the same data (i.e., no differences in missingness in the predictors) and the models must be nested.\nHowever, in contemporary Bayesian practice we don’t tend to compare models with deviance. For details on why, check out Chapter 6 in McElreath’s Statistical Rethinking. McElreath also covered the topic in several online lectures (e.g., here and here).\n\n\n4.6.3 Implementing deviance-based hypothesis tests\nIn this project, we are not going to practice comparing deviance values using frequentist \\(\\chi^2\\) tests. We will, however, cover Bayesian information criteria.\n\n\n4.6.4 AIC and BIC WAIC and LOO statistics: Comparing nonnested models using information criteria [and cross validation]\nWe do not use the AIC or the BIC within the Stan ecosystem. The AIC is frequentist and cannot handle models with priors. The BIC is interesting in it’s a double misnomer. It is neither Bayesian nor is it a proper information criterion–though it does scale like one. However, it might be useful for our purposes to walk out the AIC a bit. It’ll ground our discussion of the WAIC and LOO. From Spiegelhalter, Best, Carlin and van der Linde [@-spiegelhalterDevianceInformationCriterion2014], we read:\n\nSuppose that we have a given set of candidate models, and we would like a criterion to assess which is ‘better’ in a defined sense. Assume that a model for observed data \\(y\\) postulates a density \\(p(y \\mid \\theta)\\) (which may include covariates etc.), and call \\(D(\\theta) = -2 \\log {p(y \\mid \\theta)}\\) the deviance, here considered as a function of \\(\\theta\\). Classical model choice uses hypothesis testing for comparing nested models, e.g. the deviance (likelihood ratio) test in generalized linear models. For non nested models, alternatives include the Akaike information criterion\n\\[AIC = -2 \\log {p(y \\mid \\hat{\\theta})} + 2k\\]\nwhere \\(\\hat{\\theta}\\) is the maximum likelihood estimate and \\(k\\) is the number of parameters in the model (dimension of \\(\\Theta\\)).\nAIC is built with the aim of favouring models that are likely to make good predictions. Since we generally do not have independent validation data, we can assess which model best predicts the observed data by using the deviance, but if parameters have been estimated we need some penalty for this double use of the data. AIC’s penalty of 2k has been shown to be asymptotically equivalent to leave-one-out cross-validation. However, AIC does not work in models with informative prior information, such as hierarchical models, since the prior effectively acts to ‘restrict’ the freedom of the model parameters, so the appropriate ‘number of parameters’ is generally unclear. (pp. 485–486, emphasis in the original)\n\nFor the past two decades, the Deviance Information Criterion [DIC; Spiegelhalter et al. (2002)] has been a popular information criterion among Bayesians. Let’s define \\(D\\) as the posterior distribution of deviance values and \\(\\bar D\\) as its mean. If you compute deviance based on the posterior mean, you have \\(\\hat D\\). Within a multi-parameter model, this would be the deviance based on the collection of the posterior mean of each parameter. With these, we define the DIC as\n\\[\\text{DIC} = \\bar D + (\\bar D + \\hat D) + \\bar D + p_D,\\]\nwhere \\(p_D\\) is the number of effective parameters in the model, which is also sometimes referred to as the penalty term. As McElreath pointed out in Statistical Rethinking, the \\(p_D\\)\n\nis just the expected distance between the deviance in-sample and the deviance out-of-sample. In the case of flat priors, DIC reduces directly to AIC, because the expected distance is just the number of parameters. But more generally, \\(p_D\\) will be some fraction of the number of parameters, because regularizing priors constrain a model’s flexibility. (p. 191)\n\nAs you’ll see, you can get the \\(p_D\\) for brms::brm() models. However, the DIC is limited in that it requires a multivariate Gaussian posterior and I’m not aware of a convenience function within brms that will compute the DIC. Which is fine. The DIC has been overshadowed in recent years by newer methods. But for a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit.\n\n4.6.4.1 The Widely Applicable Information Criterion (WAIC)\nThe main information criterion within our Stan ecosystem paradigm is the Widely Applicable Information Criterion [WAIC; Watanabe (2010)]. From McElreath, again, we read:\n\nIt does not require a multivariate Gaussian posterior, and it is often more accurate than DIC. There are types of models for which it is hard to define at all, however. We’ll discuss that issue more, after defining WAIC.\nThe distinguishing feature of WAIC is that it is pointwise. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data. This is useful, because some observations are much harder to predict than others and may also have different uncertainty… You can think of WAIC as handling uncertainty where it actually matters: for each independent observation.\nDefine \\(\\Pr (y_i)\\) as the average likelihood of observation \\(i\\) in the training sample. This means we compute the likelihood of \\(y_i\\) for each set of parameters samples from the posterior distribution. Then we average the likelihoods for each observation \\(i\\) and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density,\n\\[\\text{lppd} = \\sum_{i = 1}^N \\log \\Pr (y_i)\\]\nYou might say this out loud as:\n\nThe log-pointwise-predictive-density is the total across observations of the logarithm of the average likelihood of each observation.\n\nThe lppd is just a pointwise analog of deviance, averaged over the posterior distribution. If you multiplied it by -2, it’d be similar to the deviance, in fact.\nThe second piece of WAIC is the effective number of parameters \\(p_\\text{WAIC}\\). Define \\(V(y_i)\\) as the variance in log-likelihood for observation \\(i\\) in the training sample. This means we compute the log-likelihood of \\(y_i\\) for each sample from the posterior distribution. Then we take the variance of those values. This is \\(V(y_i)\\). Now \\(p_\\text{WAIC}\\) is defined as:\n\\[p_\\text{WAIC} = \\sum_{i=1}^N V(y_i)\\]\nNow WAIC is defined as:\n\\[\\text{WAIC} = -2(\\text{lppd} - p_\\text{WAIC})\\]\nAnd this value is yet another estimate of out-of-sample deviance. (pp. 191–192, emphasis in the original)\n\nIn Chapter 6 of my ebook translating McElreath’s Statistical Rethinking into brms and tidyverse code, I walk out how to hand compute the WAIC for a brm() fit. I’m not going to repeat the exercise, here. But do see the project and McElreath’s text if you’re interested. Rather, I’d like to get down to business. In brms, you can get a model’s WAIC with the waic() function.\n\nwaic(fit4.1)\n\n\nComputed from 4000 by 246 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -312.7 12.2\np_waic        55.7  4.9\nwaic         625.4 24.4\n\n39 (15.9%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nWe’ll come back to that warning message, later. For now, notice the main output is a \\(3 \\times 2\\) data frame with named rows. For the statistic in each row, you get a point estimate and a standard error. The WAIC is on the bottom. The effective number of parameters, the \\(p_\\text{WAIC}\\), is in the middle. Notice the elpd_waic on the top. That’s what you get without the \\(-2 \\times \\dots\\) in the formula. Remember how that part is just to put things in a metric amenable to \\(\\chi^2\\)-difference testing? Well, not all Bayesians like that and within the Stan ecosystem you’ll also see the WAIC expressed instead as the \\(\\text{elpd}_\\text{WAIC}\\).\nThe current recommended workflow within brms is to attach the WAIC information to the model fit. You do it with the add_criterion() function.\n\nfit4.1 &lt;- add_criterion(fit4.1, criterion = \"waic\")\n\nAnd now you can access that information directly with good-old $ indexing.\n\nfit4.1$criteria$waic\n\n\nComputed from 4000 by 246 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -312.7 12.2\np_waic        55.7  4.9\nwaic         625.4 24.4\n\n39 (15.9%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nYou might notice how that value is similar to the AIC and BIC values for Model A in Table 4.1. But it’s not identical and we shouldn’t expect it to be. It was computed by a different formula that accounts for priors. For our purposes, this is much better than the frequentist AIC and BIC. We need statistics that can handle priors.\n\n\n4.6.4.2 Leave-one-out cross-validation (LOO-CV)\nWe have another big option for model comparison within the Stan ecosystem. It involves leave-one-out cross-validation (LOO-CV). It’s often the case that we aren’t just interested in modeling the data we have in hand. The hope is our findings would generalize to other data we could have collected or may collect in the future. We’d like our findings to tell us something more general about the world at large. But unless you’re studying something highly uniform like the weights of hydrogen atoms, chances are your data have idiosyncrasies that won’t generalize well to other data. Sure, if we had all the information on all the relevant variables, we could explain the discrepancies across samples with hidden moderators and such. But we don’t have all the data and we typically don’t even know what all the relevant variables are.\nWelcome to science.\nTo address this problem, you might recommend we collect data from two samples for each project. Starting with sample A, we’d fit a series of models and settle on one or a small subset that both speak to our scientific hypothesis and seem to fit the sample A data well. Then we’d switch to sample B and rerun our primary model(s) from A to make sure our findings generalize. In this paradigm, we might call the A data in sample and the B data out of sample–or out of sample A, anyways.\nThe problem is we often have time and funding constraints. We only have sample A and we may never collect sample B. So we’ll need to make the most out of A. Happily, tricky statisticians have our back. Instead, what we might do is divide our data into \\(k\\) equally-sized subsets. Call those subsets folds. If we leave one of the folds out, we can fit the model with the remaining data and then see how well that model speaks to the left-out fold. After doing this for every fold, we can get an average performance across folds.\nNote how as \\(k\\) increases, the number of cases with a fold get smaller. In the extreme, \\(k = N\\), the number of cases within the data. At that point, \\(k\\)-fold cross-validation turns into leave-one-out cross-validation (LOO-CV).\nBut there’s a practical difficulty with LOO-CV: it’s costly. As you may have noticed, it takes some time to fit a Bayesian multilevel model. For large data and/or complicated models, sometimes it takes hours or days. Most of us just don’t have enough time or computational resources to fit that many models. Happily, we have an approximation to pure LOO-CV. Vehtari, Gelman, and Gabry (2017) proposed Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO) as an efficient way to approximate true LOO-CV. At this point, it’s probably best to let the statisticians speak for themselves:\n\nTo maintain comparability with the given dataset and to get easier interpretation of the differences in scale of effective number of parameters, we define a measure of predictive accuracy for the \\(n\\) data points taken one at a time:\n\\[\\begin{align}\n\\text{elpd} & = \\text{expected log pointwise predictive density for a new dataset} \\\\\n& = \\sum_{i = 1}^n \\int p_t (\\tilde{y}_i) \\log p (\\tilde{y}_i \\mid y) d \\tilde{y}_i,\n\\end{align}\\]\nwhere \\(p_t (\\tilde{y}_i)\\) is the distribution representing the true data-generating process for \\(\\tilde{y}_i\\). The \\(p_t (\\tilde{y}_i)\\)’s are unknown, and we will use cross-validation or WAIC to approximate. In a regression, these distributions are also implicitly conditioned on any predictors in the model…\nThe Bayesian LOO estimate of out-of-sample predictive fit is\n\\[\\text{elpd}_{\\text{loo}} = \\sum_{i = 1}^n \\log p (y_i \\mid y - _i),\\]\nwhere\n\\[p (y_i \\mid y - _i) = \\int p (y_i \\mid \\theta) p (\\theta \\mid y - _i) d \\theta\\]\nis the leave-one-out predictive density given the data without the ith data point. (pp. 2–3)\n\nFor the rest of the details, check out the original paper. Our goal is to practice using the PSIS-LOO. Since this is the only version of the LOO we’ll be using in this project, I’m just going to refer to it as the LOO from here on. To use the LOO to evaluate a brm() fit, you just use the loo() function. Though you don’t have to save the results as an object, we’ll be forward thinking and do so here.\n\nl_fit4.1 &lt;- loo(fit4.1)\n\nprint(l_fit4.1)\n\n\nComputed from 4000 by 246 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -316.3 12.6\np_loo        59.4  5.3\nlooic       632.7 25.2\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.5]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     241   98.0%   424     \n   (0.7, 1]   (bad)        5    2.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   0    0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nRemember that warning message we got from the waic() a while back? We get more information along those lines from the loo(). As it turns out, a few of the cases in the data were unduly influential in the model fit. Within the loo() paradigm, those are indexed by the pareto_k values. As it turns out, the Pareto \\(k\\) can be used as a diagnostic tool. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)’s are low. We typically get worried when those \\(k\\)’s exceed 0.7 and the loo() function spits out a warning message when they do.\nIf you didn’t know, the brms functions like the waic() and loo() actually come from the loo package (Vehtari et al., 2017; Vehtari et al., 2022; Yao et al., 2018). Explicitly loading loo will buy us some handy convenience functions.\n\nlibrary(loo)\n\nWe’ll be leveraging those \\(k\\) values with the pareto_k_table() and pareto_k_ids() functions. Both functions take objects created by the loo() or psis() functions. Let’s take a look at the pareto_k_table() function first.\n\npareto_k_table(l_fit4.1) \n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     241   98.0%   424     \n   (0.7, 1]   (bad)        5    2.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   0    0.0%   &lt;NA&gt;    \n\n\nThis is the same table that popped out earlier after using the loo(). Recall that this data set has 246 observations (i.e., execute count(alcohol1_pp)). With pareto_k_table(), we see how the Pareto \\(k\\) values have been categorized into bins ranging from “good” to “very bad”. Clearly, we like nice and low \\(k\\)’s. In this example, most of our observations are “good” or “ok.” Two are in the “bad” \\(k\\) range. We can take a closer look by placing our loo() object into plot().\n\nplot(l_fit4.1)\n\n\n\n\n\n\n\n\nWe got back a nice diagnostic plot for those \\(k\\) values, ordered by row number. We can see that our three observations with the “bad” \\(k\\) values were earlier in the data and it appears their \\(k\\) values are just a smidge above the recommended threshold. If we wanted to further verify to ourselves which observations those were, we’d use the pareto_k_ids() function.\n\npareto_k_ids(l_fit4.1, threshold = 0.7)\n\n[1]  27  33 132 177 229\n\n\nNote our use of the threshold argument. Play around with it to see how it works. In case you’re curious, here are those rows.\n\nalcohol1_pp %&gt;% \n  slice(pareto_k_ids(l_fit4.1, threshold = 0.7))\n\n# A tibble: 5 × 9\n     id   age   coa  male age_14 alcuse  peer  cpeer   ccoa\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     9    16     1     1      2   3.46 0     -1.02   0.549\n2    11    16     1     1      2   3.16 0     -1.02   0.549\n3    44    16     0     1      2   3    0     -1.02  -0.451\n4    59    16     0     1      2   2.65 0.894 -0.124 -0.451\n5    77    14     0     1      0   0    0.894 -0.124 -0.451\n\n\nIf you want an explicit look at those \\(k\\) values, execute l_fit4.1$diagnostics$pareto_k. For the sake of space, I’m going to omit the output.\n\nl_fit4.1$diagnostics$pareto_k\n\nThe pareto_k values can be used to examine cases that are overly-influential on the model parameters, something like a Cook’s \\(D_i\\). See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in Vehtari et al. (2017) and in this presentation by Aki Vehtari.\nAnyway, the implication of all this is these values suggest fit4.1 (i.e., Model A) might not be the best model of the data. Happily, we have other models to compare it to. That leads into the next section:\n\n\n4.6.4.3 You can compare Bayesian models with the WAIC and LOO\nRemember how we used the add_criterion() function, above. That’ll work for both WAIC and the LOO. Let’s do that for Models A through E.\n\nfit4.1 &lt;- add_criterion(fit4.1, criterion = c(\"loo\", \"waic\"))\nfit4.2 &lt;- add_criterion(fit4.2, criterion = c(\"loo\", \"waic\"))\nfit4.3 &lt;- add_criterion(fit4.3, criterion = c(\"loo\", \"waic\"))\nfit4.5 &lt;- add_criterion(fit4.5, criterion = c(\"loo\", \"waic\"))\n\nAnd to refresh, we can pull the WAIC and LOO information with $ indexing. Here’s how to get the LOO info for fit4.2.\n\nfit4.2$criteria$loo\n\n\nComputed from 4000 by 246 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -289.5 12.7\np_loo        94.7  7.5\nlooic       578.9 25.4\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.2, 1.8]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     218   88.6%   88      \n   (0.7, 1]   (bad)       27   11.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   1    0.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nSigh. Turns out there are even more overly-influential cases in the unconditional growth model. In the case of a real data analysis, this might suggest we need a more robust model. One possible solution might be switching out our Gaussian likelihood for the robust Student’s \\(t\\)-distribution. For an introduction, you might check out my blog post on the topic, Robust Linear Regression with Student’s \\(t\\)-Distribution. But that’ll take us farther afield than I want to go, right now.\nThe point to focus on, here, is we can use the loo_compare() function to compare fits by their WAIC or LOO. Let’s practice with the WAIC.\n\nws &lt;- loo_compare(fit4.1, fit4.2, fit4.3, fit4.5, criterion = \"waic\")\n\nprint(ws)\n\n       elpd_diff se_diff\nfit4.5   0.0       0.0  \nfit4.3  -5.1       3.7  \nfit4.2  -6.1       4.2  \nfit4.1 -42.1       8.3  \n\n\nRemember how we said that some contemporary Bayesians aren’t fans of putting Bayesian information criteria in a \\(\\chi^2\\) metric? Well, it turns out Aki Vehtari, of the Stan team and loo package fame–and also the primary author in that PSIS-LOO paper from before–, is one of those Bayesians. So instead of getting difference scores in the WAIC metric, we get them in the \\(\\text{elpd}_\\text{WAIC}\\) metric instead. But remember, if you prefer these estimates in the traditional metric, just multiply by -2.\n\ncbind(waic_diff = ws[, 1] * -2,\n      se        = ws[, 2] *  2)\n\n       waic_diff        se\nfit4.5   0.00000  0.000000\nfit4.3  10.20884  7.354128\nfit4.2  12.14385  8.391621\nfit4.1  84.28362 16.638349\n\n\nThe reason we multiplied the se_diff column (i.e., the standard errors for the difference estimates) by 2 is because you can’t have negative standard errors. That’d be silly.\nBut anyway, notice that the brm() fits have been rank ordered with the smallest differences at the top. Each row in the output is the difference of one of the fits compared to the best-fitting fit. Since fit4.5 apparently had the lowest WAIC value, it was ranked at the top. And notice how its waic_diff is 0. That, of course, is because \\(x - x = 0\\). So all the other difference scores are follow the formula \\(\\text{Difference}_x = \\text{WAIC}_\\text{fit\\_x} - \\text{WAIC}_\\text{fit\\_4.5}\\).\nConcerning our ws object, we can get more information on our models’ WAIC information if we include a simplify = F argument within print().\n\nprint(ws, simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit4.5    0.0       0.0  -270.5      11.1         74.5    5.6     541.1   22.2 \nfit4.3   -5.1       3.7  -275.6      11.5         80.7    6.2     551.3   23.0 \nfit4.2   -6.1       4.2  -276.6      11.7         81.9    6.4     553.2   23.3 \nfit4.1  -42.1       8.3  -312.7      12.2         55.7    4.9     625.4   24.4 \n\n\nTheir WAIC estimates and the associated standard errors are in the final two columns. In the two before that, we get the \\(p_\\text{WAIC}\\) estimates and their standard errors. We can get similar information for the LOO.\n\nloo_compare(fit4.1, fit4.2, fit4.3, fit4.5, criterion = \"loo\") %&gt;% \n  print(simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit4.5    0.0       0.0  -282.3     12.0        86.3    6.8    564.5   24.1  \nfit4.3   -5.5       3.9  -287.8     12.4        92.9    7.2    575.6   24.7  \nfit4.2   -7.2       4.5  -289.5     12.7        94.7    7.5    578.9   25.4  \nfit4.1  -34.1       8.5  -316.3     12.6        59.4    5.3    632.7   25.2  \n\n\nIf you wanted a more focused comparison, say between fit1 and fit2, you’d just simplify your input.\n\nloo_compare(fit4.1, fit4.2, criterion = \"loo\") %&gt;% \n  print(simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit4.2    0.0       0.0  -289.5     12.7        94.7    7.5    578.9   25.4  \nfit4.1  -26.9       7.8  -316.3     12.6        59.4    5.3    632.7   25.2  \n\n\nWe’ll get more practice with these methods as we go along. But for your own edification, you might check out the vignettes put out by the loo team.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#using-wald-statistics-to-test-composite-hypotheses-about-fixed-effects",
    "href": "04.html#using-wald-statistics-to-test-composite-hypotheses-about-fixed-effects",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.7 Using Wald statistics to test composite hypotheses about fixed effects",
    "text": "4.7 Using Wald statistics to test composite hypotheses about fixed effects\nI’m not going to address issues of composite null-hypothesis tests using the Wald statistic. However, we can address some of these issues from a different more estimation-based perspective. Consider the initial question posed on page 123:\n\nSuppose, for example, you wanted to test whether the entire true change trajectory for a particular type of adolescent–say, a child of non-alcoholic parents with an average value of PEER–differs from a “null” trajectory (one with zero intercept and zero slope). This is tantamount to asking whether the average child of non-alcoholic parents drinks no alcohol at age 14 and remains abstinent over time.\n\nSinger and Willett then expressed their joint null hypothesis as\n\\[H_0: \\gamma_{00} = 0 \\; \\text{and} \\; \\gamma_{10} = 0.\\]\nThis is a substantive question we can address more informatively with fitted(). First, let’s provide the necessary values for our predictor variables, coa, peer, and age_14.\n\nmu_peer &lt;- mean(alcohol1_pp$peer)\n\nnd &lt;- tibble(\n  coa    = 0,\n  peer   = mu_peer,\n  age_14 = seq(from = 0, to = 2, length.out = 30))\n\nhead(nd)\n\n# A tibble: 6 × 3\n    coa  peer age_14\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     0  1.02 0     \n2     0  1.02 0.0690\n3     0  1.02 0.138 \n4     0  1.02 0.207 \n5     0  1.02 0.276 \n6     0  1.02 0.345 \n\n\nNow we use fitted() to examine the model-implied trajectory for a child of non-alcoholic parents and average peer values.\n\nf &lt;- fitted(fit4.6, \n            newdata = nd,\n            re_formula = NA) %&gt;%\n  data.frame() %&gt;%\n  bind_cols(nd) %&gt;%\n  mutate(age = age_14 + 14) \n\nf %&gt;%\n  ggplot(aes(x = age)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              size = 0, alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  scale_y_continuous(\"alcuse\", breaks = 0:2, limits = c(0, 2)) +\n  labs(subtitle = \"Zero is credible for neither\\nthe intercept nor the slope.\") +\n  coord_cartesian(xlim = c(13, 17)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nRecall that the result of our Bayesian analyses are the probability of the parameters given the data, \\(p(\\theta \\mid d)\\). Based on our plot, there is much less than a .05 probability either intercept or slope for teens of this demographic are zero. If you really wanted to fixate on zero, you could even use geom_hline() to insert a horizontal line at zero in the figure. But all that fixating on zero detracts from what to my mind are the more important parts of the model. The intercept at age = 14 is about 1/3 and the endpoint when age = 16 is almost at \\(1\\). Those are our effect sizes. If you wanted to quantify those effect sizes more precisely, just query our fitted() object, f.\n\nf %&gt;% \n  select(age, Estimate, Q2.5, Q97.5) %&gt;% \n  filter(age %in% c(14, 16)) %&gt;% \n  mutate_all(round, digits = 2)\n\n  age Estimate Q2.5 Q97.5\n1  14     0.39 0.18  0.61\n2  16     0.94 0.67  1.21\n\n\nWorks like a champ. But we haven’t fully covered part of Singer and Willett’s joint hypothesis test. They proposed a joint Null that included the \\(\\gamma_{10} = 0\\). Though it’s clear from the plot that the trajectory increases, we can address the issue more directly with a difference score. For our difference, we’ll subtract the estimate at age = 14 from the one at age = 15. But to that, we’ll have to return to fitted(). So far, we’ve been using the default output which returns summaries of the posterior. To compute a proper difference score, we’ll need to work with all the posterior draws in order to approximate the full distribution. We do that by setting summary = F. And since we’re only interested in the estimates from these two age values, we’ll streamline our nd data.\n\nnd &lt;- tibble(coa    = 0,\n             peer   = mu_peer,\n             age_14 = 0:1)\n\nf &lt;- fitted(fit4.6, \n            newdata = nd,\n            re_formula = NA,\n            summary = F) %&gt;%\n  data.frame()\n\nstr(f)\n\n'data.frame':   4000 obs. of  2 variables:\n $ X1: num  0.376 0.368 0.246 0.262 0.507 ...\n $ X2: num  0.53 0.721 0.581 0.593 0.669 ...\n\n\nNow our f object has 4,000 rows and 2 columns. Each of the rows corresponds to one of the 4,000 post-warmup posterior draws. The columns correspond to the two rows in our nd data. To get a slope based on this combination of predictor values, we simply subtract the first column from the second.\n\nf &lt;- f %&gt;% \n  transmute(difference = X2 - X1)\n\nf %&gt;% \n  ggplot(aes(x = difference)) +\n  geom_density(size = 0, fill = \"grey25\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = \"Based on 4,000 posterior draws, not a single one\\nsuggests the slope is even close to zero. Rather, the\\nposterior mass is concentrated around 0.25.\",\n       x = expression(paste(gamma[0][1], \" (i.e., the difference between the two time points)\"))) +\n  coord_cartesian(xlim = 0:1) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere are the posterior mean and 95% intervals.\n\nf %&gt;% \n  summarise(mean = mean(difference),\n            ll   = quantile(difference, probs = 0.025),\n            ul   = quantile(difference, probs = 0.975))\n\n       mean        ll        ul\n1 0.2732853 0.1501059 0.3986967\n\n\nOn page 125, Singer and Willett further mused:\n\nWhen we examined the OLS estimated change trajectories in figure 4.2, we noticed that among children of non-alcoholic parents, those with low values of CPEER tended to have a lower initial status and steeper slopes than those with high values of CPEER. We might therefore ask whether the former group “catches up” to the latter. This is a question about the “vertical” separation between these two groups[’] true change trajectories at some later age, say 16.\n\nWithin their joint hypothesis testing paradigm, they pose this as testing\n\\[H_0: 0\\gamma_{00} + 0\\gamma_{01} + 1\\gamma_{02} + 0\\gamma_{10} + 2\\gamma_{12} = 0.\\]\nFrom our perspective, this is a differences of differences analysis. That is, first we’ll compute the model implied alcuse estimates for the four combinations of the two levels of age and peer, holding coa constant at 0. Second, we’ll compute the differences between the two peer levels at each age. Third and finally, we’ll compute a difference of those differences.\nFor our first step, recall it was fit4.7 that used the cpeer variable.\n\n# first step\nnd &lt;- crossing(age_14 = c(0, 2),\n               cpeer  = c(-0.363, 0.363)) %&gt;% \n  mutate(coa = 0)\n\nhead(nd)\n\n# A tibble: 4 × 3\n  age_14  cpeer   coa\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1      0 -0.363     0\n2      0  0.363     0\n3      2 -0.363     0\n4      2  0.363     0\n\nf &lt;- fitted(fit4.7, \n            newdata = nd,\n            re_formula = NA,\n            summary = F) %&gt;% \n  data.frame()\n\nhead(f)\n\n          X1        X2        X3        X4\n1 0.22345772 0.7363258 0.9818732 1.2458892\n2 0.09779833 0.5190283 0.6832786 1.0337496\n3 0.35129959 0.7531899 0.8885316 1.2076465\n4 0.30469554 0.7947526 0.7131174 1.2165299\n5 0.18459360 0.7056289 0.7453610 1.1968860\n6 0.11122673 0.6171796 0.6591346 0.8678888\n\n\nFor our initial difference scores, we’ll subtract the estimates for the lower level of cpeer from the higher ones.\n\n# step 2\nf &lt;- f %&gt;% \n  transmute(`difference at 14` = X2 - X1,\n            `difference at 16` = X4 - X3)\n\nhead(f)\n\n  difference at 14 difference at 16\n1        0.5128680        0.2640160\n2        0.4212300        0.3504709\n3        0.4018903        0.3191149\n4        0.4900571        0.5034125\n5        0.5210353        0.4515251\n6        0.5059529        0.2087543\n\n\nFor our final difference score, we’ll subtract the first difference score from the second.\n\n# step 3\nf &lt;- f %&gt;% \n  mutate(`difference in differences` = `difference at 16` - `difference at 14`)\n\nhead(f)\n\n  difference at 14 difference at 16 difference in differences\n1        0.5128680        0.2640160               -0.24885209\n2        0.4212300        0.3504709               -0.07075906\n3        0.4018903        0.3191149               -0.08277538\n4        0.4900571        0.5034125                0.01335537\n5        0.5210353        0.4515251               -0.06951026\n6        0.5059529        0.2087543               -0.29719860\n\n\nHere we’ll plot all three.\n\nf %&gt;%\n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density(size = 0, fill = \"grey25\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"different differences\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\n\n\n\n\n\nSinger and Willett concluded they could “reject the null hypothesis at any conventional level of significance” (p. 126). If we must appeal to the Null, here are the posterior means and 95% intervals for our differences.\n\nf %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            ll   = quantile(value, probs = 0.025),\n            ul   = quantile(value, probs = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 3 × 4\n  name                        mean     ll    ul\n  &lt;chr&gt;                      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 difference at 14           0.502  0.336 0.657\n2 difference at 16           0.283  0.056 0.51 \n3 difference in differences -0.219 -0.46  0.029\n\n\nOur results contrast a bit from Singer and Willett’s. Though the bulk of our posterior mass is concentrated around -0.22, zero is a credible value within the difference of differences density. Our best bet is the differences begin to converge over time. However, that rate of that convergence is subtle and somewhat imprecise relative to the effect size. Interpret with caution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#evaluating-the-tenability-of-a-models-assumptions",
    "href": "04.html#evaluating-the-tenability-of-a-models-assumptions",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.8 Evaluating the tenability of a model’s assumptions",
    "text": "4.8 Evaluating the tenability of a model’s assumptions\n“Whenever you fit a statistical model, you invoke assumptions” (p. 127). This is the case for multilevel Bayesian models, too.\n\n4.8.1 Checking functional form\nWe’ve already checked the functional form at level-1 with our version of Figure 4.1. When we made our version of Figure 4.1, we relied on ggplot2::stat_smooth() to compute the id-level OLS trajectories. To make our variants of Figure 4.4, we’ll have to back up and compute them externally with lm(). Here we’ll do so in bulk with a nested data frame. The broom package will help us extract the results.\n\nlibrary(broom)\n\no &lt;- alcohol1_pp %&gt;% \n  nest(-id, -coa, -peer) %&gt;% \n  mutate(ols = map(data, ~lm(data = ., alcuse ~ 1 + age_14))) %&gt;% \n  mutate(tidy = map(ols, tidy)) %&gt;% \n  unnest(tidy) %&gt;% \n  # this is unnecessary, but will help with plotting\n  mutate(term = factor(term, \n                       levels = c(\"(Intercept)\", \"age_14\"),\n                       labels = c(\"pi[0]\", \"pi[1]\")))\n\nhead(o)\n\n# A tibble: 6 × 10\n     id   coa  peer data             ols    term  estimate std.error statistic p.value\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;           &lt;list&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     1     1 1.26  &lt;tibble [3 × 6]&gt; &lt;lm&gt;   pi[0]    1.78     0.0999    17.8    0.0357\n2     1     1 1.26  &lt;tibble [3 × 6]&gt; &lt;lm&gt;   pi[1]    0.134    0.0774     1.73   0.333 \n3     2     1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt;   pi[0]   -0.167    0.373     -0.447  0.732 \n4     2     1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt;   pi[1]    0.5      0.289      1.73   0.333 \n5     3     1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt;   pi[0]    0.947    0.118      8.03   0.0789\n6     3     1 0.894 &lt;tibble [3 × 6]&gt; &lt;lm&gt;   pi[1]    1.16     0.0914    12.7    0.0501\n\n\nNow plot.\n\no %&gt;% \n  select(coa:peer, term:estimate) %&gt;% \n  pivot_longer(coa:peer) %&gt;% \n  \n  ggplot(aes(x = value, y = estimate)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_point(alpha = 2/3) +\n  theme(panel.grid = element_blank(),\n        strip.text = element_text(size = 11)) +\n  facet_grid(term ~ name, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\nWith a little more wrangling, we can extract the Pearson’s correlation coefficients for each panel.\n\no %&gt;% \n  select(coa:peer, term:estimate) %&gt;% \n  pivot_longer(coa:peer) %&gt;% \n  group_by(term, name) %&gt;% \n  nest() %&gt;%\n  mutate(r = map_dbl(data, ~cor(.)[2, 1] %&gt;% round(digits = 2)))\n\n# A tibble: 4 × 4\n# Groups:   term, name [4]\n  term  name  data                  r\n  &lt;fct&gt; &lt;chr&gt; &lt;list&gt;            &lt;dbl&gt;\n1 pi[0] coa   &lt;tibble [82 × 2]&gt;  0.39\n2 pi[0] peer  &lt;tibble [82 × 2]&gt;  0.58\n3 pi[1] coa   &lt;tibble [82 × 2]&gt; -0.04\n4 pi[1] peer  &lt;tibble [82 × 2]&gt; -0.19\n\n\n\n\n4.8.2 Checking normality\nThe basic multilevel model of change yields three variance parameters, \\(\\epsilon_{ij}\\), \\(\\zeta_{0i}\\), and \\(\\zeta_{1i}\\). Each measurement occasion in the model receives a model-implied estimate for each. Singer and Willett referred to those estimates as \\(\\hat{\\epsilon}_{ij}\\), \\(\\hat{\\zeta}_{0i}\\), and \\(\\hat{\\zeta}_{1i}\\). As with frequentist software, our Bayesian software brms will return these estimates.\nTo extract our Bayesian draws for the \\(\\hat{\\epsilon}_{ij}\\)’s, we use the residuals() function.\n\ne &lt;- residuals(fit4.6)\n\nstr(e)\n\n num [1:246, 1:4] 0.3021 0.2756 -0.0456 -0.3948 -0.5975 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n\nhead(e)\n\n        Estimate Est.Error      Q2.5     Q97.5\n[1,]  0.30209342 0.6795717 -1.024312 1.6401410\n[2,]  0.27558757 0.6884036 -1.057861 1.6419175\n[3,] -0.04560829 0.7497454 -1.493929 1.4706529\n[4,] -0.39483895 0.6959227 -1.755010 0.9993772\n[5,] -0.59745626 0.6904301 -1.922366 0.7543355\n[6,]  0.19927275 0.7512627 -1.303172 1.6979116\n\n\nFor our fit5, the residuals() function returned a \\(246 \\times 4\\) numeric array. Each row corresponded to one of the rows of the original data set. The four vectors are the familiar summaries Estimate, Est.Error, Q2.5, and Q97.5. If we’d like to work with these in a ggplot2-made plot, we’ll have to convert our e object to a data frame.\nAfter we make the conversion, we then make the top left panel of Figure 4.5.\n\ne &lt;- e %&gt;% \n  data.frame()\n\ne %&gt;% \n  ggplot(aes(sample = Estimate)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_qq() +\n  ylim(-2, 2) +\n  labs(x = \"Normal score\",\n       y = expression(hat(epsilon)[italic(ij)])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFor the right plot on the top, we need to add an id index. That’s as easy as appending the one from the original data. If you followed closely with the text, you may have also noticed this panel is of the standardized residuals. That just means we’ll have to hand-standardize ours before plotting.\n\ne %&gt;% \n  bind_cols(alcohol1_pp %&gt;% select(id)) %&gt;% \n  mutate(z = (Estimate - mean(Estimate)) / sd(Estimate)) %&gt;% \n  \n  ggplot(aes(x = id, y = z)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_point() +\n  scale_y_continuous(expression(italic(std)~hat(epsilon)[italic(ij)]), limits = c(-2, 2)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe’ll need to use the ranef() function to return the estimates for the \\(\\zeta\\)’s.\n\nz &lt;- ranef(fit4.6)\n\nstr(z)\n\nList of 1\n $ id: num [1:82, 1:4, 1:2] 0.297 -0.481 0.328 -0.348 -0.557 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : chr [1:82] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n  .. ..$ : chr [1:2] \"Intercept\" \"age_14\"\n\nz[[1]][1:6, , \"Intercept\"]\n\n    Estimate Est.Error       Q2.5      Q97.5\n1  0.2968669 0.3245855 -0.3265067 0.96412445\n2 -0.4813291 0.3421962 -1.2102704 0.13077958\n3  0.3281245 0.3394782 -0.3744580 0.97867224\n4 -0.3476118 0.3655476 -1.1069642 0.30492671\n5 -0.5573537 0.3355308 -1.2333489 0.06851604\n6  0.8233490 0.3731296  0.1552689 1.61484139\n\nz[[1]][1:6, , \"age_14\"]\n\n     Estimate Est.Error        Q2.5     Q97.5\n1  0.07333257 0.2510269 -0.43441739 0.5662414\n2 -0.08818017 0.2525496 -0.56208490 0.4384224\n3  0.44834735 0.2579810 -0.03282577 0.9795671\n4  0.14511723 0.2718540 -0.33298032 0.7234516\n5 -0.31561059 0.2491656 -0.81192164 0.1687735\n6  0.25432863 0.2669678 -0.28539765 0.7742281\n\n\nFor our fit5, the ranef() function returned a list of 1, indexed by id. Therein lay a 3-dimensional array. The first two dimensions are the same as what we got from residuals(), above. The third dimension had two levels: Intercept and age_14. In other words, the third dimension is the one that differentiated between \\(\\hat{\\zeta}_{0i}\\) and \\(\\hat{\\zeta}_{1i}\\). to make this thing a little more useful, let’s convert it to a long-formatted data frame.\n\nz &lt;- rbind(z[[1]][ , , \"Intercept\"],\n           z[[1]][ , , \"age_14\"]) %&gt;% \n  data.frame() %&gt;% \n  mutate(ranef = rep(c(\"hat(zeta)[0][italic(i)]\", \"hat(zeta)[1][italic(i)]\"), each = n() / 2))\n\nglimpse(z)\n\nRows: 164\nColumns: 5\n$ Estimate  &lt;dbl&gt; 0.29686688, -0.48132911, 0.32812451, -0.34761177, -0.55735369, 0.82334901, 0.20920524, -0.28069831, 0.17368335…\n$ Est.Error &lt;dbl&gt; 0.3245855, 0.3421962, 0.3394782, 0.3655476, 0.3355308, 0.3731296, 0.3241401, 0.3272219, 0.3604507, 0.3289889, …\n$ Q2.5      &lt;dbl&gt; -0.32650672, -1.21027045, -0.37445803, -1.10696425, -1.23334893, 0.15526886, -0.39075089, -0.92943890, -0.5750…\n$ Q97.5     &lt;dbl&gt; 0.96412445, 0.13077958, 0.97867224, 0.30492671, 0.06851604, 1.61484139, 0.87662914, 0.36928980, 0.84902139, 0.…\n$ ranef     &lt;chr&gt; \"hat(zeta)[0][italic(i)]\", \"hat(zeta)[0][italic(i)]\", \"hat(zeta)[0][italic(i)]\", \"hat(zeta)[0][italic(i)]\", \"h…\n\n\nNow we’re ready to plot the remaining panels on the left of Figure 4.5.\n\nz %&gt;% \n  ggplot(aes(sample = Estimate)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_qq() +\n  ylim(-1, 1) +\n  labs(x = \"Normal score\",\n       y = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ ranef, labeller = label_parsed, ncol = 1)\n\n\n\n\n\n\n\n\nHere are the ones on the right.\n\nz %&gt;% \n  bind_cols(\n    bind_rows(\n      alcohol1_pp %&gt;% distinct(id),\n      alcohol1_pp %&gt;% distinct(id)\n      )\n    ) %&gt;%\n  mutate(ranef = str_c(\"italic(std)~\", ranef)) %&gt;% \n  # note we have to group them before standardizing\n  group_by(ranef) %&gt;% \n  mutate(z = (Estimate - mean(Estimate)) / sd(Estimate)) %&gt;% \n  \n  ggplot(aes(x = id, y = z)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_point() +\n  scale_y_continuous(NULL, limits = c(-3, 3)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ ranef, labeller = label_parsed, ncol = 1)\n\n\n\n\n\n\n\n\nIf you were paying close attention, you may have noticed that for all three of our id-level deviation estimates, they were summarized not only by a posterior mean but by standard deviations and 95% intervals, too. To give a sense of what that means, here are those last two plots, again, but this time including vertical bars defined by the 95% intervals.\n\nz %&gt;% \n  bind_cols(\n    bind_rows(\n      alcohol1_pp %&gt;% distinct(id),\n      alcohol1_pp %&gt;% distinct(id)\n      )\n    ) %&gt;%\n\n  ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_pointrange(shape = 20, size = 1/3) +\n  ylab(NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ ranef, labeller = label_parsed, ncol = 1)\n\n\n\n\n\n\n\n\nWhen you go Bayesian, even your residuals get full posterior distributions.\n\n\n4.8.3 Checking homoscedasticity\nHere we examine the homoscedasticity assumption by plotting the residual estimates against our predictors. We’ll start with the upper left panel of Figure 4.6.\n\ne %&gt;% \n  bind_cols(alcohol1_pp) %&gt;% \n  \n  ggplot(aes(x = age, y = Estimate)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_point(alpha = 1/4) +\n  ylab(expression(hat(epsilon)[italic(ij)])) +\n  coord_cartesian(xlim = c(13, 17),\n                  ylim = c(-2, 2)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s a quick way to get the remaining four panels.\n\nz %&gt;% \n  bind_cols(\n    bind_rows(\n      alcohol1_pp %&gt;% distinct(id, coa, peer),\n      alcohol1_pp %&gt;% distinct(id, coa, peer)\n      )\n    ) %&gt;%\n  select(Estimate, ranef, coa, peer) %&gt;% \n  pivot_longer(-c(Estimate, ranef)) %&gt;% \n  \n  ggplot(aes(x = value, y = Estimate)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_point(alpha = 1/3) +\n  ylim(-1, 1) +\n  labs(x = \"covariate value\",\n       y = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.text = element_text(size = 10)) +\n  facet_grid(ranef ~ name, labeller = label_parsed, scales = \"free\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#model-based-empirical-bayes-estimates-of-the-individual-growth-parameters",
    "href": "04.html#model-based-empirical-bayes-estimates-of-the-individual-growth-parameters",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "4.9 Model-based (Empirical Bayes) estimates of the individual growth parameters",
    "text": "4.9 Model-based (Empirical Bayes) estimates of the individual growth parameters\nIn this section, the authors discussed two methods for constructing id-level trajectories: a) use a weighted average of the OLS and multilevel estimates and b) rely solely on the multilevel model by making use of the three sources of residual variation. Our method will be the latter.\nHere are the data for id == 23.\n\nalcohol1_pp %&gt;% \n  select(id:coa, cpeer, alcuse) %&gt;% \n  filter(id == 23)\n\n# A tibble: 3 × 5\n     id   age   coa cpeer alcuse\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    23    14     1 -1.02   1   \n2    23    15     1 -1.02   1   \n3    23    16     1 -1.02   1.73\n\n\n\ndraws_23 &lt;- as_draws_df(fit4.7) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  # make our pis\n  mutate(`pi[0][\",23\"]` = b_Intercept + b_coa * 1 + b_cpeer * -1.018,\n         `pi[1][\",23\"]` = b_age_14 + `b_age_14:cpeer` * -1.018)\n\nhead(draws_23)\n\n# A tibble: 6 × 7\n  b_Intercept b_age_14 b_coa b_cpeer `b_age_14:cpeer` `pi[0][\",23\"]` `pi[1][\",23\"]`\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1       0.480    0.317 0.480   0.706         -0.171            0.240          0.491\n2       0.308    0.275 0.645   0.580         -0.0487           0.363          0.325\n3       0.552    0.248 0.475   0.554         -0.0570           0.463          0.306\n4       0.550    0.208 0.390   0.675          0.00920          0.253          0.198\n5       0.445    0.263 0.410   0.718         -0.0479           0.124          0.312\n6       0.364    0.200 0.563   0.697         -0.205            0.218          0.408\n\n\nIt doesn’t help us much now, but the reason we’ve formatted the names for our two \\(\\pi\\) columns so oddly is because those names will work much nicer in the figure we’ll make, below. Just wait and see.\nAnyways, more than a couple point estimates, we returned the draws from the full posterior distribution. We might summarize them.\n\ndraws_23 %&gt;% \n  pivot_longer(starts_with(\"pi\")) %&gt;%   \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            ll   = quantile(value, probs = 0.025),\n            ul   = quantile(value, probs = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 2 × 4\n  name              mean     ll    ul\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 \"pi[0][\\\",23\\\"]\" 0.262 -0.071 0.604\n2 \"pi[1][\\\",23\\\"]\" 0.426  0.216 0.63 \n\n\nOr we could plot them.\n\ndraws_23 %&gt;% \n  pivot_longer(starts_with(\"pi\")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density(size = 0, fill = \"grey25\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"participant-specific parameter estimates\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\nYet this approach neglects the \\(\\zeta\\)’s. We’ve been extracting the \\(\\zeta\\)’s with ranef(). We also get them when we use as_draws_df(). Here we’ll extract both the \\(\\gamma\\)’s as well as the \\(\\zeta\\)’s for id == 23.\n\ndraws_23 &lt;- as_draws_df(fit4.7) %&gt;% \n  select(starts_with(\"b_\"), contains(\"23\"))\n\nglimpse(draws_23)\n\nRows: 4,000\nColumns: 7\n$ b_Intercept          &lt;dbl&gt; 0.4798917, 0.3084133, 0.5522447, 0.5497241, 0.4451113, 0.3642032, 0.2128283, 0.3048192, 0.4732113, …\n$ b_age_14             &lt;dbl&gt; 0.3169947, 0.2750504, 0.2479222, 0.2075498, 0.2630061, 0.1996543, 0.3126289, 0.2732347, 0.3046803, …\n$ b_coa                &lt;dbl&gt; 0.4797408, 0.6453055, 0.4747504, 0.3904525, 0.4099225, 0.5630397, 0.5889189, 0.5730423, 0.5522002, …\n$ b_cpeer              &lt;dbl&gt; 0.7064298, 0.5802066, 0.5535679, 0.6750098, 0.7176795, 0.6969048, 0.8433452, 0.7544907, 0.5425816, …\n$ `b_age_14:cpeer`     &lt;dbl&gt; -0.171385734, -0.048732134, -0.057007839, 0.009197917, -0.047872082, -0.204682233, -0.156986925, -0…\n$ `r_id[23,Intercept]` &lt;dbl&gt; 0.12410832, 0.26418171, 0.21847831, 0.41720673, 0.60980181, 0.39750613, 0.46508962, 0.18603794, -0.…\n$ `r_id[23,age_14]`    &lt;dbl&gt; 0.410808538, 0.027602206, 0.298144537, 0.002121383, 0.036973660, 0.023954961, 0.262958592, 0.409140…\n\n\nWith the r_id prefix, brms tells you these are residual estimates for the levels in the id grouping variable. Within the brackets, we learn these particular columns are for id == 23, the first with respect to the Intercept and second with respect to the age_14 parameter. Let’s put them to use.\n\ndraws_23 &lt;- draws_23 %&gt;% \n  mutate(`pi[0][\",23\"]` = b_Intercept + b_coa * 1 + b_cpeer * -1.018 + `r_id[23,Intercept]`,\n         `pi[1][\",23\"]` = b_age_14 + `b_age_14:cpeer` * -1.018 + `r_id[23,age_14]`)\n\nglimpse(draws_23)\n\nRows: 4,000\nColumns: 9\n$ b_Intercept          &lt;dbl&gt; 0.4798917, 0.3084133, 0.5522447, 0.5497241, 0.4451113, 0.3642032, 0.2128283, 0.3048192, 0.4732113, …\n$ b_age_14             &lt;dbl&gt; 0.3169947, 0.2750504, 0.2479222, 0.2075498, 0.2630061, 0.1996543, 0.3126289, 0.2732347, 0.3046803, …\n$ b_coa                &lt;dbl&gt; 0.4797408, 0.6453055, 0.4747504, 0.3904525, 0.4099225, 0.5630397, 0.5889189, 0.5730423, 0.5522002, …\n$ b_cpeer              &lt;dbl&gt; 0.7064298, 0.5802066, 0.5535679, 0.6750098, 0.7176795, 0.6969048, 0.8433452, 0.7544907, 0.5425816, …\n$ `b_age_14:cpeer`     &lt;dbl&gt; -0.171385734, -0.048732134, -0.057007839, 0.009197917, -0.047872082, -0.204682233, -0.156986925, -0…\n$ `r_id[23,Intercept]` &lt;dbl&gt; 0.12410832, 0.26418171, 0.21847831, 0.41720673, 0.60980181, 0.39750613, 0.46508962, 0.18603794, -0.…\n$ `r_id[23,age_14]`    &lt;dbl&gt; 0.410808538, 0.027602206, 0.298144537, 0.002121383, 0.036973660, 0.023954961, 0.262958592, 0.409140…\n$ `pi[0][\",23\"]`       &lt;dbl&gt; 0.36459536, 0.62725025, 0.68194134, 0.67022340, 0.73423783, 0.61529988, 0.40831143, 0.29582792, 0.4…\n$ `pi[1][\",23\"]`       &lt;dbl&gt; 0.9022739, 0.3522619, 0.6041007, 0.2003077, 0.3487136, 0.4319757, 0.7354002, 0.8093491, 0.4672463, …\n\n\nHere are our updated summaries.\n\ndraws_23 %&gt;% \n  pivot_longer(starts_with(\"pi\")) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            ll   = quantile(value, probs = 0.025),\n            ul   = quantile(value, probs = 0.975)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 2 × 4\n  name              mean     ll    ul\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 \"pi[0][\\\",23\\\"]\" 0.566 -0.022 1.22 \n2 \"pi[1][\\\",23\\\"]\" 0.512  0.016 0.961\n\n\nAnd here are the updated density plots.\n\ndraws_23 %&gt;% \n  pivot_longer(starts_with(\"pi\")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density(size = 0, fill = \"grey25\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"participant-specific parameter estimates\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\nWe’ve been focusing on the \\(\\pi\\) parameters. Notice that when we turn our attention to Figure 4.7, we’re now shifting focus slightly to the consequences of those parameters. We’re not attending to trajectories. It’s important to pick up on this distinction because it has consequences for our programming workflow. If you wanted to keep a parameter-centric workflow, we could continue to expand on our as_draws_df() by applying the full composite formula to explicitly add in predictions for various levels of age_14. And we could do that separately or in bulk for the eight participants highlighted in the figure.\nHowever pedagogically useful that might be, it’d be very tedious. If we instead take a trajectory-centric perspective, it’ll be more natural and efficient to work with a fitted()-based workflow. Let’s define our nd data.\n\nnd &lt;- alcohol1_pp %&gt;% \n  select(id:coa, age_14:alcuse, cpeer) %&gt;% \n  filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% \n  # these next two lines will make plotting easier\n  mutate(id_label = ifelse(id &lt; 10, str_c(\"0\", id), id)) %&gt;% \n  mutate(id_label = str_c(\"id = \", id_label))\n\nglimpse(nd)\n\nRows: 24\nColumns: 7\n$ id       &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65, 82, 82, 82\n$ age      &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16\n$ coa      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ age_14   &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2\n$ alcuse   &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, 1.732051, 1.732051, 1.414214, 1…\n$ cpeer    &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1.0180000, -1.0180000, -1.018000…\n$ id_label &lt;chr&gt; \"id = 04\", \"id = 04\", \"id = 04\", \"id = 14\", \"id = 14\", \"id = 14\", \"id = 23\", \"id = 23\", \"id = 23\", \"id = 32\", \"…\n\n\nWe’ve isolated the relevant predictor variables for our eight focal participants. Next we’ll pump them through fitted() and wrangle as usual.\n\nf &lt;- fitted(fit4.7, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nglimpse(f)\n\nRows: 24\nColumns: 11\n$ Estimate  &lt;dbl&gt; 1.1478486, 1.4523890, 1.7569293, 2.3695322, 2.7069877, 3.0444432, 0.5655661, 1.0771762, 1.5887864, 0.7802236, …\n$ Est.Error &lt;dbl&gt; 0.3566131, 0.2964612, 0.4476232, 0.3549815, 0.3047143, 0.4412486, 0.3223901, 0.2934049, 0.4309445, 0.3784683, …\n$ Q2.5      &lt;dbl&gt; 0.39914377, 0.87386730, 0.89772990, 1.71848914, 2.08821839, 2.20584449, -0.02161133, 0.49193914, 0.75113194, 0…\n$ Q97.5     &lt;dbl&gt; 1.792636, 2.046117, 2.644994, 3.094280, 3.304657, 3.908907, 1.223698, 1.665103, 2.432485, 1.564490, 1.693071, …\n$ id        &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65, 82, 82, 82\n$ age       &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16\n$ coa       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ age_14    &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2\n$ alcuse    &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, 1.732051, 1.732051, 1.414214, …\n$ cpeer     &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1.0180000, -1.0180000, -1.01800…\n$ id_label  &lt;chr&gt; \"id = 04\", \"id = 04\", \"id = 04\", \"id = 14\", \"id = 14\", \"id = 14\", \"id = 23\", \"id = 23\", \"id = 23\", \"id = 32\", …\n\n\nNotice how this time we omitted the re_formula = NA argument. By default, re_formula = NULL, the consequence of which is the output is based on all the parameters in the multilevel model, not just the \\(\\gamma\\)’s. Here are what they look like.\n\nf %&gt;% \n  ggplot(aes(x = age, y = Estimate)) +\n  geom_line(size = 1) +\n  scale_y_continuous(\"alcuse\", breaks = 0:4, limits = c(-1, 4)) +\n  xlim(13, 17) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ id_label, ncol = 4)\n\n\n\n\n\n\n\n\nNow we’ve warmed up, let’s add in the data and the other lines so make the full version of Figure 4.7. Before we do so, we’ll revisit fitted(). Notice the return of the re_formula = NA argument. The trajectories in our f_gamma_only data frame will only be sensitive to the \\(\\gamma\\)s.\n\nf_gamma_only &lt;- fitted(\n  fit4.7,\n  newdata = nd,\n  re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nglimpse(f_gamma_only)\n\nRows: 24\nColumns: 11\n$ Estimate  &lt;dbl&gt; 1.4998088, 1.6557380, 1.8116671, 1.6459337, 1.7700162, 1.8940987, 0.2618197, 0.6875579, 1.1132961, 0.2618197, …\n$ Est.Error &lt;dbl&gt; 0.1389740, 0.1371476, 0.1898794, 0.1533788, 0.1514247, 0.2139575, 0.1693999, 0.1649239, 0.2191853, 0.1693999, …\n$ Q2.5      &lt;dbl&gt; 1.21982890, 1.38552999, 1.43774048, 1.33736623, 1.47845805, 1.47884062, -0.07120761, 0.37517331, 0.69487737, -…\n$ Q97.5     &lt;dbl&gt; 1.7759306, 1.9233227, 2.1935770, 1.9519514, 2.0701747, 2.3253841, 0.6037027, 1.0154950, 1.5497587, 0.6037027, …\n$ id        &lt;dbl&gt; 4, 4, 4, 14, 14, 14, 23, 23, 23, 32, 32, 32, 41, 41, 41, 56, 56, 56, 65, 65, 65, 82, 82, 82\n$ age       &lt;dbl&gt; 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16, 14, 15, 16\n$ coa       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ age_14    &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2\n$ alcuse    &lt;dbl&gt; 0.000000, 2.000000, 1.732051, 2.828427, 3.605551, 2.828427, 1.000000, 1.000000, 1.732051, 1.732051, 1.414214, …\n$ cpeer     &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.9820000, 0.9820000, 0.9820000, -1.0180000, -1.0180000, -1.0180000, -1.01800…\n$ id_label  &lt;chr&gt; \"id = 04\", \"id = 04\", \"id = 04\", \"id = 14\", \"id = 14\", \"id = 14\", \"id = 23\", \"id = 23\", \"id = 23\", \"id = 32\", …\n\n\nLet’s plot!\n\nf %&gt;% \n  ggplot(aes(x = age)) +\n  # `id`-specific lines\n  geom_line(aes(y = Estimate),\n            size = 1) +\n  # gamma-centric lines\n  geom_line(data = f_gamma_only,\n            aes(y = Estimate),\n            size = 1/2) +\n  # OLS lines\n  stat_smooth(data = nd,\n              aes(y = alcuse),\n              method = \"lm\", se = F,\n              color = \"black\", linetype = 2, size = 1/2) +\n  # data points\n  geom_point(data = nd,\n             aes(y = alcuse)) +\n  scale_y_continuous(\"alcuse\", breaks = 0:4, limits = c(-1, 4)) +\n  xlim(13, 17) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ id_label, ncol = 4)\n\n\n\n\n\n\n\n\nThough our purpose was largely to reproduce Figure 4.7, we might push ourselves a little further. Our Bayesian estimates came with measures of uncertainty, the posterior standard deviations and the 95% intervals. Whenever possible, it’s good form to include some expression of our uncertainty in our plots. Here let’s focus on the id-specific trajectories.\n\nf %&gt;% \n  ggplot(aes(x = age, y = Estimate)) +\n  # `id`-specific 95% intervals\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey75\") +\n  # `id`-specific lines\n  geom_line(size = 1) +\n  # data points\n  geom_point(data = nd,\n             aes(y = alcuse)) +\n  scale_y_continuous(\"alcuse\", breaks = 0:4, limits = c(-1, 4)) +\n  xlim(13, 17) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ id_label, ncol = 4)\n\n\n\n\n\n\n\n\nThis also clarifies an important visualization point. If you only care about plotting straight lines, you only need two points. However, if you want to express shapes with curves, such as the typically-bowtie-shaped 95% intervals, you need estimates over a larger number of predictor values. Back to fitted()!\n\n# we need an expanded version of the `nd`\nnd_expanded &lt;- alcohol1_pp %&gt;% \n  select(id, coa, cpeer) %&gt;% \n  filter(id %in% c(4, 14, 23, 32, 41, 56, 65, 82)) %&gt;% \n  # this part is important!\n  expand(nesting(id, coa, cpeer),\n         age_14 = seq(from = 0, to = 2, length.out = 30)) %&gt;% \n  mutate(id_label = ifelse(id &lt; 10, str_c(\"0\", id), id)) %&gt;% \n  mutate(id_label = str_c(\"id = \", id_label),\n         age      = age_14 + 14)\n\n# pump our `nd_expanded` into `fitted()`\nf &lt;- fitted(fit4.7, newdata = nd_expanded) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd_expanded)\n\nglimpse(f)\n\nRows: 240\nColumns: 10\n$ Estimate  &lt;dbl&gt; 1.147849, 1.168851, 1.189854, 1.210857, 1.231860, 1.252863, 1.273865, 1.294868, 1.315871, 1.336874, 1.357876, …\n$ Est.Error &lt;dbl&gt; 0.3566131, 0.3458200, 0.3357556, 0.3264874, 0.3180849, 0.3106184, 0.3041569, 0.2987655, 0.2945031, 0.2914191, …\n$ Q2.5      &lt;dbl&gt; 0.3991438, 0.4419543, 0.4889595, 0.5358052, 0.5876218, 0.6237729, 0.6558085, 0.6990207, 0.7377322, 0.7633090, …\n$ Q97.5     &lt;dbl&gt; 1.792636, 1.797907, 1.801834, 1.809506, 1.824976, 1.836936, 1.854064, 1.872640, 1.893665, 1.915683, 1.936258, …\n$ id        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 14, 14, 14, 14, 14, …\n$ coa       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ cpeer     &lt;dbl&gt; 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, 0.7708544, …\n$ age_14    &lt;dbl&gt; 0.00000000, 0.06896552, 0.13793103, 0.20689655, 0.27586207, 0.34482759, 0.41379310, 0.48275862, 0.55172414, 0.…\n$ id_label  &lt;chr&gt; \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", \"id = 04\", …\n$ age       &lt;dbl&gt; 14.00000, 14.06897, 14.13793, 14.20690, 14.27586, 14.34483, 14.41379, 14.48276, 14.55172, 14.62069, 14.68966, …\n\n\nNotice how we now have many more rows. Let’s plot.\n\nf %&gt;% \n  ggplot(aes(x = age, y = Estimate)) +\n  # `id`-specific 95% intervals\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey75\") +\n  # `id`-specific lines\n  geom_line(size = 1) +\n  # data points\n  geom_point(data = nd,\n             aes(y = alcuse)) +\n  scale_y_continuous(\"alcuse\", breaks = 0:4, limits = c(-1, 4)) +\n  xlim(13, 17) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ id_label, ncol = 4)\n\n\n\n\n\n\n\n\nSinger and Willett pointed out that one of the ways in which the multilevel model is more parsimonious than a series of id-specific single-level models is that all id levels share the same \\(\\sigma_\\epsilon\\) parameter. For now, we should just point out that it’s possible to relax this assumption with modern Bayesian software, such as brms. For ideas on how, check out Donald Williams’ work (e.g., Williams et al., 2019).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#session-info",
    "href": "04.html#session-info",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] broom_1.0.10    loo_2.9.0.9000  ggdist_3.3.3    brms_2.23.0     Rcpp_1.1.0      lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        farver_2.1.2            S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4          \n [6] tensorA_0.36.2.1        digest_0.6.39           timechange_0.3.0        estimability_1.5.1      lifecycle_1.0.5        \n[11] StanHeaders_2.36.0.9000 survival_3.8-3          magrittr_2.0.4          posterior_1.6.1.9000    compiler_4.5.1         \n[16] rlang_1.1.7             tools_4.5.1             utf8_1.2.6              knitr_1.51              labeling_0.4.3         \n[21] bridgesampling_1.2-1    htmlwidgets_1.6.4       curl_7.0.0              bit_4.6.0               pkgbuild_1.4.8         \n[26] plyr_1.8.9              RColorBrewer_1.1-3      abind_1.4-8             multcomp_1.4-29         withr_3.0.2            \n[31] grid_4.5.1              stats4_4.5.1            xtable_1.8-4            inline_0.3.21           emmeans_1.11.2-8       \n[36] scales_1.4.0            MASS_7.3-65             cli_3.6.5               mvtnorm_1.3-3           rmarkdown_2.30         \n[41] crayon_1.5.3            generics_0.1.4          RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5         \n[46] tzdb_0.5.0              rstan_2.36.0.9000       splines_4.5.1           bayesplot_1.15.0.9000   parallel_4.5.1         \n[51] matrixStats_1.5.0       vctrs_0.6.5             V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1         \n[56] jsonlite_2.0.0          hms_1.1.4               bit64_4.6.0-1           glue_1.8.0              codetools_0.2-20       \n[61] distributional_0.5.0    stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1          \n[66] htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1                vroom_1.6.6             evaluate_1.0.5         \n[71] lattice_0.22-7          backports_1.5.0         rstantools_2.5.0.9000   coda_0.19-4.1           gridExtra_2.3          \n[76] nlme_3.1-168            checkmate_2.3.3         mgcv_1.9-3              xfun_0.55               zoo_1.8-14             \n[81] pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "04.html#comments",
    "href": "04.html#comments",
    "title": "4  Doing Data Analysis with the Multilevel Model for Change",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBürkner, P.-C. (2021). brms reference manual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/\n\n\nJaeger, B. C., Edwards, L. J., Das, K., & Sen, P. K. (2017). An R2 statistic for fixed effects in the generalized linear mixed model. Journal of Applied Statistics, 44(6), 1086–1105. https://doi.org/10.1080/02664763.2016.1193725\n\n\nKay, M. (2021). ggdist: Visualizations of distributions and uncertainty [Manual]. https://CRAN.R-project.org/package=ggdist\n\n\nKreft, I. G., & de Leeuw, J. (1998). Introducing multilevel modeling. SAGE Publications, Inc. https://doi.org/https://dx.doi.org/10.4135/9781849209366\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nLambert, B. (2018). A student’s guide to Bayesian statistics. SAGE Publications, Inc. https://ben-lambert.com/a-students-guide-to-bayesian-statistics/\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114\n\n\nPearl, J., Glymour, M., & Jewell, N. P. (2016). Causal Inference in Statistics - A Primer (1st Edition). Wiley. https://www.wiley.com/en-us/Causal+Inference+in+Statistics%3A+A+Primer-p-9781119186847\n\n\nRights, Jason D., & Cole, D. A. (2018). Effect size measures for multilevel models in clinical child and adolescent research: New R-squared methods and recommendations. Journal of Clinical Child & Adolescent Psychology, 47(6), 863–873. https://doi.org/10.1080/15374416.2018.1528550\n\n\nRights, Jason D., & Sterba, S. K. (2020). New recommendations on the use of R-squared differences in multilevel model comparisons. Multivariate Behavioral Research, 55(4), 568–599. https://doi.org/10.1080/00273171.2019.1660605\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nSnijders, T. A. B., & Bosker, R. J. (1994). Modeled variance in two-level models. Sociological Methods & Research, 22(3), 342–363. https://doi.org/10.1177/0049124194022003004\n\n\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. V. D. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4), 583–639. https://doi.org/10.1111/1467-9868.00353\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637\n\n\nVehtari, A., Gabry, J., Magnusson, M., Yao, Y., & Gelman, A. (2022). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4\n\n\nWatanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html\n\n\nWilliams, D. R., Rouder, J., & Rast, P. (2019). Beneath the surface: Unearthing within-Person variability and mean relations with Bayesian mixed models. https://doi.org/10.31234/osf.io/gwatq\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Doing Data Analysis with the Multilevel Model for Change</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Treating Time More Flexibly",
    "section": "",
    "text": "5.1 Variably spaced measurement occasions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "05.html#variably-spaced-measurement-occasions",
    "href": "05.html#variably-spaced-measurement-occasions",
    "title": "5  Treating Time More Flexibly",
    "section": "",
    "text": "Many researchers design their studies with the goal of assessing each individual on an identical set of occasions…\nYet sometimes, despite a valiant attempt to collect time-structured data, actual measurement occasions will differ. Variation often results from the realities of fieldwork and data collection…\nSo, too, many researchers design their studies knowing full well that the measurement occasions may differ across participants. This is certainly true, for example, of those who use an accelerated cohort design in which an age-heterogeneous cohort of individuals is followed for a constant period of time. Because respondents initial vary in age, and age, not wave, is usually the appropriate metric for analyses (see the discussion of time metrics in section 1.3.2), observed measurement occasions will differ across individuals. (p. 139, emphasis in the original)\n\n\n5.1.1 The structure of variably spaced data sets\nYou can find the PIAT data from the CNLSY study in the reading_pp.csv file.\n\nlibrary(tidyverse)\n\nreading_pp &lt;- read_csv(\"data/reading_pp.csv\")\n\nhead(reading_pp)\n\n# A tibble: 6 × 5\n     id  wave agegrp   age  piat\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1    6.5  6       18\n2     1     2    8.5  8.33    35\n3     1     3   10.5 10.3     59\n4     2     1    6.5  6       18\n5     2     2    8.5  8.5     25\n6     2     3   10.5 10.6     28\n\n\nOn pages 141 and 142, Singer and Willett discussed the phenomena of occasion creep, which is when “the temporal separations of occasions widens as the actual ages exceed design projections”. Here’s what that might look like.\n\nreading_pp %&gt;% \n  ggplot(aes(x = age, y = wave)) +\n  geom_vline(xintercept = c(6.5, 8.5, 10.5), color = \"white\") +\n  geom_jitter(alpha = 0.5, height = 0.33, width = 0) +\n  scale_x_continuous(breaks = c(6.5, 8.5, 10.5)) +\n  scale_y_continuous(breaks = 1:3) +\n  ggtitle(\"This is what occasion creep looks like.\",\n          subtitle = \"As the waves go by, the variation of the ages widens and their central tendency\\ncreeps away from the ideal point.\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s how we might make our version of Figure 5.1.\n\nset.seed(5)\n\n# wrangle\nreading_pp %&gt;% \n  nest(data = c(wave, agegrp, age, piat)) %&gt;% \n  sample_n(size = 9) %&gt;% \n  unnest(data) %&gt;% \n  # this will help format and order the facets\n  mutate(id = ifelse(id &lt; 10, str_c(\"0\", id), id) %&gt;% str_c(\"id = \", .)) %&gt;% \n  pivot_longer(contains(\"age\")) %&gt;% \n  \n  # plot\n  ggplot(aes(x = value, y = piat, color = name)) +\n  geom_point(alpha = 2/3) +\n  stat_smooth(method = \"lm\", se = F, linewidth = 1/2) +\n  scale_color_viridis_d(NULL, option = \"B\", end = 0.5, direction = -1) +\n  xlab(\"measure of age\") +\n  coord_cartesian(xlim = c(5, 12),\n                  ylim = c(0, 80)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id)\n\n\n\n\n\n\n\n\nSince it wasn’t clear which id values the authors used in the text, we just randomized. Change the seed to view different samples.\n\n\n5.1.2 Postulating and fitting multilevel models with variably spaced waves of data\nThe composite formula for our first model is\n\\[\\begin{align}\n\\text{piat}_{ij} & = \\gamma_{00} + \\gamma_{10} (\\text{agegrp}_{ij} - 6.5) + \\zeta_{0i} + \\zeta_{1i} (\\text{agegrp}_{ij} - 6.5) + \\epsilon_{ij} \\\\\n\\epsilon_{ij}    & \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{1i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\mathbf \\Sigma\n\\end{pmatrix}, \\text{where} \\\\\n\\mathbf \\Sigma  & = \\mathbf D \\mathbf\\Omega \\mathbf D', \\text{where} \\\\\n\\mathbf D       & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\text{and} \\\\\n\\mathbf \\Omega  & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix}\n\\end{align}\\]\nIt’s the same for the twin model using age rather than agegrp. Notice how we’ve switched from Singer and Willett’s \\(\\sigma^2\\) parameterization to the \\(\\sigma\\) parameterization typical of brms.\n\nreading_pp &lt;- reading_pp %&gt;% \n  mutate(agegrp_c = agegrp - 6.5,\n         age_c    = age    - 6.5)\n  \nhead(reading_pp)\n\n# A tibble: 6 × 7\n     id  wave agegrp   age  piat agegrp_c age_c\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     1     1    6.5  6       18        0 -0.5 \n2     1     2    8.5  8.33    35        2  1.83\n3     1     3   10.5 10.3     59        4  3.83\n4     2     1    6.5  6       18        0 -0.5 \n5     2     2    8.5  8.5     25        2  2   \n6     2     3   10.5 10.6     28        4  4.08\n\n\nIn the last chapter, we began familiarizing ourselves with brms::brm() default priors. It’s time to level up. Another approach is to use domain knowledge to set weakly-informative priors. Let’s start with the PIAT. The Peabody Individual Achievement Test is a standardized individual test of scholastic achievement. It yields several subtest scores. The reading subtest is the one we’re focusing on, here. As is typical for such tests, the PIAT scores are normed to yield a population mean of 100 and a standard deviation of 15.\nWith that information alone, even a PIAT novice should have an idea about how to specify the priors. Since our sole predictor variables are versions of age centered at 6.5, we know that the model intercept is interpreted as the expected value on the PIAT when the children are 6.5 years old. If you knew nothing else, you’d guess the mean score would be 100 with a standard deviation around 15. One way to use a weakly-informative prior on the intercept would be to multiply that \\(SD\\) by a number like 2.\nNext we need a prior for the time variables, age_c and agegrp_c. A one-unit increase in either of these is the expected increase in the PIAT with one year’s passage of age. Bringing in a little domain knowledge, IQ and achievement tests tend to be rather stable over time. However, we also expect children to get better as they age and we also don’t know exactly how these data have been adjusted for the children’s ages. It’s also important to know that it’s typical within the Bayesian world to place Normal priors on \\(\\beta\\) parameters. So one approach would be to center the Normal prior on 0 and put something like twice the PIAT’s standard deviation on the prior’s \\(\\sigma\\). If we were PIAT researchers, we could do much better. But with minimal knowledge of the test, this approach is certainly beats defaults.\nNext we have the variance parameters. Recall that brms::brm() defaults are Student’s \\(t\\)-distributions with \\(\\nu = 3\\) and \\(\\mu = 0\\). Let’s start there. Now we just need to put values on \\(\\sigma\\). Since the PIAT has a standard deviation of 15 in the population, why not just use 15? If you felt insecure about this, multiply if by a factor of 2 or so. Also recall that when Student’s \\(t\\)-distributions has a \\(\\nu = 3\\), the tails are quite fat. Within the context of Bayesian priors, those fat tails make it easy for the likelihood to dominate the prior even when it’s a good way into the tail.\nFinally, we have the correlation among the group-level variance parameters, \\(\\sigma_0\\) and \\(\\sigma_1\\). Recall that last chapter we learned the brms::brm() default was lkj(1). To get a sense of what the LKJ does, we’ll simulate from it. McElreath’s rethinking package contains a handy rlkjcorr() function, which will allow us to simulate n draws from a K by K correlation matrix for which \\(\\eta\\) is defined by eta. Let’s take n &lt;- 1e6 draws from two LKJ prior distributions, one with \\(\\eta = 1\\) and the other with \\(\\eta = 4\\).\n\nlibrary(rethinking)\n\nn &lt;- 1e6\nset.seed(5)\n\nlkj &lt;- tibble(eta = c(1, 4)) %&gt;% \n  mutate(draws = purrr::map(eta, ~ rlkjcorr(n, K = 2, eta = .)[, 2, 1])) %&gt;% \n  unnest(draws)\n\nglimpse(lkj)\n\nRows: 2,000,000\nColumns: 2\n$ eta   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ draws &lt;dbl&gt; 0.59957109, -0.83375155, 0.79069974, -0.05591997, -0.91300025, 0.45343010, 0.36319196, 0.47481372, 0.22494851, -0.…\n\n\nNow plot that lkj.\n\nlkj %&gt;% \n  mutate(eta = factor(eta)) %&gt;% \n  \n  ggplot(aes(x = draws, fill = eta, color = eta)) +\n  geom_density(linewidth = 0, alpha = 2/3) +\n  geom_text(data = tibble(\n    draws = c(0.75, 0.35),\n    y     = c(0.6, 1.05),\n    label = c(\"eta = 1\", \"eta = 4\"),\n    eta   = c(1, 4) %&gt;% as.factor()),\n    aes(y = y, label = label)) +\n  scale_fill_viridis_d(option = \"A\", end = 0.5) +\n  scale_color_viridis_d(option = \"A\", end = 0.5) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(rho)) +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen we use lkj(1), the prior is flat over the parameter space. However, setting lkj(4) is tantamount to a prior with a probability mass concentrated a bit towards zero. It’s a prior that’s skeptical of extremely large or small correlations. Within the context of our multilevel model \\(\\rho\\) parameters, this will be our weakly-regularizing prior.\nLet’s prepare to fit our models and load brms.\n\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\nFit the models. Following the same form, the differ in that the first uses agegrp_c and the second uses age_c.\n\nfit5.1 &lt;- brm(\n  data = reading_pp, \n  family = gaussian,\n  piat ~ 0 + Intercept + agegrp_c + (1 + agegrp_c | id),\n  prior = c(prior(normal(100, 30), class = b, coef = Intercept),\n            prior(normal(0, 30),   class = b, coef = agegrp_c),\n            prior(student_t(3, 0, 15), class = sd),\n            prior(student_t(3, 0, 15), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.01\")\n\nfit5.2 &lt;- brm(\n  data = reading_pp, \n  family = gaussian,\n  piat ~ 0 + Intercept + age_c + (1 + age_c | id),\n  prior = c(prior(normal(100, 30), class = b, coef = Intercept),\n            prior(normal(0, 30),   class = b, coef = age_c),\n            prior(student_t(3, 0, 15), class = sd),\n            prior(student_t(3, 0, 15), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.02\")\n\nFocusing first on fit5.1, our analogue to the \\((AGEGRP – 6.5)\\) model displayed in Table 5.2, here is our model summary.\n\nprint(fit5.1, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: piat ~ 0 + Intercept + agegrp_c + (1 + agegrp_c | id) \n   Data: reading_pp (Number of observations: 267) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 89) \n                        Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)              3.456     0.780    1.890    4.949 1.007     1093     2204\nsd(agegrp_c)               2.187     0.282    1.655    2.767 1.002     1173     2283\ncor(Intercept,agegrp_c)    0.182     0.218   -0.217    0.640 1.009      638     1239\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept   21.204     0.625   19.984   22.389 1.001     4616     2849\nagegrp_c     5.022     0.303    4.438    5.605 1.001     3038     2796\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    5.248     0.347    4.588    5.948 1.005     1210     2568\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere’s the age_c model.\n\nprint(fit5.2, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: piat ~ 0 + Intercept + age_c + (1 + age_c | id) \n   Data: reading_pp (Number of observations: 267) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 89) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           2.751     0.827    1.102    4.279 1.010      822      926\nsd(age_c)               1.984     0.246    1.515    2.479 1.006     1216     2285\ncor(Intercept,age_c)    0.238     0.238   -0.214    0.715 1.029      238      459\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept   21.116     0.597   19.956   22.335 1.000     5502     2791\nage_c        4.539     0.268    3.998    5.061 1.001     3181     2363\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    5.187     0.337    4.564    5.875 1.004     1236     2398\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor a more focused look, we can use fixef() compare our \\(\\gamma\\)’s to each other and those in the text.\n\nfixef(fit5.1) %&gt;% round(digits = 3)\n\n          Estimate Est.Error   Q2.5  Q97.5\nIntercept   21.204     0.625 19.984 22.389\nagegrp_c     5.022     0.303  4.438  5.605\n\nfixef(fit5.2) %&gt;% round(digits = 3)\n\n          Estimate Est.Error   Q2.5  Q97.5\nIntercept   21.116     0.597 19.956 22.335\nage_c        4.539     0.268  3.998  5.061\n\n\nHere are our \\(\\sigma_\\epsilon\\) summaries.\n\nVarCorr(fit5.1)$residual$sd %&gt;% round(digits = 3)\n\n Estimate Est.Error  Q2.5 Q97.5\n    5.248     0.347 4.588 5.948\n\nVarCorr(fit5.2)$residual$sd %&gt;% round(digits = 3)\n\n Estimate Est.Error  Q2.5 Q97.5\n    5.187     0.337 4.564 5.875\n\n\nFrom a quick glance, you can see they are about the square of the \\(\\sigma_\\epsilon^2\\) estimates in the text.\nLet’s go ahead and compute the LOO and WAIC.\n\nfit5.1 &lt;- add_criterion(fit5.1, criterion = c(\"loo\", \"waic\"))\nfit5.2 &lt;- add_criterion(fit5.2, criterion = c(\"loo\", \"waic\"))\n\nCompare the models with a WAIC difference.\n\nloo_compare(fit5.1, fit5.2, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit5.2    0.0       0.0  -865.8      15.4         77.6    7.3    1731.6   30.7 \nfit5.1   -3.5       3.6  -869.3      13.5         78.2    6.5    1738.6   27.1 \n\n\nThe WAIC difference between the two isn’t that large relative to its standard error. The LOO tells a similar story.\n\nloo_compare(fit5.1, fit5.2, criterion = \"loo\") %&gt;% \n  print(simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit5.2    0.0       0.0  -879.4     16.0        91.2    8.3   1758.7   32.1  \nfit5.1   -3.4       3.7  -882.7     14.2        91.7    7.4   1765.5   28.3  \n\n\nThe uncertainty in our WAIC and LOO estimates and their differences provides information that was not available for the AIC and the BIC comparisons in the text. We can also compare the WAIC and the LOO with model weights. Given the WAIC, from McElreath (2015) we learn\n\nA total weight of 1 is partitioned among the considered models, making it easier to compare their relative predictive accuracy. The weight for a model \\(i\\) in a set of \\(m\\) models is given by:\n\\[w_i = \\frac{\\exp(-\\frac{1}{2} \\text{dWAIC}_i)}{\\sum_{j = 1}^m \\exp(-\\frac{1}{2} \\text{dWAIC}_i)}\\]\nwhere dWAIC is the dWAIC in the compare table output. This example uses WAIC but the formula is the same for any other information criterion, since they are all on the deviance scale. (p. 199)\n\nThe compare() function McElreath referenced is from his (2020) rethinking package, which is meant to accompany his texts. We don’t have that function with brms. A rough analogue to the rethinking::compare() function is loo_compare(). We don’t quite have a dWAIC column from loo_compare(). Remember how last chapter we discussed how Aki Vehtari isn’t a fan of converting information criteria to the \\(\\chi^2\\) difference metric with that last \\(-2 \\times ...\\) step? That’s why we have an elpd_diff instead of a dWAIC. But to get the corresponding value, you just multiply those values by -2. And yet if you look closely at the formula for \\(w_i\\), you’ll see that each time the dWAIC term appears, it’s multiplied by \\(-\\frac{1}{2}\\). So we don’t really need that dWAIC value anyway. As it turns out, we’re good to go with our elpd_diff. Thus the above equation simplifies to\n\\[\nw_i = \\frac{\\exp(\\text{elpd\\_diff}_i)}{\\sum_{j = 1}^m \\exp(\\text{elpd\\_diff}_i)}\n\\]\nBut recall you don’t have to do any of this by hand. We have the brms::model_weights() function, which we can use to compute weights with the WAIC or the LOO.\n\nmodel_weights(fit5.1, fit5.2, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit5.1 fit5.2 \n  0.03   0.97 \n\nmodel_weights(fit5.1, fit5.2, weights = \"loo\") %&gt;% round(digits = 3)\n\nfit5.1 fit5.2 \n 0.033  0.967 \n\n\nBoth put the lion’s share of the weight on the age_c model. Back to McElreath McElreath (2015):\n\nBut what do these weights mean? There actually isn’t a consensus about that. But here’s Akaike’s interpretation, which is common.\n\nA model’s weight is an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models considered.\n\nHere’s the heuristic explanation. First, regard WAIC as the expected deviance of a model on future data. That is to say that WAIC gives us an estimate of \\(\\text{E} (D_\\text{test})\\). Akaike weights convert these deviance values, which are log-likelihoods, to plain likelihoods and then standardize them all. This is just like Bayes’ theorem uses a sum in the denominator to standardize the produce of the likelihood and prior. Therefore the Akaike weights are analogous to posterior probabilities of models, conditional on expected future data. (p. 199, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "05.html#varying-numbers-of-measurement-occasions",
    "href": "05.html#varying-numbers-of-measurement-occasions",
    "title": "5  Treating Time More Flexibly",
    "section": "5.2 Varying numbers of measurement occasions",
    "text": "5.2 Varying numbers of measurement occasions\nAs Singer and Willett pointed out,\n\nonce you allow the spacing of waves to vary across individuals, it is a small leap to allow their number to vary as well. Statisticians say that such data sets are unbalanced. As you would expect, balance facilitates analysis: models can be parameterized more easily, random effects can be estimated more precisely, and computer algorithms will converge more rapidly.\nYet a major advantage of the multilevel model for change is that it is easily fit to unbalanced data. (p. 146, emphasis in the original)\n\n\n5.2.1 Analyzing data sets in which the number of waves per person varies\nHere we load the wages_pp.csv data.\n\nwages_pp &lt;- read_csv(\"data/wages_pp.csv\")\n\nglimpse(wages_pp)\n\nRows: 6,402\nColumns: 15\n$ id            &lt;dbl&gt; 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 53, 53, 53, 53, 53, 53, 53, 53, 12…\n$ lnw           &lt;dbl&gt; 1.491, 1.433, 1.469, 1.749, 1.931, 1.709, 2.086, 2.129, 1.982, 1.798, 2.256, 2.573, 1.819, 2.928, 2.443, 2…\n$ exper         &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040, 3.021, 4.021, 5.521, 6.733, 7…\n$ ged           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ postexp       &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040, 3.021, 4.021, 5.521, 6.733, 7…\n$ black         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hispanic      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hgc           &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, 12, 12, 12, 12, 12, …\n$ hgc.9         &lt;dbl&gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -2, -2, -2, -2, -2, -2, -2, 3, 3, 3, 3, …\n$ uerate        &lt;dbl&gt; 3.215, 3.215, 3.215, 3.295, 2.895, 2.495, 2.595, 4.795, 4.895, 7.400, 7.400, 5.295, 4.495, 2.895, 2.595, 2…\n$ ue.7          &lt;dbl&gt; -3.785, -3.785, -3.785, -3.705, -4.105, -4.505, -4.405, -2.205, -2.105, 0.400, 0.400, -1.705, -2.505, -4.1…\n$ ue.centert1   &lt;dbl&gt; 0.000, 0.000, 0.000, 0.080, -0.320, -0.720, -0.620, 1.580, 0.000, 2.505, 2.505, 0.400, -0.400, -2.000, -2.…\n$ ue.mean       &lt;dbl&gt; 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 5.096500, 5.096500, 5.0965…\n$ ue.person.cen &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.08000000, -0.32000000, -0.72000000, -0.62000000, 1.58000000, -0.2015…\n$ ue1           &lt;dbl&gt; 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 4.895, 4.895, 4.895, 4.895, 4.895, 4.895, 4.895, 4…\n\n\nHere’s a more focused look along the lines of Table 5.3.\n\nwages_pp %&gt;% \n  select(id, exper, lnw, black, hgc, uerate) %&gt;% \n  filter(id %in% c(206, 332, 1028))\n\n# A tibble: 20 × 6\n      id exper   lnw black   hgc uerate\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1   206 1.87  2.03      0    10   9.2 \n 2   206 2.81  2.30      0    10  11   \n 3   206 4.31  2.48      0    10   6.30\n 4   332 0.125 1.63      0     8   7.1 \n 5   332 1.62  1.48      0     8   9.6 \n 6   332 2.41  1.80      0     8   7.2 \n 7   332 3.39  1.44      0     8   6.20\n 8   332 4.47  1.75      0     8   5.60\n 9   332 5.18  1.53      0     8   4.60\n10   332 6.08  2.04      0     8   4.30\n11   332 7.04  2.18      0     8   3.40\n12   332 8.20  2.19      0     8   4.39\n13   332 9.09  4.04      0     8   6.70\n14  1028 0.004 0.872     1     8   9.3 \n15  1028 0.035 0.903     1     8   7.4 \n16  1028 0.515 1.39      1     8   7.3 \n17  1028 1.48  2.32      1     8   7.4 \n18  1028 2.14  1.48      1     8   6.30\n19  1028 3.16  1.70      1     8   5.90\n20  1028 4.10  2.34      1     8   6.9 \n\n\nTo get a sense of the diversity in the number of occasions per id, use group_by() and count().\n\nwages_pp %&gt;% \n  count(id) %&gt;% \n   \n  ggplot(aes(y = n)) +\n  geom_bar() +\n  scale_y_continuous(\"# measurement occasions\", breaks = 1:13) +\n  xlab(\"count of cases\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe spacing of the measurement occasions also differs a lot across cases. Recall that exper “identifies the specific moment–to the nearest day–in each man’s labor force history associated with each observed value of” lnw (p. 147). Here’s a sense of what that looks like.\n\nwages_pp %&gt;% \n  filter(id %in% c(206, 332, 1028)) %&gt;% \n  mutate(id = factor(id)) %&gt;% \n  \n  ggplot(aes(x = exper, y = lnw, color = id)) +\n  geom_point() +\n  geom_line() +\n  scale_color_viridis_d(option = \"B\", begin = 0.35, end = 0.8) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nUneven for dayz.\nHere’s the brms version of the composite formula for Model A, the unconditional growth model for lnw.\n\\[\\begin{align}\n\\text{lnw}_{ij} & = \\gamma_{00} + \\gamma_{10} \\text{exper}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n  \\zeta_{0i} \\\\ \\zeta_{1i}\n  \\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\mathbf\\Sigma\n\\end{pmatrix}, \\text{where} \\\\\n\\mathbf\\Sigma & = \\mathbf D \\mathbf \\Omega \\mathbf D', \\text{where} \\\\\n\\mathbf D     & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix}, \\text{and} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix}\n\\end{align}\\]\nTo attempt setting priors for this, we need to review what lnw is. From the text: “To adjust for inflation, each hourly wage is expressed in constant 1990 dollars. To address the skewness commonly found in wage data and to linearize the individual wage trajectories, we analyze the natural logarithm of wages, LNW” (p. 147). So it’s the log of participant wages in 1990 dollars. From the official US Social Secutiry website, we learn the average yearly wage in 1990 was $20,172.11. Here’s that natural log for that.\n\nlog(20172.11)\n\n[1] 9.912056\n\n\nHowever, that’s the yearly wage. In the text, this is conceptualized as rate per hour. If we presume a 40 hour week for 52 weeks, this translates to a little less than $10 per hour.\n\n20172.11 / (40 * 52)\n\n[1] 9.69813\n\n\nHere’s what that looks like in a log metric.\n\nlog(20172.11 / (40 * 52))\n\n[1] 2.271933\n\n\nBut keep in mind that “to track wages on a common temporal scale, Murnane and colleagues decided to clock time from each respondent’s first day of work” (p. 147). So the wages at one’s initial point in the study were often entry-level wages. From the official website for the US Department of Labor, we learn the national US minimum wage in 1990 was $3.80 per hour. Here’s what that looks like on the log scale.\n\nlog(3.80)\n\n[1] 1.335001\n\n\nSo perhaps this is a better figure to center our prior for the model intercept on. If we stay with a conventional Gaussian prior and put \\(\\mu = 1.335\\), what value should we use for the standard deviation? Well, if that’s the log minimum and 2.27 is the log mean, then there’s less than a log value of 1 between the minimum and the mean. If we’d like to continue our practice of weakly regularizing priors a value of 1 or even 0.5 on the log scale would seem reasonable. For simplicity, we’ll use normal(1.335, 1).\nNext we need a prior for the expected increase over a single year’s employment. A conservative default might be to center it on zero—no change from year to year. Since as we’ve established a 1 on the log scale is more than the difference between the minimum and average hourly wages in 1990 dollars, we might just use normal(0, 0.5) as a starting point.\nSo then what about our variance parameters? Given these are all entry-level workers and given how little we’d expect them to increase from year to year, a student_t(3, 0, 1) on the log scale would seem pretty permissive.\nSo then here’s how we might formally specify our model priors:\n\\[\n\\begin{align}\n\\gamma_{00}     & \\sim \\operatorname{Normal}(1.335, 1) \\\\\n\\gamma_{10}     & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma_\\epsilon & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\sigma_0        & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\sigma_1        & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\rho_{01}       & \\sim \\operatorname{LKJ} (4)\n\\end{align}\n\\]\nFor a point of comparison, here are the brms::brm() default priors.\n\nget_prior(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + (1 + exper | id))\n\n                prior class      coef group resp dpar nlpar lb ub tag       source\n               (flat)     b                                                default\n               (flat)     b     exper                                 (vectorized)\n               (flat)     b Intercept                                 (vectorized)\n               lkj(1)   cor                                                default\n               lkj(1)   cor              id                           (vectorized)\n student_t(3, 0, 2.5)    sd                                  0             default\n student_t(3, 0, 2.5)    sd              id                  0        (vectorized)\n student_t(3, 0, 2.5)    sd     exper    id                  0        (vectorized)\n student_t(3, 0, 2.5)    sd Intercept    id                  0        (vectorized)\n student_t(3, 0, 2.5) sigma                                  0             default\n\n\nEven though our priors are still quite permissive on the scale of the data, they’re much more informative than the defaults. If we had formal backgrounds in the entry-level economy of the US in the early 1900s, we’d be able to specify even better priors. But hopefully this walk-through gives a sense of how to start thinking about model priors.\nLet’s fit the model. To keep the size of the fits/fit05.03.rds file below the 100MB GitHub limit, we’ll set chains = 3 and compensate by upping iter a little.\n\nfit5.3 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b, coef = exper),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.03\")\n\nHere are the results.\n\nprint(fit5.3, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + exper + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.232     0.011    0.211    0.254 1.003     1618     2914\nsd(exper)               0.041     0.003    0.036    0.047 1.005      589     1395\ncor(Intercept,exper)   -0.285     0.068   -0.412   -0.146 1.006      642     1451\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    1.716     0.011    1.694    1.737 1.001     2814     3601\nexper        0.046     0.002    0.041    0.050 1.001     2897     3972\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.309     0.003    0.303    0.315 1.001     3739     3071\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince the criterion lnw is on the log scale, Singer and Willett pointed out our estimate for \\(\\gamma_{10}\\) indicates a nonlinear growth rate on the natural dollar scale. They further explicated that “if an outcome in a linear relationship, \\(Y\\), is expressed as a natural logarithm and \\(\\hat \\gamma_{01}\\) is the regression coefficient for a predictor \\(X\\), then \\(100(e^{\\hat{\\gamma}_{01}} - 1)\\) is the percentage change in \\(Y\\) per unit difference in \\(X\\)” (p. 148, emphasis in the original). Here’s how to do that conversion with our brms output.\n\ndraws &lt;- as_draws_df(fit5.3) %&gt;%\n  transmute(percent_change = 100 * (exp(b_exper) - 1))\n\nhead(draws)\n\n# A tibble: 6 × 1\n  percent_change\n           &lt;dbl&gt;\n1           5.24\n2           4.49\n3           4.61\n4           4.50\n5           4.54\n6           5.32\n\n\nFor our plot, let’s break out Matthew Kay’s handy tidybayes package (Kay, 2023). With the tidybayes::stat_halfeye() function, it’s easy to put horizontal point intervals beneath out parameter densities. Here we’ll use 95% intervals.\n\nlibrary(tidybayes)\n\ndraws %&gt;% \n  ggplot(aes(x = percent_change)) +\n  stat_halfeye(.width = 0.95) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Percent change\",\n       x = expression(100*(italic(e)^(hat(gamma)[1][0])-1))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe tidybayes package also has a group of functions that make it easy to summarize posterior parameters with measures of central tendency (i.e., mean, median, mode) and intervals (i.e., percentile based, highest posterior density intervals). Here we’ll use median_qi() to get the posterior median and percentile-based 95% intervals.\n\ndraws %&gt;% \n  median_qi(percent_change)\n\n# A tibble: 1 × 6\n  percent_change .lower .upper .width .point .interval\n           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1           4.67   4.19   5.16   0.95 median qi       \n\n\nFor our next model, Model B in Table 5.4, we add two time-invariant covariates. In the data, these are listed as black and hgc.9. Before we proceed, let’s rename hgc.9 to be more consistent with tidyverse style.\n\nwages_pp &lt;- wages_pp %&gt;% \n  rename(hgc_9 = hgc.9)\n\nThere we go. Let’s take a look at the distributions of our covariates.\n\nwages_pp %&gt;% \n  pivot_longer(c(black, hgc_9)) %&gt;% \n\n  ggplot(aes(x = value)) +\n  geom_bar() +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nWe see black is a dummy variable coded “Black” = 1, “Non-black” = 0. hgc_9 is a somewhat Gaussian ordinal centered around zero. For context, it might also help to check its standard deviation.\n\nsd(wages_pp$hgc_9)\n\n[1] 1.347135\n\n\nWith a mean near 0 and an \\(SD\\) near 1, hgc_9 is almost in a standardized metric. If we wanted to keep with our weakly-regularizing approach, normal(0, 1) or even normal(0, 0.5) would be pretty permissive for both these variables. Recall that we’re predicting wage on the log scale. A \\(\\gamma\\) value of 1 or even 0.5 would be humongous for the social sciences. Since we already have the \\(\\gamma\\) for exper set to normal(0, 0.5), let’s just keep with that. Here’s how we might describe our model in statistical terms:\n\\[\\begin{align}\n\\text{lnw}_{ij} & = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_{i} - 9) + \\gamma_{02} \\text{black}_{i} \\\\\n& \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{11} \\text{exper}_{ij} \\times (\\text{hgc}_{i} - 9) + \\gamma_{12} \\text{exper}_{ij} \\times \\text{black}_{i} \\\\\n& \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{1i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n  \\mathbf D \\mathbf\\Omega \\mathbf D'\n  \\end{pmatrix} \\\\\n\\mathbf D     & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                                      & \\sim \\operatorname{Normal}(1.335, 1) \\\\\n\\gamma_{01}, \\dots, \\gamma_{12}                  & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 & \\sim \\operatorname{Student-t} (3, 0, 1) \\\\\n\\rho_{01}                                        & \\sim \\operatorname{LKJ} (4).\n\\end{align}\\]\nThe top portion up through the \\(\\mathbf\\Omega\\) line is the likelihood. Starting with \\(\\gamma_{00} \\sim \\text{Normal}(1.335, 1)\\) on down, we’ve listed our priors. Here’s how to fit the model with brms.\n\nfit5.4 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.04\")\n\nLet’s take a look at the results.\n\nprint(fit5.4, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.227     0.011    0.205    0.249 1.001     1120     2617\nsd(exper)               0.040     0.003    0.035    0.046 1.002      448     1177\ncor(Intercept,exper)   -0.290     0.070   -0.417   -0.142 1.001      532     1163\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.717     0.012    1.693    1.741 1.000     1994     2848\nhgc_9          0.035     0.008    0.020    0.050 1.000     2494     2456\nblack          0.016     0.024   -0.031    0.063 1.000     2062     2823\nexper          0.049     0.003    0.044    0.054 1.001     1746     3151\nhgc_9:exper    0.001     0.002   -0.002    0.005 1.001     2793     3103\nblack:exper   -0.018     0.006   -0.029   -0.008 1.001     2108     3323\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.309     0.003    0.303    0.315 1.000     2553     2872\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe \\(\\gamma\\)’s are on par with those in the text. When we convert the \\(\\sigma\\) parameters to the \\(\\sigma^2\\) metric, here’s what they look like.\n\ndraws &lt;- as_draws_df(fit5.4) \n\ndraws %&gt;% \n  transmute(`sigma[0]^2` = sd_id__Intercept^2,\n            `sigma[1]^2` = sd_id__exper^2,\n            `sigma[epsilon]^2` = sigma^2) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value, y = name)) +\n  stat_halfeye(.width = 0.95, normalize = \"xy\") +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  coord_cartesian(ylim = c(1.4, 3.4)) +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe might plot our \\(\\gamma\\)’s, too. Here we’ll use tidybayes::stat_pointinterval() to just focus on the points and intervals.\n\ndraws %&gt;% \n  select(b_Intercept:`b_black:exper`) %&gt;% \n  set_names(str_c(\"gamma\", c(\"[0][0]\", \"[0][1]\", \"[0][2]\", \"[1][0]\", \"[1][1]\", \"[1][2]\"))) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value, y = name)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  stat_pointinterval(.width = 0.95, size = 1/2) +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAs in the text, our \\(\\gamma_{02}\\) and \\(\\gamma_{11}\\) parameters hovered around zero. For our next model, Model C in Table 5.4, we’ll drop those parameters.\n\nfit5.5 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.05\")\n\nLet’s take a look at the results.\n\nprint(fit5.5, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.227     0.011    0.208    0.248 1.002     1876     3267\nsd(exper)               0.041     0.003    0.036    0.046 1.004      680     1244\ncor(Intercept,exper)   -0.296     0.068   -0.421   -0.153 1.005      755     1551\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.721     0.011    1.701    1.743 1.000     2830     3610\nhgc_9          0.038     0.006    0.026    0.051 1.000     2399     2904\nexper          0.049     0.003    0.044    0.054 1.000     2616     3250\nexper:black   -0.016     0.005   -0.025   -0.007 1.000     2565     2905\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.309     0.003    0.303    0.315 1.000     3764     3286\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nPerhaps unsurprisingly, the parameter estimates for fit5.5 ended up quite similar to those from fit5.4. Happily, they’re also similar to those in the text. Let’s compute the WAIC estimates.\n\nfit5.3 &lt;- add_criterion(fit5.3, criterion = \"waic\")\nfit5.4 &lt;- add_criterion(fit5.4, criterion = \"waic\")\nfit5.5 &lt;- add_criterion(fit5.5, criterion = \"waic\")\n\nCompare their WAIC estimates using \\(\\text{elpd}\\) difference scores.\n\nloo_compare(fit5.3, fit5.4, fit5.5, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.5     0.0       0.0 -2050.7     103.8        867.7    27.0    4101.5   207.6\nfit5.4    -3.5       1.4 -2054.2     103.8        867.9    27.2    4108.4   207.6\nfit5.3    -5.6       4.2 -2056.3     103.6        879.6    27.2    4112.7   207.3\n\n\nThe differences are subtle. Here are the WAIC weights.\n\nmodel_weights(fit5.3, fit5.4, fit5.5, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.3 fit5.4 fit5.5 \n 0.004  0.030  0.966 \n\n\nWhen we use weights, almost all goes to fit5.4 and fit5.5. Focusing on the trimmed model, fit5.5, let’s get ready to make our version of Figure 5.2. We’ll start with fitted() work.\n\nnd &lt;- crossing(black = 0:1,\n               hgc_9 = c(0, 3)) %&gt;% \n  expand_grid(exper = seq(from = 0, to = 11, length.out = 30))\n\nf &lt;- fitted(fit5.5, \n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nhead(f)\n\n  Estimate   Est.Error     Q2.5    Q97.5 black hgc_9     exper\n1 1.721384 0.010516393 1.701001 1.742686     0     0 0.0000000\n2 1.739933 0.010060519 1.720338 1.760306     0     0 0.3793103\n3 1.758481 0.009677787 1.739591 1.777891     0     0 0.7586207\n4 1.777029 0.009377155 1.758659 1.795574     0     0 1.1379310\n5 1.795578 0.009166706 1.777841 1.813822     0     0 1.5172414\n6 1.814126 0.009052731 1.796440 1.831905     0     0 1.8965517\n\n\nHere it is, our two-panel version of Figure 5.2.\n\nf %&gt;%\n  mutate(black = factor(black,\n                        labels = c(\"Latinos and Whites\", \"Blacks\")),\n         hgc_9 = factor(hgc_9, \n                        labels = c(\"9th grade dropouts\", \"12th grade dropouts\"))) %&gt;% \n  \n  ggplot(aes(x = exper,\n             color = black, fill = black)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4, linewidth = 0) +\n  geom_line(aes(y = Estimate)) +\n  scale_fill_viridis_d(NULL, option = \"C\", begin = 0.25, end = 0.75) +\n  scale_color_viridis_d(NULL, option = \"C\", begin = 0.25, end = 0.75) +\n  ylab(\"lnw\") +\n  coord_cartesian(ylim = c(1.6, 2.4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ hgc_9)\n\n\n\n\n\n\n\n\nThis leads in nicely to a brief discussion of posterior predictive checks (PPC). The basic idea is that good models should be able to retrodict the data used to produce them. Table 5.3 in the text introduced the data set by highlighting three participants and we went ahead and looked at their data in a plot. One way to do a PPC might be to plot their original data atop their model estimates. The fitted() function will help us with the preparatory work.\n\nnd &lt;- wages_pp %&gt;% \n  filter(id %in% c(206, 332, 1028))\n\nf &lt;- fitted(fit5.5, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nhead(f)\n\n  Estimate Est.Error     Q2.5    Q97.5  id   lnw exper ged postexp black hispanic hgc hgc_9 uerate   ue.7 ue.centert1  ue.mean\n1 2.057673 0.1390933 1.788865 2.330283 206 2.028 1.874   0       0     0        0  10     1  9.200  2.200       0.000 8.831667\n2 2.118934 0.1376507 1.855088 2.389486 206 2.297 2.814   0       0     0        0  10     1 11.000  4.000       1.800 8.831667\n3 2.216692 0.1545631 1.917386 2.518895 206 2.482 4.314   0       0     0        0  10     1  6.295 -0.705      -2.905 8.831667\n4 1.460931 0.1398149 1.191638 1.735435 332 1.630 0.125   0       0     0        1   8    -1  7.100  0.100       0.000 5.906500\n5 1.644951 0.1135030 1.423806 1.869218 332 1.476 1.625   0       0     0        1   8    -1  9.600  2.600       2.500 5.906500\n6 1.741623 0.1027752 1.539905 1.944975 332 1.804 2.413   0       0     0        1   8    -1  7.200  0.200       0.100 5.906500\n  ue.person.cen ue1\n1     0.3683333 9.2\n2     2.1683333 9.2\n3    -2.5366667 9.2\n4     1.1935000 7.1\n5     3.6935000 7.1\n6     1.2935000 7.1\n\n\nHere’s the plot.\n\nf %&gt;% \n  mutate(id = str_c(\"id = \", id)) %&gt;% \n  \n  ggplot(aes(x = exper)) +\n  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5,\n                      color = id)) +\n  geom_point(aes(y = lnw)) +\n  scale_color_viridis_d(option = \"B\", begin = 0.35, end = 0.8) +\n  labs(subtitle = \"The black dots are the original data. The colored points and vertical lines are the participant-specific posterior\\nmeans and 95% intervals.\") +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ id)\n\n\n\n\n\n\n\n\nAlthough each participant got their own intercept and slope, the estimates all fall in straight lines. Since we’re only working with time-invariant covariates, that’s about the best we can do. Though our models can express gross trends over time, they’re unable to speak to variation from occasion to occasion. Just a little later on in this chapter and we’ll learn how to do better.\n\n\n5.2.2 Practical problems that may arise when analyzing unbalanced data sets\nWith HMC, the issues with non-convergence aren’t quite the same as with maximum likelihood estimation. However, the basic issue still remains:\n\nEstimation of variance components requires that enough people have sufficient data to allow quantification of within-person residual variation–variation in the residuals over and above the fixed effects. If too many people have too little data, you will be unable to quantify [have difficulty quantifying] this residual variability. (p. 152)\n\nThe big difference is that as Bayesians, our priors add additional information that will help us define the posterior distributions of our variance components. Thus our challenge will choosing sensible priors for our \\(\\sigma\\)’s.\n\n5.2.2.1 Boundary constraints\nUnlike with the frequentist multilevel software discussed in the text, brms will not yield negative values on the \\(\\sigma\\) parameters. This is because the brms default is to set a lower limit of zero on those parameters. For example, see what happens when we execute fit5.3$model.\n\nfit5.3$model\n\n// generated with brms 2.23.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  vector[N] Z_1_2;\n  int&lt;lower=1&gt; NC_1;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  matrix[M_1, N_1] z_1;  // standardized group-level effects\n  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  matrix[N_1, M_1] r_1;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_1] r_1_1;\n  vector[N_1] r_1_2;\n  // prior contributions to the log posterior\n  real lprior = 0;\n  // compute actual group-level effects\n  r_1 = scale_r_cor(z_1, sd_1, L_1);\n  r_1_1 = r_1[, 1];\n  r_1_2 = r_1[, 2];\n  lprior += normal_lpdf(b[1] | 1.335, 1);\n  lprior += normal_lpdf(b[2] | 0, 0.5);\n  lprior += student_t_lpdf(sigma | 3, 0, 1)\n    - 1 * student_t_lccdf(0 | 3, 0, 1);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 1)\n    - 2 * student_t_lccdf(0 | 3, 0, 1);\n  lprior += lkj_corr_cholesky_lpdf(L_1 | 4);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n];\n    }\n    target += normal_id_glm_lpdf(Y | X, mu, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(to_vector(z_1));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n  vector&lt;lower=-1,upper=1&gt;[NC_1] cor_1;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_1) {\n    for (j in 1:(k - 1)) {\n      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n    }\n  }\n}\n\n\nThat returned the Stan code corresponding to our brms::brm() code, above. Notice the second and third lines in the parameters block. Both contained &lt;lower=0&gt;, which indicated the lower bounds for those parameters was zero. See? Stan has you covered.\nLet’s load the wages_small_pp.csv data.\n\nwages_small_pp &lt;- read_csv(\"data/wages_small_pp.csv\") %&gt;% \n  rename(hgc_9 = hcg.9)\n\nglimpse(wages_small_pp)\n\nRows: 257\nColumns: 5\n$ id    &lt;dbl&gt; 206, 206, 206, 266, 304, 329, 329, 329, 336, 336, 336, 394, 394, 394, 518, 518, 541, 541, 541, 832, 832, 832, 911,…\n$ lnw   &lt;dbl&gt; 2.028, 2.297, 2.482, 1.808, 1.842, 1.422, 1.308, 1.885, 1.892, 1.279, 2.224, 2.383, 1.929, 1.609, 1.272, 1.613, 1.…\n$ exper &lt;dbl&gt; 1.874, 2.814, 4.314, 0.322, 0.580, 0.016, 0.716, 1.756, 1.910, 2.514, 3.706, 1.890, 2.770, 3.010, 0.525, 1.926, 1.…\n$ black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ hgc_9 &lt;dbl&gt; 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 2, -1, 0, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2,…\n\n\nHere’s the distribution of the number of measurement occasions for our small data set.\n\nwages_small_pp %&gt;% \n  count(id) %&gt;% \n   \n  ggplot(aes(y = n)) +\n  geom_bar() +\n  scale_y_continuous(\"# measurement occasions\", breaks = 1:13, limits = c(0.5, 13)) +\n  xlab(\"count of cases\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nOur brm() code is the same as that for fit5.5, above, with just a slightly different data argument. If we wanted to, we could be hasty and just use update(), instead. But since we’re still practicing setting our priors and such, here we’ll be exhaustive.\n\nfit5.6 &lt;- brm(\n  data = wages_small_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.06\")\n\n\nprint(fit5.6)\n\nWarning: There were 35 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) \n   Data: wages_small_pp (Number of observations: 257) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 124) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.29      0.05     0.19     0.38 1.01      721     1308\nsd(exper)                0.04      0.03     0.00     0.10 1.01      458      372\ncor(Intercept,exper)    -0.04      0.32    -0.61     0.58 1.00     2672     2450\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       1.73      0.05     1.64     1.83 1.00     2455     2493\nhgc_9           0.05      0.03    -0.00     0.10 1.00     2718     2656\nexper           0.05      0.02     0.01     0.09 1.00     2257     1984\nexper:black    -0.05      0.04    -0.13     0.02 1.00     2326     1322\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.34      0.02     0.30     0.39 1.01     1150     1923\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s walk through this slow.\nYou may have noticed that warning message about divergent transitions. We’ll get to that in a bit. First focus on the parameter estimates for sd(exper). Unlike in the text, our posterior mean is not 0.000. But do remember that our posterior is parameterized in the \\(\\sigma\\) metric. Let’s do a little converting and look at it in a plot.\n\ndraws &lt;- as_draws_df(fit5.6)\n\nv &lt;- draws %&gt;%\n  transmute(sigma_1 = sd_id__exper) %&gt;% \n  mutate(sigma_2_1 = sigma_1^2) %&gt;% \n  set_names(\"sigma[1]\", \"sigma[1]^2\") %&gt;% \n  pivot_longer(everything())\n\nPlot.\n\nv %&gt;% \n  ggplot(aes(x = value, y = name)) +\n  stat_halfeye(.width = 0.95, normalize = \"xy\") +\n  scale_y_discrete(NULL, labels = parse(text = c(\"sigma[1]\", \"sigma[1]^2\"))) +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIn the \\(\\sigma\\) metric, the posterior is bunched up a little on the boundary, but much of its mass is a gently right-skewed mound concentrated in the 0—0.1 range. When we convert the posterior to the \\(\\sigma^2\\) metric, the parameter appears much more bunched up against the boundary. Because we typically summarize our posteriors with means or medians, the point estimate still moves away from zero.\n\nv %&gt;% \n  group_by(name) %&gt;% \n  mean_qi() %&gt;% \n  mutate_if(is.double, round, digits = 4)\n\n# A tibble: 2 × 7\n  name        value .lower .upper .width .point .interval\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 sigma[1]   0.0376 0.0014 0.0966   0.95 mean   qi       \n2 sigma[1]^2 0.0021 0      0.0093   0.95 mean   qi       \n\nv %&gt;% \n  group_by(name) %&gt;% \n  median_qi() %&gt;% \n  mutate_if(is.double, round, digits = 4)\n\n# A tibble: 2 × 7\n  name        value .lower .upper .width .point .interval\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 sigma[1]   0.0334 0.0014 0.0966   0.95 median qi       \n2 sigma[1]^2 0.0011 0      0.0093   0.95 median qi       \n\n\nBut it really does start to shoot to zero if we attempt to summarize the central tendency with the mode, as within the maximum likelihood paradigm.\n\nv %&gt;% \n  group_by(name) %&gt;% \n  mode_qi() %&gt;% \n  mutate_if(is.double, round, digits = 4)\n\n# A tibble: 2 × 7\n  name        value .lower .upper .width .point .interval\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 sigma[1]   0.0103 0.0014 0.0966   0.95 mode   qi       \n2 sigma[1]^2 0      0      0.0093   0.95 mode   qi       \n\n\nBacking up to that warning message, we were informed that “Increasing adapt_delta above 0.8 may help.” The adapt_delta parameter ranges from 0 to 1. The brm() default is 0.8. In my experience, increasing to 0.9 or 0.99 is often a good place to start. For this model, 0.9 wasn’t quite enough, but 0.99 worked. Here’s how to do it.\n\nfit5.7 &lt;- brm(\n  data = wages_small_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit05.07\")\n\nNow look at the summary.\n\nprint(fit5.7)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) \n   Data: wages_small_pp (Number of observations: 257) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 124) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.29      0.05     0.19     0.38 1.00     1011     1360\nsd(exper)                0.04      0.03     0.00     0.10 1.01      593     1334\ncor(Intercept,exper)    -0.04      0.31    -0.61     0.56 1.00     3367     3205\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       1.74      0.05     1.64     1.83 1.00     2862     3020\nhgc_9           0.05      0.03    -0.00     0.10 1.00     2556     3262\nexper           0.05      0.02     0.01     0.10 1.00     3160     2770\nexper:black    -0.06      0.04    -0.13     0.02 1.00     2664     2980\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.34      0.02     0.30     0.39 1.00     1520     2157\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur estimates were pretty much the same as before. Happily, this time we got our summary without any warning signs. It won’t always be that way, so make sure to take adapt_delta warnings seriously.\nNow do note that for both fit5.6 and fit5.7, our effective sample sizes for \\(\\sigma_0\\) and \\(\\sigma_1\\) aren’t terribly large relative to the total number of post-warmup draws, 4,000. If it was really important that you had high-quality summary statistics for these parameters, you might need to refit the model with something like iter = 20000, warmup = 2000.\nIn Model B in Table 5.5, Singer and Willett gave the results of a model with the boundary constraints on the \\(\\sigma^2\\) parameters removed. I am not going to attempt something like that with brms. If you’re interested, you’re on your own.\nBut we will fit a version of their Model C where we’ve removed the \\(\\sigma_1\\) parameter. Notice that this results in our removal of the LKJ prior for \\(\\rho_{01}\\), too. Without a \\(\\sigma_1\\), there’s no other parameter with which our lonely \\(\\sigma_0\\) might covary.\n\nfit5.8 &lt;- brm(\n  data = wages_small_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.08\")\n\nHere is the basic model summary.\n\nprint(fit5.8)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 | id) \n   Data: wages_small_pp (Number of observations: 257) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 124) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.30      0.04     0.22     0.37 1.00     1097     1983\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       1.74      0.05     1.64     1.83 1.00     2377     2814\nhgc_9           0.05      0.03    -0.00     0.09 1.00     1867     2528\nexper           0.05      0.02     0.01     0.09 1.00     2753     2854\nexper:black    -0.06      0.03    -0.13     0.01 1.00     2839     2996\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.34      0.02     0.30     0.39 1.00     1492     2007\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNo warning messages and our effective samples for \\(\\sigma_0\\) improved a bit. Compute the WAIC for both models.\n\nfit5.7 &lt;- add_criterion(fit5.7, criterion = \"waic\")\nfit5.8 &lt;- add_criterion(fit5.8, criterion = \"waic\")\n\nCompare.\n\nloo_compare(fit5.7, fit5.8, criterion = \"waic\") %&gt;% \n  print(simplify = F, digits = 3)\n\n       elpd_diff se_diff  elpd_waic se_elpd_waic p_waic   se_p_waic waic     se_waic \nfit5.8    0.000     0.000 -132.407    20.449       70.116   11.986   264.814   40.898\nfit5.7   -0.346     1.384 -132.753    21.410       71.609   13.144   265.506   42.820\n\n\nYep. Those WAIC estimates are quite similar and when you compare them with formal \\(\\text{elpd}\\) difference scores, the standard error is about the same size as the difference itself.\nThough we’re stepping away from the text a bit, we should explore more alternatives for this boundary issue. The Stan team has put together a Prior Choice Recommendations wiki at https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations. In the Boundary-avoiding priors for modal estimation (posterior mode, MAP, marginal posterior mode, marginal maximum likelihood, MML) section, we read:\n\n\nThese are for parameters such as group-level scale parameters, group-level correlations, group-level covariance matrix\nWhat all these parameters have in common is that (a) they’re defined on a space with a boundary, and (b) the likelihood, or marginal likelihood, can have a mode on the boundary. Most famous example is the group-level scale parameter tau for the 8-schools hierarchical model.\nWith full Bayes the boundary shouldn’t be a problem (as long as you have any proper prior).\nBut with modal estimation, the estimate can be on the boundary, which can create problems in posterior predictions. For example, consider a varying-intercept varying-slope multilevel model which has an intercept and slope for each group. Suppose you fit marginal maximum likelihood and get a modal estimate of 1 for the group-level correlation. Then in your predictions the intercept and slope will be perfectly correlated, which in general will be unrealistic.\nFor a one-dimensional parameter restricted to be positive (e.g., the scale parameter in a hierarchical model), we recommend Gamma(2,0) prior (that is, p(tau) proportional to tau) which will keep the mode away from 0 but still allows it to be arbitrarily close to the data if that is what the likelihood wants. For details see this paper by Chung et al.: http://www.stat.columbia.edu/~gelman/research/published/chung_etal_Pmetrika2013.pdf\n\nGamma(2,0) biases the estimate upward. When number of groups is small, try Gamma(2,1/A), where A is a scale parameter representing how high tau can be.\n\n\n\nWe should walk those Gamma priors out, a bit. The paper by Chung et al. (2013) is quite helpful. We’ll first let them give us a little more background in the topic:\n\nZero group-level variance estimates can cause several problems. Zero variance can go against prior knowledge of researchers and results in underestimation of uncertainty in fixed coefficient estimates. Inferences for groups are often of interest to researchers, but when the group-level variance is estimated as zero, the resulting predictions of the group-level errors will all be zero, so one fails to find unexplained differences between groups. In addition, uncertainty in predictions for new and existing groups is also understated. (p. 686)\n\nThey expounded further on page 687.\n\nWhen a variance parameter is estimated as zero, there is typically a large amount of uncertainty about this variance. One possibility is to declare in such situations that not enough information is available to estimate a multilevel model. However, the available alternatives can be unappealing since, as noted in the introduction, discarding a variance component or setting the variance to zero understates the uncertainty. In particular, standard errors for coefficients of covariates that vary between groups will be too low as we will see in Section 2.2. The other extreme is to fit a regression with indicators for groups (a fixed-effects model), but this will overcorrect for group effects (it is mathematically equivalent to a mixed-effects model with variance set to infinity), and also does not allow predictions for new groups.\nDegenerate variance estimates lead to complete shrinkage of predictions for new and existing groups and yield estimated prediction standard errors that understate uncertainty. This problem has been pointed out by Li and Lahiri (2010) and Morris and Tang (2011) in small area estimation….\nIf zero variance is not a null hypothesis of interest, a boundary estimate, and the corresponding zero likelihood ratio test statistic, should not necessarily lead us to accept the null hypothesis and to proceed as if the true variance is zero.\n\nIn their paper, they covered both penalized maximum likelihood and full Bayesian estimation. We’re just going to focus on Bayes, but some of the quotes will contain ML talk. Further, we read:\n\nWe recommend a class of log-gamma penalties (or gamma priors) that in our default setting (the log-gamma(2, \\(\\lambda\\)) penalty with \\(\\lambda \\rightarrow 0\\)) produce maximum penalized likelihood (MPL) estimates (or Bayes modal estimates) approximately one standard error away from zero when the maximum likelihood estimate is at zero. We consider these priors to be weakly informative in the sense that they supply some direction but still allow inference to be driven by the data. The penalty has little influence when the number of groups is large or when the data are informative about the variance, and the asymptotic mean squared error of the proposed estimator is the same as that of the maximum likelihood estimator. (p. 686)\n\nIn the upper left panel of Figure 3, Chung and colleagues gave an example of what they mean by \\(\\lambda \\rightarrow 0\\): \\(\\lambda = 0.1\\). Here’s an example of what \\(\\operatorname{Gamma}(2, 0.1)\\) looks like across the parameter space of 0 to 100.\n\nlibrary(ggdist)\n\nprior(gamma(2, 0.1)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.95, p_limits = c(0.0001, 0.9999)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = \"Gamma(2, 0.1)\",\n       x = \"parameter space\") +\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nGiven we’re working with data on the log scale, that’s a massively permissive prior. Let’s zoom in and see what it means for the parameter space of possible values for our data.\n\nprior(gamma(2, 0.1)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.95, p_limits = c(0.000001, 0.99)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = \"Gamma(2, 0.1)\",\n       x = \"parameter space (zoomed in)\") +\n  coord_cartesian(xlim = c(0, 2)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNow keep that picture in mind as we read further along in the paper:\n\nIn addition, with \\(\\lambda \\rightarrow 0\\), the gamma density function has a positive constant derivative at zero, which allows the likelihood to dominate if it is strongly curved near zero. The positive constant derivative implies that the prior is linear at zero so that there is no dead zone near zero. The top-left panel of Figure 3 shows that the gamma(2,0.1) density increases linearly from zero with a gentle slope. The shape will be even flatter with a smaller rate parameter. (p. 691)\n\nIn case you’re not familiar with the gamma distribution, the rate parameter is what we’ve been calling \\(\\lambda\\). Let’s test this baby out with our model. Here’s how to specify it in brms.\n\nfit5.9 &lt;- brm(\n  data = wages_small_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(gamma(2, 0.1), class = sd, group = id, coef = exper),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  control = list(adapt_delta = 0.85),\n  file = \"fits/fit05.09\")\n\nNotice how we had to increase adapt_delta a bit. Here are the results.\n\nprint(fit5.9, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) \n   Data: wages_small_pp (Number of observations: 257) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 124) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.280     0.052    0.173    0.379 1.005      856     1418\nsd(exper)               0.059     0.029    0.012    0.123 1.004      587     1009\ncor(Intercept,exper)   -0.087     0.305   -0.626    0.544 1.001     2127     2513\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.733     0.049    1.639    1.832 1.001     3454     3264\nhgc_9          0.048     0.025   -0.002    0.095 1.001     3805     3329\nexper          0.051     0.024    0.002    0.098 1.002     3545     2649\nexper:black   -0.051     0.038   -0.126    0.023 1.000     3811     3274\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.344     0.023    0.303    0.392 1.002     1330     2112\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur sd(exper) is still quite close to zero. But notice how not the lower level of the 95% interval is higher than zero. Here’s what it looks like in both \\(\\sigma\\) and \\(\\sigma^2\\) metrics.\n\nas_draws_df(fit5.9) %&gt;%\n  transmute(sigma_1 = sd_id__exper) %&gt;% \n  mutate(sigma_2_1 = sigma_1^2) %&gt;% \n  set_names(\"sigma[1]\", \"sigma[1]^2\") %&gt;% \n  pivot_longer(everything()) %&gt;%\n  \n  ggplot(aes(x = value, y = name)) +\n  stat_halfeye(.width = 0.95, normalize = \"xy\") +\n  scale_y_discrete(NULL, labels = parse(text = c(\"sigma[1]\", \"sigma[1]^2\"))) +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLet’s zoom in on the leftmost part of the plot.\n\nas_draws_df(fit5.9) %&gt;%\n  transmute(sigma_1 = sd_id__exper) %&gt;% \n  mutate(sigma_2_1 = sigma_1^2) %&gt;% \n  set_names(\"sigma[1]\", \"sigma[1]^2\") %&gt;% \n  pivot_longer(everything()) %&gt;%\n  \n  ggplot(aes(x = value, y = name)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  stat_halfeye(.width = 0.95, normalize = \"xy\") +\n  scale_y_discrete(NULL, labels = parse(text = c(\"sigma[1]\", \"sigma[1]^2\"))) +\n  coord_cartesian(xlim = c(0, 0.01)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAlthough we are still brushing up on the boundary with \\(\\sigma_1^2\\), the mode is no longer at zero. In the discussion, Chung and colleagues pointed out “sometimes weak prior information is available about a variance parameter. When \\(\\alpha = 2\\), the gamma density has its mode at \\(1 / \\lambda\\), and so one can use the \\(\\operatorname{gamma}(\\alpha, \\lambda)\\) prior with \\(1 / \\lambda\\) set to the prior estimate of \\(\\sigma_\\theta\\)” (p. 703). Let’s say we only had our wages_small_pp, but the results of something like the wages_pp data were published by some earlier group of researchers. In this case, we do have good prior data; we have the point estimate from the model of the wages_pp data! Here’s what that was in terms of the median.\n\nas_draws_df(fit5.3) %&gt;%\n  median_qi(sd_id__exper)\n\n# A tibble: 1 × 6\n  sd_id__exper .lower .upper .width .point .interval\n         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1       0.0413 0.0364 0.0466   0.95 median qi       \n\n\nAnd here’s what that value is when set as the divisor of 1.\n\n1 / 0.04154273\n\n[1] 24.0716\n\n\nWhat does that distribution look like?\n\nprior(gamma(2, 24.0716)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.95, p_limits = c(0.0001, 0.9999)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = \"Gamma(2, 24.0716)\",\n       x = \"parameter space\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nSo this is much more informative than our gamma(2, 0.1) prior from before. But given the magnitude of the estimate from fit5.3, it’s still fairly liberal. Let’s practice using it.\n\nfit5.10 &lt;- brm(\n  data = wages_small_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(gamma(2, 24.0716), class = sd, group = id, coef = exper),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.10\")\n\nCheck out the results.\n\nprint(fit5.10, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + exper + exper:black + (1 + exper | id) \n   Data: wages_small_pp (Number of observations: 257) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 124) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.286     0.047    0.190    0.376 1.004     1080     1485\nsd(exper)               0.042     0.024    0.007    0.098 1.009      534      683\ncor(Intercept,exper)   -0.036     0.310   -0.602    0.578 1.001     2253     2162\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.736     0.048    1.639    1.831 1.002     2415     2578\nhgc_9          0.047     0.025   -0.001    0.095 1.003     2169     2676\nexper          0.050     0.023    0.004    0.095 1.001     2251     2162\nexper:black   -0.053     0.037   -0.126    0.021 1.004     2230     2432\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.344     0.022    0.302    0.391 1.002     1468     2467\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we compare the three ways to specify the \\(\\sigma_1\\) prior with the posterior from the original model fit with the full data set. For simplicity, we’ll just look at the results in the brms-like \\(\\sigma\\) metric. Hopefully by now you’ll know how to do the conversions to get the values into the \\(\\sigma^2\\) metric.\n\ntibble(\n  `full data, student_t(3, 0, 1) prior`  = VarCorr(fit5.3, summary = F)[[1]][[1]][1:4000, 2],\n  `small data, student_t(3, 0, 1) prior` = VarCorr(fit5.7, summary = F)[[1]][[1]][, 2],\n  `small data, gamma(2, 0.1) prior`      = VarCorr(fit5.9, summary = F)[[1]][[1]][, 2],\n  `small data, gamma(2, 24.0716) prior`  = VarCorr(fit5.10, summary = F)[[1]][[1]][, 2]) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name, \n                       levels = c(\"full data, student_t(3, 0, 1) prior\", \n                                  \"small data, student_t(3, 0, 1) prior\", \n                                  \"small data, gamma(2, 0.1) prior\", \n                                  \"small data, gamma(2, 24.0716) prior\"))) %&gt;% \n  \n  ggplot(aes(x = value, fill = name == \"full data, student_t(3, 0, 1) prior\")) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  stat_halfeye(.width = 0.95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(values = c(\"grey75\", \"darkgoldenrod2\")) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~ name, ncol = 1)\n\n\n\n\n\n\n\n\nOne thing to notice is that when you’re working with full Bayesian estimation with even a rather vague prior with a boundary on zero, the measure of central tendency in the posterior is away from zero. Things get more compact when you’re working in the \\(\\sigma^2\\) metric. But remember that when we’re fitting our models with brms, we’re in the \\(\\sigma\\) metric, anyway. And with either of these three options, you don’t have a compelling reason to set the \\(\\sigma_\\theta\\) parameter to zero the way you would with ML. Even a rather vague prior will add enough information to the model that we can feel confident about keeping our theoretically-derived \\(\\sigma_1\\) parameter.\n\n\n5.2.2.2 Nonconvergence [i.e., it’s time to talk chains and such]\nAs discussed in Section 4.3, all multilevel modeling programs implement iterative numeric algorithms for model fitting. (p. 155). This is also true for our Stan-propelled brms software. However, what’s going on under the hood, here, is not what’s happening with the frequentist packages discussed by Singer and Willett. We’re using Hamiltonian Monte Carlo (HMC) to draw from the posterior. To my eye, Bürkner gave in a (2020) preprint probably the clearest and most direct introduction to why we need fancy algorithms like HMC to fit Bayesian models. First, Bürkner warmed up by contrasting Bayes with conventional frequentist inference:\n\nIn frequentist statistics, parameter estimates are usually obtained by finding those parameter values that maximise the likelihood. In contrast, Bayesian statistics aims to estimate the full (joint) posterior distribution of the parameters. This is not only fully consistent with probability theory, but also much more informative than a single point estimate (and an approximate measure of uncertainty commonly known as ‘standard error’). (p. 9)\n\nThose iterative algorithms Singer and Willett discussed in this section, that’s what they’re doing. They are maximizing the likelihood. But with Bayes, we have the more challenging goal of describing the entire posterior distribution, which is the product of the likelihood and the prior. As such,\n\nObtaining the posterior distribution analytically is only possible in certain cases of carefully chosen combinations of prior and likelihood, which may considerably limit modeling flexibilty but yield a computational advantage. However, with the increased power of today’s computers, Markov-Chain Monte-Carlo (MCMC) sampling methods constitute a powerful and feasible alternative to obtaining posterior distributions for complex models in which the majority of modeling decisions is made based on theoretical and not computational grounds. Despite all the computing power, these sampling algorithms are computationally very intensive and thus fitting models using full Bayesian inference is usually much slower than in point estimation techniques. However, advantages of Bayesian inference – such as greater modeling flexibility, prior distributions, and more informative results – are often worth the increased computational cost (Gelman, Carlin, Stern, and Rubin 2013). (pp. 9–10)\n\nThe gritty details are well beyond the scope of this project. If you’d like a more thorough walk-through on why it’s analytically and computationally challenging to get the posterior, I recommend working through the first several chapters in Kruschke’s (2015) Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.\nBut so anyways, our primary algorithm is HMC as implemented by Stan. You can find all kinds of technical details at https://mc-stan.org/users/documentation/. Because it’s rather difficult to describe our Bayesian multilevel models analytically, we use HMC to draw from the posterior instead. We then summarize the marginal and joint distributions of those parameters with things like measures of central tendency (i.e., means, medians, modes) and spread (i.e., standard deviations, percentile-based intervals). We make lots of plots.\nAnd somewhat like with the frequentist iterative algorithms, we need to make sure our sweet Stan-based HMC is working well, too. One way is with trace plots.\n\n5.2.2.2.1 Trace plots\nWe can get the trace plots for a model by placing a brm() fit object into the plot() function. Here’s an example with the full model, fit5.4.\n\nplot(fit5.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll notice we get two plots for each of the major model parameters, the \\(\\gamma\\)’s, the \\(\\sigma\\)’s and the \\(\\rho\\)’s. The plots on the left are the density plots for each parameter. On the right, we have the actual trace plots. On the \\(x\\)-axis, we have an ordering of the posterior draws; on the \\(y\\), we have the parameter space. Since we requested three HMC chains to draw from the posterior (chains = 3), those three chains are depicted by different colored lines. We generally like it when the lines for our chains all overlap with each other in a stable zig-zag sort of way. Trace plots are sometimes called caterpillar plots because, when things are going well, they often resemble nice multicolored fuzzy caterpillars.\nIf you’d like more control over your trace plot visuals, you might check out the bayesplot package (Gabry et al., 2019; Gabry & Mahr, 2022).\n\nlibrary(bayesplot)\n\nOur main function will be mcmc_trace(). Unlike with the brms::plot() method, bayesplog::mcmc_trace() takes the posterior draws themselves as input. So we’ll have to use as_draws_df() first.\n\ndraws &lt;- as_draws_df(fit5.4)\n\nWe can use the pars argument to focus on particular parameters.\n\nmcmc_trace(draws, pars = \"sigma\")\n\n\n\n\n\n\n\n\nIf we use the pars = vars(...) format, we can use function helpers from dplyr to select subsets of parameters. For example, here’s how we might single out the \\(\\gamma\\)’s.\n\nmcmc_trace(draws,\n           pars = vars(starts_with(\"b_\")),\n           facet_args = list(ncol = 2))\n\n\n\n\n\n\n\n\nNotice how we used the facet_args argument to adjust the number of columns in the output. We can also use familiar ggplot2 functions to customize the plots further.\n\ndraws %&gt;% \n  mutate(`sigma[0]`       = sd_id__Intercept,\n         `sigma[1]`       = sd_id__exper,\n         `sigma[epsilon]` = sigma) %&gt;%\n\nmcmc_trace(pars = vars(starts_with(\"sigma[\")),\n           facet_args = list(labeller = label_parsed)) +\n  scale_color_viridis_d(option = \"A\") +\n  scale_x_continuous(NULL, breaks = NULL) +\n  ggtitle(\"I can't wait to show these traceplots to my mom.\") +\n  theme_grey() +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank(),\n        panel.grid.major.y = element_line(color = \"white\", size = 1/4),\n        strip.text = element_text(size = 12))\n\n\n\n\n\n\n\n\nTrace plots are connected to other important concepts, like autocorrelation and effective sample size.\n\n\n5.2.2.2.2 Autocorrelation\nWhen using Markov chain Monte Carlo methods, of which HMC is a special case, the notions of autocorrelation and effective sample size are closely connected. Both have to do with the question, How many post-warmup draws from the posterior do I need to take? If you take too few, you won’t have a good sense of the shape of the posterior. If you take more than necessary, you’re just wasting time and computer memory. Here’s how McElreath introduced the topic in his (2015) text:\n\nSo how many samples do we need for accurate inference about the posterior distribution? It depends. First, what really matters is the effective number of samples, not the raw number. The effective number of samples is an estimate of the number of independent samples from the posterior distribution. Markov chains are typically autocorrelated, so that sequential samples are not entirely independent. Stan chains tend to be less autocorrelated than those produced by other engines [e.g., the Gibbs sampler], but there is always some autocorrelation. (p. 255, emphasis in the original)\n\nI’m not aware of a way to query the autocorrelations from a brm() fit using brms convenience functions. However, we can get those diagnostics from the bayesplot::mcmc_acf() function.\n\nmcmc_acf(draws, pars = vars(starts_with(\"b_\")), lags = 10)  +\n  theme_grey() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe mcmc_acf() function gives a wealth of granular output. The columns among the plots are the specified parameters. The rows are the chains, one for each. In this particular case, the autocorrelations were quite low for all our \\(\\gamma\\) parameters by the second or third lag. That’s really quite good and not uncommon for HMC. Do note, however, that this won’t always be the case. For example, here are the plots for our variance parameters and \\(\\rho_{01}\\).\n\nmcmc_acf(draws, pars = vars(starts_with(\"sd_\"), \"sigma\", starts_with(\"cor\")), lags = 10)  +\n  theme_grey() +\n  theme(panel.grid = element_blank(),\n        strip.text = element_text(size = 7))\n\n\n\n\n\n\n\n\nOn the whole, all of them are pretty okay. But notice how the autocorrelations for \\(\\sigma_1\\) and \\(\\rho_{01}\\) remained relatively high up until the 10th lag.\nThe plots from mcmc_act() are quite handy for focused diagnostics. But if you want a more global perspective, they’re too tedious. Fortunately for us, we have other diagnostic tools.\n\n\n5.2.2.2.3 Effective sample size\nAbove we quoted McElreath as pointing out “what really matters is the effective number of samples, not the raw number.” With brms, you typically get the effective number of samples in the print() or summary() output. Here it is again for fit4.\n\nsummary(fit5.4)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.23      0.01     0.21     0.25 1.00     1120     2617\nsd(exper)                0.04      0.00     0.04     0.05 1.00      448     1177\ncor(Intercept,exper)    -0.29      0.07    -0.42    -0.14 1.00      532     1163\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       1.72      0.01     1.69     1.74 1.00     1994     2848\nhgc_9           0.03      0.01     0.02     0.05 1.00     2494     2456\nblack           0.02      0.02    -0.03     0.06 1.00     2062     2823\nexper           0.05      0.00     0.04     0.05 1.00     1746     3151\nhgc_9:exper     0.00      0.00    -0.00     0.00 1.00     2793     3103\nblack:exper    -0.02      0.01    -0.03    -0.01 1.00     2108     3323\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.31      0.00     0.30     0.31 1.00     2553     2872\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSee the last two columns on the right: Bulk_ESS and Tail_ESS? Following Vehtari et al. (2019), those describe our effective sample size in two ways. From the paper, we read:\n\nIf you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original)\n\nFor more technical details, see the paper. In short, Bulk_ESS indexes the number of effective samples in ‘the center of the’ posterior distribution (i.e., the posterior mean or median). But since we also care about uncertainty in our parameters, we care about stability in the 95% intervals and such. The Tail_ESS column allows us to gauge the effective sample size for those intervals. Like with the autocorrelations, each parameter gets its own estimate for both ESS measures. You might compare the numbers to the number of post-warmup iterations, 4,500 in this case.\nYou may wonder, how many effective samples do I need? Back to McElreath:\n\nIf all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. (p. 255)\n\nAt the moment, brms does not offer a convenience function that allows users to collect the Bulk_ESS and Tail_ESS values in a data frame. However you can do so with help from the posterior package (Bürkner et al., 2022). For our purposes, the function of interest is summarise_draws(), which will take the output from as_draws_df() as input. We’ll save the results as draws_sum.\n\nlibrary(posterior)\n\ndraws_sum &lt;- as_draws_df(fit5.4) %&gt;% \n  summarise_draws()\n\ndraws_sum %&gt;% \n  head(n = 10)\n\n# A tibble: 10 × 10\n   variable                     mean   median      sd     mad       q5      q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;                       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 b_Intercept               1.72     1.72    0.0123  0.0124   1.70     1.74    1.000    1994.    2848.\n 2 b_hgc_9                   0.0349   0.0349  0.00788 0.00801  0.0220   0.0476  1.00     2494.    2456.\n 3 b_black                   0.0158   0.0158  0.0240  0.0237  -0.0241   0.0547  1.00     2062.    2823.\n 4 b_exper                   0.0494   0.0494  0.00262 0.00263  0.0450   0.0536  1.00     1746.    3151.\n 5 b_hgc_9:exper             0.00131  0.00129 0.00173 0.00174 -0.00148  0.00417 1.00     2793.    3103.\n 6 b_black:exper            -0.0183  -0.0183  0.00552 0.00556 -0.0273  -0.00920 1.00     2108.    3323.\n 7 sd_id__Intercept          0.227    0.227   0.0109  0.0107   0.209    0.245   1.00     1120.    2617.\n 8 sd_id__exper              0.0403   0.0403  0.00269 0.00266  0.0360   0.0446  1.00      448.    1177.\n 9 cor_id__Intercept__exper -0.290   -0.293   0.0703  0.0690  -0.398   -0.169   1.00      532.    1163.\n10 sigma                     0.309    0.309   0.00313 0.00321  0.304    0.314   1.00     2553.    2872.\n\n\nNote how the last two columns are the ess_bulk and the ess_tail. Here we summarize them with histograms.\n\ndraws_sum %&gt;% \n  pivot_longer(starts_with(\"ess\")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 100) +\n  xlim(0, NA) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nIf you wanted a focused plot of the effective sample sizes for our primary summary parameters, you would just wrangle the output a little. Since it’s fun, we’ll switch to a lollipop plot.\n\ndraws_sum %&gt;% \n  slice(1:10) %&gt;% \n  pivot_longer(contains(\"ess\")) %&gt;% \n  mutate(ess = str_remove(name, \"ess_\")) %&gt;% \n\n  ggplot(aes(y = reorder(variable, value))) +\n  geom_linerange(aes(xmin = 0, xmax = value)) +\n  geom_point(aes(x = value)) +\n  labs(x = \"effective sample size\",\n       y = NULL) +\n  xlim(0, 4000) +\n  theme(panel.grid  = element_blank(),\n        axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank()) +\n  facet_wrap(~ ess)\n\n\n\n\n\n\n\n\nYou may have noticed from the ESS histogram that many of the parameters seemed to have bulk ESS values well above the total number of posterior draws (4,500). What’s that about? As is turns out, the sampling in Stan is so good that sometimes the HMC chains for a parameter can be negatively autocorrelated. When this is the case, your effective sample size can be larger than your actual sample size. Madness, I know. This was the case for many of our random effects. Here’s a look at the parameters with ten largest values.\n\ndraws_sum %&gt;% \n  arrange(desc(ess_bulk)) %&gt;% \n  select(variable, ess_bulk) %&gt;% \n  slice(1:10)\n\n# A tibble: 10 × 2\n   variable              ess_bulk\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 r_id[4619,exper]        10209.\n 2 r_id[7311,exper]         9035.\n 3 r_id[6794,Intercept]     8879.\n 4 r_id[304,Intercept]      8844.\n 5 r_id[7918,exper]         8826.\n 6 r_id[1679,exper]         8799.\n 7 r_id[7117,exper]         8784.\n 8 r_id[12152,exper]        8747.\n 9 r_id[12200,Intercept]    8725.\n10 r_id[9471,Intercept]     8679.\n\n\nLet’s take the first 4 and check their autocorrelation plots.\n\nmcmc_acf(draws, \n         lags = 5, \n         pars = vars(`r_id[12335,exper]`, `r_id[5968,Intercept]`, \n                     `r_id[10476,Intercept]`, `r_id[7117,Intercept]`))  +\n  theme_grey() +\n  theme(panel.grid = element_blank(),\n        strip.text = element_text(size = 10))\n\n\n\n\n\n\n\n\nSee those dips below zero for the first lag in each? That’s what a negative autocorrelation looks like. Beautiful. For more on negative autocorrelations within chains and how it influences the number of effective samples, check out this thread on the Stan forums where many members of the Stan team chimed in.\n\n\n5.2.2.2.4 \\(\\widehat R\\)\nReturn again to the default print() output for a brms::brm() fit.\n\nprint(fit5.4)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + hgc_9 + black + exper + exper:hgc_9 + exper:black + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.23      0.01     0.21     0.25 1.00     1120     2617\nsd(exper)                0.04      0.00     0.04     0.05 1.00      448     1177\ncor(Intercept,exper)    -0.29      0.07    -0.42    -0.14 1.00      532     1163\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       1.72      0.01     1.69     1.74 1.00     1994     2848\nhgc_9           0.03      0.01     0.02     0.05 1.00     2494     2456\nblack           0.02      0.02    -0.03     0.06 1.00     2062     2823\nexper           0.05      0.00     0.04     0.05 1.00     1746     3151\nhgc_9:exper     0.00      0.00    -0.00     0.00 1.00     2793     3103\nblack:exper    -0.02      0.01    -0.03    -0.01 1.00     2108     3323\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.31      0.00     0.30     0.31 1.00     2553     2872\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe third last column for each parameter is Rhat. “Rhat is a complicated estimate of the convergence of the Markov chains to the target distribution. It should approach 1.00 from above, when all is well” (McElreath, 2015, p. 250). We can extract \\(\\widehat R\\) directly with the brms::rhat() function.\n\nbrms::rhat(fit5.4) %&gt;% \n  str()\n\n Named num [1:1788] 1 1 1 1 1 ...\n - attr(*, \"names\")= chr [1:1788] \"b_Intercept\" \"b_hgc_9\" \"b_black\" \"b_exper\" ...\n\n\nFor our fit5.4, the brms::rhat() function returned a named numeric vector, with one row for each of the 1787 parameters in the model. You can subset the rhat() output to focus on a few parameters.\n\nbrms::rhat(fit5.4)[1:10]\n\n             b_Intercept                  b_hgc_9                  b_black                  b_exper            b_hgc_9:exper \n               0.9997128                1.0000086                1.0002903                1.0007257                1.0010926 \n           b_black:exper         sd_id__Intercept             sd_id__exper cor_id__Intercept__exper                    sigma \n               1.0007023                1.0005135                1.0018936                1.0012967                1.0004566 \n\n\nNote also that our draws_sum object from above has an rhat column, too.\n\ndraws_sum %&gt;% \n  select(variable, rhat) %&gt;% \n  slice(1:4)\n\n# A tibble: 4 × 2\n  variable     rhat\n  &lt;chr&gt;       &lt;dbl&gt;\n1 b_Intercept 1.000\n2 b_hgc_9     1.00 \n3 b_black     1.00 \n4 b_exper     1.00 \n\n\nFor a more global perspective, just plot.\n\ndraws_sum %&gt;% \n  ggplot(aes(x = rhat)) +\n  geom_vline(xintercept = 1, color = \"white\") +\n  geom_histogram(binwidth = 0.0001) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe bayesplot package offers a convenience function for plotting brms::rhat() output. Here we’ll focus on the first 20 parameters.\n\nmcmc_rhat(brms::rhat(fit5.4)[1:20]) +\n  yaxis_text(hjust = 0)\n\n\n\n\n\n\n\n\nBy default, mcmc_rhat() does not return text on the \\(y\\)-axis. But you can retrieve that text with the yaxis_text() function. For more on the \\(\\widehat R\\), you might check out the Rhat: potential scale reduction statistic subsection of Gabry and Modrák’s (2020) vignette, Visual MCMC diagnostics using the bayesplot package.\nWe should also point out that the Stan team has found some deficiencies with the \\(\\widehat R\\). They’ve made recommendations that will be implemented in the Stan ecosystem sometime soon. In the meantime, you can read all about it in their preprint (Vehtari et al., 2019) and in Dan Simpson’s blog post, Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it.\n\n\n\n\n5.2.3 Distinguishing among different types of missingness\n\nMissingness, in and of itself, is not necessarily problematic. It all depends upon what statisticians call the type of missingness. In seminal work on this topic, R. J. Little (1995), refining earlier work with Rubin (R. J. A. Little & Rubin, 1987), distinguished among three types of missingness: (1) missing completely at random (MCAR); (2) covariate-dependent dropout (CDD); and (3) missing at random (MAR) (see also Schafer, 1997).\nWhen we say that data are MCAR, we argue that the observed values are a random sample of all the values that could have been observed (according ot plan), had there been no missing data.\n…Covariate dependent dropout (CDD) is a less restrictive assumption that permits associations between the probability of missingness and observed predictor values (“covariates”). Data can be CDD even if the probability of missingness is systematically related to either TIME or observed substantive predictors.\n…When data are MAR, the probability of missingness can depend upon any observed data, for either the predictors or any outcome values. It cannot, however, depend upon an unobserved value of either any predictor or the outcome. (pp. 157–158, emphasis in the original)\n\nFor some more current introductions to missing data methods, I recommend Enders’ (2010) Applied missing data analysis, for which you can find a free sample chapter here, and Little and Rubin’s (2019) Statistical analysis with missing data, 3rd Edition. You might also check out van Burren’s great (2018) online text Flexible imputation of missing data. Second edition. If you’re a fan of the podcast medium, you might listen to episode 16 from the first season of the Quantitude podcast, IF EPISODE=16 THEN EPISODE=-999;, in which Patrick Curran and Greg Hancock do a fine job introducing the basics of missing data. And very happily, brms has several ways to handle missing data, about which you can learn more from Bürkner’s (2021) vignette, Handle missing values with brms.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "05.html#time-varying-predictors",
    "href": "05.html#time-varying-predictors",
    "title": "5  Treating Time More Flexibly",
    "section": "5.3 Time-varying predictors",
    "text": "5.3 Time-varying predictors\n\nA time-varying predictor is a variable whose values may differ over time. Unlike their time-invariant cousins, which record an individual’s static status, time-varying predictors record an individual’s potentially differing status on each associated measurement occasion. Some time-varying predictors have values that change naturally; others have values that change by design. (pp. 159–160, emphasis in the original)\n\n\n5.3.1 Including the main effect of a time-varying predictor\nYou can find Ginexi and colleagues’ (2000) unemployment study data in the reading_pp.csv file.\n\nunemployment_pp &lt;- read_csv(\"data/unemployment_pp.csv\")\n\nhead(unemployment_pp)\n\n# A tibble: 6 × 4\n     id months  cesd unemp\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   103  1.15     25     1\n2   103  5.95     16     1\n3   103 12.9      33     1\n4   641  0.789    27     1\n5   641  4.86      7     0\n6   641 11.8      25     0\n\n\nWe have 254 unique participants.\n\nunemployment_pp %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   254\n\n\nHere’s one way to compute the number of participants who were never employed during the study.\n\nunemployment_pp %&gt;% \n  filter(unemp == 0) %&gt;% \n  distinct(id) %&gt;% \n  count() %&gt;% \n  summarise(never_employed = 254 - n)\n\n# A tibble: 1 × 1\n  never_employed\n           &lt;dbl&gt;\n1            132\n\n\nIn case it wasn’t clear, participants had up to 3 interviews.\n\nBy recruiting 254 participants from local unemployment offices, the researchers were able to interview individuals soon after job loss (within the first 2 months). Follow-up interviews were conducted between 3 and 8 months and 10 and 16 months after job loss. (p. 161)\n\nThose times were encoded in the months variable. Here’s what that looks like.\n\nunemployment_pp %&gt;% \n  ggplot(aes(x = months))  +\n  geom_vline(xintercept = c(3, 8), color = \"white\") +\n  geom_histogram(binwidth = 0.5) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo make some of our data questions easier, we can use those 3- and 8-month thresholds to make an interview variable to indicate the periods during which the interviews were conducted.\n\nunemployment_pp &lt;- unemployment_pp %&gt;% \n  mutate(interview = ifelse(months &lt; 3, 1,\n                            ifelse(months &gt; 8, 3, 2))) \n  \nunemployment_pp %&gt;% \n  ggplot(aes(x = interview))  +\n  geom_bar() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWith a little wrangling, we can display all possible employment patterns along with counts on how many followed them.\n\nunemployment_pp %&gt;%\n  select(-months, -cesd) %&gt;%\n  mutate(interview = str_c(\"int_\", interview)) %&gt;%\n  spread(key = interview, value = unemp) %&gt;%\n  group_by(int_1, int_2, int_3) %&gt;%\n  count() %&gt;%\n  arrange(desc(n)) %&gt;%\n  flextable::flextable()\n\nint_1int_2int_3n111781005511041127112210119104114102101111\n\n\nIt takes a little work to see how Singer and Willett came to the conclusion “62 were always working after the first interview” (p. 161). Based on an analysis of those who had complete data, that corresponds to the pattern in the top row, [1, 0, 0], which we have counted as 55 (i.e., row 2). If you add to that the two rows with missingness on one of the critical values (i.e., [1, 0, NA], [1, NA, 0], and [NA, 1, 1]), that gets you \\(55 + 4 + 2 + 1 = 62\\).\nWe can confirm that “41 were still unemployed at the second interview but working by the third” (p. 161). That’s our pattern [1, 1, 0], shown in row 3. We can also confirm “19 were working by the second interview but unemployed at the third” (p. 161). That’s shown in our pattern [1, 0, 1], shown in row 6.\nBefore we configure our unconditional growth model, we might familiarize ourselves with our criterion variable, cesd. Singer and Willett informed us:\n\nEach time participants completed the Center for Epidemiologic Studies’ Depression (CES-D) scale (Radloff, 1977), which asks them to rate, on a four-point scale, the frequency with which they experience each of the 20 depressive symptoms. The CES-D scores can vary from a low or 0 for someone with no symptoms to a high of 80 for someone in serious distress. (p. 161)\n\nIn addition to Radloff’s original article, you can get a copy of the CES-D here.\nTo help us pick our priors, [Brown and Gary (1985) listed the means and standard deviations of the CES-D scores for unemployed African-American adults. They gave the summary statistics broken down by sex:\n\nMales: 14.05 (8.86), \\(n = 37\\)\nFemales: 15.35 (9.39), \\(n = 72\\)\n\nBased the variables in the data set and the descriptions of it in the text, we don’t have a good sense of the demographic backgrounds of the participants. But with the information we have in hand, a reasonable empirically-based but nonetheless noncommittal prior for baseline CES-D might be something like normal(14.5, 20). A weakly-regularizing prior on change over 1 month might be normal(0, 10). It’d be fair if you wanted to argue about these priors. Try your own! But if you are willing to go along with me, we might write the statistical formula for the unconditional growth model as\n\\[\\begin{align}\n\\text{cesd}_{ij} & = \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\\\\n\\epsilon_{ij}    & \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n  \\zeta_{0i} \\\\ \\zeta_{1i}\n  \\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n  \\mathbf D \\mathbf\\Omega \\mathbf D'\n  \\end{pmatrix} \\\\\n\\mathbf D       & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf \\Omega  & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                                      & \\sim \\operatorname{Normal}(14.5, 20) \\\\\n\\gamma_{10}                                      & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 & \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\\n\\rho_{01}                                        & \\sim \\operatorname{LKJ} (4).\n\\end{align}\\]\nThose \\(\\operatorname{Student-t}(3, 0, 11.9)\\) priors for the \\(\\sigma\\)’s are the defaults, which you can confirm with get_prior(). The \\(\\operatorname{LKJ} (4)\\) for \\(\\rho_{01}\\) will weakly regularize the correlation towards zero.\nHere’s how we might fit that model.\n\nfit5.11 &lt;- brm(\n  data = unemployment_pp, \n  family = gaussian,\n  cesd ~ 0 + Intercept + months + (1 + months | id),\n  prior = c(prior(normal(14.5, 20), class = b, coef = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 11.9), class = sd),\n            prior(student_t(3, 0, 11.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit05.11\")\n\nHere are the results.\n\nprint(fit5.11, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: cesd ~ 0 + Intercept + months + (1 + months | id) \n   Data: unemployment_pp (Number of observations: 674) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 254) \n                      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            8.715     0.837    7.153   10.394 1.018      404     1400\nsd(months)               0.429     0.212    0.035    0.799 1.033      178      522\ncor(Intercept,months)   -0.373     0.232   -0.687    0.236 1.004      853      792\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept   17.661     0.772   16.145   19.222 1.001     2006     2642\nmonths      -0.418     0.082   -0.586   -0.254 1.000     3504     2805\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    8.560     0.409    7.761    9.327 1.026      312     1272\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere are the posteriors for the CES-D at the first day of job loss (i.e., \\(\\gamma_{00}\\)) and the expected rate of change over one month (i.e., \\(\\gamma_{10}\\)).\n\nas_draws_df(fit5.11) %&gt;% \n  transmute(`first day of job loss` = b_Intercept,\n            `linear decline by month` = b_months) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = 0.95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"CES-D composite score\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nWe might use conditional_effects() to get a quick view on what that might looks like.\n\nplot(conditional_effects(fit5.11),\n       plot = FALSE)[[1]] +\n  geom_hline(yintercept = 14.5, color = \"grey50\", linetype = 2) +\n  coord_cartesian(ylim = c(0, 20)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFor reference, the dashed gray line is the value we centered our prior for initial status on.\n\n5.3.1.1 Using a composite specification\nWe might specify Model B, our first model with a time-varying covariate, like this:\n\\[\\begin{align}\n\\text{cesd}_{ij} & = \\big [ \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\gamma_{20} \\text{unemp}_{ij} \\big ] + \\big [  \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\big ]\\\\\n\\epsilon_{ij}    & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n  \\zeta_{0i} \\\\ \\zeta_{1i}\n  \\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n  \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n  \\mathbf D \\mathbf\\Omega \\mathbf D'\n  \\end{pmatrix} \\\\\n\\mathbf D      & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf \\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                                      & \\sim \\operatorname{Normal}(14.5, 20) \\\\\n\\gamma_{10} \\text{ and } \\gamma_{20}             & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 & \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\\n\\rho_{01}                                        & \\sim \\operatorname{LKJ}(4).\n\\end{align}\\]\nNote a few things about the priors. First, we haven’t changed any of the priors from the previous model. All we did was add \\(\\gamma_{20} \\sim \\text{Normal}(0, 10)\\) for our new parameter. Given how weakly-informative our other priors have been for these data, this isn’t an unreasonable approach. However, the meaning for our intercept, \\(\\gamma_{01}\\), has changed. Now it’s the initial status for someone who is employed at baseline. But remember that for Model A, we set that prior with unemployed people in mind. A careful researcher might want to dive back into the literature to see if some lower value than 14.5 would be more reasonable to set for the mean of that prior. However, since the standard deviations for our intercepts priors and the covariate priors are all rather wide and permissive, this just won’t be much of a problem, for us. Buy anyway, second, note that we’ve centered our prior for \\(\\gamma_{20}\\) on zero. This is a weakly-regularizing prior, slightly favoring smaller effects over larger ones. And like before, one could easily argue for different priors.\nHere’s how to fit the model in brms.\n\nfit5.12 &lt;- brm(\n  data = unemployment_pp, \n  family = gaussian,\n  cesd ~ 0 + Intercept + months + unemp + (1 + months | id),\n  prior = c(prior(normal(14.5, 20), class = b, coef = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 11.9), class = sd),\n            prior(student_t(3, 0, 11.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  control = list(adapt_delta = 0.9),\n  file = \"fits/fit05.12\")\n\nIf you compare our results to those in Table 5.7, you’ll see they’re quite similar.\n\nprint(fit5.12)\n\nWarning: There were 4 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: cesd ~ 0 + Intercept + months + unemp + (1 + months | id) \n   Data: unemployment_pp (Number of observations: 674) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 254) \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)             9.12      0.85     7.48    10.77 1.00      487      941\nsd(months)                0.55      0.19     0.10     0.86 1.01      270      392\ncor(Intercept,months)    -0.47      0.17    -0.71    -0.01 1.00      943      667\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    12.81      1.26    10.33    15.30 1.00     2527     2572\nmonths       -0.21      0.09    -0.39    -0.03 1.00     3710     3489\nunemp         4.97      1.03     2.94     7.02 1.00     3264     2903\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.15      0.42     7.37     9.00 1.00      388      994\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBefore we make our versions of Figure 5.3, let’s first compare \\(\\gamma_{01}\\) posteriors by model. On page 166 of the text, Singer and Willett reported the monthly rate of decline “had been cut in half (to 0.20 from 0.42 in Model A)”.\n\nfixef(fit5.11)[\"months\", ]\n\n   Estimate   Est.Error        Q2.5       Q97.5 \n-0.41776049  0.08218066 -0.58584541 -0.25422993 \n\nfixef(fit5.12)[\"months\", ]\n\n   Estimate   Est.Error        Q2.5       Q97.5 \n-0.20769610  0.09197149 -0.38691201 -0.02650551 \n\n\nYou might be wondering why the quote from Singer and Willett used positive numbers while our parameter estimates have negative ones. No, there’s no mistake, there. Negative parameter estimates for monthly trajectories are then same thing as expressing a rate of decline with a positive number. But anyways, you see our estimates are on par with theirs. With our Bayesian paradigm, it’s also easy to get a formal difference distribution.\n\ntibble(fit5.11 = as_draws_df(fit5.11) %&gt;% pull(\"b_months\"),\n       fit5.12 = as_draws_df(fit5.12) %&gt;% pull(\"b_months\"))\n\n# A tibble: 4,000 × 2\n   fit5.11 fit5.12\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1  -0.436 -0.312 \n 2  -0.507 -0.264 \n 3  -0.491 -0.277 \n 4  -0.340 -0.412 \n 5  -0.374 -0.178 \n 6  -0.476 -0.257 \n 7  -0.313 -0.0517\n 8  -0.541 -0.278 \n 9  -0.445 -0.150 \n10  -0.317 -0.0567\n# ℹ 3,990 more rows\n\n\n\ntibble(fit5.11 = as_draws_df(fit5.11) %&gt;% pull(\"b_months\"),\n       fit5.12 = as_draws_df(fit5.12) %&gt;% pull(\"b_months\")) %&gt;% \n  mutate(dif = fit5.12 - fit5.11) %&gt;%\n  \n  ggplot(aes(x = dif)) +\n  stat_halfeye(.width = 0.95) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(paste(\"Difference in \", gamma[1][0]))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s our posterior for \\(\\gamma_{20}\\), b_unemp.\n\nas_draws_df(fit5.12) %&gt;% \n  ggplot(aes(x = b_unemp)) +\n  stat_halfeye(.width = 0.95) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLet’s compute our WAIC estimates for fit5.11 and fit5.12.\n\nfit5.11 &lt;- add_criterion(fit5.11, criterion = \"waic\")\nfit5.12 &lt;- add_criterion(fit5.12, criterion = \"waic\")\n\nNow we’ll compare the models by both their WAIC differences and their WAIC weights.\n\nloo_compare(fit5.11, fit5.12, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.12     0.0       0.0 -2492.2      21.5        196.6    10.7    4984.4    43.0\nfit5.11   -20.5       4.6 -2512.7      21.5        180.9    10.2    5025.4    43.0\n\nmodel_weights(fit5.11, fit5.12, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.11 fit5.12 \n      0       1 \n\n\nBy both metrics, fit5.12 came out as the clear favorite.\nIt’s finally time to make our version of the upper left panel of Figure 5.3. We’ll do so using fitted().\n\nnd &lt;- tibble(unemp = 1,\n             months = seq(from = 0, to = 14, by = 0.5))\n\nf &lt;- fitted(fit5.12,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nf %&gt;% \n  ggplot(aes(x = months)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate)) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Remain unemployed\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe upper right panel will take more care. We’ll still use fitted(), but we’ll have to be tricky with how we define the two segments. When we defined the sequence of months values over which we wanted to plot the model trajectory, we just casually set length.out = 30 within the seq() function. But now we need to make sure two of those sequential points are at 5. One way to do so is to use the by = 0.5 argument within seq(), instead. Since we’ll be defining the end points in our range with integer values, dividing up the sequence by every .5th value will ensure we’ll both be able to stop at 5 and that we’ll have a reasonable amount of values in the sequence to ensure the bowtie-shaped 95% intervals don’t look chunky.\nBut anyway, that also means we’ll need to do a good job determining how many values we’ll need to repeat our desired unemp values over. So here’s a quick way to do the math. Since we’re using every .5 in the sequence, you just subtract the integer at the beginning of the sequence from the integer at the end of the sequence, multiply that value by 2, and then add 1 to the product. Like this:\n\n2 * (5 - 0) + 1\n\n[1] 11\n\n2 * (14 - 5) + 1\n\n[1] 19\n\n\nThose are the number of times we need to repeat unemp == 1 and unemp == 0, respectively. You’ll see. Now wrangle and plot.\n\nnd &lt;- tibble(\n  unemp  = rep(1:0, times = c(11, 19)),\n  months = c(seq(from = 0, to = 5, by = 0.5),\n             seq(from = 5, to = 14, by = 0.5)))\n\nf &lt;- fitted(fit5.12,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nf %&gt;% \n  ggplot(aes(x = months, group = unemp)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate)) +\n  geom_segment(x = 5, xend = 5,\n               y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 5,\n               yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 5 + fixef(fit5.12)[3, 1],\n               linewidth = 1/3, linetype = 2) +\n  annotate(geom = \"text\",\n           x = 8, y = 14.5, label = \"gamma[2][0]\",\n           parse = T) +\n  geom_segment(x = 7, xend = 5.5,\n               y = 14.5, yend = 14.5,\n               arrow = arrow(length = unit(0.05, \"inches\"))) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Reemployed at 5 months\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nSame deal for the lower left panel of Figure 5.3.\n\n2 * (10 - 0) + 1\n\n[1] 21\n\n2 * (14 - 10) + 1\n\n[1] 9\n\n\nNow wrangle and plot.\n\nnd &lt;- tibble(\n  unemp  = rep(1:0, times = c(21, 9)),\n  months = c(seq(from = 0, to = 10, by = 0.5),\n             seq(from = 10, to = 14, by = 0.5)))\n\nf &lt;- fitted(fit5.12,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nf %&gt;% \n  ggplot(aes(x = months, group = unemp)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate)) +\n  geom_segment(x = 10, xend = 10,\n               y = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 10,\n               yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * 10 + fixef(fit5.12)[3, 1],\n               linetype = 2, linewidth = 1/3) +\n  annotate(geom = \"text\",\n           x = 7, y = 13.5, label = \"gamma[2][0]\",\n           parse = T) +\n  geom_segment(x = 8, xend = 9.5,\n               y = 13.5, yend = 13.5,\n               arrow = arrow(length = unit(0.05, \"inches\"))) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Reemployed at 10 months\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIt’s just a little bit trickier to get that lower right panel. Now we need to calculate three values.\n\n2 * (5 - 0) + 1\n\n[1] 11\n\n2 * (10 - 5) + 1\n\n[1] 11\n\n2 * (14 - 10) + 1\n\n[1] 9\n\n\nGet that plot.\n\nnd &lt;- tibble(\n  unemp  = rep(c(1, 0, 1), times = c(11, 11, 9)),\n  months = c(seq(from = 0, to = 5, by = 0.5),\n             seq(from = 5, to = 10, by = 0.5),\n             seq(from = 10, to = 14, by = 0.5)),\n  group  = rep(letters[1:3], times = c(11, 11, 9)))\n\nf &lt;- fitted(fit5.12,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nlines &lt;- tibble(group = letters[1:2],\n                x     = c(5, 10)) %&gt;% \n  mutate(xend = x,\n         y    = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * x,\n         yend = fixef(fit5.12)[1, 1] + fixef(fit5.12)[2, 1] * x + fixef(fit5.12)[3, 1])\n\narrow &lt;- tibble(x    = c(6.75, 8.25),\n                y    = 14,\n                xend = c(5.5, 9.5),\n                yend = c(14.5, 13.5))\n\nf %&gt;% \n  ggplot(aes(x = months)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = group),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate, group = group)) +\n  geom_segment(data = lines,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend, \n                   group = group),\n               linewidth = 1/3, linetype = 2) +\n  annotate(geom = \"text\",\n           x = 7.5, y = 14, label = \"gamma[2][0]\",\n           parse = T) +\n  geom_segment(data = arrow,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               arrow = arrow(length = unit(0.05, \"inches\"))) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Reemployed at 5 months\\nunemployed again at 10\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNow we’ve been on a plotting roll, let’s knock out the leftmost panel of Figure 5.4. It’s just a small extension of what we’ve been doing.\n\n2 * (14 - 0) + 1\n\n[1] 29\n\n2 * (3.5 - 0) + 1\n\n[1] 8\n\n2 * (14 - 3.5) + 1\n\n[1] 22\n\n\n\nnd &lt;- tibble(\n  unemp  = rep(1:0, times = c(29, 22)),\n  months = c(seq(from = 0, to = 14, by = 0.5),\n             seq(from = 3.5, to = 14, by = 0.5)))\n\nf &lt;- fitted(fit5.12,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(label = str_c(\"unemp = \", unemp))\n\nf %&gt;% \n  ggplot(aes(x = months, group = unemp)) +\n  # new trick\n  geom_abline(intercept = fixef(fit5.12)[1, 1],\n              slope = fixef(fit5.12)[2, 1],\n              color = \"grey80\", linetype = 2) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate)) +\n  # another new trick\n  geom_text(data = f %&gt;% filter(months == 14), \n            aes(label = label, y = Estimate), hjust = -.05) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Main effects of unemp and time\") +\n  # don't forget this part\n  coord_cartesian(clip = \"off\") +\n  theme(panel.grid = element_blank(),\n        plot.margin = margin(6, 55, 6, 6))\n\n\n\n\n\n\n\n\n\n\n5.3.1.2 Using a level-1/level-2 specification\nIf we wanted to reexpress our composite equation for fit5.12 using the level-1/level-2 form, the level-1 model would be\n\\[\n\\text{cesd}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{months}_{ij} + \\pi_{2i} \\text{unemp}_{ij} + \\epsilon_{ij}.\n\\]\nHere’s the corresponding level-2 model:\n\\[\n\\begin{align}\n\\pi_{0i} & = \\gamma_{00} + \\zeta_{0i} \\\\\n\\pi_{1i} & = \\gamma_{10} + \\zeta_{1i} \\\\\n\\pi_{2i} & = \\gamma_{20}.\n\\end{align}\n\\]\nIf we wanted the effects of the time-varying covariate unemp to vary across individuals, then we’d expand the definition of \\(\\pi_{2i}\\) to be\n\\[\n\\pi_{2i} = \\gamma_{20} + \\zeta_{2i}.\n\\]\nAlthough this doesn’t change the way we model \\(\\epsilon_{ij}\\), which remains\n\\[\n\\epsilon_{ij} \\sim \\text{Normal} (0, \\sigma_\\epsilon),\n\\]\nit does change the model for the \\(\\zeta\\)s. Within our Stan/brms paradigm, that would now be\n\\[\\begin{align}\n\\begin{bmatrix}\n  \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{2i}\n  \\end{bmatrix} & \\sim \\text{Normal}\n\\begin{pmatrix}\n  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n  \\mathbf D \\mathbf\\Omega \\mathbf D'\n  \\end{pmatrix}, \\text{where} \\\\\n\\mathbf D      & = \\begin{bmatrix} \\sigma_0 & 0 & 0 \\\\ 0 & \\sigma_1 & 0 \\\\ 0 & 0 & \\sigma_2 \\end{bmatrix} \\text{and} \\\\\n\\mathbf \\Omega & = \\begin{bmatrix} 1 & \\rho_{01} & \\rho_{02} \\\\ \\rho_{01} & 1 & \\rho_{12} \\\\ \\rho_{02} & \\rho_{12} & 1 \\end{bmatrix}.\n\\end{align}\\]\nReworded slightly from the text (p. 169), by adding one residual parameter, \\(\\zeta_{2i}\\), we got an additional corresponding standard deviation parameter, \\(\\sigma_2\\), and two more correlation parameters, \\(\\rho_{02}\\) and \\(\\rho_{12}\\). Staying with our weakly-regularizing prior approach, the priors for the updated model might look like\n\\[\n\\begin{align}\n\\gamma_{00}              & \\sim \\operatorname{Normal}(14.5, 20) \\\\\n\\gamma_{10}, \\gamma_{20} & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma_\\epsilon, \\dots, \\sigma_2 & \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\\n\\Omega                           & \\sim \\operatorname{LKJ} (4).\n\\end{align}\n\\]\nSinger and Willett then cautioned readers about hastily adding \\(\\zeta\\) parameters to their models, particularly in cases where you’re likely to run into estimation issues, such as boundary constraints. Within our Stan/brms paradigm, we still have to be aware of these difficulties. However, with skillfully-chosen priors, I think you’ll find we can fit more ambitious models than would typically be possible with frequentist estimators. But do beware that as you stretch your data further and further, your choices in likelihoods and priors more heavily influence the results. For more on the topic, check out Michael Frank’s blog post, Mixed effects models: Is it time to go Bayesian by default?, and make sure not to miss the action in the comments section.\n\n\n5.3.1.3 Time-varying predictors and variance components\nWhen you add a time-varying predictor, it’s not uncommon to see a reduction in \\(\\sigma_\\epsilon^2\\). Here we compare fit5.11 and fit5.12.\n\nv &lt;- cbind(VarCorr(fit5.11, summary = F)[[2]][[1]],\n           VarCorr(fit5.12, summary = F)[[2]][[1]]) %&gt;% \n  data.frame() %&gt;% \n  set_names(str_c(\"fit5.\", 11:12)) %&gt;% \n  transmute(fit5.11 = fit5.11^2,\n            fit5.12 = fit5.12^2) %&gt;% \n  mutate(`fit5.11 - fit5.12` = fit5.11 - fit5.12)\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name, levels = c(\"fit5.11\", \"fit5.12\", \"fit5.11 - fit5.12\"))) %&gt;% \n  \n  ggplot(aes(x = value, y = 0)) +\n  stat_halfeye(.width = 0.95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(sigma[epsilon]^2)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nUsing the full posterior for both models, here is the percent variance in CES-D explained by unemp.\n\nv %&gt;% \n  transmute(percent = (fit5.11 - fit5.12) / fit5.11) %&gt;% \n  median_qi() %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 1 × 6\n  percent .lower .upper .width .point .interval\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1   0.096 -0.204  0.309   0.95 median qi       \n\n\nWhen you go beyond point estimates and factor in full posterior uncertainty, it becomes clear how fragile ad hoc statistics like this can be. Interpret them with caution.\n\n\n\n5.3.2 Allowing the effect of a time-varying predictor to vary over time\n“Might unemployment status also affect the trajectory’s slope” (p. 171)? Here’s the statistical model:\n\\[\\begin{align}\n\\text{cesd}_{ij} & = \\big [ \\gamma_{00} + \\gamma_{10} \\text{months}_{ij} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{months}_{ij} \\times \\text{unemp}_{ij} \\big ] \\\\\n& \\;\\;\\; + \\big [  \\zeta_{0i} + \\zeta_{1i} \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\\n\\epsilon_{ij}    & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n  \\zeta_{0i} \\\\ \\zeta_{1i}\n  \\end{bmatrix} & \\sim \\operatorname{Normal}\n  \\begin{pmatrix}\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\mathbf D \\mathbf \\Omega \\mathbf D'\n  \\end{pmatrix}\\\\\n\\mathbf D      & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf \\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                                        & \\sim \\operatorname{Normal}(14.5, 20) \\\\\n\\gamma_{10}, \\gamma_{20}, \\gamma_{30} & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma_\\epsilon, \\sigma_0, \\sigma_1   & \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\\n\\rho_{01}                                          & \\sim \\operatorname{LKJ}(4).\n\\end{align}\\]\nSince \\(\\gamma_{30}\\) is an interaction term, it might make sense to give it an ever tighter prior, something like \\(\\text{Normal}(0, 5)\\). Here we’ll just stay wide and loose.\n\nfit5.13 &lt;- brm(\n  data = unemployment_pp, \n  family = gaussian,\n  cesd ~ 0 + Intercept + months + unemp + months:unemp + (1 + months | id),\n  prior = c(prior(normal(14.5, 20), class = b, coef = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 11.9), class = sd),\n            prior(student_t(3, 0, 11.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  control = list(adapt_delta = 0.9),\n  file = \"fits/fit05.13\")\n\nHere are the results.\n\nprint(fit5.13)\n\n Family: gaussian \n  Links: mu = identity \nFormula: cesd ~ 0 + Intercept + months + unemp + months:unemp + (1 + months | id) \n   Data: unemployment_pp (Number of observations: 674) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 254) \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)             9.16      0.81     7.60    10.74 1.00      650     1552\nsd(months)                0.56      0.17     0.15     0.85 1.01      335      443\ncor(Intercept,months)    -0.48      0.16    -0.71    -0.10 1.00     1430     1217\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        9.93      1.93     6.11    13.72 1.00     1759     1898\nmonths           0.13      0.20    -0.26     0.52 1.00     1843     2278\nunemp            8.17      1.93     4.25    11.98 1.00     1871     2173\nmonths:unemp    -0.43      0.22    -0.86     0.00 1.00     2021     2524\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.11      0.40     7.37     8.93 1.00      509     1445\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere’s how we might make our version of the middle panel of Figure 5.4.\n\nnd &lt;- tibble(\n  unemp  = rep(1:0, times = c(29, 22)),\n  months = c(seq(from = 0, to = 14, by = 0.5),\n             seq(from = 3.5, to = 14, by = 0.5)))\n\nf &lt;- fitted(fit5.13,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(label = str_c(\"unemp = \", unemp))\n\nf %&gt;% \n  ggplot(aes(x = months, group = unemp)) +\n  geom_abline(intercept = fixef(fit5.13)[1, 1],\n              slope = fixef(fit5.13)[2, 1],\n              color = \"grey80\", linetype = 2) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate)) +\n  geom_text(data = f %&gt;% filter(months == 14), \n            aes(label = label, y = Estimate), hjust = -.05) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Main effects of unemp and time\") +\n  coord_cartesian(clip = \"off\") +\n  theme(panel.grid = element_blank(),\n        plot.margin = margin(6, 55, 6, 6))\n\n\n\n\n\n\n\n\nHere’s the posterior for \\(\\gamma_{10}\\).\n\nas_draws_df(fit5.13) %&gt;% \n  ggplot(aes(x = b_months, y = 0)) +\n  stat_halfeye(.width = 0.95) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(paste(gamma[1][0], \", the main effect for time\"))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIt’s quite uncertain and almost symmetrically straddles the parameter space between -0.5 and 0.5.\nThe next model follows the form\n\\[\\begin{align}\n\\text{cesd}_{ij} & = \\big [ \\gamma_{00} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{unemp}_{ij} \\times \\text{months}_{ij} \\big ] \\\\\n& \\;\\;\\; + \\big [  \\zeta_{0i} + \\zeta_{3i} \\text{unemp}_{ij} \\times \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\\n\\epsilon_{ij}    & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{3i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n\\mathbf D \\mathbf\\Omega \\mathbf D'\n\\end{pmatrix} \\\\\n\\mathbf D     & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_3 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{03} \\\\ \\rho_{03} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                         & \\sim \\operatorname{Normal}(14.5, 20) \\\\\n\\gamma_{20}, \\gamma_{30}            & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma_\\epsilon, \\sigma_0, \\sigma_3 & \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\\n\\rho_{03}                           & \\sim \\operatorname{LKJ}(4).\n\\end{align}\\]\nHere’s how to fit it with brms::brm().\n\nfit5.14 &lt;- brm(\n  data = unemployment_pp, \n  family = gaussian,\n  cesd ~ 0 + Intercept + unemp + months:unemp + (1 + months:unemp | id),\n  prior = c(prior(normal(14.5, 20), class = b, coef = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 11.9), class = sd),\n            prior(student_t(3, 0, 11.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  control = list(adapt_delta = 0.95),\n  file = \"fits/fit05.14\")\n\nIt’s easy to miss this if you’re not following along quite carefully with the text, but this model, which corresponds to Equation 5.9 in the text, is NOT Model D. Rather, it’s an intermediary model between Model C and Model D. All this means we can’t compare our results with those in Table 5.7. But here they are, anyway.\n\nprint(fit5.14)\n\nWarning: There were 39 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: cesd ~ 0 + Intercept + unemp + months:unemp + (1 + months:unemp | id) \n   Data: unemployment_pp (Number of observations: 674) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 254) \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                   7.99      0.65     6.79     9.35 1.00      855      791\nsd(months:unemp)                0.40      0.21     0.03     0.80 1.02      308      570\ncor(Intercept,months:unemp)    -0.11      0.25    -0.51     0.48 1.00     1521     2074\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       11.15      0.89     9.44    12.88 1.00     2559     2703\nunemp            6.95      0.91     5.20     8.77 1.00     4273     2995\nunemp:months    -0.30      0.11    -0.52    -0.09 1.00     2853     1942\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.54      0.34     7.87     9.21 1.00      933     1634\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe’ve already computed the WAIC for fit5.11 and fit5.12. Here we do so for fit5.13 and fit5.14.\n\nfit5.13 &lt;- add_criterion(fit5.13, criterion = \"waic\")\nfit5.14 &lt;- add_criterion(fit5.14, criterion = \"waic\")\n\n\nloo_compare(fit5.11, fit5.12, fit5.13, fit5.14, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.13     0.0       0.0 -2490.1      21.6        198.8    10.9    4980.1    43.2\nfit5.12    -2.2       2.2 -2492.2      21.5        196.6    10.7    4984.4    43.0\nfit5.14   -14.8       3.4 -2504.8      21.8        170.5     9.8    5009.7    43.6\nfit5.11   -22.6       4.9 -2512.7      21.5        180.9    10.2    5025.4    43.0\n\nmodel_weights(fit5.11, fit5.12, fit5.13, fit5.14, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.11 fit5.12 fit5.13 fit5.14 \n  0.000   0.103   0.897   0.000 \n\n\nYep, it appears fit5.14 is not an improvement on fit5.13 (i.e., our analogue to Model C in the text). Here’s our version of Model D:\n\\[\\begin{align}\n\\text{cesd}_{ij} & = \\big [ \\gamma_{00} + \\gamma_{20} \\text{unemp}_{ij} + \\gamma_{30} \\text{unemp}_{ij} \\times \\text{months}_{ij} \\big ] \\\\\n& \\;\\;\\; + \\big [  \\zeta_{0i} + \\zeta_{3i} \\text{unemp}_{ij} \\times \\text{months}_{ij} + \\epsilon_{ij} \\big ] \\\\\n\\epsilon_{ij}    & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{2i} \\\\ \\zeta_{3i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\mathbf D \\mathbf\\Omega \\mathbf D'\n\\end{pmatrix} \\\\\n\\mathbf D     & = \\begin{bmatrix} \\sigma_0 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ 0 & 0 & \\sigma_3 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{02} & \\rho_{03} \\\\ \\rho_{02} & 1 & \\rho_{23} \\\\ \\rho_{03} & \\rho_{23} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                        & \\sim \\operatorname{Normal}(14.5, 20) \\\\\n\\gamma_{20}, \\gamma_{30} & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma_\\epsilon, \\dots , \\sigma_3  & \\sim \\operatorname{Student-t}(3, 0, 11.9) \\\\\n\\mathbf\\Omega                      & \\sim \\operatorname{LKJ}(4).\n\\end{align}\\]\nWe’ll call it fit5.15. Here’s the brm() code.\n\nfit5.15 &lt;- brm(\n  data = unemployment_pp, \n  family = gaussian,\n  cesd ~ 0 + Intercept + unemp + months:unemp + (1 + unemp + months:unemp | id),\n  prior = c(prior(normal(14.5, 20), class = b, coef = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 11.9), class = sd),\n            prior(student_t(3, 0, 11.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.15\")\n\n\nprint(fit5.15)\n\n Family: gaussian \n  Links: mu = identity \nFormula: cesd ~ 0 + Intercept + unemp + months:unemp + (1 + unemp + months:unemp | id) \n   Data: unemployment_pp (Number of observations: 674) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 254) \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                   6.86      0.84     5.27     8.57 1.00     1647     2391\nsd(unemp)                       5.03      1.59     1.76     8.03 1.04      145      510\nsd(unemp:months)                0.66      0.23     0.09     1.04 1.02      253      286\ncor(Intercept,unemp)            0.23      0.22    -0.18     0.66 1.02      501     1500\ncor(Intercept,unemp:months)    -0.18      0.21    -0.57     0.24 1.00      834     1270\ncor(unemp,unemp:months)        -0.41      0.27    -0.80     0.25 1.01      345      811\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       11.15      0.83     9.52    12.78 1.00     1903     2647\nunemp            6.95      0.95     5.10     8.77 1.00     3438     3061\nunemp:months    -0.29      0.11    -0.51    -0.07 1.00     2762     3163\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.03      0.44     7.21     8.90 1.02      281      916\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we can finally make our version of the right panel of Figure 5.4.\n\nf &lt;- fitted(fit5.15,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(label = str_c(\"unemp = \", unemp))\n\nf %&gt;% \n  ggplot(aes(x = months, group = unemp)) +\n  geom_abline(intercept = fixef(fit5.15)[1, 1],\n              slope = 0,\n              color = \"grey80\", linetype = 2) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey67\", alpha = 1/2) +\n  geom_line(aes(y = Estimate)) +\n  geom_text(data = f %&gt;% filter(months == 14), \n            aes(label = label, y = Estimate), hjust = -.05) +\n  scale_x_continuous(\"Months since job loss\", breaks = seq(from = 0, to = 14, by = 2)) +\n  scale_y_continuous(\"CES-D\", limits = c(5, 20)) +\n  labs(subtitle = \"Constraining the effects time\\namong the re-employed\") +\n  coord_cartesian(clip = \"off\") +\n  theme(panel.grid = element_blank(),\n        plot.margin = margin(6, 55, 6, 6))\n\n\n\n\n\n\n\n\nBy carefully using filter(), we can extract the posterior for summary the expected CES-D value for “the average unemployed person in the population”, “immediately upon layoff” (p. 173).\n\nf %&gt;% \n  filter(unemp == 1 & months == 0)\n\n  Estimate Est.Error     Q2.5    Q97.5 unemp months     label\n1 18.10409 0.7924139 16.56084 19.69005     1      0 unemp = 1\n\n\nTo get the decline rate per month, just use fixef() and subset.\n\nfixef(fit5.15)[\"unemp:months\", ]\n\n  Estimate  Est.Error       Q2.5      Q97.5 \n-0.2897911  0.1096620 -0.5075830 -0.0731679 \n\n\nHow much lower, on average, are “CES-D scores among those who find a job” right after layoff (p. 173)? Again, just use fixef().\n\nfixef(fit5.15)[\"unemp\", ]\n\n Estimate Est.Error      Q2.5     Q97.5 \n6.9531506 0.9460358 5.0978707 8.7743153 \n\n\nBut if we’d like to get the posterior for the difference at 12 months later, we’ll need to go back to fitted().\n\nnd &lt;- tibble(unemp  = 1:0,\n             months = 12)\n\nfitted(fit5.15,\n       newdata = nd,\n       re_formula = NA,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(str_c(c(\"un\", \"\"), \"employed_cesd_at_12\")) %&gt;% \n  transmute(difference = unemployed_cesd_at_12 - employed_cesd_at_12) %&gt;% \n\n  median_qi() %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 1 × 6\n  difference .lower .upper .width .point .interval\n       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1       3.48    0.9   6.09   0.95 median qi       \n\n\nThere’s more posterior uncertainty, there, that you might expect from simply using the point estimates. Always beware the posterior uncertainty. We may as well finish off with a little WAIC.\n\nfit5.15 &lt;- add_criterion(fit5.15, criterion = \"waic\")\n\nloo_compare(fit5.11, fit5.12, fit5.13, fit5.14, fit5.15, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.15     0.0       0.0 -2487.0      21.7        203.4    11.0    4973.9    43.4\nfit5.13    -3.1       3.6 -2490.1      21.6        198.8    10.9    4980.1    43.2\nfit5.12    -5.2       3.6 -2492.2      21.5        196.6    10.7    4984.4    43.0\nfit5.14   -17.9       3.8 -2504.8      21.8        170.5     9.8    5009.7    43.6\nfit5.11   -25.7       6.6 -2512.7      21.5        180.9    10.2    5025.4    43.0\n\nmodel_weights(fit5.11, fit5.12, fit5.13, fit5.14, fit5.15, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.11 fit5.12 fit5.13 fit5.14 fit5.15 \n  0.000   0.005   0.043   0.000   0.952 \n\n\nfit5.15, fit5.13, and fit5.12 are all very close, with a modest edge for fit5.15.\n\n\n5.3.3 Recentering time-varying predictors\nBack to the wages_pp data! Here’s the generic statistical model we’ll be fooling with:\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\gamma_{00} + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{01} (\\text{hgc}_i - 9) + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\\n                & \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\epsilon_{ij}.\n\\end{align}\n\\]\nWe will fit the model with three versions of uerate. If you execute head(wages_pp), you’ll discover they’re already in the data. But it might be worth walking out how to compute those variables. First, centering uerate at 7 is easy enough. Just subtract.\n\nwages_pp &lt;- wages_pp %&gt;% \n  mutate(uerate_7 = uerate - 7)\n\nContinuing on with the same priors from before, here’s how to fit the new model, our version of Model A on page 175.\n\nfit5.16 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.16\")\n\n\nprint(fit5.16, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.224     0.011    0.203    0.245 1.001     1757     2761\nsd(exper)               0.040     0.003    0.035    0.046 1.003      638     1001\ncor(Intercept,exper)   -0.301     0.070   -0.430   -0.156 1.001      729     1554\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.749     0.011    1.727    1.770 1.001     2545     3177\nexper          0.044     0.003    0.039    0.049 1.001     2325     2972\nhgc_9          0.040     0.006    0.028    0.052 1.003     2622     3067\nuerate_7      -0.012     0.002   -0.015   -0.008 1.001     4783     4011\nexper:black   -0.018     0.005   -0.027   -0.009 1.001     2664     3284\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.308     0.003    0.302    0.314 1.000     3389     3284\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor the next model, we add a couple variables to the wages_pp data.\n\nwages_pp &lt;- wages_pp %&gt;% \n  group_by(id) %&gt;% \n  mutate(uerate_id_mu  = mean(uerate)) %&gt;% \n  ungroup() %&gt;% \n  mutate(uerate_id_dev = uerate - uerate_id_mu)\n\nIn the original data set, these were the ue.mean and ue.person.cen variables, respectively. Here’s how to fit the model.\n\nfit5.17 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_mu + uerate_id_dev + \n    (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5),   class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.17\")\n\n\nprint(fit5.17, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_mu + uerate_id_dev + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.225     0.011    0.205    0.247 1.002     1562     2482\nsd(exper)               0.040     0.003    0.035    0.045 1.009      650     1422\ncor(Intercept,exper)   -0.313     0.068   -0.438   -0.173 1.005      667     1662\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept        1.873     0.029    1.815    1.930 1.001     2361     2849\nexper            0.045     0.003    0.040    0.050 1.000     2877     3676\nhgc_9            0.040     0.006    0.027    0.053 1.001     2841     3599\nuerate_id_mu    -0.018     0.004   -0.024   -0.011 1.000     2332     2588\nuerate_id_dev   -0.010     0.002   -0.014   -0.006 1.000     5295     3690\nexper:black     -0.019     0.005   -0.028   -0.010 1.001     2628     3115\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.308     0.003    0.302    0.314 1.001     3221     2891\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWithin the tidyverse, probably the easiest way to center on the first value for each id is to first group_by(id) and then make use of the dplyr::first() function, which you can learn more about here.\n\nwages_pp &lt;- wages_pp %&gt;% \n  group_by(id) %&gt;% \n  mutate(uerate_id_1 = first(uerate)) %&gt;% \n  ungroup() %&gt;% \n  mutate(uerate_id_1_dev = uerate - uerate_id_1)\n\nIn the original data set, these were the ue1 and ue.centert1 variables, respectively. Here’s how to fit the updated model.\n\nfit5.18 &lt;- update(\n  fit5.16,\n  newdata = wages_pp,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_id_1 + uerate_id_1_dev + \n    (1 + exper | id),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.18\")\n\nThe desired updates require recompiling the model\n\n\n\nprint(fit5.18, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ Intercept + exper + hgc_9 + uerate_id_1 + uerate_id_1_dev + (1 + exper | id) + exper:black - 1 \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.224     0.011    0.203    0.245 1.001     1715     2320\nsd(exper)               0.040     0.003    0.035    0.046 1.002      711     1523\ncor(Intercept,exper)   -0.301     0.070   -0.429   -0.158 1.003      717     1369\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept          1.870     0.025    1.819    1.920 1.001     2757     3232\nexper              0.045     0.003    0.039    0.050 1.001     2537     3154\nhgc_9              0.040     0.006    0.027    0.053 1.001     3271     3375\nuerate_id_1       -0.016     0.003   -0.021   -0.011 1.001     2848     3220\nuerate_id_1_dev   -0.010     0.002   -0.014   -0.006 1.000     5858     4132\nexper:black       -0.018     0.005   -0.027   -0.009 1.000     2990     3407\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.308     0.003    0.302    0.314 1.002     3648     3446\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere are the WAIC comparisons.\n\nfit5.16 &lt;- add_criterion(fit5.16, criterion = \"waic\")\nfit5.17 &lt;- add_criterion(fit5.17, criterion = \"waic\")\nfit5.18 &lt;- add_criterion(fit5.18, criterion = \"waic\")\n\nloo_compare(fit5.16, fit5.17, fit5.18, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.18     0.0       0.0 -2034.4     104.2        861.5    26.8    4068.7   208.4\nfit5.16    -3.8       2.0 -2038.1     104.3        863.4    27.2    4076.3   208.7\nfit5.17    -4.4       1.5 -2038.8     104.3        865.7    27.1    4077.5   208.5\n\nmodel_weights(fit5.16, fit5.17, fit5.18, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.16 fit5.17 fit5.18 \n  0.022   0.012   0.966 \n\n\nLike in the text, these three models are all close with respect to the WAIC. Based on their WAIC weights, fit5.18 (Model C) and fit5.16 (Model A) seen the be the best depictions of the data.\n\n\n5.3.4 An important caveat: The problem of reciprocal causation\nIn this section, Singer and Willett gave a typology for time-varying covariates.\n\nA variable is defined if, “in advance to data collection, its values are predetermined for everyone under study” (p. 177). Examples are time and the seasons.\nA variable is ancillary if “its values cannot be influenced by study participants because they are determined by a stochastic process totally external to them” (p. 178). Within the context of a study on people within monogamous relationships, the availability of potential mates within the region would be an example.\nA variable is contextual if “it describes an ‘external’ stochastic process, but the connection between units is closer–between husbands and wives, parents and children, teachers and students, employers and employees. Because of this proximity, contextual predictors can be influenced by an individual’s contemporaneous outcome values; if so, they are susceptible to issues of reciprocal causation” (p. 179).\nA variable is internal if it describes an “individual’s potentially changeable status over time” (p. 179). Examples include mood, psychiatric syndromes, and employment status.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "05.html#recentering-the-effect-of-time",
    "href": "05.html#recentering-the-effect-of-time",
    "title": "5  Treating Time More Flexibly",
    "section": "5.4 Recentering the effect of TIME",
    "text": "5.4 Recentering the effect of TIME\nOur version of the data from Tomarken, Shelton, Elkins, and Anderson (1997) is saved as medication_pp.csv.\n\nmedication_pp &lt;- read_csv(\"data/medication_pp.csv\")\n\nglimpse(medication_pp)\n\nRows: 1,242\nColumns: 11\n$ id          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, …\n$ treat       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ wave        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n$ day         &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, …\n$ time.of.day &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000…\n$ time        &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 1.0000000, 1.3333333, 1.6666667, 2.0000000, 2.3333333, 2.6666667, 3.0000000…\n$ time333     &lt;dbl&gt; -3.3333333, -3.0000000, -2.6666667, -2.3333333, -2.0000000, -1.6666667, -1.3333333, -1.0000000, -0.6666667, …\n$ time667     &lt;dbl&gt; -6.6666667, -6.3333333, -6.0000000, -5.6666667, -5.3333333, -5.0000000, -4.6666667, -4.3333333, -4.0000000, …\n$ initial     &lt;dbl&gt; 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.10, …\n$ final       &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.90, …\n$ pos         &lt;dbl&gt; 106.6667, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 106.6667, 103.3333…\n\n\nThe medication_pp data do not come with a reading variable, as in Table 5.9. Here we’ll make one and display the time variables highlighted in Table 5.9.\n\nmedication_pp &lt;- medication_pp %&gt;% \n  mutate(reading = ifelse(time.of.day == 0, \"8 am\",\n                          ifelse(time.of.day &lt; .6, \"3 pm\", \"10 pm\")))\n\nmedication_pp %&gt;% \n  select(wave, day, reading, time.of.day:time667) %&gt;% \n  head(n = 10)\n\n# A tibble: 10 × 7\n    wave   day reading time.of.day  time time333 time667\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1     0 8 am          0     0      -3.33    -6.67\n 2     2     0 3 pm          0.333 0.333  -3       -6.33\n 3     3     0 10 pm         0.667 0.667  -2.67    -6   \n 4     4     1 8 am          0     1      -2.33    -5.67\n 5     5     1 3 pm          0.333 1.33   -2       -5.33\n 6     6     1 10 pm         0.667 1.67   -1.67    -5   \n 7     7     2 8 am          0     2      -1.33    -4.67\n 8     8     2 3 pm          0.333 2.33   -1       -4.33\n 9     9     2 10 pm         0.667 2.67   -0.667   -4   \n10    10     3 8 am          0     3      -0.333   -3.67\n\n\nTo help get a sense of the balance in the data, here is a bar plot of the distribution of the numbers of measurement occasions within participants. It’s color coded by treatment status.\n\nmedication_pp %&gt;% \n  mutate(treat = str_c(\"treat = \", treat)) %&gt;% \n  group_by(treat, id) %&gt;% \n  count() %&gt;% \n  \n  ggplot(aes(y = n, fill = treat)) +\n  geom_bar() +\n  scale_y_continuous(breaks = c(2,12, 15:21)) +\n  scale_fill_viridis_d(NULL, end = 0.8) +\n  labs(x = \"count of cases\",\n       y = \"# measurement occasions\") +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nFor this section, our basic model will be\n\\[\n\\begin{align}\n\\text{pos}_{ij} & = \\pi_{0i} + \\pi_{1i} (\\text{time}_{ij} - c) + \\epsilon_{ij} \\\\\n\\pi_{0i}        & = \\gamma_{00} + \\gamma_{01} \\text{treat}_i + \\zeta_{0i} \\\\\n\\pi_{1i}        & = \\gamma_{10} + \\gamma_{11} \\text{treat}_i + \\zeta_{1i},\n\\end{align}\n\\]\nwhere \\(c\\) is a generic constant. For our three variants of \\(\\text{time}_{ij}\\), \\(c\\) will be\n\n0 for time,\n3.3333333 for time333, and\n6.666667 for time667.\n\nOur criterion variable is pos, positive mood rating. The text told us these were from “a package of mood diaries (which use a five-point scale to assess positive and negative moods)” (p. 182). However, we don’t know what numerals were assigned to the points on the scale, we don’t know how many items were used, and we don’t even know whether the items were taken from an existing questionnaire. And unfortunately, the citation Singer and Willett gave for the study is from a conference presentation, making it a pain to track down background information on the internet. In such a situation, it’s difficult to figure out how to set our priors. Though suboptimal, we might first get a sense of the pos data with a histogram.\n\nmedication_pp %&gt;% \n  ggplot(aes(x = pos)) +\n  geom_histogram(binwidth = 10) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s the range().\n\nrange(medication_pp$pos)\n\n[1] 100.0000 466.6667\n\n\nStarting with the prior for our intercept, I think there’s a lot of room for argument, here. To keep with our weakly-regularizing approach to priors, it might make sense to use something like normal(200, 100). But then again, we know something about the study design. At the beginning, participants were in treatment for depression, so we’d expect the starting point to be closer to the lower end of the scale. In that case, we might update our approach to something like normal(150, 50). Feel free to play with alternatives. What I hope this illustrates is that our task would be much easier with more domain knowledge.\nAnyway, given the scale of the data, weakly-regularizing priors for the predictor variables might take the form of something like normal(0, 25). We’ll use student_t(0, 50) on the \\(\\sigma\\)s and stay steady with lkj(4) on \\(\\rho_{01}\\).\nHere we fit the model with all three versions of time.\n\nfit5.19 &lt;- brm(\n  data = medication_pp, \n  family = gaussian,\n  pos ~ 0 + Intercept + time + treat + time:treat + (1 + time | id),\n  prior = c(prior(normal(150, 50), class = b, coef = Intercept),\n            prior(normal(0, 25), class = b),\n            prior(student_t(3, 0, 50), class = sd),\n            prior(student_t(3, 0, 50), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.19\")\n\nfit5.20 &lt;- update(\n  fit5.19,\n  newdata = medication_pp,\n  pos ~ 0 + Intercept + time333 + treat + time333:treat + (1 + time333 | id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.20\")\n\nfit5.21 &lt;- update(\n  fit5.19,\n  newdata = medication_pp,\n  pos ~ 0 + Intercept + time667 + treat + time667:treat + (1 + time667 | id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.21\")\n\nGiven the size of our posterior standard deviations, our \\(\\gamma\\) posteriors are quite comparable to the point estimates (and their standard errors) in the text.\n\nprint(fit5.19)\n\n Family: gaussian \n  Links: mu = identity \nFormula: pos ~ 0 + Intercept + time + treat + time:treat + (1 + time | id) \n   Data: medication_pp (Number of observations: 1242) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 64) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)          47.08      4.81    38.68    57.48 1.01     1014     1886\nsd(time)                8.23      0.95     6.56    10.20 1.00     1336     2155\ncor(Intercept,time)    -0.28      0.13    -0.51    -0.02 1.00     1258     1872\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    166.54      8.95   149.01   184.76 1.00      906     1259\ntime          -2.33      1.79    -5.75     1.18 1.00     1136     2053\ntreat         -1.93     11.28   -24.41    19.65 1.00      985     1407\ntime:treat     5.41      2.33     0.91     9.98 1.00     1172     1870\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    35.13      0.76    33.68    36.67 1.00     5894     2553\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nGiven the scales we’re working with, it’s difficult to compare our \\(\\sigma\\) summaries with the \\(\\sigma^2\\) summaries in Table 5.10 of the text. Here we convert and re-summarize.\n\nv &lt;- as_draws_df(fit5.19) %&gt;%\n  transmute(sigma_2_0 = sd_id__Intercept^2,\n            sigma_2_1 = sd_id__time^2,\n            sigma_2_epsilon = sigma^2)\n\nv %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi() %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 3 × 7\n  name             value .lower .upper .width .point .interval\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 sigma_2_0       2178.  1496.   3304.   0.95 median qi       \n2 sigma_2_1         66.9   43.0   104.   0.95 median qi       \n3 sigma_2_epsilon 1234.  1134.   1345.   0.95 median qi       \n\n\nTurns out the posterior medians are quite similar to the ML estimates in the text. Here’s what the entire distributions looks like.\n\nv %&gt;% \n  set_names(\"sigma[0]^2\", \"sigma[1]^2\", \"sigma[epsilon]^2\") %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = 0.95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"posterior\") +\n  theme(panel.grid = element_blank(),\n        strip.text = element_text(size = 11)) +\n  facet_wrap(~ name, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\nHere’s our version of Figure 5.5.\n\nnd &lt;- crossing(\n  treat = 0:1,\n  time  = seq(from = 0, to = 7, length.out = 30))\n\ntext &lt;- tibble(\n  treat = 0:1,\n  time  = 4,\n  y     = c(135, 197),\n  label = c(\"control\", \"treatment\"),\n  angle = c(350, 15))\n\nfitted(fit5.19,\n       newdata = nd,\n       re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  ggplot(aes(x = time, fill = treat, color = treat, group = treat)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3, linewidth = 0) +\n  geom_line(aes(y = Estimate)) +\n  geom_text(data = text,\n            aes(y = y, label = label, angle = angle)) +\n  scale_fill_viridis_c(option = \"A\", begin = 0.3, end = 0.6) +\n  scale_color_viridis_c(option = \"A\", begin = 0.3, end = 0.6) +\n  labs(x = \"days\",\n       y = \"pos\") +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAlthough we still have clear evidence of an interaction, adding those 95% intervals makes it look less impressive, doesn’t it?\nHere are the summaries for the models with the alternative versions of \\((\\text{time}_{ij} - c)\\).\n\nprint(fit5.20)\n\n Family: gaussian \n  Links: mu = identity \nFormula: pos ~ Intercept + time333 + treat + (1 + time333 | id) + time333:treat - 1 \n   Data: medication_pp (Number of observations: 1242) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 64) \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)             46.16      4.39    38.58    56.04 1.02      423      822\nsd(time333)                8.34      0.95     6.65    10.30 1.01     1069     1518\ncor(Intercept,time333)     0.21      0.13    -0.06     0.45 1.01      962     1368\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       160.31      8.23   144.36   177.19 1.01      314      477\ntime333          -2.41      1.83    -5.97     1.06 1.01      616     1278\ntreat            12.67     10.49    -8.91    33.20 1.02      330      632\ntime333:treat     5.49      2.41     0.83    10.18 1.00      625     1206\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    35.13      0.73    33.77    36.59 1.00     3224     2843\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit5.21)\n\n Family: gaussian \n  Links: mu = identity \nFormula: pos ~ Intercept + time667 + treat + (1 + time667 | id) + time667:treat - 1 \n   Data: medication_pp (Number of observations: 1242) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 64) \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)             57.82      5.36    48.39    69.59 1.00     1138     1638\nsd(time667)                8.08      0.90     6.46     9.98 1.00     1306     2038\ncor(Intercept,time667)     0.59      0.09     0.39     0.75 1.00     1520     2361\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       157.08     10.61   136.19   177.45 1.01      784     1272\ntime667          -1.88      1.72    -5.31     1.44 1.00     1157     1855\ntreat            24.40     13.05    -1.16    49.84 1.01      920     1503\ntime667:treat     4.60      2.21     0.28     8.94 1.01     1239     2161\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    35.13      0.75    33.69    36.62 1.00     5910     3353\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt might be easier to compare the \\(\\gamma\\) summaries across models with a well-designed coefficient plot. Here’s an attempt.\n\ntibble(name = str_c(\"fit5.\", 19:21)) %&gt;% \n  mutate(fixef = map(name, ~ get(.) %&gt;% \n                       fixef() %&gt;% \n                       data.frame() %&gt;% \n                       rownames_to_column(\"parameter\"))) %&gt;% \n  unnest(fixef) %&gt;% \n  mutate(gamma = rep(c(\"gamma[0][0]\", \"gamma[1][0]\", \"gamma[0][1]\", \"gamma[1][1]\"), times = 3)) %&gt;% \n  \n  ggplot(aes(x = name, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_pointrange() +\n  xlab(NULL) +\n  coord_flip() +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        strip.text = element_text(size = 11)) +\n  facet_wrap(~ gamma, scales = \"free_x\", labeller = label_parsed, ncol = 4)\n\n\n\n\n\n\n\n\nSince we’re juggling less information, we might compare the posteriors for \\(\\sigma_1^2\\) across the three models with good old tidybayes::geom_halfeyeh().\n\ntibble(name = str_c(\"fit5.\", 19:21)) %&gt;% \n  mutate(fit = map(name, get)) %&gt;% \n  mutate(sigma_2_1 = map(fit, ~ VarCorr(., summary = F)[[1]][[1]][, 1]^2 %&gt;% \n                           data.frame() %&gt;% \n                           set_names(\"sigma_2_1\"))) %&gt;% \n  unnest(sigma_2_1) %&gt;% \n  \n  ggplot(aes(x = sigma_2_1, y = name)) +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  scale_x_continuous(expression(sigma[1]^2), limits = c(0, NA)) +\n  ylab(NULL) +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFor kicks and giggles, we marked off both 50% and 95% intervals for each. Here’s the same for \\(\\rho_{01}\\).\n\ntibble(name = str_c(\"fit5.\", 19:21)) %&gt;% \n  mutate(fit = map(name, get)) %&gt;% \n  mutate(rho = map(fit, ~ VarCorr(., summary = F)[[1]][[2]][, 2, \"Intercept\"] %&gt;% \n                           data.frame() %&gt;% \n                           set_names(\"rho\"))) %&gt;% \n  unnest(rho) %&gt;% \n  \n  ggplot(aes(x = rho, y = name)) +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  labs(x = expression(rho[0][1]),\n       y = NULL) +\n  coord_cartesian(xlim = c(-1, 1)) +\n  theme(axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n“As Rogosa and Willett (1985) demonstrate, you can always alter the correlation between the level-1 growth parameters simply by changing the centering constant” (p. 186).\nHere are the WAIC comparisons.\n\nfit5.19 &lt;- add_criterion(fit5.19, criterion = \"waic\")\nfit5.20 &lt;- add_criterion(fit5.20, criterion = \"waic\")\nfit5.21 &lt;- add_criterion(fit5.21, criterion = \"waic\")\n\nloo_compare(fit5.19, fit5.20, fit5.21, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.21     0.0       0.0 -6241.8      41.4        109.9     7.6   12483.5    82.9\nfit5.20    -0.4       0.5 -6242.2      41.4        110.6     7.6   12484.3    82.8\nfit5.19    -0.5       0.7 -6242.3      41.4        110.0     7.6   12484.6    82.9\n\nmodel_weights(fit5.19, fit5.20, fit5.21, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.19 fit5.20 fit5.21 \n  0.257   0.298   0.445 \n\n\nAs one might hope, not much going on.\nAs Singer and Willett alluded to in the text (p. 187), the final model in this chapter is an odd one. Do note the initial and final columns in the data.\n\nglimpse(medication_pp)\n\nRows: 1,242\nColumns: 12\n$ id          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, …\n$ treat       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ wave        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n$ day         &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, …\n$ time.of.day &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000, 0.3333333, 0.6666667, 0.0000000…\n$ time        &lt;dbl&gt; 0.0000000, 0.3333333, 0.6666667, 1.0000000, 1.3333333, 1.6666667, 2.0000000, 2.3333333, 2.6666667, 3.0000000…\n$ time333     &lt;dbl&gt; -3.3333333, -3.0000000, -2.6666667, -2.3333333, -2.0000000, -1.6666667, -1.3333333, -1.0000000, -0.6666667, …\n$ time667     &lt;dbl&gt; -6.6666667, -6.3333333, -6.0000000, -5.6666667, -5.3333333, -5.0000000, -4.6666667, -4.3333333, -4.0000000, …\n$ initial     &lt;dbl&gt; 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.10, …\n$ final       &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.90, …\n$ pos         &lt;dbl&gt; 106.6667, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 106.6667, 103.3333…\n$ reading     &lt;chr&gt; \"8 am\", \"3 pm\", \"10 pm\", \"8 am\", \"3 pm\", \"10 pm\", \"8 am\", \"3 pm\", \"10 pm\", \"8 am\", \"3 pm\", \"10 pm\", \"8 am\", …\n\n\nWith this model\n\\[\n\\begin{align}\n\\text{pos}_{ij} & = \\pi_{0i} \\bigg (\\frac{6.67 - \\text{time}_{ij}}{6.67} \\bigg ) + \\pi_{1i} \\bigg (\\frac {\\text{time}_{ij}}{6.67} \\bigg ) + \\epsilon_{ij} \\\\\n\\pi_{0i}        & = \\gamma_{00} + \\gamma_{01} \\text{treat}_i + \\zeta_{0i} \\\\\n\\pi_{1i}        & = \\gamma_{10} + \\gamma_{11} \\text{treat}_i + \\zeta_{1i}.\n\\end{align}\n\\]\nBecause of the parameterization, we’ll use both variables simultaneously to indicate time. In the code, below, you’ll notice we no longer have an intercept parameter. Rather, we just have initial and final. As such, both of those parameters get the same prior we’ve been using for the intercept in the previous models.\n\nfit5.22 &lt;- brm(\n  data = medication_pp, \n  family = gaussian,\n  pos ~ 0 + initial + final + initial:treat + final:treat + (0 + initial + final | id),\n  prior = c(prior(normal(150, 50), class = b, coef = initial),\n            prior(normal(150, 50), class = b, coef = final),\n            prior(normal(0, 25), class = b),\n            prior(student_t(3, 0, 50), class = sd),\n            prior(student_t(3, 0, 50), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/fit05.22\")\n\nOur results are quite similar to those in the text.\n\nprint(fit5.22)\n\n Family: gaussian \n  Links: mu = identity \nFormula: pos ~ 0 + initial + final + initial:treat + final:treat + (0 + initial + final | id) \n   Data: medication_pp (Number of observations: 1242) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 64) \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(initial)           47.21      4.70    39.08    57.42 1.00     1885     2601\nsd(final)             59.16      5.83    48.97    71.95 1.00     1579     1974\ncor(initial,final)     0.42      0.11     0.19     0.62 1.00     1305     2084\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ninitial         167.63      8.66   150.84   184.70 1.00     1587     2228\nfinal           156.07     10.43   135.87   176.73 1.00     1951     2388\ninitial:treat    -4.11     10.83   -25.18    18.18 1.00     1248     2261\nfinal:treat      24.82     13.02    -0.86    50.00 1.00     2224     2560\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    35.10      0.75    33.65    36.59 1.00     5182     2738\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s finish up with the WAIC.\n\nfit5.22 &lt;- add_criterion(fit5.22, criterion = \"waic\")\n\nloo_compare(fit5.19, fit5.20, fit5.21, fit5.22, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit5.22     0.0       0.0 -6241.2      41.5        110.3     7.6   12482.4    83.0\nfit5.21    -0.6       0.7 -6241.8      41.4        109.9     7.6   12483.5    82.9\nfit5.20    -1.0       0.6 -6242.2      41.4        110.6     7.6   12484.3    82.8\nfit5.19    -1.1       0.7 -6242.3      41.4        110.0     7.6   12484.6    82.9\n\nmodel_weights(fit5.19, fit5.20, fit5.21, fit5.22, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit5.19 fit5.20 fit5.21 fit5.22 \n  0.144   0.167   0.249   0.441 \n\n\nAll about the same.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "05.html#session-info",
    "href": "05.html#session-info",
    "title": "5  Treating Time More Flexibly",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] bayesplot_1.15.0.9000 ggdist_3.3.3          tidybayes_3.0.7       brms_2.23.0           Rcpp_1.1.0           \n [6] posterior_1.6.1.9000  cmdstanr_0.9.0        lubridate_1.9.4       forcats_1.0.1         stringr_1.6.0        \n[11] dplyr_1.1.4           purrr_1.2.1           readr_2.1.5           tidyr_1.3.2           tibble_3.3.1         \n[16] ggplot2_4.0.1         tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n  [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          mgcv_1.9-3              loo_2.9.0.9000         \n [11] systemfonts_1.3.1       vctrs_0.6.5             reshape2_1.4.5          pkgconfig_2.0.3         shape_1.4.6.1          \n [16] arrayhelpers_1.1-0      crayon_1.5.3            fastmap_1.2.0           backports_1.5.0         labeling_0.4.3         \n [21] utf8_1.2.6              rmarkdown_2.30          tzdb_0.5.0              ps_1.9.1                ragg_1.5.0             \n [26] bit_4.6.0               xfun_0.55               jsonlite_2.0.0          uuid_1.2-1              R6_2.6.1               \n [31] stringi_1.8.7           RColorBrewer_1.1-3      StanHeaders_2.36.0.9000 estimability_1.5.1      rstan_2.36.0.9000      \n [36] knitr_1.51              zoo_1.8-14              Matrix_1.7-3            splines_4.5.1           timechange_0.3.0       \n [41] tidyselect_1.2.1        rstudioapi_0.17.1       abind_1.4-8             codetools_0.2-20        curl_7.0.0             \n [46] processx_3.8.6          pkgbuild_1.4.8          lattice_0.22-7          plyr_1.8.9              withr_3.0.2            \n [51] bridgesampling_1.2-1    S7_0.2.1                askpass_1.2.1           flextable_0.9.10        coda_0.19-4.1          \n [56] evaluate_1.0.5          survival_3.8-3          RcppParallel_5.1.11-1   zip_2.3.3               xml2_1.4.0             \n [61] pillar_1.11.1           tensorA_0.36.2.1        checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0   \n [66] generics_0.1.4          vroom_1.6.6             hms_1.1.4               rstantools_2.5.0.9000   scales_1.4.0           \n [71] xtable_1.8-4            glue_1.8.0              gdtools_0.4.4           emmeans_1.11.2-8        tools_4.5.1            \n [76] data.table_1.17.8       mvtnorm_1.3-3           grid_4.5.1              QuickJSR_1.8.1          nlme_3.1-168           \n [81] cli_3.6.5               textshaping_1.0.4       officer_0.7.2           fontBitstreamVera_0.1.1 svUnit_1.0.8           \n [86] viridisLite_0.4.2       Brobdingnag_1.2-9       V8_8.0.1                gtable_0.3.6            digest_0.6.39          \n [91] fontquiver_0.2.1        TH.data_1.1-4           htmlwidgets_1.6.4       farver_2.1.2            htmltools_0.5.9        \n [96] lifecycle_1.0.5         openssl_2.3.4           fontLiberation_0.1.0    bit64_4.6.0-1           MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "05.html#comments",
    "href": "05.html#comments",
    "title": "5  Treating Time More Flexibly",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBrown, D. R., & Gary, L. E. (1985). Predictors of depressive symptoms among unemployed Black adults. Journal of Sociology and Social Welfare, 12, 736. https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=1721&amp=&context=jssw&amp=&sei-redir=1&referer=https%253A%252F%252Fscholar.google.com%252Fscholar%253Fq%253D%252522CES-D%252522%252Bunemployment%2526hl%253Den%2526as_sdt%253D0%25252C44%2526as_ylo%253D1977%2526as_yhi%253D2000#search=%22CES-D%20unemployment%22\n\n\nBürkner, P.-C. (2020). Bayesian item response modeling in R with brms and Stan. http://arxiv.org/abs/1905.09501\n\n\nBürkner, P.-C. (2021). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html\n\n\nBürkner, P.-C., Gabry, J., Kay, M., & Vehtari, A. (2022). posterior: Tools for working with posterior distributions. https://CRAN.R-project.org/package=posterior\n\n\nChung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., & Liu, J. (2013). A nondegenerate penalized likelihood estimator for variance parameters in multilevel models. Psychometrika, 78(4), 685–709. https://doi.org/10.1007/s11336-013-9328-2\n\n\nEnders, C. K. (2010). Applied missing data analysis. Guilford Press. http://www.appliedmissingdata.com/\n\n\nGabry, J., & Mahr, T. (2022). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot\n\n\nGabry, J., & Modrák, M. (2020). Visual MCMC diagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378\n\n\nGinexi, E. M., Howe, G. W., & Caplan, R. D. (2000). Depression and control beliefs in relation to reemployment: What are the directions of effect? Journal of Occupational Health Psychology, 5(3), 323–336. https://doi.org/10.1037/1076-8998.5.3.323\n\n\nKay, M. (2023). tidybayes: Tidy data and ’geoms’ for Bayesian models. https://CRAN.R-project.org/package=tidybayes\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nLi, H., & Lahiri, P. (2010). An adjusted maximum likelihood method for solving small area estimation problems. Journal of Multivariate Analysis, 101(4), 882–892. https://doi.org/10.1016/j.jmva.2009.10.009\n\n\nLittle, R. J. (1995). Modeling the drop-out mechanism in repeated-measures studies. Journal of the American Statistical Association, 90(431), 1112–1121. https://doi.org/10.1080/01621459.1995.10476615\n\n\nLittle, R. J. A., & Rubin, D., B. (1987). Statistical analysis with missing data. Wiley.\n\n\nLittle, R. J., & Rubin, D. B. (2019). Statistical analysis with missing data (3rd ed., Vol. 793). John Wiley & Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). rethinking R package. https://xcelab.net/rm/software/\n\n\nMorris, C., & Tang, R. (2011). Estimating random effects via adjustment for density maximization. Statistical Science, 26(2), 271–287. https://doi.org/10.1214/10-STS349\n\n\nRadloff, L. S. (1977). The CES-D Scale: A self-report depression scale for research in the general population. Applied Psychological Measurement, 1(3), 385–401. https://doi.org/10.1177/014662167700100306\n\n\nRogosa, D. R., & Willett, J. B. (1985). Understanding correlates of change by modeling individual differences in growth. Psychometrika, 50(2), 203–228. https://doi.org/10.1007/BF02294247\n\n\nSchafer, J. L. (1997). Analysis of incomplete multivariate data. CRC press. https://www.routledge.com/Analysis-of-Incomplete-Multivariate-Data/Schafer/p/book/9780412040610\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nTomarken, A., Shelton, R., Elkins, L., & Anderson, T. (1997). Sleep deprivation and anti-depressant medication: Unique effects on positive and negative affect. American Psychological Society Meeting, Washington, DC.\n\n\nvan Buuren, S. (2018). Flexible imputation of missing data (Second Edition). CRC Press. https://stefvanbuuren.name/fimd/\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved for assessing convergence of MCMC. https://arxiv.org/abs/1903.08008?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Treating Time More Flexibly</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "",
    "text": "6.1 Discontinuous individual change",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#discontinuous-individual-change",
    "href": "06.html#discontinuous-individual-change",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "",
    "text": "Not all individual change trajectories are continuous functions of time…\nIf you have reason to believe that individual change trajectories might shift in elevation and/or slope, your level-1 model should reflect this hypothesis. Doing so allows you to test ideas about how the trajectory’s shape might change over time…\nTo postulate a discontinuous individual change trajectory, you need to know not just why the shift might occur but also when. This is because your level-1 individual growth model must include one (or more) time-varying predictor(s) that specify whether and if so, when each person experiences the hypothesized shift. (pp. 190–191, emphasis in the original)\n\n\n6.1.1 Alternative discontinuous level-1 models for change\n\nTo postulate a discontinuous level-1 individual growth model, you must first decide on its functional form. Although you can begin empirically, we prefer to focus on substance and the longitudinal process that gave rise to the data. What kind of discontinuity might the precipitating event create? What would a plausible level-1 trajectory look like? Before parameterizing models and constructing variables, we suggest that you: (1) take a pen and paper and sketch some options; and (2) articulate–in words, not equations–the rationale for each. We recommend these steps because, as we demonstrate, the easiest models to specify may not display the type of discontinuity you expect to find. (pp. 191–192)\n\nI’ll leave the pen and paper scribbling to you. Here we load the wages_pp.csv data.\n\nlibrary(tidyverse)\n\nwages_pp &lt;- read_csv(\"data/wages_pp.csv\")\n\nglimpse(wages_pp)\n\nRows: 6,402\nColumns: 15\n$ id            &lt;dbl&gt; 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 53, 53, 53, 53, 53, 53, 53, 53, 12…\n$ lnw           &lt;dbl&gt; 1.491, 1.433, 1.469, 1.749, 1.931, 1.709, 2.086, 2.129, 1.982, 1.798, 2.256, 2.573, 1.819, 2.928, 2.443, 2…\n$ exper         &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040, 3.021, 4.021, 5.521, 6.733, 7…\n$ ged           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ postexp       &lt;dbl&gt; 0.015, 0.715, 1.734, 2.773, 3.927, 4.946, 5.965, 6.984, 0.315, 0.983, 2.040, 3.021, 4.021, 5.521, 6.733, 7…\n$ black         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hispanic      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hgc           &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, 12, 12, 12, 12, 12, …\n$ hgc.9         &lt;dbl&gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -2, -2, -2, -2, -2, -2, -2, 3, 3, 3, 3, …\n$ uerate        &lt;dbl&gt; 3.215, 3.215, 3.215, 3.295, 2.895, 2.495, 2.595, 4.795, 4.895, 7.400, 7.400, 5.295, 4.495, 2.895, 2.595, 2…\n$ ue.7          &lt;dbl&gt; -3.785, -3.785, -3.785, -3.705, -4.105, -4.505, -4.405, -2.205, -2.105, 0.400, 0.400, -1.705, -2.505, -4.1…\n$ ue.centert1   &lt;dbl&gt; 0.000, 0.000, 0.000, 0.080, -0.320, -0.720, -0.620, 1.580, 0.000, 2.505, 2.505, 0.400, -0.400, -2.000, -2.…\n$ ue.mean       &lt;dbl&gt; 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 3.215000, 5.096500, 5.096500, 5.0965…\n$ ue.person.cen &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.08000000, -0.32000000, -0.72000000, -0.62000000, 1.58000000, -0.2015…\n$ ue1           &lt;dbl&gt; 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 3.215, 4.895, 4.895, 4.895, 4.895, 4.895, 4.895, 4.895, 4…\n\n\nHere’s a more focused look along the lines of Table 6.1.\n\nwages_pp %&gt;% \n  select(id, lnw, exper, ged, postexp) %&gt;% \n  mutate(`ged by exper` = ged * exper) %&gt;% \n  filter(id %in% c(206, 2365, 4384))\n\n# A tibble: 22 × 6\n      id   lnw exper   ged postexp `ged by exper`\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1   206  2.03  1.87     0    0              0   \n 2   206  2.30  2.81     0    0              0   \n 3   206  2.48  4.31     0    0              0   \n 4  2365  1.78  0.66     0    0              0   \n 5  2365  1.76  1.68     0    0              0   \n 6  2365  1.71  2.74     0    0              0   \n 7  2365  1.74  3.68     0    0              0   \n 8  2365  2.19  4.68     1    0              4.68\n 9  2365  2.04  5.72     1    1.04           5.72\n10  2365  2.32  6.72     1    2.04           6.72\n# ℹ 12 more rows\n\n\nSimilar to what we did in Section 5.2.1, here is a visualization of the two primary variables, exper and lnw, for those three participants.\n\nwages_pp %&gt;% \n  filter(id %in% c(206, 2365, 4384)) %&gt;% \n  mutate(id = factor(id)) %&gt;% \n  \n  ggplot(aes(x = exper, y = lnw)) +\n  geom_point(aes(color = id),\n             size = 4) +\n  geom_line(aes(color = id)) +\n  geom_text(aes(label = ged),\n            size = 3) +\n  scale_x_continuous(breaks = 1:13) +\n  scale_color_viridis_d(option = \"B\", begin = 0.6, end = 0.9) +\n  labs(caption = expression(italic(\"Note\")*'. GED status is coded 0 = \"not yet\", 1 = \"yes.\"')) +\n  theme(panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nNote how the time-varying predictor ged is depicted as either an 0 or a 1 in the center of the dots. Maybe we might describe that change as linear with the simple model \\(\\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}\\). Maybe a different model that included ged would be helpful.\nIn the data, \\(n = 581\\) never got their GED’s, while \\(n = 307\\) did.\n\nwages_pp %&gt;% \n  group_by(id) %&gt;% \n  summarise(got_ged = sum(ged) &gt; 0) %&gt;% \n  count(got_ged)\n\n# A tibble: 2 × 2\n  got_ged     n\n  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE     581\n2 TRUE      307\n\n\nHowever, those who did get their GED’s did so at different times. Here’s their distribution.\n\nwages_pp %&gt;% \n  filter(ged == 1) %&gt;% \n  group_by(id) %&gt;% \n  slice(1) %&gt;%\n  \n  ggplot(aes(x = exper)) +\n  geom_histogram(binwidth = 0.5) +\n  labs(subtitle = expression(The~italic(timing)~of~the~GED~attainment~varies)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s another, more focused, look at the GED status for our three focal participants, over exper.\n\nwages_pp %&gt;% \n  select(id, lnw, exper, ged, postexp) %&gt;% \n  filter(id %in% c(206, 2365, 4384)) %&gt;% \n  mutate(id = factor(id)) %&gt;%\n  \n  ggplot(aes(x = exper, y = ged)) +\n  geom_point(aes(color = id),\n             size = 4) +\n  scale_x_continuous(breaks = 1:13) +\n  scale_y_continuous(breaks = 0:1, limits = c(-0.2, 1.2)) +\n  scale_color_viridis_d(option = \"B\", begin = 0.6, end = 0.9, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0)) +\n  facet_wrap(~ id)\n\n\n\n\n\n\n\n\nOne might wonder: “How might GED receipt affect individual \\(i\\)’s wage trajectory?” (p. 193). Here we reproduce Figure 6.1, which entertains four possibilities.\n\ntibble(exper = c(0, 3, 3, 10),\n       ged   = rep(0:1, each = 2)) %&gt;% \n  expand(model = letters[1:4],\n         nesting(exper, ged)) %&gt;% \n  mutate(exper2 = if_else(ged == 0, 0, exper - 3)) %&gt;% \n  mutate(lnw = case_when(\n    model == \"a\" ~ 1.60 + 0.04 * exper,\n    model == \"b\" ~ 1.65 + 0.04 * exper + 0.05 * ged,\n    model == \"c\" ~ 1.75 + 0.04 * exper + 0.02 * exper2 * ged,\n    model == \"d\" ~ 1.85 + 0.04 * exper + 0.01 * ged + 0.02 * exper * ged\n  ),\n  model = fct_rev(model)) %&gt;% \n  \n  ggplot(aes(x = exper, y = lnw)) +\n  geom_line(aes(color = model),\n            linewidth = 1) +\n  scale_color_viridis_d(option = \"D\", begin = 1/4, end = 3/4) +\n  ylim(1.5, 2.5) +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\n6.1.1.1 Including a discontinuity in elevation, not slope\nWe can write the level-1 formula for when there is a change in elevation, but not slope, as\n\\[\n\\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\epsilon_{ij}.\n\\]\nBecause we are equating ged values as relating to the intercept, but not the slope, it might be helpful to rewrite that formula as\n\\[\n\\text{lnw}_{ij} = (\\pi_{0i} + \\pi_{2i} \\text{ged}_{ij}) + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij},\n\\]\nwhere the portion inside of the parentheses concerns initial status and discontinuity in elevation, but not slope. Because the ged values only come in 0’s and 1’s, we can express the two versions of this equation as\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = [\\pi_{0i} + \\pi_{2i} (0)] + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij} \\;\\;\\; \\text{and} \\\\\n                & = [\\pi_{0i} + \\pi_{2i} (1)] + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}.\n\\end{align}\n\\]\nIn other words, whereas the pre-GED intercept is \\(\\pi_{0i}\\), the post-GED intercept is \\(\\pi_{0i} + \\pi_{2i}\\). To get a better sense of this, we might make a version of the upper left panel of Figure 6.2. Since we’ll be making four of these over the next few sections, we might reduce the redundancies in the code by making a custom plotting function. We’ll call it plot_figure_6.2().\n\nplot_figure_6.2 &lt;- function(data, \n                            mapping, \n                            linewidths = c(1, 1/4), \n                            linetypes = c(1, 2), \n                            ...) {\n  ggplot(data, mapping) +\n    geom_line(aes(linewidth = model, linetype = model)) +\n    geom_text(data = text,\n              aes(label = label, hjust = hjust),\n              size = 3, parse = T) +\n    geom_segment(data = arrow,\n                 aes(xend = xend, yend = yend),\n                 arrow = arrow(length = unit(0.075, \"inches\"), type = \"closed\"),\n                 linewidth = 1/4) +\n    scale_linewidth_manual(values = linewidths) +\n    scale_linetype_manual(values = linetypes) +\n    scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +\n    scale_y_continuous(breaks = 0:4 * 0.2 + 1.6, expand = c(0, 0)) +\n    coord_cartesian(ylim = c(1.6, 2.4)) +\n    theme(legend.position = \"none\",\n          panel.grid = element_blank())\n}\n\nNow we have our custom plotting function, let’s take it for a spin.\n\ntext &lt;- tibble(\n  exper = c(4.5, 4.5, 7.5, 7.5, 1),\n  lnw   = c(2.24, 2.2, 1.82, 1.78, 1.62),\n  label = c(\"Common~rate~of~change\",\n            \"Pre-Post~GED~(pi[1][italic(i)])\",\n            \"Elevation~differential\",\n            \"on~GED~receipt~(pi[2][italic(i)])\",\n            \"italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])\"),\n  hjust = c(0.5, 0.5, 0.5, 0.5, 0))\n\narrow &lt;- tibble(\n  exper = c(2.85, 5.2, 5.5, 1.7),\n  xend  = c(2, 6.8, 3.1, 0.05),\n  lnw   = c(2.18, 2.18, 1.8, 1.64),\n  yend  = c(1.84, 2.08, 1.9, 1.74))\n\np1 &lt;- tibble(exper = c(0, 3, 3, 10),\n             ged   = rep(0:1, each = 2)) %&gt;% \n  expand(model = letters[1:2],\n         nesting(exper, ged)) %&gt;% \n  mutate(exper2 = if_else(ged == 0, 0, exper - 3)) %&gt;% \n  mutate(lnw = case_when(\n    model == \"a\" ~ 1.75 + 0.04 * exper,\n    model == \"b\" ~ 1.75 + 0.04 * exper + 0.05 * ged),\n  model = fct_rev(model)) %&gt;%\n  \n  plot_figure_6.2(aes(x = exper, y = lnw))\n  \np1  \n\n\n\n\n\n\n\n\nWorked like a dream!\n\n\n6.1.1.2 Including a discontinuity in slope, not elevation\n\nTo specify a level-1 individual growth model that includes a discontinuity in slope, not elevation, you need a different time-varying predictor. Unlike GED, this predictor must clock the passage of time (like EXPER). But unlike EXPER, it must do so within only one of the two epochs (pre- or post-GED receipt). Adding a second temporal predictor allows each individual change trajectory to have two distinct slopes: one before the hypothesized discontinuity an another after. (p. 195, emphasis in the original)\n\nIn the wages_pp data, postexp is this second variable. Here is how it compares to the other relevant variables.\n\nwages_pp %&gt;% \n  select(id, ged, exper, postexp) %&gt;% \n  filter(id &gt;= 53)\n\n# A tibble: 6,384 × 4\n      id   ged exper postexp\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    53     0 0.781   0    \n 2    53     0 0.943   0    \n 3    53     1 0.957   0    \n 4    53     1 1.04    0.08 \n 5    53     1 1.06    0.1  \n 6    53     1 1.11    0.152\n 7    53     1 1.18    0.227\n 8    53     1 1.78    0.82 \n 9   122     0 2.04    0    \n10   122     0 2.64    0    \n# ℹ 6,374 more rows\n\n\nSinger and Willett then went on to report “construction of a suitable time-varying predictor to register the desired discontinuity is often the hardest part of the model specification” (p. 195). They weren’t kidding.\nThis concept caused me a good bit of frustration when learning about these models. Let’s walk through this slowly. In the last code block, we looked at four relevant variables. You may wonder why we executed filter(id &gt;= 53). This is because the first two participants always had ged == 1. They’re valid cases and all, but those data won’t be immediately helpful for understanding what’s going on with postexp. Happily, the next case, id == 53, is perfect for our goal. First, notice how that person’s postexp values are always 0 when ged == 0. Second, notice how the first time where ged == 1, postexp is still a 0. Third, notice that after that first initial row, postexp increases. If you caught all that, go you!\nTo make the next point, it’ll come in handy to subset the data. Because we’re trying to understand the relationship between exper and postesp conditional on ged, cases for which ged is always the same will be of little use. Let’s drop them.\n\nwages_pp_subset &lt;- wages_pp %&gt;% \n  group_by(id) %&gt;% \n  filter(mean(ged) &gt; 0) %&gt;% \n  filter(mean(ged) &lt; 1) %&gt;% \n  ungroup() %&gt;% \n  select(id, ged, exper, postexp)\n\nwages_pp_subset\n\n# A tibble: 819 × 4\n      id   ged exper postexp\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    53     0 0.781   0    \n 2    53     0 0.943   0    \n 3    53     1 0.957   0    \n 4    53     1 1.04    0.08 \n 5    53     1 1.06    0.1  \n 6    53     1 1.11    0.152\n 7    53     1 1.18    0.227\n 8    53     1 1.78    0.82 \n 9   134     0 0.192   0    \n10   134     0 0.972   0    \n# ℹ 809 more rows\n\n\nWhat might not be obvious yet is exper and postexp scale together. To show how this works, we’ll make two new columns. First, we’ll mark the minimum exper value for each level of id. Then we’ll make a exper - postexp which is exactly what the name implies. Here’s what that looks like.\n\nwages_pp_subset %&gt;% \n  filter(ged == 1) %&gt;% \n  group_by(id) %&gt;% \n  mutate(min_exper         = min(exper),\n         `exper - postexp` = exper - postexp)\n\n# A tibble: 525 × 6\n# Groups:   id [107]\n      id   ged exper postexp min_exper `exper - postexp`\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n 1    53     1 0.957   0         0.957             0.957\n 2    53     1 1.04    0.08      0.957             0.957\n 3    53     1 1.06    0.1       0.957             0.957\n 4    53     1 1.11    0.152     0.957             0.958\n 5    53     1 1.18    0.227     0.957             0.958\n 6    53     1 1.78    0.82      0.957             0.957\n 7   134     1 3.92    0         3.92              3.92 \n 8   134     1 4.64    0.72      3.92              3.92 \n 9   134     1 5.64    1.72      3.92              3.92 \n10   134     1 6.75    2.84      3.92              3.92 \n# ℹ 515 more rows\n\n\nHuh. For each case, the min_exper value is (near)identical with exper - postexp. The reason they’re not always identical is simply rounding error. Had we computed them by hand without rounding, they would always be the same. This relationship is the consequence of our having coded postexp == 0 the very first time ged == 1, but allowed it to linearly increase afterward. Within each level of id–and conditional on ged == 1–, the way it increases is simply exper – min(exper). Here’s that value.\n\nwages_pp_subset %&gt;% \n  filter(ged == 1) %&gt;% \n  group_by(id) %&gt;% \n  mutate(min_exper         = min(exper),\n         `exper - postexp` = exper - postexp) %&gt;% \n  mutate(`exper - min_exper` = exper - min_exper)\n\n# A tibble: 525 × 7\n# Groups:   id [107]\n      id   ged exper postexp min_exper `exper - postexp` `exper - min_exper`\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n 1    53     1 0.957   0         0.957             0.957              0     \n 2    53     1 1.04    0.08      0.957             0.957              0.0800\n 3    53     1 1.06    0.1       0.957             0.957              0.1   \n 4    53     1 1.11    0.152     0.957             0.958              0.153 \n 5    53     1 1.18    0.227     0.957             0.958              0.228 \n 6    53     1 1.78    0.82      0.957             0.957              0.82  \n 7   134     1 3.92    0         3.92              3.92               0     \n 8   134     1 4.64    0.72      3.92              3.92               0.72  \n 9   134     1 5.64    1.72      3.92              3.92               1.72  \n10   134     1 6.75    2.84      3.92              3.92               2.84  \n# ℹ 515 more rows\n\n\nSee? Our new exper - min_exper column is the same, within rounding error, as postexp.\n\nA fundamental feature of POSTEXP–indeed, any temporal predictor designed to register a shift in slope–is that the difference between each non-zero pair of consecutive values must be numerically identical to the difference between the corresponding pair of values for the basic predictor (here, EXPER). (p. 197, emphasis in the original)\n\n\nwages_pp %&gt;% \n  group_by(id) %&gt;% \n  filter(mean(ged) &gt; 0 & mean(ged) &lt; 1) %&gt;% \n  ungroup() %&gt;%\n  pivot_longer(c(exper, postexp),\n               names_to = \"temporal predictor\") %&gt;% \n  filter(id &lt; 250) %&gt;% \n  \n  ggplot(aes(x = value, y = lnw)) +\n  geom_line(aes(linetype = `temporal predictor`, color = `temporal predictor`)) +\n  scale_color_viridis_d(option = \"A\", begin = 1/3, end = 2/3, direction = -1) +\n  theme(legend.position = c(5/6, 0.25),\n        panel.grid = element_blank()) +\n  facet_wrap(~id, scales = \"free_x\")\n\n\n\n\n\n\n\n\nSee how those two scale together within each level of id?\nAll of this work is a setup for the level-1 equation\n\\[\n\\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{3i} \\text{postexp}_{ij} + \\epsilon_{ij},\n\\]\nwhere \\(\\pi_{0i}\\) is the only intercept parameter and \\(\\pi_{1i}\\) and \\(\\pi_{3i}\\) are two slope parameters. Singer and Willett explained\n\neach slope assesses the effect of work experience, but it does so from a different origin: (1) \\(\\pi_{1i}\\) captures the effects of total work experience (measured from labor force entry); and (2) \\(\\pi_{3i}\\) captures the added effect of post-GED work experience (measured from GED receipt). (p. 197, emphasis in the original)\n\nTo get a sense of what this looks like, here’s our version of the upper right panel of Figure 6.2.\n\ntext &lt;- tibble(\n  exper = c(5, 5, 0.5, 0.5, 1),\n  lnw   = c(2.24, 2.2, 2, 1.96, 1.62),\n  label = c(\"Slope~differential\",\n            \"Pre-Post~GED~(pi[3][italic(i)])\",\n            \"Rate~of~change\",\n            \"Pre~GED~(pi[1][italic(i)])\",\n            \"italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])\"),\n  hjust = c(0.5, 0.5, 0, 0, 0))\n\narrow &lt;- tibble(\n  exper = c(5.2, 1.7, 1.7),\n  xend  = c(9.1, 1.7, 0.05),\n  lnw   = c(2.18, 1.93, 1.64),\n  yend  = c(2.15, 1.84, 1.74))\n\np2 &lt;- tibble(exper = c(0, 3, 3, 10),\n             ged   = rep(0:1, each = 2)) %&gt;% \n  expand(model = letters[1:2],\n         nesting(exper, ged)) %&gt;% \n  mutate(postexp = ifelse(exper == 10, 1, 0)) %&gt;% \n  mutate(lnw = case_when(\n    model == \"a\" ~ 1.75 + 0.04 * exper,\n    model == \"b\" ~ 1.75 + 0.04 * exper + 0.15 * postexp),\n  model = fct_rev(model)) %&gt;%\n  \n  plot_figure_6.2(aes(x = exper, y = lnw)) +\n  annotate(geom = \"curve\",\n           x = 8.5, xend = 8.8,\n           y = 2.195, yend = 2.109,\n           arrow = arrow(length = unit(0.05, \"inches\"), type = \"closed\", ends = \"both\"),\n           curvature = -0.85, linetype = 2, linewidth = 1/4)\n  \np2  \n\n\n\n\n\n\n\n\n\n\n6.1.1.3 Including discontinuities in both elevation and slope\nThere are (at least) two ways to do this. They are similar, but not identical. The first is an extension of the model from the last subsection where we retain postexp from our second slope parameter. We can express this as the equation\n\\[\n\\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} \\text{postexp}_{ij} + \\epsilon_{ij}.\n\\]\nFor those without a GED, the equation reduces to\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (0) + \\pi_{3i} (0) + \\epsilon_{ij} \\\\\n                & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}.\n\\end{align}\n\\]\nOnce people secure their GED, \\(\\pi_{2i}\\) is always multiplied by 1 (i.e., \\(\\pi_{2i} (1)\\)) and the values by which we multiply \\(\\pi_{3i}\\) scale linearly with exper, but with the offset the way we discussed in the previous subsection. To emphasize that, we might rewrite the equation as\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (1) + \\pi_{3i} \\text{postexp} + \\epsilon_{ij} \\\\\n                & = (\\pi_{0i} + \\pi_{2i}) + \\pi_{1i} \\text{exper}_{ij} + \\pi_{3i} \\text{postexp} + \\epsilon_{ij}.\n\\end{align}\n\\]\nTo get a sense of what this looks like, here’s our version of the lower left panel of Figure 6.2.\n\ntext &lt;- tibble(\n  exper = c(5, 5, 0.5, 0.5, 1, 7, 7),\n  lnw   = c(2.24, 2.2, 2, 1.96, 1.62, 1.78, 1.74),\n  label = c(\"Slope~differential\",\n            \"Pre-Post~GED~(pi[3][italic(i)])\",\n            \"Rate~of~change\",\n            \"Pre~GED~(pi[1][italic(i)])\",\n            \"italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])\",\n            \"Constant~elevation~differential\",\n            \"on~GED~receipt~(pi[2][italic(i)])\"),\n  hjust = c(0.5, 0.5, 0, 0, 0, 0.5, 0.5))\n\narrow &lt;- tibble(\n  exper = c(5.2, 1.7, 1.7, 6),\n  xend  = c(9.1, 1.7, 0.05, 3.1),\n  lnw   = c(2.18, 1.93, 1.64, 1.8),\n  yend  = c(2.15, 1.84, 1.74, 1.885))\n\np3 &lt;- tibble(exper = c(0, 3, 3, 10),\n             ged   = rep(0:1, each = 2)) %&gt;%\n  expand(model = letters[1:3],\n         nesting(exper, ged)) %&gt;% \n  mutate(postexp = ifelse(exper == 10, 1, 0)) %&gt;% \n  mutate(lnw = case_when(\n    model == \"a\" ~ 1.75 + 0.04 * exper,\n    model == \"b\" ~ 1.75 + 0.04 * exper + 0.02 * ged,\n    model == \"c\" ~ 1.75 + 0.04 * exper + 0.02 * ged + 0.1 * postexp),\n  model = fct_rev(model)) %&gt;%\n  \n  plot_figure_6.2(aes(x = exper, y = lnw),\n                  linewidths = c(1, 1/4, 1/4),\n                  linetypes = c(1, 2, 2)) +\n  annotate(geom = \"curve\",\n           x = 8.5, xend = 8.8,\n           y = 2.185, yend = 2.125,\n           arrow = arrow(length = unit(0.05, \"inches\"), type = \"closed\", ends = \"both\"),\n           curvature = -0.85, linetype = 2, linewidth = 1/4)\n  \np3  \n\n\n\n\n\n\n\n\nThe second way to include discontinuities in both elevation and slope replaces the postexp variable with an interaction between exper and ged. Here’s the equation:\n\\[\n\\text{lnw}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} (\\text{exper}_{ij} \\times \\text{ged}_{ij}) + \\epsilon_{ij}.\n\\]\nFor those without a GED, the equation simplifies to\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (0) + \\pi_{3i} (\\text{exper}_{ij} \\times 0) + \\epsilon_{ij} \\\\\n                & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\epsilon_{ij}.\n\\end{align}\n\\]\nOnce a participant secures their GED, the equation changes to\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} (1) + \\pi_{3i} (\\text{exper}_{ij} \\times 1) + \\epsilon_{ij} \\\\\n               & = (\\pi_{0i} + \\pi_{2i}) + (\\pi_{1i} + \\pi_{3i}) \\text{exper}_{ij} + \\epsilon_{ij}.\n\\end{align}\n\\]\nSo again, the two ways we might include discontinuities in both elevation and slope are\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} \\text{postexp}_{ij} + \\epsilon_{ij} & \\text{and} \\\\\n\\text{lnw}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{exper}_{ij} + \\pi_{2i} \\text{ged}_{ij} + \\pi_{3i} (\\text{exper}_{ij} \\times \\text{ged}_{ij}) + \\epsilon_{ij}.\n\\end{align}\n\\]\nThe \\(\\pi_{0i}\\) and \\(\\pi_{1i}\\) terms have the same meaning in both. Even though \\(\\pi_{3i}\\) is multiplied by different values in the two equations, it has the same interpretation: “it represents the increment (or decrement) of the slope in the post-GED epoch” (p. 200). However, the big difference is the behavior and interpretation for \\(\\pi_{2i}\\). In the equation for the first approach, it “assesses the magnitude of the instantaneous increment (or decrement) associated with GED attainment” (p. 200). But in the equation for the second approach, “\\(\\pi_{2i}\\) assesses the magnitude of the increment (or decrement) associated with GED attainment at a particular–and not particularly meaningful–moment: the day of labor force entry” (p. 220, emphasis added). That is, whereas \\(\\pi_{2i}\\) has a fixed value for the first approach, its magnitude changes with time in the second.\nTo get a sense of what this looks like, here’s our version of the lower right panel of Figure 6.2.\n\ntext &lt;- tibble(\n  exper = c(5, 5, 0.5, 0.5, 1, 7, 7, 8, 8, 8),\n  lnw   = c(2.28, 2.24, 2.1, 2.06, 1.62, 1.76, 1.72, 1.94, 1.9, 1.86),\n  label = c(\"Slope~differential\",\n            \"Pre-Post~GED~(pi[3][italic(i)])\",\n            \"Rate~of~change\",\n            \"Pre~GED~(pi[1][italic(i)])\",\n            \"italic(LNW)~at~labor~force~entry~(pi[0][italic(i)])\",\n            \"GED~differential~at\",\n            \"labor~force~entry~(pi[2][italic(i)])\",\n            \"Elevation~differential\",\n            \"on~GED~receipt\",\n            \"(pi[2][italic(i)]+pi[3][italic(i)]*italic(EXPER))\"),\n  hjust = c(0.5, 0.5, 0, 0, 0, 0.5, 0.5, 0.5, 0.5, 0.5))\n\narrow &lt;- tibble(\n  exper = c(5.2, 1.7, 1.7, 4.9, 6.2),\n  xend  = c(8.8, 1.7, 0.05, 0.05, 3.1),\n  lnw   = c(2.22, 2.03, 1.64, 1.745, 1.9),\n  yend  = c(2.18, 1.825, 1.74, 1.775, 1.9))\n\np4 &lt;- crossing(model = letters[1:4],\n               point = 1:4) %&gt;% \n  mutate(exper = ifelse(point == 1, 0,\n                        ifelse(point == 4, 10, 3)),\n         ged   = c(0, 0, 1, 1,  1, 1, 1, 1,  0, 0, 0, 0,  1, 1, 1, 1)) %&gt;% \n  mutate(lnw = case_when(\n    model %in% letters[1:3] ~ 1.75 + 0.04 * exper + 0.04 * ged + 0.01 * exper * ged,\n    model == \"d\"            ~ 1.75 + 0.04 * exper + 0.04 * ged)) %&gt;% \n  \n  plot_figure_6.2(aes(x = exper, y = lnw),\n                  linewidths = c(1, 1/4, 1/4, 1/4),\n                  linetypes = c(1, 2, 2, 2))  +\n  annotate(geom = \"curve\",\n           x = 8.5, xend = 8.8,\n           y = 2.205, yend = 2.145,\n           arrow = arrow(length = unit(0.05, \"inches\"), type = \"closed\", ends = \"both\"),\n           curvature = -0.85, linetype = 2, linewidth = 1/4)\n\np4\n\n\n\n\n\n\n\n\nYou may have noticed we’ve been saving the various subplot panels as objects. Here we combine them to make the full version of Figure 6.2.\n\nlibrary(patchwork)\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\nGlorious.\n\n\n\n6.1.2 Selecting among the alternative discontinuous models\nOur first model in this section will be a call back from the last chapter, fit5.16.\n\nlibrary(brms)\n\n# model a\nfit5.16 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 5,\n  file = \"fits/fit05.16\")\n\nReview the summary.\n\nprint(fit5.16, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + (1 + exper | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           0.224     0.011    0.203    0.245 1.001     1757     2761\nsd(exper)               0.040     0.003    0.035    0.046 1.003      638     1001\ncor(Intercept,exper)   -0.301     0.070   -0.430   -0.156 1.001      729     1554\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.749     0.011    1.727    1.770 1.001     2545     3177\nexper          0.044     0.003    0.039    0.049 1.001     2325     2972\nhgc_9          0.040     0.006    0.028    0.052 1.003     2622     3067\nuerate_7      -0.012     0.002   -0.015   -0.008 1.001     4783     4011\nexper:black   -0.018     0.005   -0.027   -0.009 1.001     2664     3284\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.308     0.003    0.302    0.314 1.000     3389     3284\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBefore we move forward with the next models, we’ll need to wrangle the data a bit. First, we rename hgc.9 to the more tidyverse-centric hgc_9. Then we compute uerate_7, which is uerate centered on 7.\n\nwages_pp &lt;- wages_pp %&gt;% \n  rename(hgc_9 = hgc.9) %&gt;% \n  mutate(uerate_7 = uerate - 7)\n\nOur model A (fit5.16) and the rest of the models B through J (fit6.1 through fit6.9) can all be thought of as variants of two parent models. The first parent model is model F (fit6.5), which follows the form\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_i - 9) \\\\\n                & \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\\n                & \\;\\;\\; + \\gamma_{20} (\\text{uerate}_{ij} - 7) \\\\\n                & \\;\\;\\; + \\gamma_{30} \\text{ged}_{ij} \\\\\n                & \\;\\;\\; + \\gamma_{40} \\text{postexp}_{ij}\\\\\n                & \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\zeta_{3i} \\text{ged}_{ij} + \\zeta_{4i} \\text{postexp}_{ij} + \\epsilon_{ij}, \\;\\;\\; \\text{where} \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{3i} \\\\ \\zeta_{4i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\mathbf D \\mathbf \\Omega \\mathbf D'\n\\end{pmatrix} \\\\\n\\mathbf D & = \\begin{bmatrix} \\sigma_0 & 0 & 0 & 0 \\\\ 0 & \\sigma_1 & 0 & 0 \\\\ 0 & 0 & \\sigma_3 & 0 \\\\ 0 & 0 & 0 & \\sigma_4 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} & \\rho_{03} & \\rho_{04} \\\\\n\\rho_{10} & 1 & \\rho_{13} & \\rho_{14} \\\\\n\\rho_{30} & \\rho_{31} & 1 & \\rho_{34} \\\\\n\\rho_{40} & \\rho_{41} & \\rho_{43} &1 \\end{bmatrix} \\\\\n\\gamma_{00} & \\sim \\operatorname{Normal}(1.335, 1) \\\\\n\\gamma_{01}, \\dots, \\gamma_{40} & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma_0, \\dots, \\sigma_4 & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\sigma_\\epsilon & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\mathbf\\Omega   & \\sim \\operatorname{LKJ}(4),\n\\end{align}\n\\]\nwhich uses the postexp-based approach for discontinuity in slopes. Notice how we’re using the same basic prior specification as with fit5.16. The second parent model is model I (fit6.8), which follows the form\n\\[\n\\begin{align}\n\\text{lnw}_{ij} & = \\gamma_{00} + \\gamma_{01} (\\text{hgc}_i - 9) \\\\\n                & \\;\\;\\; + \\gamma_{10} \\text{exper}_{ij} + \\gamma_{12} \\text{black}_i \\times \\text{exper}_{ij} \\\\\n                & \\;\\;\\; + \\gamma_{20} (\\text{uerate}_{ij} - 7) \\\\\n                & \\;\\;\\; + \\gamma_{30} \\text{ged}_{ij} \\\\\n                & \\;\\;\\; + \\gamma_{50} \\text{ged}_{ij} \\times \\text{exper}_{ij} \\\\\n                & \\;\\;\\; + \\zeta_{0i} + \\zeta_{1i} \\text{exper}_{ij} + \\zeta_{3i} \\text{ged}_{ij} + \\zeta_{5i} \\text{ged}_{ij} \\times \\text{exper}_{ij} + \\epsilon_{ij}, \\;\\;\\; \\text{where} \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{3i} \\\\ \\zeta_{5i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}\n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\mathbf D \\mathbf \\Omega \\mathbf D'\n\\end{pmatrix} \\\\\n\\mathbf D & = \\begin{bmatrix} \\sigma_0 & 0 & 0 & 0 \\\\ 0 & \\sigma_1 & 0 & 0 \\\\ 0 & 0 & \\sigma_3 & 0 \\\\ 0 & 0 & 0 & \\sigma_5 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} & \\rho_{03} & \\rho_{05} \\\\\n\\rho_{10} & 1 & \\rho_{13} & \\rho_{15} \\\\\n\\rho_{30} & \\rho_{31} & 1 & \\rho_{35} \\\\\n\\rho_{50} & \\rho_{51} & \\rho_{53} &1 \\end{bmatrix} \\\\\n\\gamma_{00} & \\sim \\operatorname{Normal}(1.335, 1) \\\\\n\\gamma_{01}, \\dots, \\gamma_{50} & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma_0, \\dots, \\sigma_5 & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\sigma_\\epsilon & \\sim \\operatorname{Student-t}(3, 0, 1) \\\\\n\\mathbf\\Omega   & \\sim \\operatorname{LKJ}(4).\n\\end{align}\n\\]\nwhich uses the ged:exper-based approach for discontinuity in slopes. Here we fit the models in bulk.\n\n# model b\nfit6.1 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + (1 + exper + ged | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.01\")\n\n# model c\nfit6.2 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.02\")\n\n# model d\nfit6.3 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + postexp + (1 + exper + postexp | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit06.03\")\n\n# model e\nfit6.4 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + postexp + (1 + exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.04\")\n\n# model f\nfit6.5 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + ged + postexp | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.05\")\n\n# model g\nfit6.6 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + ged | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.06\")\n\n# model h\nfit6.7 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + postexp | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit06.07\")\n\n# model i\nfit6.8 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + ged:exper + (1 + exper + ged + ged:exper | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit06.08\")\n\n# model j\nfit6.9 &lt;- brm(\n  data = wages_pp, \n  family = gaussian,\n  lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + ged:exper + (1 + exper + ged | id),\n  prior = c(prior(normal(1.335, 1), class = b, coef = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(student_t(3, 0, 1), class = sd),\n            prior(student_t(3, 0, 1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.09\")\n\nTo keep from cluttering up this ebook, I’m not going to show the summary print() output for fit6.1 through fit6.7. If you go through that output yourself, you’ll see that several of them had low effective sample size estimates for one or a few of the \\(\\sigma\\) parameters. For our pedagogical purposes, I’m okay with moving forward with these. But do note that when fitting a model for your scientific or other real-world projects, make sure you attend to your effective sample size issues (i.e., extract more posterior draws, as needed, by adjusting the warmup, iter, and chains arguments).\nThough we’re avoiding print() output, we might take a birds-eye perspective and summarize the parameters for our competing models with a faceted coefficient plot. First we extract and save the relevant information as an object, post, and then we wrangle and plot.\n\n# compute\npost &lt;- tibble(\n  model = letters[1:10],\n  fit   = c(\"fit5.16\", str_c(\"fit6.\", 1:9))) %&gt;% \n  mutate(p = map(fit, ~ get(.) %&gt;% \n                   posterior_summary() %&gt;% \n                   data.frame() %&gt;% \n                   rownames_to_column(\"parameter\") %&gt;% \n                   filter(!str_detect(parameter, \"r_id\\\\[\") & \n                            parameter != \"lp__\" &\n                            parameter != \"lprior\"))) %&gt;% \n  unnest(p)\n\n# wrangle\npost %&gt;% \n  mutate(greek = case_when(\n    parameter == \"b_Intercept\"   ~ \"gamma[0][0]\",\n    parameter == \"b_hgc_9\"       ~ \"gamma[0][1]\",\n    parameter == \"b_exper\"       ~ \"gamma[1][0]\",\n    parameter == \"b_exper:black\" ~ \"gamma[1][2]\",\n    parameter == \"b_uerate_7\"    ~ \"gamma[2][0]\",\n    parameter == \"b_ged\"         ~ \"gamma[3][0]\",\n    parameter == \"b_postexp\"     ~ \"gamma[4][0]\",\n    parameter == \"b_exper:ged\"   ~ \"gamma[5][0]\",\n    parameter == \"sd_id__Intercept\" ~ \"sigma[0]\",\n    parameter == \"sd_id__exper\"     ~ \"sigma[1]\",\n    parameter == \"sd_id__ged\"       ~ \"sigma[3]\",\n    parameter == \"sd_id__postexp\"   ~ \"sigma[4]\",\n    parameter == \"sd_id__exper:ged\" ~ \"sigma[5]\",\n    parameter == \"sigma\"            ~ \"sigma[epsilon]\",\n    parameter == \"cor_id__Intercept__exper\"     ~ \"rho[0][1]\",\n    parameter == \"cor_id__Intercept__ged\"       ~ \"rho[0][3]\",\n    parameter == \"cor_id__exper__ged\"           ~ \"rho[1][3]\",\n    parameter == \"cor_id__Intercept__postexp\"   ~ \"rho[0][4]\",\n    parameter == \"cor_id__Intercept__exper:ged\" ~ \"rho[0][5]\",\n    parameter == \"cor_id__exper__postexp\"       ~ \"rho[1][4]\",\n    parameter == \"cor_id__exper__exper:ged\"     ~ \"rho[1][5]\",\n    parameter == \"cor_id__ged__postexp\"         ~ \"rho[3][4]\",\n    parameter == \"cor_id__ged__exper:ged\"       ~ \"rho[3][5]\"\n  )) %&gt;% \n  mutate(model = fct_rev(model)) %&gt;% \n  \n  # plot\n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = model)) +\n  geom_pointrange(size = 1/6, linewidth = 1/2) +\n  labs(title = \"Marginal posteriors for models a thorugh j\",\n       x = NULL, y = NULL) +\n  theme(axis.text = element_text(size = 6),\n        axis.text.y = element_text(hjust = 0),\n        panel.grid = element_blank()) +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_x\")\n\n\n\n\n\n\n\n\nFor our model comparisons, let’s compute the WAIC estimates for each.\n\nfit5.16 &lt;- add_criterion(fit5.16, criterion = \"waic\")\nfit6.1 &lt;- add_criterion(fit6.1, criterion = \"waic\")\nfit6.2 &lt;- add_criterion(fit6.2, criterion = \"waic\")\nfit6.3 &lt;- add_criterion(fit6.3, criterion = \"waic\")\nfit6.4 &lt;- add_criterion(fit6.4, criterion = \"waic\")\nfit6.5 &lt;- add_criterion(fit6.5, criterion = \"waic\")\nfit6.6 &lt;- add_criterion(fit6.6, criterion = \"waic\")\nfit6.7 &lt;- add_criterion(fit6.7, criterion = \"waic\")\nfit6.8 &lt;- add_criterion(fit6.8, criterion = \"waic\")\nfit6.9 &lt;- add_criterion(fit6.9, criterion = \"waic\")\n\nOn pages 202 through 204, Singer and Willett performed a number of model comparisons with deviance tests and the frequentist AIC and BIC. Here we do the analogous comparisons with WAIC difference estimates.\n\n# a vs b\nloo_compare(fit5.16, fit6.1, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.1      0.0       0.0 -2029.1     103.8        878.4    27.5    4058.1   207.6\nfit5.16    -9.1       6.4 -2038.1     104.3        863.4    27.2    4076.3   208.7\n\n# b vs c\nloo_compare(fit6.1, fit6.2, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.1     0.0       0.0 -2029.1     103.8        878.4    27.5    4058.1   207.6\nfit6.2    -7.3       5.1 -2036.3     103.9        862.6    27.1    4072.7   207.8\n\n# a vs d\nloo_compare(fit5.16, fit6.3, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.3      0.0       0.0 -2037.4     104.4        867.6    27.4    4074.8   208.7\nfit5.16    -0.8       2.6 -2038.1     104.3        863.4    27.2    4076.3   208.7\n\n# d vs e\nloo_compare(fit6.3, fit6.4, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.4     0.0       0.0 -2036.6     104.1        861.3    27.1    4073.2   208.2\nfit6.3    -0.8       1.7 -2037.4     104.4        867.6    27.4    4074.8   208.7\n\n# f vs b\nloo_compare(fit6.5, fit6.1, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.5     0.0       0.0 -2022.4     104.0        886.8    27.9    4044.9   208.0\nfit6.1    -6.6       3.8 -2029.1     103.8        878.4    27.5    4058.1   207.6\n\n# f vs d\nloo_compare(fit6.5, fit6.3, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.5     0.0       0.0 -2022.4     104.0        886.8    27.9    4044.9   208.0\nfit6.3   -14.9       7.6 -2037.4     104.4        867.6    27.4    4074.8   208.7\n\n# f vs g\nloo_compare(fit6.5, fit6.6, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.5     0.0       0.0 -2022.4     104.0        886.8    27.9    4044.9   208.0\nfit6.6   -10.2       3.8 -2032.6     103.9        879.6    27.7    4065.3   207.7\n\n# f vs h\nloo_compare(fit6.5, fit6.7, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.5     0.0       0.0 -2022.4     104.0        886.8    27.9    4044.9   208.0\nfit6.7   -12.6       6.3 -2035.0     103.9        867.5    27.0    4070.1   207.7\n\n# i vs b\nloo_compare(fit6.8, fit6.1, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.8     0.0       0.0 -2027.2     103.8        887.9    27.8    4054.5   207.6\nfit6.1    -1.8       2.8 -2029.1     103.8        878.4    27.5    4058.1   207.6\n\n# j vs i\nloo_compare(fit6.9, fit6.8, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.8     0.0       0.0 -2027.2     103.8        887.9    27.8    4054.5   207.6\nfit6.9    -2.4       2.8 -2029.7     104.0        881.1    27.7    4059.3   208.0\n\n# i vs f\nloo_compare(fit6.8, fit6.5, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.5     0.0       0.0 -2022.4     104.0        886.8    27.9    4044.9   208.0\nfit6.8    -4.8       2.5 -2027.2     103.8        887.9    27.8    4054.5   207.6\n\n\nWe might also just look at all of the WAIC estimates, plus and minus their standard errors, in a coefficient plot.\n\nloo_compare(fit5.16, fit6.1, fit6.2, fit6.3, fit6.4, fit6.5, fit6.6, fit6.7, fit6.8, fit6.9, criterion = \"waic\") %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(\"fit\") %&gt;% \n  arrange(fit) %&gt;% \n  mutate(model = letters[1:n()]) %&gt;% \n  \n  ggplot(aes(x = waic, xmin = waic - se_waic, xmax = waic + se_waic, y = reorder(model, waic))) +\n  geom_pointrange(size = 1/4) +\n  labs(x = expression(WAIC%+-%s.e.),\n       y = \"model\") +\n  theme(axis.text.y = element_text(hjust = 0),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIn the text, model I had the lowest (best) deviance and information criteria values, with model F coming in a close second. Our WAIC results are the reverse. However, look at the widths of the standard error intervals, for each, relative to their point estimates. I wouldn’t get too upset about differences in our results versus those in the text. Our Bayesian models reveal there’s massive uncertainty in each estimate, an insight missing from the frequent analyses reported in the text.\nHere’s a focused look at the parameter summary for model F.\n\nprint(fit6.5, digits = 3)\n\nWarning: There were 26 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: lnw ~ 0 + Intercept + exper + hgc_9 + black:exper + uerate_7 + ged + postexp + (1 + exper + ged + postexp | id) \n   Data: wages_pp (Number of observations: 6402) \n  Draws: 3 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 4500\n\nMultilevel Hyperparameters:\n~id (Number of levels: 888) \n                       Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)             0.205     0.011    0.183    0.228 1.002     1763     3180\nsd(exper)                 0.037     0.003    0.032    0.043 1.002      714     1584\nsd(ged)                   0.159     0.039    0.087    0.238 1.003      639     1331\nsd(postexp)               0.040     0.013    0.014    0.066 1.003      285      583\ncor(Intercept,exper)     -0.222     0.089   -0.381   -0.030 1.003      717     1412\ncor(Intercept,ged)        0.203     0.207   -0.185    0.614 1.003      761     1416\ncor(exper,ged)           -0.036     0.260   -0.511    0.488 1.003      592     1280\ncor(Intercept,postexp)   -0.413     0.194   -0.749   -0.007 1.001     1009     1845\ncor(exper,postexp)       -0.096     0.244   -0.523    0.401 1.002      392      697\ncor(ged,postexp)         -0.316     0.230   -0.723    0.167 1.002      484     1059\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept      1.739     0.012    1.715    1.762 1.000     5559     3411\nexper          0.041     0.003    0.036    0.047 1.001     4631     3448\nhgc_9          0.039     0.006    0.026    0.051 1.001     4287     3329\nuerate_7      -0.012     0.002   -0.015   -0.008 1.000     6425     4196\nged            0.042     0.022   -0.002    0.085 1.000     6428     3366\npostexp        0.009     0.006   -0.002    0.020 1.001     7009     3710\nexper:black   -0.019     0.005   -0.028   -0.010 1.000     5805     3479\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.307     0.003    0.301    0.313 1.001     2519     1817\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBefore we dive into the \\(\\gamma\\)-based counterfactual trajectories of Figure 6.3, we might use a plot to get at sense of the \\(\\zeta\\)’s, as summarized by the \\(\\sigma\\) and \\(\\rho\\) parameters. Here we extract the \\(\\zeta\\)’s. Since there’s such a large number, we’ll focus on their posterior means.\n\nr &lt;- tibble(\n  `zeta[0]` = ranef(fit6.5)$id[, 1, \"Intercept\"],\n  `zeta[1]` = ranef(fit6.5)$id[, 1, \"exper\"],\n  `zeta[3]` = ranef(fit6.5)$id[, 1, \"ged\"],\n  `zeta[4]` = ranef(fit6.5)$id[, 1, \"postexp\"]) \n\nglimpse(r)\n\nRows: 888\nColumns: 4\n$ `zeta[0]` &lt;dbl&gt; -0.151918936, 0.105099372, 0.058937916, 0.035597105, 0.160955444, -0.123178825, 0.014464113, -0.012726016, 0.1…\n$ `zeta[1]` &lt;dbl&gt; 0.0071607571, 0.0093993198, -0.0049914713, -0.0021965079, 0.0284329574, -0.0083366421, 0.0274901934, 0.0184619…\n$ `zeta[3]` &lt;dbl&gt; -0.0999147923, 0.0842183444, 0.0826745272, 0.0043012382, 0.0375689027, -0.0741122120, 0.0043084981, -0.0036352…\n$ `zeta[4]` &lt;dbl&gt; 0.0178409615, 0.0010656807, -0.0125612620, -0.0020479638, -0.0049819868, 0.0012346864, -0.0095743647, -0.00387…\n\n\nWe’ll be plotting with help from the GGally package (Schloerke et al., 2021), which does a nice job displaying a grid of bivariate plots via the ggpairs(). We’re going to get fancy with our ggpairs() plot by using a handful of custom settings. Here we save them as two functions.\n\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(fill = \"black\", linewidth = 0) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nmy_upper &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_point(size = 1/10, alpha = 1/2) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nNow visualize the model F \\(\\zeta\\)’s with GGally::ggpairs().\n\nlibrary(GGally)\n\nr %&gt;% \n  ggpairs(upper = list(continuous = my_upper),\n          diag = list(continuous = my_diag),\n          lower = NULL,\n          labeller = label_parsed)\n\n\n\n\n\n\n\n\nNow we’re ready to make our version of Figure 6.3. Since we will be expressing the uncertainty of our counterfactual trajectories with 95% interval bands, we’ll be faceting the plot by the two levels of hgc_9. Otherwise, the overplotting would become too much.\n\n# define the new data\nnd &lt;- crossing(black = 0:1,\n               hgc_9 = c(0, 3)) %&gt;% \n  expand_grid(exper = seq(from = 0, to = 11, by = 0.02)) %&gt;% \n  mutate(ged      = ifelse(exper &lt; 3, 0, 1),\n         postexp  = ifelse(ged == 0, 0, exper - 3),\n         uerate_7 = 0)\n\n# compute the fitted draws\nfitted(fit6.5, \n       re_formula = NA,\n       newdata = nd) %&gt;% \n  # wrangle\n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(race  = ifelse(black == 0, \"White/Latino\", \"Black\"),\n         hgc_9 = ifelse(hgc_9 == 0, \"9th grade dropouts\", \"12th grade dropouts\")) %&gt;% \n  mutate(race  = fct_rev(race),\n         hgc_9 = fct_rev(hgc_9)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = exper, y = Estimate, ymin = Q2.5, ymax = Q97.5,\n             fill = race, color = race)) +\n  geom_ribbon(linewidth = 0, alpha = 1/4) +\n  geom_line() +\n  scale_fill_viridis_d(NULL, option = \"C\", begin = 0.25, end = 0.75) +\n  scale_color_viridis_d(NULL, option = \"C\", begin = 0.25, end = 0.75) +\n  scale_x_continuous(breaks = 0:5 * 2, expand = c(0, 0)) +\n  scale_y_continuous(\"lnw\", breaks = 1.6 + 0:4 * 0.2, \n                     expand = expansion(mult = c(0, 0.05))) +\n  coord_cartesian(ylim = c(1.6, 2.4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ hgc_9)\n\n\n\n\n\n\n\n\n\n\n6.1.3 Further extensions of the discontinuous growth model\n“It is easy to generalize these strategies to models with other discontinuities” (p. 206).\n\n6.1.3.1 Dividing TIME into multiple phases.\n“You can divide TIME into multiple epochs, allowing the trajectories to differ in elevation (and perhaps slope) during each” (p. 206, emphasis in the original). With our examples, above, we divided time into two epochs: before and (possibly) after receipt of one’s GED. More possible epochs might after completing college or graduate school. All such epochs might influence intercepts and/or slopes (by either of the slope methods, above).\n\n\n6.1.3.2 Discontinuities at common points in time\n\nIn some data sets, the timing of the discontinuity will not be person-specific; instead, everyone will experience the hypothesized transition at a common point in time. You can hypothesize a similar discontinuous change trajectory for such data sets by applying the strategies outlined above. (p. 207)\n\nExamples might include months, seasons, and academic quarters or semesters.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#using-transformations-to-model-nonlinear-individual-change",
    "href": "06.html#using-transformations-to-model-nonlinear-individual-change",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "6.2 Using transformations to model nonlinear individual change",
    "text": "6.2 Using transformations to model nonlinear individual change\n\nWhen confronted by obviously nonlinear trajectories, we usually begin with the transformation approach for two reasons. First, a straight line–even on a transformed scale–is a simple mathematical form whose two parameters have clear interpretations. Second, because the metrics of many variables are ad hoc to begin with, transformation to another ad hoc scale may sacrifice little. (p. 208)\n\nAs an example, consider fit4.6 from back in Chapter 4.\n\nfit4.6 &lt;- brm(\n  data = alcohol1_pp, \n  family = gaussian,\n  alcuse ~ 0 + Intercept + age_14 + coa + peer + age_14:peer + (1 + age_14 | id),\n  prior = c(prior(student_t(3, 0, 2.5), class = sd),\n            prior(student_t(3, 0, 2.5), class = sigma),\n            prior(lkj(1), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/fit04.06\")\n\nIn the alcohol1_pp data, the criterion alcuse was transformed by taking its square root. Since we fit a linear model on that transformed variable, our model is actually non-linear on the original metric of alcuse. To see, we’ll square the fitted()-based counterfactual trajectories and their intervals to make our version of Figure 6.4.\n\n# define the new data\nnd &lt;- crossing(coa  = 0:1,\n               peer = c(0.655, 1.381)) %&gt;% \n  expand_grid(age_14 = seq(from = 0, to = 2, length.out = 30))\n\n# compute the counterfactual trajectories\nfitted(fit4.6, \n       newdata = nd,\n       re_formula = NA) %&gt;%\n  data.frame() %&gt;%\n  bind_cols(nd) %&gt;%\n  # transform the predictions by squaring them\n  mutate(Estimate = Estimate^2,\n         Q2.5     = Q2.5^2,\n         Q97.5    = Q97.5^2) %&gt;% \n  # a little wrangling will make plotting much easier\n  mutate(age  = age_14 + 14,\n         coa  = ifelse(coa == 0, \"coa = 0\", \"coa = 1\"),\n         peer = factor(peer)) %&gt;%\n  \n  # plot!\n  ggplot(aes(x = age, color = peer, fill = peer)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/4) +\n  geom_line(aes(y = Estimate, linewidth = peer)) +\n  scale_linewidth_manual(values = c(1/2, 1)) +\n  scale_fill_manual(values = c(\"blue3\", \"red3\")) +\n  scale_color_manual(values = c(\"blue3\", \"red3\")) +\n  scale_y_continuous(\"alcuse\", breaks = 0:3, expand = c(0, 0)) +\n  labs(subtitle = \"High peer values are in red; low ones are in blue.\") +\n  coord_cartesian(xlim = c(13, 17),\n                  ylim = c(0, 3)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank()) +\n  facet_wrap(~coa)\n\n\n\n\n\n\n\n\nCompare these to the linear trajectories depicted back in Figure 4.3c.\n\n6.2.1 The ladder of transformations and the rule of the bulge\nI just not a fan of the “ladder of powers” idea and I’m not interested in reproducing Figure 6.5. However, we will make the next one, real quick. Load the berkeley_pp data.\n\nberkeley_pp &lt;- read_csv(\"data/berkeley_pp.csv\") %&gt;% \n  mutate(time = age)\n\nglimpse(berkeley_pp)\n\nRows: 18\nColumns: 3\n$ age  &lt;dbl&gt; 5, 7, 9, 10, 11, 12, 13, 14, 15, 18, 21, 24, 27, 36, 42, 48, 54, 60\n$ iq   &lt;dbl&gt; 37, 65, 85, 88, 95, 101, 103, 107, 113, 121, 148, 161, 165, 187, 205, 218, 218, 228\n$ time &lt;dbl&gt; 5, 7, 9, 10, 11, 12, 13, 14, 15, 18, 21, 24, 27, 36, 42, 48, 54, 60\n\n\nHere’s how we might make Figure 6.6.\n\n# left\np1 &lt;- berkeley_pp %&gt;% \n  ggplot(aes(x = time, y = iq)) +\n  geom_point() +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 250))\n\n# middle\np2 &lt;- berkeley_pp %&gt;% \n  ggplot(aes(x = time, y = iq^2.3)) +\n  geom_point() +\n  scale_y_continuous(expression(iq^(2.3)), breaks = 0:6 * 5e4, \n                     limits = c(0, 3e5), expand = c(0, 0))\n\n# right\np3 &lt;- berkeley_pp %&gt;% \n  ggplot(aes(x = time^(1/2.3), y = iq)) +\n  geom_point() +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 250)) +\n  xlab(expression(time^(1/2.3)))\n\n# combine\n(p1 + p2 + p3) &\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) &\n  theme(panel.grid = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#representing-individual-change-using-a-polynomial-function-of-time",
    "href": "06.html#representing-individual-change-using-a-polynomial-function-of-time",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "6.3 Representing individual change using a polynomial function of TIME",
    "text": "6.3 Representing individual change using a polynomial function of TIME\n\nWe can also model curvilinear change by including several level-1 predictors that collectively represent a polynomial function of time. Although the resulting polynomial growth model can be cumbersome, it can capture an even wider array of complex patterns of change over time. (p. 213, emphasis in the original)\n\nTo my eye, it will be easiest to make Table by dividing it up into columns, making each column individually, and then combining them on the back end. For our first step, here’s our first column, the “Shape” column.\n\np1 &lt;- tibble(\n  x     = 1,\n  y     = 9.5,\n  label = c(\"No\\nchange\", \"Linear\\nchange\", \"Quadratic\\nchange\", \"Cubic\\nchange\"),\n  row   = 1:4,\n  col   = \"Shape\") %&gt;% \n  \n  ggplot(aes(x = x, y = y, label = label)) +\n  geom_text(hjust = 0, vjust = 1, size = 3.25) +\n  scale_x_continuous(expand = c(0, 0), limits = 1:2) +\n  scale_y_continuous(expand = c(0, 0), limits = c(1, 10)) +\n  theme_void() +\n  theme(strip.background.y = element_blank(),\n        strip.text.x = element_text(hjust = 0, size = 12),\n        strip.text.y = element_blank()) +\n  facet_grid(row ~ col)\n\nFor our second step, here’s the “Level-1 model” column.\n\np2 &lt;- tibble(\n  x     = c(1, 1, 1, 2, 1, 2, 2),\n  y     = c(9.5, 9.5, 9.5, 8.5, 9.5, 8.5, 7.25),\n  label = c(\"italic(Y[i][j])==pi[0][italic(i)]+epsilon[italic(ij)]\",\n            \"italic(Y[i][j])==pi[0][italic(i)]+pi[1][italic(i)]*italic(TIME[ij])+epsilon[italic(ij)]\",\n            \"italic(Y[i][j])==pi[0][italic(i)]+pi[1][italic(i)]*italic(TIME[ij])\",\n            \"+pi[2][italic(i)]*italic(TIME[ij])^2+epsilon[italic(ij)]\",\n            \"italic(Y[i][j])==pi[0][italic(i)]+pi[1][italic(i)]*italic(TIME[ij])\",\n            \"+pi[2][italic(i)]*italic(TIME[ij])^2+pi[3][italic(i)]*italic(TIME[ij])^3\",\n            \"+epsilon[italic(ij)]\"),\n  row   = c(1:3, 3:4, 4, 4),\n  col   = \"Level-1 model\") %&gt;% \n  \n  ggplot(aes(x = x, y = y, label = label)) +\n  geom_text(hjust = 0, vjust = 1, size = 3.25, parse = T) +\n  scale_x_continuous(expand = c(0, 0), limits = c(1, 10)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(1, 10)) +\n  theme_void() +\n  theme(strip.background.y = element_blank(),\n        strip.text.x = element_text(hjust = 0, size = 12),\n        strip.text.y = element_blank()) +\n  facet_grid(row ~ col)\n\nFor our third step, here’s the “Parameter values” column.\n\np3 &lt;- tibble(\n  x     = 1,\n  y     = c(9.5, 9.5, 8.5, 9.5, 8.5, 7.5, 9.5, 8.5, 7.5, 6.5),\n  label = c(\"pi[0][italic(i)]==71\",\n            \"pi[0][italic(i)]==71\",\n            \"pi[1][italic(i)]==1.2\",\n            \"pi[0][italic(i)]==50\",\n            \"pi[1][italic(i)]==3.8\",\n            \"pi[2][italic(i)]==-0.03\",\n            \"pi[0][italic(i)]==30\",\n            \"pi[1][italic(i)]==10\",\n            \"pi[2][italic(i)]==-0.2\",\n            \"pi[3][italic(i)]==0.0012\"),\n  row   = rep(1:4, times = 1:4),\n  col   = \"Parameter\\nvalues\") %&gt;% \n  \n  ggplot(aes(x = x, y = y, label = label)) +\n  geom_text(hjust = 0, vjust = 1, size = 3.25, parse = T) +\n  scale_x_continuous(expand = c(0, 0), limits = c(1, 10)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(1, 10)) +\n  theme_void() +\n  theme(strip.background.y = element_blank(),\n        strip.text.x = element_text(hjust = 0, size = 12),\n        strip.text.y = element_blank()) +\n  facet_grid(row ~ col)\n\nWe’ll do our fourth step in three stages. First, we’ll make and save four data sets, one for each of the plot panels. Second, we’ll combine those into a single data set, which we’ll wrangle a bit. Third, we’ll make the plots in the final column.\n\n# make the four small data sets\npi0 &lt;- 71\n\nd1 &lt;- tibble(time = 0:100) %&gt;% \n  mutate(y = pi0)\n\npi0 &lt;- 71\npi1 &lt;- 1.2\n\nd2 &lt;- tibble(time = 0:100) %&gt;% \n  mutate(y = pi0 + pi1 * time) \n\npi0 &lt;- 50\npi1 &lt;- 3.8\npi2 &lt;- -0.03\n\nd3 &lt;- tibble(time = 0:100) %&gt;% \n  mutate(y = pi0 + pi1 * time + pi2 * time^2)\n\npi0 &lt;- 30\npi1 &lt;- 10\npi2 &lt;- -0.2\npi3 &lt;- 0.0012\n\nd4 &lt;- tibble(time = 0:100) %&gt;% \n  mutate(y = pi0 + pi1 * time + pi2 * time^2 + pi3 * time^3)\n\n# combine the data sets\np4 &lt;- bind_rows(d1, d2, d3, d4) %&gt;% \n  # wrangle\n  mutate(row = rep(1:4, each = n() / 4),\n         col = \"Plot of the true change trajectory\") %&gt;% \n  \n  # plot!\n  ggplot(aes(x = time, y = y)) + \n  geom_line() +\n  scale_x_continuous(expression(italic(TIME)), expand = c(0, 0),\n                     breaks = 0:2 * 50, limits = c(0, 100)) +\n  scale_y_continuous(expression(italic(Y)), expand = c(0, 0), \n                     breaks = 0:2 * 100, limits = c(0, 205)) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text.x = element_text(hjust = 0, size = 12),\n        strip.text.y = element_blank()) +\n  facet_grid(row ~ col)\n\nNow we’re finally ready to combine all the elements to make Table 6.4.\n\n(p1 | p2 | p3 | p4) +\n  plot_annotation(title = \"A taxonomy of polynomial individual change trajectories\") +\n  plot_layout(widths = c(1, 3, 2, 4))\n\n\n\n\n\n\n\n\n\n6.3.1 The shapes of polynomial individual change trajectories\n“The ‘no change’ and ‘linear change’ models are familiar; the remaining models, which contain quadratic and cubic functions of TIME, are new” (p. 213, emphasis in the original).\n\n6.3.1.1 “No change” trajectory\n\nThe “no change” trajectory is known as a polynomial function of “zero order” because TIME raised to the 0th power is 1 (i.e., \\(TIME^0 = 1\\)). This model is tantamount to including a constant predictor, 1, in the level-1 model, as a multiplier of the sole individual growth parameter, the intercept, \\(\\pi_{0i}\\)… Even though each trajectory is flat, different individuals can have different intercepts and so a collection of true “no change” trajectories is a set of vertically scattered horizontal lines. (p. 215, emphasis in the original)\n\n\n\n6.3.1.2 “Linear change” trajectory\n\nThe “linear change” trajectory is known as a “first order” polynomial in time because TIME raised to the 1st power equals TIME itself (i.e., \\(TIME^1 = TIME\\)). Linear TIME is the sole predictor and the two individual growth parameters have the usual interpretations. (p. 215, emphasis in the original)\n\n\n\n6.3.1.3 “Quadratic change” trajectory\n\nAdding \\(TIME^2\\) to a level-1 individual growth model that already includes linear TIME yields a second order polynomial for quadratic change. Unlike a level-1 model that includes only \\(TIME^2\\), a second order polynomial change trajectory includes two TIME predictors and three growth parameters (\\(\\pi_{0i}\\), \\(\\pi_{1i}\\) and \\(\\pi_{2i}\\)). The first two parameters have interpretations that are similar, but not identical, to those in the linear change trajectory; the third is new. (p. 215, emphasis in the original)\n\nIn this model, \\(\\pi_{0i}\\) is still the intercept. The \\(\\pi_{1i}\\) parameter is now the instantaneous rate of change when \\(TIME = 0\\). The new \\(\\pi_{2i}\\) parameter, sometimes called the curvature parameter, describes the change in the rate of change.\n\n\n6.3.1.4 Higher order change trajectories\n“Adding higher powers of TIME increases the complexity of the polynomial trajectory” (p. 216).\n\n\n\n6.3.2 Selecting a suitable level-1 polynomial trajectory for change\nIt appears that both the external_pp.csv and external_pp.txt files within the data folder (here) are missing a few occasions. Happily, you can download a more complete version of the data from the good people at stats.idre.ucla.edu.\n\nexternal_pp &lt;- read.table(\n  \"https://stats.idre.ucla.edu/wp-content/uploads/2020/01/external_pp.txt\", \n  header = T, sep = \",\")\n\nglimpse(external_pp)\n\nRows: 270\nColumns: 5\n$ id       &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, …\n$ external &lt;int&gt; 50, 57, 51, 48, 43, 19, 4, 6, 3, 3, 5, 12, 0, 1, 9, 26, 10, 24, 14, 5, 18, 31, 31, 23, 26, 25, 34, 10, 26, 32, …\n$ female   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ time     &lt;int&gt; 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, …\n$ grade    &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, …\n\n\nThere are data from 45 children.\n\nexternal_pp %&gt;% \n  distinct(id) %&gt;% \n  nrow()\n\n[1] 45\n\n\nThe 45 kids were composed of 28 boys and 17 girls.\n\nexternal_pp %&gt;% \n  distinct(id, female) %&gt;% \n  count(female) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n  female  n  percent\n1      0 28 62.22222\n2      1 17 37.77778\n\n\nThe data were collected over the children’s first through sixth grades.\n\nexternal_pp %&gt;% \n  distinct(grade)\n\n  grade\n1     1\n2     2\n3     3\n4     4\n5     5\n6     6\n\n\nOur criterion, external, is the sum of the 34 items in the Externalizing subscale of the Child Behavior Checklist. Each item is rated on a 3-point Likert-type scale, ranging from 0 (rarely/never) to 2 (often). The possible range for the sum score of the Externalizing subscale is 0 to 68. Here’s the overall distribution.\n\nexternal_pp %&gt;% \n  ggplot(aes(x = external)) +\n  geom_histogram(binwidth = 1, boundary = 0) +\n  xlim(0, 68) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe data are strongly bound to the left, which is a good thing in this case. We generally like it when our children exhibit fewer externalizing behaviors.\nFigure 6.7 is based on a subset of the cases in the data. It might make our job easier if we just make a subset of the data, called external_pp_subset.\n\nsubset &lt;- c(1, 6, 11, 25, 34, 36, 40, 26)\n\nexternal_pp_subset &lt;- external_pp %&gt;% \n  filter(id %in% subset) %&gt;% \n  # this is for the facets in the plot\n  mutate(case = factor(id,\n                       levels = subset,\n                       labels = LETTERS[1:8]))\n\nglimpse(external_pp_subset)\n\nRows: 48\nColumns: 6\n$ id       &lt;int&gt; 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 11, 11, 11, 11, 11, 11, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 34,…\n$ external &lt;int&gt; 50, 57, 51, 48, 43, 19, 9, 10, 9, 29, 21, 50, 11, 8, 4, 1, 0, 0, 11, 9, 9, 13, 10, 10, 19, 32, 25, 40, 20, 23, …\n$ female   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ time     &lt;int&gt; 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, …\n$ grade    &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, …\n$ case     &lt;fct&gt; A, A, A, A, A, A, B, B, B, B, B, B, C, C, C, C, C, C, D, D, D, D, D, D, H, H, H, H, H, H, E, E, E, E, E, E, F, …\n\n\nSince the order of the polynomials in Figure 6.7 is tailored to each case, we’ll have to first build the subplots in pieces, and then combine then at the end. Make the pieces.\n\n# a and e\np1 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"A\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/4, linetype = 2) +  # quartic\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2),  # quadratic\n              se = F, linewidth = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL, \n                     limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# e\np2 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"E\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/4, linetype = 2) +           # quartic\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3),  # cubic\n              se = F, linewidth = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL, \n                     limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# b\np3 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"B\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/4, linetype = 2) +  # quartic\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2),  # quadratic\n              se = F, linewidth = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL, \n                     limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# f\np4 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"F\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/2) +  # quartic\n  scale_x_continuous(breaks = 0:7, limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# c\np5 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"C\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/4, linetype = 2) +  # quartic\n  stat_smooth(method = \"lm\", formula = y ~ x,           # linear\n              se = F, linewidth = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL, \n                     limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# g\np6 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"G\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/4, linetype = 2) +  # quartic\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2),  # quadratic\n              se = F, linewidth = 1/2) +\n  scale_x_continuous(breaks = 0:7, limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# d\np7 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"D\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/4, linetype = 2) +  # quartic\n  stat_smooth(method = \"lm\", formula = y ~ x,           # linear\n              se = F, linewidth = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL, \n                     limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\n# h\np8 &lt;- external_pp_subset %&gt;% \n  filter(case %in% c(\"H\")) %&gt;% \n  \n  ggplot(aes(x = grade, y = external)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4), \n              se = F, linewidth = 1/2) +  # quartic\n  scale_x_continuous(breaks = 0:7, limits = c(0, 7), expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ case)\n\nNow combine the subplots to make the full Figure 6.7.\n\n((p1 / p2) | (p3 / p4) | (p5 / p6) | (p7 / p8)) &\n  coord_cartesian(ylim = c(0, 60)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n6.3.3 Testing higher order terms in a polynomial level-1 model\nLet’s talk about priors, first focusing on the overall intercept \\(\\pi_{01}\\). At the time of the original article by Keiley et al. (2000), it was known that boys tended to show more externalizing behaviors than girls, and that boys tend to either increase or remain fairly stable during primary school. Less was known about typical trajectories for girls. However, Sandberg et al. (1991) can give us a sense of what values are reasonable to center on. I their paper, they compared externalizing in two groups if children, broken down between boys and girls. From their second and third tables, we get the following descriptive statistics:\n\nsample_statistics &lt;- crossing(\n  gender = c(\"boys\", \"girls\"),\n  sample = c(\"School\", \"CBCL Nonclinical\")) %&gt;% \n  mutate(n    = c(300, 261, 300, 267),\n         mean = c(10.8, 14.5, 10.7, 16.6),\n         sd   = c(8.4, 10.4, 8.6, 12.1))\n\nsample_statistics %&gt;% \n  flextable::flextable()\n\ngendersamplenmeansdboysCBCL Nonclinical30010.88.4boysSchool26114.510.4girlsCBCL Nonclinical30010.78.6girlsSchool26716.612.1\n\n\nHere’s the weighted mean of the externalizing scores.\n\nsample_statistics %&gt;% \n  summarise(weighted_average = sum(n * mean) / sum(n))\n\n# A tibble: 1 × 1\n  weighted_average\n             &lt;dbl&gt;\n1             13.0\n\n\nHere’s the pooled standard deviation.\n\nsample_statistics %&gt;% \n  summarise(pooled_sd = sqrt(sum((n - 1) * sd^2) / (sum(n) - 4)))\n\n# A tibble: 1 × 1\n  pooled_sd\n      &lt;dbl&gt;\n1      9.91\n\n\nThus, I propose a good place to start with is with a \\(\\operatorname{Normal}(13, 9.9)\\) prior on the overall intercept, \\(\\gamma_{00}\\). So far, we’ve been using the Student-\\(t\\) distribution for the priors on our variance parameters. Another option favored by McElreath in the second edition of his text is the exponential distribution. The exponential distribution has a single parameter, \\(\\lambda\\), which is often called the rate. The mean of the exponential distribution is the inverse of the rate, \\(1 / \\lambda\\). When you’re working with standardized data, a nice weakly-regularizing priors on the variance parameters is \\(\\operatorname{Exponential}(1)\\), which has a mean of 1. Since our data are not standardized, we might use the prior \\(\\operatorname{Exponential}(1 / s_p)\\), where \\(s_p\\) is the pooled standard deviation. In our case, that would be \\(\\operatorname{Exponential}(1 / 9.9)\\). Here’s what those priors would look like.\n\n# left\np1 &lt;- tibble(x = seq(from = -30, to = 50, length.out = 200)) %&gt;% \n  mutate(d = dnorm(x, mean = 13, sd = 9.9)) %&gt;% \n  ggplot(aes(x = x, y = d)) +\n  geom_area(fill = \"black\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(\"prior for \"*gamma[0][0]),\n       x = (expression(Normal(13*', '*9.9)))) +\n  theme(panel.grid = element_blank())\n\n# right\np2 &lt;- tibble(x = seq(from = 0, to = 60, length.out = 200)) %&gt;% \n  mutate(d = dexp(x, rate = 1.0 / 9.9)) %&gt;% \n  ggplot(aes(x = x, y = d)) +\n  geom_area(fill = \"black\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(\"priors for \"*sigma[0]~and~sigma[epsilon]),\n       x = (expression(Exponential(1/9.9)))) +\n  theme(panel.grid = element_blank())\n\n# combine\np1 | p2\n\n\n\n\n\n\n\n\nNow using those priors, here’s how to fit the model.\n\nfit6.10 &lt;- brm(\n  data = external_pp, \n  family = gaussian,\n  external ~ 1 + (1 | id),\n  prior = c(prior(normal(13, 9.9), class = Intercept),\n            prior(exponential(1.0 / 9.9), class = sd),\n            prior(exponential(1.0 / 9.9), class = sigma)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.10\")\n\nFor the next three models, we’ll keep those priors for \\(\\gamma_{00}\\) and the \\(\\sigma\\) parameters. Yet now we have to consider the new \\(\\gamma_{00}\\) parameters which will account for the population-level effects if time, from a linear, quadratic, and cubic perspective. Given that time takes on integer values 0 through 5, the simple linear slope \\(\\gamma_{10}\\) is the expected change from one grade to the next. Since we know from the previous literature that boys tend to have either stable or slightly-increasing trajectories for their externalizing behaviors (recall we’re uncertain about girls), a mildly conservative prior might be centered on zero with, say, half of the pooled standard deviation on the scale, \\(\\operatorname{Normal}(0, 4.95)\\). This is the rough analogue to a \\(\\operatorname{Normal}(0, 0.5)\\) prior on standardized data. Without better information, we’ll extend that same \\(\\operatorname{Normal}(0, 4.95)\\) prior on \\(\\gamma_{20}\\) and \\(\\gamma_{30}\\). As to the new \\(\\rho\\) parameters, we’ll use our typical weakly-regularizing \\(\\operatorname{LKJ}(4)\\) prior on the level-2 correlation matrices.\n\n# linear\nfit6.11 &lt;- brm(\n  data = external_pp, \n  family = gaussian,\n  external ~ 0 + Intercept + time + (1 + time | id),\n  prior = c(prior(normal(0, 4.95), class = b),\n            prior(normal(13, 9.9), class = b, coef = Intercept),\n            prior(exponential(1.0 / 9.9), class = sd),\n            prior(exponential(1.0 / 9.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  file = \"fits/fit06.11\")\n\n# quadratic\nfit6.12 &lt;- brm(\n  data = external_pp, \n  family = gaussian,\n  external ~ 0 + Intercept + time + I(time^2) + (1 + time + I(time^2) | id),\n  prior = c(prior(normal(0, 4.95), class = b),\n            prior(normal(13, 9.9), class = b, coef = Intercept),\n            prior(exponential(1.0 / 9.9), class = sd),\n            prior(exponential(1.0 / 9.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  control = list(adapt_delta = 0.995),\n  file = \"fits/fit06.12\")\n\n# cubic\nfit6.13 &lt;- brm(\n  data = external_pp, \n  family = gaussian,\n  external ~ 0 + Intercept + time + I(time^2) + I(time^3) + (1 + time + I(time^2) + I(time^3) | id),\n  prior = c(prior(normal(0, 4.95), class = b),\n            prior(normal(13, 9.9), class = b, coef = Intercept),\n            prior(exponential(1.0 / 9.9), class = sd),\n            prior(exponential(1.0 / 9.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  control = list(adapt_delta = 0.85),\n  file = \"fits/fit06.13\")\n\nCompute and save the WAIC estimates.\n\nfit6.10 &lt;- add_criterion(fit6.10, criterion = \"waic\")\nfit6.11 &lt;- add_criterion(fit6.11, criterion = \"waic\")\nfit6.12 &lt;- add_criterion(fit6.12, criterion = \"waic\")\nfit6.13 &lt;- add_criterion(fit6.13, criterion = \"waic\")\n\nHere we’ll make a simplified version of the WAIC output, saving the results as waic_summary.\n\n# order the parameters, which will come in handy in the next block\norder &lt;- c(\n  \"gamma[0][0]\", \"gamma[1][0]\", \"gamma[2][0]\", \"gamma[3][0]\", \n  \"sigma[epsilon]\", \"sigma[0]\", \"sigma[1]\", \"sigma[2]\", \"sigma[3]\", \n  \"rho[0][1]\", \"rho[0][2]\", \"rho[0][3]\", \"rho[1][2]\", \"rho[1][3]\", \n  \"rho[2][3]\", \"WAIC\")\n\nwaic_summary &lt;- loo_compare(fit6.10, fit6.11, fit6.12, fit6.13, criterion = \"waic\") %&gt;%\n  data.frame() %&gt;% \n  rownames_to_column(\"fit\") %&gt;% \n  arrange(fit) %&gt;% \n  transmute(model   = str_c(\"model \", letters[1:n()]),\n            summary = str_c(formatC(waic, digits = 2, format = \"f\"), \" (\", \n                            formatC(se_waic, digits = 2, format = \"f\"), \")\"),\n            parameter = factor(\"WAIC\", levels = order)) \n\nwaic_summary\n\n    model         summary parameter\n1 model a 1960.08 (29.72)      WAIC\n2 model b 1923.95 (28.40)      WAIC\n3 model c 1904.54 (26.38)      WAIC\n4 model d 1901.61 (26.07)      WAIC\n\n\nHere we’ll use a little summarizing, wrangling, and creative plotting, we’ll make a streamlined version of Table 6.5 with ggplot2.\n\n# extract and wrangle the posterior summaries\ntibble(model = str_c(\"model \", letters[1:4]),\n         fit   = str_c(\"fit6.1\", 0:3)) %&gt;% \n  mutate(p = map(fit, ~ get(.) %&gt;% \n                   posterior_summary() %&gt;% \n                   data.frame() %&gt;% \n                   rownames_to_column(\"parameter\") %&gt;% \n                   filter(!str_detect(parameter, \"r_id\\\\[\") & \n                            parameter != \"lp__\" & \n                            parameter != \"lprior\"))) %&gt;% \n  unnest(p) %&gt;% \n  mutate(greek = case_when(\n    parameter == \"b_Intercept\" ~ \"gamma[0][0]\",\n    parameter == \"b_time\"      ~ \"gamma[1][0]\",\n    parameter == \"b_ItimeE2\"   ~ \"gamma[2][0]\",\n    parameter == \"b_ItimeE3\"   ~ \"gamma[3][0]\",\n    parameter == \"sd_id__Intercept\" ~ \"sigma[0]\",\n    parameter == \"sd_id__time\"      ~ \"sigma[1]\",\n    parameter == \"sd_id__ItimeE2\"   ~ \"sigma[2]\",\n    parameter == \"sd_id__ItimeE3\"   ~ \"sigma[3]\",\n    parameter == \"sigma\"            ~ \"sigma[epsilon]\",\n    parameter == \"cor_id__Intercept__time\"    ~ \"rho[0][1]\",\n    parameter == \"cor_id__Intercept__ItimeE2\" ~ \"rho[0][2]\",\n    parameter == \"cor_id__Intercept__ItimeE3\" ~ \"rho[0][3]\",\n    parameter == \"cor_id__time__ItimeE2\"      ~ \"rho[1][2]\",\n    parameter == \"cor_id__time__ItimeE3\"      ~ \"rho[1][3]\",\n    parameter == \"cor_id__ItimeE2__ItimeE3\"   ~ \"rho[2][3]\"\n  )) %&gt;% \n  mutate(summary = str_c(formatC(Estimate, digits = 2, format = \"f\"), \" (\", formatC(Est.Error, digits = 2, format = \"f\"), \")\"),\n         parameter = factor(greek, levels = order)) %&gt;% \n  select(model, summary, parameter) %&gt;% \n  # add in the WAIC summary information\n  bind_rows(waic_summary) %&gt;% \n  mutate(parameter = fct_rev(parameter)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = model, y = parameter, label = summary)) +\n  geom_text(hjust = 1, size = 3) +\n  scale_x_discrete(NULL, position = \"top\") +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  coord_cartesian(xlim = c(NA, 3.5)) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(hjust = 1, color = \"black\"),\n        axis.text.y = element_text(hjust = 0, color = \"black\"),\n        axis.line.x = element_line(linewidth = 1/4),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere are the formal WAIC difference estimates.\n\nloo_compare(fit6.10, fit6.11, fit6.12, fit6.13, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit6.13    0.0       0.0  -950.8      13.0         67.5    5.7    1901.6   26.1 \nfit6.12   -1.5       1.4  -952.3      13.2         66.0    5.8    1904.5   26.4 \nfit6.11  -11.2       5.2  -962.0      14.2         62.4    6.3    1924.0   28.4 \nfit6.10  -29.2       9.2  -980.0      14.9         38.9    4.1    1960.1   29.7 \n\n\nThe WAIC difference estimates suggest the cubic and quadratic models are about the same, but both are notably better than the intercepts-only and linear models.\nIn the text, Singer and Willett preferred the quadratic model (fit6.12). Now we’ve fit our Bayesian version of the model, why not use the posterior distribution to make a high-quality model-based version of Figure 6.7?\n\n# define the new data\nnd &lt;- external_pp_subset %&gt;% \n  distinct(id, case) %&gt;% \n  expand_grid(time = seq(from = 0, to = 5, length.out = 50)) %&gt;% \n  mutate(grade = time + 1)\n\n# extract the fitted trajectories and wrangle\nfitted(fit6.12, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = grade, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = external_pp_subset,\n             aes(y = external)) +\n  scale_x_continuous(breaks = 0:7, labels = 0:7,\n                     expand = c(0, 0), limits = c(0, 7)) +\n  ylab(\"external\") +\n  coord_cartesian(ylim = c(0, 60)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ case, ncol = 4)\n\n\n\n\n\n\n\n\nAt the end of this section in the text, Singer and Willett briefly discussed fitting an expansion of the quadratic model which included the variable female as time-invariant predictor for the individual differences in initial status (\\(\\pi_{0i}\\)), the instantaneous rate of change (\\(\\pi_{1i}\\)), and curvature (\\(\\pi_{2i}\\)). We might express the model in formal notation as\n\\[\n\\begin{align}\n\\text{external}_{ij} & = \\gamma_{00} + \\gamma_{01} \\text{female}_i \\\\\n& \\;\\;\\; + \\gamma_{10} \\text{time}_{ij} + \\gamma_{11} ( \\text{female}_i \\times \\text{time}_{ij} ) \\\\\n& \\;\\;\\; + \\gamma_{20} \\text{time}_{ij}^2 + \\gamma_{21} ( \\text{female}_i \\times \\text{time}_{ij}^2 ) \\\\\n& \\;\\;\\; + [\\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij} + \\zeta_{2i} \\text{time}_{ij}^2 + \\epsilon_{ij}] \\\\\n\\epsilon_{ij} & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix}\n\\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\zeta_{2i}\n\\end{bmatrix} & \\sim \\operatorname{Normal}(\\mathbf 0, \\mathbf D \\mathbf \\Omega \\mathbf D') \\\\\n\\mathbf D & = \\begin{bmatrix} \\sigma_0 & 0 & 0 \\\\ 0 & \\sigma_1 & 0 \\\\ 0 & 0 & \\sigma_2 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} & \\rho_{02} \\\\\n\\rho_{10} & 1 & \\rho_{12} \\\\\n\\rho_{20} & \\rho_{21} & 1 \\end{bmatrix} \\\\\n\\gamma_{00}                     & \\sim \\operatorname{Normal}(13, 9.9) \\\\\n\\gamma_{01}, \\dots, \\gamma_{21} & \\sim \\operatorname{Normal}(0, 4.95) \\\\\n\\sigma_0, \\dots, \\sigma_2 & \\sim \\operatorname{Exponential}(1 / 9.9) \\\\\n\\sigma_\\epsilon           & \\sim \\operatorname{Exponential}(1 / 9.9) \\\\\n\\mathbf\\Omega   & \\sim \\operatorname{LKJ}(4).\n\\end{align}\n\\]\nNow fit the model.\n\nfit6.14 &lt;- brm(\n  data = external_pp, \n  family = gaussian,\n  external ~ 0 + Intercept + time + I(time^2) + female + time:female + I(time^2):female +\n    (1 + time + I(time^2) | id),\n  prior = c(prior(normal(0, 4.95), class = b),\n            prior(normal(13, 9.9), class = b, coef = Intercept),\n            prior(exponential(1.0 / 9.9), class = sd),\n            prior(exponential(1.0 / 9.9), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2500, warmup = 1000, chains = 3, cores = 3,\n  seed = 6,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit06.14\")\n\nSinger and Willett reported the new parameters \\(\\gamma_{01}\\), \\(\\gamma_{02}\\) and \\(\\gamma_{03}\\), were unimpressive. Let’s check their posteriors with a coefficient plot.\n\nposterior_summary(fit6.14)[4:6, ] %&gt;% \n  data.frame() %&gt;% \n  mutate(parameter = str_c(\"gamma[\", 0:2, \"][1]\")) %&gt;% \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = parameter)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  geom_pointrange(linewidth = 1/2, size = 1/4) +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  labs(subtitle = \"How well did 'female' do?\",\n       x = \"marginal posterior\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nYeah, they were either small or highly uncertain. Let’s go all the way with a WAIC comparison with fit6.12.\n\nfit6.14 &lt;- add_criterion(fit6.14, criterion = \"waic\")\n\nloo_compare(fit6.12, fit6.14, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit6.12    0.0       0.0  -952.3      13.2         66.0    5.8    1904.5   26.4 \nfit6.14   -1.2       1.0  -953.4      13.2         67.4    6.0    1906.8   26.5 \n\n\nYep, there’s no compelling reason to prefer fit6.14 over fit6.12. The predictor female looks like a dud, which suggests the externalizing behavior trajectories are about the same for the boys and the girls.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#truly-nonlinear-trajectories",
    "href": "06.html#truly-nonlinear-trajectories",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "6.4 Truly nonlinear trajectories",
    "text": "6.4 Truly nonlinear trajectories\n\nAll the individual growth models described so far—including the curvilinear ones presented in this chapter–share an important mathematical property: they are linear in the individual growth parameters. Why do we use the label “linear” to describe trajectories that are blatantly nonlinear? The explanation for this apparent paradox is that this mathematical property depends not on the shape of the underlying growth trajectory but rather where–in which portion of the model–the nonlinearity arises. In all previous model, nonlinearity (or discontinuity) stems from the representation of the predictors. To allow the hypothesized trajectory to deviate from a straight line, TIME is either transformed or expressed using higher order polynomial terms. In the truly nonlinear models we now discuss, nonlinearity arises in a different way–through the parameters. (pp. 223–224, emphasis in the original)\n\n\n6.4.1 What do we mean by truly nonlinear models?\nLinear models are linear in the sense that the expected value for the criterion (\\(\\operatorname E(y)\\)) is the sum of the \\(\\gamma\\)’s multiplies by either a constant (in the case of \\(\\gamma_{00}\\)) or b the regression weight (the point estimate for frequentists or the measure of central tendency in the posterior, often the mean or median, for Bayesians). In other words, \\(\\operatorname E(y)\\) is a weighted linear composite of the \\(\\gamma\\)’s multiplied by a given set of predictor values. Truly nonlinear models do not have this property.\n\n\n6.4.2 The logistic individual growth curve\nLoad the data from Tivan’s (1980) unpublished dissertation.\n\nlibrary(tidyverse)\n\nfoxngeese_pp &lt;- read_csv(\"data/foxngeese_pp.csv\")\n\nglimpse(foxngeese_pp)\n\nRows: 445\nColumns: 4\n$ id     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ game   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 1, 2, 3, 4, 5,…\n$ nmoves &lt;dbl&gt; 4, 7, 8, 3, 3, 3, 7, 6, 3, 7, 5, 3, 8, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 5, 3, 3, 6, 3, 3, 3, 3, 3, 6, 6, 3, 3,…\n$ read   &lt;dbl&gt; 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4…\n\n\nThere are responses from 17 participants in these data.\n\nfoxngeese_pp %&gt;% \n  distinct(id)\n\n# A tibble: 17 × 1\n      id\n   &lt;dbl&gt;\n 1     1\n 2     2\n 3     3\n 4     4\n 5     5\n 6     6\n 7     7\n 8     8\n 9     9\n10    10\n11    11\n12    12\n13    13\n14    14\n15    15\n16    16\n17    17\n\n\nIn this experiment, each child played up to 27 games, with some variation among the children.\n\nfoxngeese_pp %&gt;% \n  count(id, name = \"# games, per kid\") %&gt;% \n  count(`# games, per kid`)\n\n# A tibble: 3 × 2\n  `# games, per kid`     n\n               &lt;int&gt; &lt;int&gt;\n1                 14     1\n2                 26     1\n3                 27    15\n\n\nOur criterion is nmoves, the number of a child made within a game “before making a catastrophic error” (p. 226). Here’s the overall distribution of nmoves.\n\nfoxngeese_pp %&gt;% \n  ggplot(aes(x = nmoves)) +\n  geom_bar() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe can get a sense of the child-level data with our version of Figure 6.8.\n\nsubset &lt;- c(\"1\", \"4\", \"6\", \"7\", \"8\", \"11\", \"12\", \"15\")\n\nfoxngeese_pp %&gt;% \n  filter(id %in% subset) %&gt;% \n  mutate(id = if_else(id &lt; 10, str_c(\"0\", id), as.character(id))) %&gt;% \n\n  ggplot(aes(x = game, y = nmoves)) +\n  geom_point(size = 2/3) +\n  scale_y_continuous(breaks = 0:5 * 5, limits = c(0, 25)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id, ncol = 4, labeller = label_both)\n\n\n\n\n\n\n\n\nIn these data, nmoves always lies between 1 and 20. Based on what we know about the structure of these data and the experiment which produced them, Singer and Willett proposed our statistical model should include:\n\na lower asymptote, which captures how each child had to make at least one move;\nan upper asymptote, which captures the maximum number of moves allowed, which seems to be 20; and\na smooth curve showing growth from the lower asymptote to the upper, which involves a period of accelerated learning somewhere in the middle.\n\nThey then point out these features are captured in a logistic trajectory. Their proposed logistic model of change follows the form\n\\[\n\\begin{align}\n\\text{nmoves}_{ij} & = 1 + \\frac{19}{1 + \\pi_{0i} e^{-(\\pi_{1i} \\text{game}_{ij})}} + \\epsilon_{ij} \\\\\n\\epsilon_{ij}      & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon^2).\n\\end{align}\n\\]\nThis equation is set up so that as the value for \\(\\text{game}_{ij} \\rightarrow \\infty\\), the denominator in the equation shrinks to 1, which leave \\(1 + 19/1 = 20\\), the upper asymptote. Yet as \\(\\text{game}_{ij} \\rightarrow -\\infty\\), the denominator inflates to \\(\\infty\\), leaving \\(1 + 0 = 1\\), the lower asymptote. The \\(\\pi_{0i}\\) and \\(\\pi_{1i}\\) terms are no longer simply the intercepts and time slopes, as in earlier models. To get a sense, we’ll make Figure 6.9. To help, we might make a custom function that follows the equation, above.\n\nsw_logistic &lt;- function(pi0 = 15, pi1 = 0.3, game = 10) {\n  1 + (19 / (1 + pi0 * exp(-pi1 * game)))\n}\n\nNow use our sw_logistic() function1 to make Figure 6.9.\n1 You might also use our sw_logistic() to investigate the claims on what happens when \\(\\text{game}_{ij} \\rightarrow \\infty\\) or \\(\\text{game}_{ij} \\rightarrow -\\infty\\).\ncrossing(pi0 = c(1.5, 15, 150),\n         pi1 = c(0.1, 0.3, 0.5)) %&gt;% \n  expand(nesting(pi0, pi1),\n         game = 0:30) %&gt;% \n  mutate(y     = sw_logistic(pi0, pi1, game),\n         pi0_f = factor(str_c(\"pi[0]==\", pi0), \n                        levels = c(\"pi[0]==150\", \"pi[0]==15\", \"pi[0]==1.5\"))) %&gt;%\n  \n  ggplot(aes(x = game, y = y, group = pi1)) +\n  geom_line(aes(linewidth = pi1)) + \n  scale_linewidth_continuous(expression(pi[1]), range = c(1/3, 1), breaks = c(0.1, 0.3, 0.5)) +\n  scale_y_continuous(\"nmoves\", limits = c(0, 25)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~pi0_f, labeller = label_parsed) \n\n\n\n\n\n\n\n\nThough \\(\\pi_{0i}\\) has clear implications for the intercept, it’s not quite the same thing as the intercept. Similarly, though \\(\\pi_{1i}\\) has an influence on how rapidly the trajectories approach the upper asymptote, it’s not the same thing as the slope. Once you go nonlinear, it can become tricky to interpret the parameters directly.\nSince we’ll be fitting this model as Bayesians, let’s talk about priors. Based on the plots in Figure 6.9, it seems like we should center the \\(\\gamma_{00}\\) prior on a relatively large value. If you compare the three panels of the plot, it seems like there’s a diminishing returns effect as the value for \\(\\pi_0\\) went from 1.5 to 15 to 150. I propose something like \\(\\operatorname N(15, 3)\\).\nNow let’s talk about the prior for \\(\\gamma_{10}\\). In each of the three panels of Figure 6.9, it looks like the values of 0.1 to 0.5 cover a good range of plausible values. If we were conservative about how quickly the children might learn the game, perhaps we’d settle for a prior like \\(\\operatorname N(0.2, 0.1)\\).\nA nonlinear model like this can be hard to understand even with the benefit of Figure 6.9. To get a clear sense of our priors, we might do a graphical prior predictive check. Here we simulate 100 draws from the prior predictive distribution of\n\\[\n\\begin{align}\ny_j & = 1 + \\frac{19}{1 + \\gamma_{00} e^{-(\\gamma_{10} \\text{game}_j)}} \\\\\n\\gamma_{00} & \\sim \\operatorname N(15, 3) \\\\\n\\gamma_{10} & \\sim \\operatorname N(0.2, 0.1).\n\\end{align}\n\\]\n\n# how many do you want?\nn &lt;- 100\n\n# simulate\nset.seed(6)\n\ntibble(n   = 1:n,\n       gamma00 = rnorm(n, mean = 15, sd = 3),\n       gamma10 = rnorm(n, mean = 0.2, sd = 0.1)) %&gt;% \n  expand(nesting(n, gamma00, gamma10),\n         game = 0:30) %&gt;% \n  mutate(y = sw_logistic(gamma00, gamma10, game)) %&gt;%\n  \n  # plot!\n  ggplot(aes(x = game, y = y, group = n)) +\n  geom_hline(yintercept = c(1, 20), color = \"white\") +\n  geom_line(alpha = 1/2, linewidth = 1/4) + \n  scale_y_continuous(limits = c(1, 20)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo my eye, this looks like a respectable starting point. As for our variance parameters, I’d be comfortable just setting \\(\\sigma_0 \\sim \\operatorname{Exponential}(1)\\) and \\(\\sigma_\\epsilon \\sim \\operatorname{Exponential}(1)\\). Since the range for our \\(\\gamma_{10}\\) prior is about an order of magnitude smaller, I’d argue \\(\\operatorname{Exponential}(10)\\) would be a decent starting point for our \\(\\sigma_1\\) prior. As is typical, I recommend the weakly regularizing \\(\\operatorname{LKJ}(4)\\) for the level-2 correlation matrix.\nHere’s how to fit the model with brms.\n\nfit6.15 &lt;- brm(\n  data = foxngeese_pp, \n  family = gaussian,\n  bf(nmoves ~ 1 + (19.0 / (1.0 + g0 * exp(-g1 * game))),\n     g0 + g1 ~ 1 + (1 |i| id), \n     nl = TRUE),\n  prior = c(prior(normal(15, 3), nlpar = g0),\n            prior(normal(0.2, 0.1), nlpar = g1),\n            prior(exponential(1), class = sd, nlpar = g0),\n            prior(exponential(10), class = sd, nlpar = g1),\n            prior(exponential(1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, cores = 3, chains = 3,\n  init = 0, \n  control = list(adapt_delta = 0.995),\n  file = \"fits/fit06.15\")\n\nBefore we explore the results, you might have noticed a few odd things about our syntax. For example, notice how we wrapped our model formula within a bf() statement, that the actual formula is composed of both variable names (e.g., game) AND parameter names (e.g., g0, which is an abbreviation for \\(\\gamma_{00}\\)), and that we set nl = TRUE. Further, did you notice how we used the nlpar argument in several of our prior() lines? Taken as a whole, these indicate we used the brms non-linear syntax. Since so few of the models in this book require the brms non-linear syntax, I’m not going to explain it in detail, here. However, you can learn all about it in Bürkner’s (2021) vignette, Estimating non-linear models with brms. I also use the non-linear syntax quite a bit in my (2026a) translation of the second edition of McElreath’s text.\nBut as to the task at hand, let’s look at the summary for our model.\n\nprint(fit6.15, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: nmoves ~ 1 + (19/(1 + g0 * exp(-g1 * game))) \n         g0 ~ 1 + (1 | i | id)\n         g1 ~ 1 + (1 | i | id)\n   Data: foxngeese_pp (Number of observations: 445) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 17) \n                               Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(g0_Intercept)                  1.306     1.277    0.033    4.759 1.002     1424     1716\nsd(g1_Intercept)                  0.059     0.012    0.040    0.087 1.003      945     1653\ncor(g0_Intercept,g1_Intercept)    0.100     0.330   -0.543    0.693 1.015      298      729\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\ng0_Intercept   13.595     1.968   10.137   17.908 1.002     3037     2087\ng1_Intercept    0.124     0.017    0.090    0.157 1.002     1045     1328\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    3.750     0.133    3.504    4.031 1.000     3899     2209\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur posterior summaries are similar to the results Singer and Willett reported in their Table 6.6. To further explore the model, let’s use the fitted() approach to make our version of Figure 6.10a.\n\nnd &lt;- tibble(game = seq(from = 0, to = 30, by = 0.1))\n\np1 &lt;- fitted(fit6.15,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  ggplot(aes(x = game, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = c(1, 20), color = \"white\") +\n  geom_ribbon(alpha = 1/4) +\n  geom_line() + \n  scale_y_continuous(\"nmoves\", limits = c(0, 25), expand = c(0, 0)) +\n  labs(subtitle = \"Model A\") +\n  theme(panel.grid = element_blank())\n\np1\n\n\n\n\n\n\n\n\nFor the next model, Singer and Willett proposed we use a mean-centered version of read as predictor of the level-2 intercepts and slopes. Here we make a version of that variable, which we’ll call read_c.\n\nfoxngeese_pp &lt;- foxngeese_pp %&gt;% \n  mutate(read_c = read - mean(read))\n\nFor our new coefficients, I propose we continue to take a weakly-regularizing approach. Since the level-2 variables they’re predicting are on different scales, it seems like these priors should be on different scales, too. I suggest we specify \\(\\gamma_{01} \\sim \\operatorname{Normal}(0, 1)\\) and \\(\\gamma_{11} \\sim \\operatorname{Normal}(0, 0.1)\\). If you follow along, we might express our statistical model in formal notation as\n\\[\n\\begin{align}\n\\text{nmoves}_{ij} & = 1 + \\frac{19}{1 + \\pi_{0i} e^{-(\\pi_{1i} \\text{game}_{ij})}} + \\epsilon_{ij} \\\\\n\\pi_{0i} & = \\gamma_{00} + \\gamma_{01} \\left (\\text{read}_i - \\overline{\\text{read}} \\right ) + \\zeta_{0i} \\\\\n\\pi_{1i} & = \\gamma_{10} + \\gamma_{11} \\left (\\text{read}_i - \\overline{\\text{read}} \\right ) + \\zeta_{1i} \\\\\n\\epsilon_{ij}      & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &\n\\sim \\operatorname{Normal}  \\begin{pmatrix} \\begin{bmatrix}\n   0 \\\\ 0  \\end{bmatrix}, \\mathbf D \\mathbf \\Omega\n   \\mathbf D' \\end{pmatrix} \\\\\n\\mathbf D & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1\\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\\n\\rho_{10} & 1 \\end{bmatrix}, \\\\\n\\gamma_{00} & \\sim \\operatorname{Normal}(15, 3) \\\\\n\\gamma_{10} & \\sim \\operatorname{Normal}(0.2, 0.1) \\\\\n\\gamma_{01} & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\gamma_{11} & \\sim \\operatorname{Normal}(0, 0.1) \\\\\n\\sigma_0 & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_1 & \\sim \\operatorname{Exponential}(10) \\\\\n\\sigma_\\epsilon & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf \\Omega & \\sim \\operatorname{LKJ}(4).\n\\end{align}\n\\]\nFit the model.\n\nfit6.16 &lt;- brm(\n  data = foxngeese_pp, \n  family = gaussian,\n  bf(nmoves ~ 1 + (19.0 / (1.0 + g0 * exp(-g1 * game))),\n     g0 + g1 ~ 1 + read_c + (1 |i| id), \n     nl = TRUE),\n  prior = c(prior(normal(15, 3), nlpar = g0, coef = Intercept),\n            prior(normal(0, 1), nlpar = g0, coef = read_c),\n            prior(normal(0.2, 0.1), nlpar = g1, coef = Intercept),\n            prior(normal(0, 0.1), nlpar = g1, coef = read_c),\n            prior(exponential(1), class = sd, nlpar = g0),\n            prior(exponential(10), class = sd, nlpar = g1),\n            prior(exponential(1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, cores = 3, chains = 3,\n  init = 0, \n  control = list(adapt_delta = 0.999),\n  file = \"fits/fit06.16\")\n\nCheck the results.\n\nprint(fit6.16)\n\n Family: gaussian \n  Links: mu = identity \nFormula: nmoves ~ 1 + (19/(1 + g0 * exp(-g1 * game))) \n         g0 ~ 1 + read_c + (1 | i | id)\n         g1 ~ 1 + read_c + (1 | i | id)\n   Data: foxngeese_pp (Number of observations: 445) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 17) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(g0_Intercept)                   1.32      1.30     0.04     4.81 1.00     1654     1528\nsd(g1_Intercept)                   0.06      0.01     0.04     0.09 1.00     1121     1685\ncor(g0_Intercept,g1_Intercept)     0.10      0.33    -0.55     0.69 1.01      308     1062\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ng0_Intercept    13.61      1.94    10.14    17.71 1.00     3686     2347\ng0_read_c        0.49      0.96    -1.34     2.38 1.00     3363     2234\ng1_Intercept     0.13      0.02     0.09     0.16 1.00     1293     1825\ng1_read_c        0.02      0.02    -0.02     0.06 1.00     1511     1883\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.75      0.13     3.51     4.01 1.00     3659     2196\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBefore you get worried about how our posterior mean for \\(\\gamma_{01}\\) has the opposite sign as the point estimate Singer and Willett reported in the text, take a look at the whole posterior.\n\nlibrary(tidybayes)\n\nas_draws_df(fit6.16) %&gt;% \n  ggplot(aes(x = b_g0_read_c)) +\n  stat_halfeye(.width = c(0.8, 0.95)) +\n  geom_vline(xintercept = -0.3745, linetype = 2) +\n  annotate(geom = \"text\",\n           x = -0.3745, y = 0.03,\n           label = \"Singer and Willett's\\npoint estimate\",\n           angle = 90, hjust = 0, size = 3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(gamma[0][1])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe posterior is wide and the point estimate in the text fits comfortably within our inner 80% interval. Okay, let’s explore this model further by making the rest of Figure 6.10.\n\nnd &lt;- crossing(\n  game   = seq(from = 0, to = 30, by = 0.1),\n  read_c = c(-1.58, 1.58))\n\np2 &lt;- fitted(fit6.16,\n            newdata = nd,\n            re_formula = NA) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(read_c = ifelse(read_c &lt; 0, \"low (-1.58)\", \"high (1.58)\")) %&gt;% \n  \n  ggplot(aes(x = game, y = Estimate, ymin = Q2.5, ymax = Q97.5, \n             fill = read_c, color = read_c)) +\n  geom_hline(yintercept = c(1, 20), color = \"white\") +\n  geom_ribbon(alpha = 1/4, linewidth = 0) +\n  geom_line() + \n  scale_fill_viridis_d(option = \"A\", begin = 0.25, end = 0.75) +\n  scale_color_viridis_d(option = \"A\", begin = 0.25, end = 0.75) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 25), expand = c(0, 0)) +\n  labs(subtitle = \"Model B\") +\n  theme(panel.grid = element_blank())\n\n# combine\np1 + p2\n\n\n\n\n\n\n\n\nNotice how unimpressive the expected trajectories between the two levels of read_c are when you include the 95% interval bands. Beware plots of fitted lines that do not include the 95% intervals!\nWe might compare our two nonlinear models with their WAIC estimates.\n\nfit6.15 &lt;- add_criterion(fit6.15, criterion = \"waic\")\nfit6.16 &lt;- add_criterion(fit6.16, criterion = \"waic\")\n\nloo_compare(fit6.15, fit6.16, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.16     0.0       0.0 -1235.8      18.4         28.8     3.3    2471.6    36.7\nfit6.15    -0.4       0.5 -1236.2      18.3         28.9     3.3    2472.5    36.7\n\n\nThey’re nearly the same, suggesting fit6.16 was overfit. However, now that we have it, we might use our overfit model fit6.16 to an updated version of Figure 6.8.\n\n# define the new data\nnd &lt;- foxngeese_pp %&gt;% \n  distinct(id, read_c) %&gt;% \n  filter(id %in% subset) %&gt;% \n  expand_grid(game = seq(from = 0, to = 30, by = 0.1))\n  \n\n# extract the fitted trajectories\nfitted(fit6.16, newdata = nd) %&gt;% \n  # wrangle\n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = game)) +\n  geom_hline(yintercept = c(1, 20), color = \"white\") +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = read_c),\n              alpha = 1/4, linewidth = 0) +\n  geom_line(aes(y = Estimate, color = read_c)) + \n  geom_point(data = foxngeese_pp %&gt;% filter(id %in% subset),\n             aes(y = nmoves),\n             size = 2/3) +\n  scale_fill_viridis_c(option = \"A\", begin = 0.15, end = 0.85, \n                       limits = range(foxngeese_pp$read_c)) +\n  scale_color_viridis_c(option = \"A\", begin = 0.15, end = 0.85, \n                        limits = range(foxngeese_pp$read_c)) +\n  scale_y_continuous(breaks = 0:5 * 5, limits = c(0, 25), expand = c(0, 0)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id, ncol = 4, labeller = label_both)\n\n\n\n\n\n\n\n\n\n\n6.4.3 A survey of truly nonlinear change trajectories\n\nBy now you should realize that you can represent individual change using a virtually limitless number of mathematical functions….\nHow then can you possibly specify a suitable model for your data and purposes? Clearly, you need more than empirical evidence. Among a group of well-fitting growth models, blind numeric comparison of descriptive statistics, goodness-of-fit, and regression diagnostics will rarely pick out the best one. As you might expect, we recommend that you blend theory and empirical evidence, articulating a rationale that you can translate into a statistical model. This recommendation underscores an important point that can often be overlooked in the heat of data analysis: substance is paramount. The best way to select an appropriate individual growth model is to work within an explicit theoretical framework. We suggest that you ask not “What is the best model for the job?” but, rather, “What model is most theoretically sound?” (pp. 232–233, emphasis in the original)\n\nBased on the work of Mead and Pike (1975), Singer and Willett suggested we might divide up the range of nonlinear models into four bins:\n\npolynomial,\nhyperbolic,\ninverse polynomial, and\nexponential.\n\n\n6.4.3.1 Hyperbolic growth\n\nThe rectangular hyperbola is one of the simplest nonlinear models for individual change. TIME enters as a reciprocal in the denominator of the model’s right side. This model possesses an important property for modeling biological and agricultural growth: over time, its outcome smoothly approaches–but never reaches–an asymptote. (p. 234, emphasis in the original)\n\nThe example of a hyperbolic growth from Table 6.7 (p. 234) followed the form\n\\[Y_{ij} = \\alpha_i - \\frac{1}{\\pi_{1i} TIME_{ij}} + \\epsilon_{ij},\\]\nwhere \\(\\pi_{1i}\\) is kinda like a growth slope and there is no \\(\\pi_{0i}\\) because \\(\\pi_{0i}\\) parameters are for losers. To get a sense of what this model provides, here’s our version of the upper-left panel of Figure 6.11.\n\ntext &lt;- tibble(\n  time  = c(0, 1.2, 4.6, 8),\n  y     = c(102.5, 90, 87, 85),\n  label = c(\"alpha==100\",\n            \"pi[1]==0.01\",\n            \"pi[1]==0.02\",\n            \"pi[1]==0.1\"))\n\np1 &lt;- crossing(\n  alpha = 100,\n  pi1   = c(0.01, 0.02, 0.1)) %&gt;% \n  expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% \n  mutate(y = alpha - (1 / (pi1 * time))) %&gt;% \n  \n  ggplot(aes(x = time, y = y, group = pi1)) +\n  geom_hline(yintercept = 100, color = \"white\") +\n  geom_line() +\n  geom_text(data = text,\n            aes(label = label),\n            size = 3, hjust = 0, parse = T) +\n  labs(title = \"Hyperbola\",\n       y = expression(E(italic(Y)))) +\n  coord_cartesian(ylim = c(0, 100)) +\n  theme(panel.grid = element_blank())\n\np1\n\n\n\n\n\n\n\n\nNote how, in this plot, \\(\\alpha\\) is the upper asymptote.\n\n\n6.4.3.2 Inverse polynomial growth\n“The family of inverse polynomials extends the rectangular hyperbola by adding higher powers of TIME to the denominator of the quotient on the model’s right side” (p. 236, emphasis in the original). The example of a hyperbolic growth from Table 6.7 followed the form\n\\[Y_{ij} = \\alpha_i - \\frac{1}{\\pi_{1i} TIME_{ij} + \\pi_{2i} TIME_{ij}^2} + \\epsilon_{ij}.\\]\nTo get a sense of what this model provides, here’s our version of the upper-right panel of Figure 6.11.\n\ntext &lt;- tibble(\n  time  = c(0, 3.5, 7.5, 10, 1.5),\n  y     = c(102.5, 94, 91, 77, 8),\n  label = c(\"alpha==100\",\n            \"pi[2]==0.015\",\n            \"pi[2]==0\",\n            \"pi[2]==-0.0015\",\n            \"pi[1]==0.02~('for all curves')\"),\n  hjust = c(0, 0, 0, 1, 0))\n\np2 &lt;- crossing(\n  alpha = 100,\n  pi1   =  0.02,\n  pi2   = c(0.015, 0, -0.0015)) %&gt;% \n  expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% \n  mutate(y = alpha - (1 / (pi1 * time + pi2 * time^2))) %&gt;% \n  \n  ggplot(aes(x = time, y = y, group = pi2)) +\n  geom_hline(yintercept = 100, color = \"white\") +\n  geom_line() +\n  geom_text(data = text,\n            aes(label = label, hjust = hjust),\n            size = 3, parse = T) +\n  labs(title = \"Inverse polynomial\",\n       y = expression(E(italic(Y)))) +\n  coord_cartesian(ylim = c(0, 100)) +\n  theme(panel.grid = element_blank())\n\np2\n\n\n\n\n\n\n\n\n\n\n6.4.3.3 Exponential growth\n\nExponential growth is probably the most widely used class of truly nonlinear models. This theoretically compelling group has been used for centuries to model biological, agricultural, and physical growth. This class includes a wide range of different functional forms, but all contain an exponent of \\(e\\), the base of the natural logarithm. (p. 237, emphasis in the original)\n\nThe example of simple exponential growth from Table 6.7 followed the form\n\\[Y_{ij} = \\pi_{0i} e^{\\pi_{1i} TIME_{ij}} + \\epsilon_{ij},\\]\nwhere now we have the triumphant return of \\(\\pi_{0i}\\), which still isn’t quite an intercept but is close to one. The of negative exponential growth from Table 6.7 followed the form\n\\[Y_{ij} = \\alpha_i -(\\alpha_i - \\pi_{0i}) e^{\\pi_{1i} TIME_{ij}} + \\epsilon_{ij}.\\]\nLet’s make the panels of the lower row of Figure 6.11.\n\n# left\ntext &lt;- tibble(\n  time  = 10,\n  y     = c(100.5, 39, 16, 1),\n  label = c(\"pi[1]==0.3\",\n            \"pi[1]==0.2\",\n            \"pi[1]==0.1\",\n            \"pi[0]==5~('for all curves')\"),\n  vjust = c(0, 0.5, 0.5, 0))\n\np3 &lt;- crossing(\n  pi0 = 5,\n  pi1 = c(0.1, 0.2, 0.3)) %&gt;% \n  expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% \n  mutate(y = pi0 * exp(pi1 * time)) %&gt;% \n  \n  ggplot(aes(x = time, y = y, group = pi1)) +\n  geom_hline(yintercept = 100, color = \"white\") +\n  geom_line() +\n  geom_text(data = text,\n            aes(label = label, vjust = vjust),\n            size = 3, hjust = 1, parse = T) +\n  labs(title = \"Exponential (simple)\",\n       y = expression(E(italic(Y)))) +\n  coord_cartesian(ylim = c(0, 100)) +\n  theme(panel.grid = element_blank())\n\n# right\ntext &lt;- tibble(\n  time  = c(0, 10, 10, 10, 7),\n  y     = c(102.5, 98, 83, 63, 20),\n  label = c(\"alpha==100\",\n            \"pi[1]==0.3\",\n            \"pi[1]==0.2\",\n            \"pi[1]==0.1\",\n            \"pi[0]==20~('for all curves')\"),\n  hjust = c(0, 1, 1, 1, 1))\n\np4 &lt;- crossing(\n  alpha = 100,\n  pi0   = 20,\n  pi1   = c(0.1, 0.2, 0.3)) %&gt;% \n  expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% \n  mutate(y = alpha - (alpha - pi0) * exp(-pi1 * time)) %&gt;% \n  \n  ggplot(aes(x = time, y = y, group = pi1)) +\n  geom_hline(yintercept = 100, color = \"white\") +\n  geom_line() +\n  geom_text(data = text,\n            aes(label = label, hjust = hjust),\n            size = 3, parse = T) +\n  labs(title = \"Negative exponential\",\n       y = expression(E(italic(Y)))) +\n  coord_cartesian(ylim = c(0, 100)) +\n  theme(panel.grid = element_blank())\n\nNow combine the ggplots to make the full version of Figure 6.11.\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\nAs covered, above, the logistic trajectory is great when you have need of both lower and upper asymptotic. I might note that the logistic models we fit were not what you’d expect when thinking of what is typically called logistic regression. Conventional logistic regression models don’t add a normally-distributed error term. Rather, error is simply a function of the expected value. It’s something of a shame Singer and Willett didn’t cover logistic multilevel growth models. It’s such a shame, in fact, that we’ll go rogue and cover them in Section 6.5.\n\n\n\n6.4.4 From substantive theory to mathematical representations of individual growth\nHere’s how to make Figure 6.12.\n\ntext &lt;- tibble(\n  time  = c(0.2, 9.8, 9.8, 9.8, 1),\n  y     = c(102.5, 87, 63, 46, 10),\n  label = c(\"alpha==100\",\n            \"pi[1]==1\",\n            \"pi[1]==5\",\n            \"pi[1]==10\",\n            \"pi[0]==10~('for all curves')\"),\n  hjust = c(0, 1, 1, 1, 0))\n\ncrossing(alpha = 100,\n         pi0   = 10,\n         pi1   = c(1, 5, 10)) %&gt;% \n  expand_grid(time = seq(from = 0, to = 10, by = 0.01)) %&gt;% \n  mutate(y = pi0 + ((alpha - pi0) * time) / (pi1 + time)) %&gt;% \n  \n  ggplot(aes(x = time, y = y, group = pi1)) +\n  geom_hline(yintercept = 100, color = \"white\") +\n  geom_line() +\n  geom_text(data = text,\n            aes(label = label, hjust = hjust),\n            size = 3, parse = T) +\n  scale_x_continuous(breaks = 0:5 * 2, expand = c(0, 0), limits = c(0, 10)) +\n  scale_y_continuous(expression(E(italic(Y))), \n                     expand = c(0, 0), limits = c(0, 110)) +\n  labs(title = \"Thurstone's learning equation\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nJust to get in a little more practice with non-linear models and the brms non-liner syntax, we might practice applying Thurstone’s learning equation to our children’s behavioral data from the last section. As before, our preliminary task to to pick our prior. Based on what we already know about the data and also based on the curves we see in the plot, above, I recommend we select something like \\(\\gamma_{00} \\sim \\operatorname{Normal}(2, 1)\\) and \\(\\gamma_{10} \\sim \\operatorname{Normal}(5, 2)\\). To get a sense of what that’d mean, here’s a prior predictive check.\n\n# how many do you want?\nn &lt;- 100\n\n# simulate\nalpha &lt;- 20\n\nset.seed(6)\n\ntibble(n   = 1:n,\n       pi0 = rnorm(n, mean = 2, sd = 1),\n       pi1 = rnorm(n, mean = 5, sd = 2)) %&gt;% \n  expand_grid(game = 0:30) %&gt;% \n  mutate(y = pi0 + (((alpha - pi0) * game) / (pi1 + game))) %&gt;%\n  \n  # plot!\n  ggplot(aes(x = game, y = y, group = n)) +\n  geom_hline(yintercept = c(1, 20), color = \"white\") +\n  geom_line(linewidth = 1/4, alpha = 1/2) + \n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nI recommend you play around with different priors on your own. Here’s how to fit the model to the foxngeese_pp data.\n\nfit6.17 &lt;- brm(\n  data = foxngeese_pp, \n  family = gaussian,\n  bf(nmoves ~ g0 + (((20 - g0) * game) / (g1 + game)),\n     g0 + g1 ~ 1 + (1 |i| id), \n     nl = TRUE),\n  prior = c(prior(normal(2, 1), nlpar = g0),\n            prior(normal(5, 2), nlpar = g1),\n            prior(exponential(1), class = sd, nlpar = g0),\n            prior(exponential(1), class = sd, nlpar = g1),\n            prior(exponential(1), class = sigma),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, cores = 3, chains = 3,\n  init = 0, \n  control = list(adapt_delta = 0.99),\n  file = \"fits/fit06.17\")\n\nCheck the parameter summary.\n\nprint(fit6.17)\n\n Family: gaussian \n  Links: mu = identity \nFormula: nmoves ~ g0 + (((20 - g0) * game)/(g1 + game)) \n         g0 ~ 1 + (1 | i | id)\n         g1 ~ 1 + (1 | i | id)\n   Data: foxngeese_pp (Number of observations: 445) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 17) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(g0_Intercept)                   0.39      0.34     0.01     1.24 1.00     1523     1611\nsd(g1_Intercept)                  17.22      3.01    11.84    23.54 1.00     1816     2096\ncor(g0_Intercept,g1_Intercept)    -0.00      0.33    -0.64     0.61 1.01      210      607\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ng0_Intercept     0.52      0.52    -0.49     1.55 1.00     1573     2024\ng1_Intercept     9.21      1.97     5.31    13.03 1.00     1712     1990\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.30      0.15     4.01     4.60 1.00     3840     2038\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe might use our Thurstone model to an updated version of Figure 6.8.\n\n# define the new data\nnd &lt;- foxngeese_pp %&gt;% \n  distinct(id) %&gt;% \n  filter(id %in% subset) %&gt;% \n  expand_grid(game = seq(from = 1, to = 30, by = 0.1))\n\n# extract the fitted trajectories\nfitted(fit6.17, newdata = nd) %&gt;% \n  # wrangle\n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = game)) +\n  geom_hline(yintercept = c(1, 20), color = \"white\") +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4, linewidth = 0) +\n  geom_line(aes(y = Estimate)) + \n  geom_point(data = foxngeese_pp %&gt;% filter(id %in% subset),\n             aes(y = nmoves),\n             size = 2/3) +\n  scale_y_continuous(breaks = 0:5 * 5, expand = c(0, 0)) +\n  coord_cartesian(ylim = c(0, 25)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ id, ncol = 4, labeller = label_both)\n\n\n\n\n\n\n\n\nDoesn’t look great. Notice how the parameters in this model did not restrict the fitted trajectories to respect the lower asymptote. And indeed, Thurstone’s model has no lower asymptote. Let’s see how this model compares with the unconditional logistic model, fit6.15, by way of the WAIC.\n\nfit6.17 &lt;- add_criterion(fit6.17, criterion = \"waic\")\n\nloo_compare(fit6.15, fit6.17, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit6.15     0.0       0.0 -1236.2      18.3         28.9     3.3    2472.5    36.7\nfit6.17   -54.8      10.5 -1291.0      13.9         17.1     1.2    2582.1    27.9\n\n\nYep, the logistic model captured the patterns in these data better than Thurstone’s learning model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#sec-Bonus-The-logistic-growth-model",
    "href": "06.html#sec-Bonus-The-logistic-growth-model",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "6.5 Bonus: The logistic growth model",
    "text": "6.5 Bonus: The logistic growth model\nIn the social sciences, binary data are a widely-collected data type which are often best analyzed with a nonlinear model. In this section, we’ll practice fitting a multilevel growth model on binary data using logistic regression. This will differ from the logistic curve models, above, in that we will not be presuming normally distributed errors, \\(\\epsilon_{ij} \\sim \\operatorname N(0, \\sigma_\\epsilon)\\). Rather, we’ll be using a different likelihood altogether.\n\n6.5.1 We need data\nIn this section, we’ll be borrowing a data file from the supplemental material from Gelman & Hill (2006), Data analysis using regression and multilevel/hierarchical models.\n\ndogs &lt;- read_delim(\n  \"extra_data/dogs.txt\", \n  \"\\t\", \n  escape_double = FALSE, \n  trim_ws = TRUE) %&gt;% \n  rename(dog = Dog)\n\nglimpse(dogs)\n\nRows: 30\nColumns: 26\n$ dog  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30\n$ T.0  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ T.1  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0\n$ T.2  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0\n$ T.3  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0\n$ T.4  &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0\n$ T.5  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1\n$ T.6  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0\n$ T.7  &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1\n$ T.8  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0\n$ T.9  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1\n$ T.10 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1\n$ T.11 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0\n$ T.12 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1\n$ T.13 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0\n$ T.14 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0\n$ T.15 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0\n$ T.16 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1\n$ T.17 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1\n$ T.18 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1\n$ T.19 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ T.20 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ T.21 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0\n$ T.22 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ T.23 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ T.24 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0\n\n\nThe data are initially in the wide format. Here we make the dogs data long.\n\ndogs &lt;- dogs %&gt;% \n  pivot_longer(-dog, values_to = \"y\") %&gt;% \n  mutate(trial = str_remove(name, \"T.\") %&gt;% as.double())\n\nhead(dogs)\n\n# A tibble: 6 × 4\n    dog name      y trial\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 T.0       0     0\n2     1 T.1       0     1\n3     1 T.2       1     2\n4     1 T.3       0     3\n5     1 T.4       1     4\n6     1 T.5       0     5\n\n\nAs Gelman and Hill described (p. 515), these data were taken from a behavioral experiment in the 1950’s. Thirty dogs learned a new trick. Each of the 30 dogs was given 25 trials to learn the trick. In these data, time is captured by the tial column, which ranges from 0 (the first trial) to 24 (the final 25th trial). The criterion variable is y, which is coded 0 = fail, 1 = success. To give a sense of the data, here’s a descriptive plot of a randomly-chosen subset of eight of the dogs.\n\n# define the dog subset\nset.seed(6)\n\nsubset &lt;- sample(1:30, size = 8)\n\n# subset the data\ndogs %&gt;% \n  filter(dog %in% subset) %&gt;% \n\n  # plot!\n  ggplot(aes(x = trial, y = y)) +\n  geom_point() +\n  scale_y_continuous(breaks = 0:1) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ dog, ncol = 4, labeller = label_both)\n\n\n\n\n\n\n\n\nAt the beginning of the experiment (trial == 0), none of the dogs new how to do the trick (i.e., for each one, y == 0). However, the dogs tended to learn the trick (i.e., get y == 1) by around the fifth or tenth trial. When modeling data of this kind, the conventional \\(\\epsilon_{ij} \\sim \\operatorname N(0, \\sigma_\\epsilon)\\) assumption has no hope of working. It’s just inappropriate. To get a sense of why, we’ll need to get technical.\n\n\n6.5.2 Have you heard of the generalized linear [mixed] model?\nThis wasn’t really addressed in Singer and Willett, but not all data kinds produce nicely Gaussian residuals. Though multilevel models can help with this, they won’t always do the job, particularly with binary data. Happily, the family of approaches described as the generalized linear mixed model (GLMM) will allow you to handle all kinds of wacky time-series data types. I’m not really going to offer a thorough introduction to the GLMM, here. For that, see Gelman & Hill (2006) or McElreath (2020). However, we will explore one strategy from the GLMM framework, which replaces the Gaussian likelihood with the Bernoulli.\n\n6.5.2.1 Sometimes data come in 0’s and 1’s\nSay you have a set of binary data \\(y_i\\). You can summarize those data as \\(N\\) trials, for which \\(z\\) were \\(1\\)’s. So if we had five coin flips, for which we assigned heads as 1 and tails as 0, we might describe those data as \\(N = 5\\) Bernoulli trials with \\(z = 4\\) successes. In ‘Bernoulli trials talk’, success is often a synonym for \\(y_i = 1\\). Further, you can use the Bernoulli likelihood to describe the probability \\(y_i = 1\\), given an underlying probability \\(p\\), as\n\\[\\Pr(y_i = 1 \\mid p) = p^z (1 - p)^{N - z}.\\]\nLet’s see how this plays out. First, we’ll make a custom bernoulli_likelihood() function.\n\nbernoulli_likelihood &lt;- function(p, data) {\n  n &lt;- length(data)\n  z &lt;- sum(data)\n  \n  p^z * (1 - p)^(n - sum(data))\n}\n\nNow we’ll apply this function to a range of possible \\(p\\) values and data of the kind we just discussed, \\(N = 5\\) trials, for which \\(z = 4\\).\n\ndata &lt;- c(1, 1, 0, 1, 1)\n\ntibble(p = seq(from = 0, to = 1, length.out = 200)) %&gt;% \n  mutate(d = bernoulli_likelihood(p, data)) %&gt;% \n  \n  ggplot(aes(x = p, y = d)) +\n  geom_line() +\n  geom_vline(xintercept = 0.8, linetype = 3) +\n  scale_x_continuous(expression(italic(p)), breaks = 0:5 / 5) +\n  ylab(\"likelihood\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNote how the maximum value for this likelihood, given our data, is at \\(p = .8\\). It’s also the case that the sample mean of a set of Bernoulli trials is the same as the sample estimate of \\(p\\). Let’s confirm.\n\nmean(data)\n\n[1] 0.8\n\n\nAlthough the sample mean gave us a point estimate for \\(p\\), it was our use of the Bernoulli likelihood function that gave us a sense of the relative likelihoods of all possible values of \\(p\\). It also gave us a sense of the certainty for our sample estimate of \\(p\\). As is usually the case, our certainty will increase when \\(N\\) increases. For example, consider the case of \\(N = 100\\) and \\(z = 80\\).\n\nn &lt;- 100\nz &lt;- 80\n\ndata &lt;- rep(1:0, times = c(z, n - z))\n\ntibble(p = seq(from = 0, to = 1,  length.out = 200)) %&gt;% \n  mutate(d = bernoulli_likelihood(p, data)) %&gt;% \n  \n  ggplot(aes(x = p, y = d)) +\n  geom_line() +\n  geom_vline(xintercept = z / n, linetype = 3) +\n  scale_x_continuous(expression(italic(p)), breaks = 0:5 / 5) +\n  ylab(\"likelihood\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe same ratio of \\(z/N\\) produced the same point estimate of \\(p\\), but the overall shape of the likelihood was more concentrated around that point estimate (the mode) than in our first example.\n\n\n6.5.2.2 Recall how we can express models in terms of the Gaussian likelihood\nThroughout the text, Singer and Willett express their models in terms true scores and stochastic elements. For a simple single-level model of one predictor \\(x_i\\) for a criterion \\(y_i\\), Singer and Willett might express that model as\n\\[\n\\begin{align}\ny_i & = [b_0 + b_1 x_i] + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{N}(0, \\sigma_\\epsilon^2),\n\\end{align}\n\\]\nwhere the \\(y_i = [b_0 + b_1 x_i]\\) portion describes the deterministic part of the model (the true score) and the \\(\\epsilon_i\\) portion describes the stochastic part of the model (the residual variance). As we briefly covered in Section 3.2.2, it turns out that another way to express this model is\n\\[\n\\begin{align}\ny_i & \\sim \\operatorname{N}(\\mu_i, \\sigma_\\epsilon^2)\\\\\n\\mu_i & = b_0 + b_1 x_i,\n\\end{align}\n\\]\nwhere we more explicitly declare that the criterion \\(y_i\\) follows a conditional Normal distribution for which we model the mean with the equation \\(\\mu_i = b_0 + b_1 x_i\\) and we express the variation around the conditional mean as \\(\\sigma_\\epsilon^2\\). The advantage of adopting this style of model notation is it generalizes well to data of other kinds for which we might want to use other likelihoods, such as the Bernoulli.\n\n\n6.5.2.3 The simple logistic regression model using the Bernoulli likelihood\nNow consider our example of \\(N = 100\\) Bernoulli trials data, \\(y_i\\), for which \\(z = 80\\). We can use the likelihood-based notation to express that as\n\\[\n\\begin{align}\ny_i & \\sim \\operatorname{Bernoulli}(p_i)\\\\\n\\operatorname{logit}(p_i) & = b_0,\n\\end{align}\n\\]\nwhere, since we don’t have a predictor variable, the intercept \\(b_0\\) is the log-odds probability. Why ‘log-odds’? you say. Well, the problem of fitting regression models for probabilities (\\(p_i\\)) is that probabilities are logically bounded between 0 and 1. Yet there’s nothing inherent in the machinery of the conventional linear regression model to prevent predictions outside of those bound. Happily, our friends the statisticians can help us get around that with the aid of link functions. If we model the logit of \\(p\\), \\(\\operatorname{logit}(p_i)\\), instead of modeling \\(p\\) directly, we end up with an inherently nonlinear model that will always produce estimates within the bounds of 0 and 1. Another way of expressing this model is by nesting the right-hand part of the equation within the inverse logit function,\n\\[\n\\begin{align}\ny_i & \\sim \\operatorname{Bernoulli}(p_i) \\\\\np_i & = \\operatorname{logit}^{-1}(b_0),\n\\end{align}\n\\]\nwhere \\(\\operatorname{logit}^{-1}\\) is called the inverse logit. This use of the logit/inverse-logit function is where logistic regression get’s its name. Getting to the functions themselves, the logistic function is\n\\[\\operatorname{logit}(x) = \\log \\left ( \\frac{x}{1 - x} \\right ).\\]\nYou may recall that the odds of a probability is defined as \\(\\left ( \\frac{p}{1 - p}\\right )\\). Therefore, we can describe \\(\\operatorname{logit}(p)\\) as the log odds of the probability, or \\(p\\) in a log-odds metric. Anyway, the inverse of the logistic function is\n\\[\\operatorname{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}.\\]\nTo bring this all down to earth, here’s what happens when we apply the logistic function to a series of values ranging between 0 and 1.\n\ntibble(p = seq(from = 0.001, to = 0.999, by = 0.001)) %&gt;% \n  mutate(odds = p / (1 - p)) %&gt;% \n  mutate(log_odds = log(odds)) %&gt;% \n  \n  ggplot(aes(x = p, y = log_odds)) +\n  geom_vline(xintercept = 0.5, color = \"white\", linetype = 2) +\n  geom_hline(yintercept = 0, color = \"white\", linetype = 2) +\n  geom_line() +\n  scale_x_continuous(expression(italic(p)), \n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\"),\n                     expand = c(0, 0), limits = 0:1) +\n  scale_y_continuous(expression(italic(p)~(log~odds)), breaks = -2:2 * 2) +\n  coord_cartesian(ylim = c(-4.5, 4.5)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe consequence of the logistic function is it can take the probability values, which are necessarily bounded within 0 and 1, and transform then to the unbounded log-odds space. Thus, if you fit a linear regression model on \\(\\operatorname{logit}(p)\\), you will avoid making predictions that \\(p\\) is less than 0 or greater than 1.\nHopefully this is all just review. Now let’s consider the longitudinal multilevel model version of this model type for our dogs data.\n\n\n\n6.5.3 Define the simple logistic multilevel growth model\nIf we have \\(j\\) Bernoulli trials nested within \\(i\\) participants over time, we can express the generic logistic multilevel growth model as\n\\[\n\\begin{align}\ny_{ij} & \\sim \\operatorname{Bernoulli}(p_{ij}) \\\\\n\\operatorname{logit}(p_{ij}) & = \\pi_{0i} + \\pi_{1i} \\text{time}_{ij} \\\\\n\\pi_{0i} & = \\gamma_{00} + \\zeta_{0i} \\\\\n\\pi_{1i} & = \\gamma_{10} + \\zeta_{1i} \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &\n\\sim \\operatorname{Normal}  \\begin{pmatrix} \\begin{bmatrix}\n   0 \\\\ 0  \\end{bmatrix}, \\mathbf D \\mathbf \\Omega\n   \\mathbf D' \\end{pmatrix} \\\\\n\\mathbf D & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1\\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\\n\\rho_{10} & 1 \\end{bmatrix},   \n\\end{align}\n\\]\nwhere we make the usual multivariate-normal assumptions about \\(\\zeta_{0i}\\) and \\(\\zeta_{1i}\\). Yet because we are modeling the \\(y_{ij}\\) with the Bernoulli likelihood, we end up setting the linear model to \\(\\operatorname{logit}(p_{ij})\\). And also, because the Bernoulli distribution doesn’t itself have a \\(\\sigma\\) parameter, we have no \\(\\sigma_\\epsilon\\) term. That is, though the \\(\\zeta\\) parameters get a multivariate normal distribution, the \\(y_{ij}\\) data is considered Bernoulli. We have multiple distributional assumptions within a single model and this is totally okay.\nNow let’s see how this will work with our dogs data.\n\n\n6.5.4 Ease into the model with a prior predictive check\nIf we focus just on the top parts of the model, we can express the logistic multilevel growth model for our dogs data as\n\\[\n\\begin{align}\ny_{ij} & \\sim \\operatorname{Bernoulli}(p_{ij}) \\\\\n\\operatorname{logit}(p_{ij}) & = \\gamma_{00} + \\gamma_{10} \\text{trial}_{ij} + [\\zeta_{0i} + \\zeta_{1i} \\text{trial}_{ij}],  \n\\end{align}\n\\]\nwhere the Bernoulli trials \\(y\\) are nested within \\(i\\) dogs across \\(j\\) trials. As with all Bayesian models, we have to place priors on all model parameters. Before we get all technical with the model, let’s review what we know about the data:\n\nthere were 25 trials;\nthe trial variable is coded such that the first of the trials is trial == 0 and the 25th is trial == 24;\nthe dogs didn’t know how to do the trick, at first; and\nthe dogs all tended to learn the trick after several trials.\n\nNow if we were the actual scientists who collected these data and wanted to model them for a scientific publication, we wouldn’t have known that last bullet point before collecting the data. But perhaps we could have hypothesized that the dogs would tend to learn after a handful of trials. Anyway, this information tells us that because of the scaling of the predictor variable trial, the \\(\\gamma_{00}\\) parameter will be the expected value at the first trial. Since we know the dogs didn’t know how to do the trick at this point in the experiment, the probability value should be close to zero. Also, since we know the trials are scaled from 0 to 24, we also know that the coefficient increases in time, \\(\\gamma_{10}\\), should be positive since we expect improvement, but it shouldn’t be overly large since we don’t expect a lot of learning from one trial to the next. Rather, we expect to see learning unfold across several trials.\nThe difficulty is how to encode these insights into parameters that will make good predictions in the log-odds probability space. If you look up at our last plot, you’ll see that \\(p = .5\\) is the same as \\(\\operatorname{logit}(p) = 0\\). You’ll also notice that as \\(p \\rightarrow 0\\), \\(\\operatorname{logit}(p) \\rightarrow -\\infty\\). Further, \\(p = .25 \\approx \\operatorname{logit}(p) = -1.1\\) and \\(p = .1 \\approx \\operatorname{logit}(p) = -2.2\\). Thus, we might express our expectation that the initial probability of success will be low with a prior centered somewhere around -2. I propose we consider \\(\\gamma_{00} \\sim \\operatorname N(-2, 1)\\).\nNow we consider our effect size from trial to trial, \\(\\gamma_{10}\\). This should be modest on the log-odds space. Somewhere between 0 and 1 would be weakly regularizing. I propose we consider \\(\\gamma_{10} \\sim \\operatorname N(0.25, 0.25)\\).\nTo get a sense of what these priors would predict, let’s do a prior predictive check. McElreath covered this strategy extensively in his (2020) text. Here we’ll simulate 1,000 draws using these two priors, given 25 trials.\n\n# how many simulations?\nn &lt;- 1e3\n\nset.seed(6)\n\ntibble(iter = 1:n,\n       # notice we set the parameters on the log-odds scale\n       b0   = rnorm(n, mean = -2, sd = 1),\n       b1   = rnorm(n, mean = 0.25, sd = 0.25)) %&gt;% \n  expand_grid(trial = seq(from = 0, to = 24, length.out = 50)) %&gt;% \n  # here we use the inverse logit function to convert the liner model to the probability metric\n  mutate(y = inv_logit_scaled(b0 + b1 * trial)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = trial, y = y, group = iter)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  geom_line(linewidth = 1/4, alpha = 1/4) +\n  scale_y_continuous(expression(italic(p[i][j])), \n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\"),\n                     expand = c(0, 0), limits = 0:1) +\n  labs(subtitle = \"prior predictive check\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo my eye, this is about what we want. Our \\(\\gamma_{00} \\sim \\operatorname N(-2, 1)\\) prior concentrated the probability values close to zero at the start, but it was permissive enough to allow for the possibility that the success probability could be nearly as high as .5, or so. Also, our \\(\\gamma_{10} \\sim \\operatorname N(0.25, 0.25)\\) prior was largely consistent with moderate growth across trials, but it was not so heavy-handed that it prevented the possibility of no growth or even getting worse, over time.`\nEspecially if you’re new to this, see what happens if you fool with the priors, a little.\nNext, we’ll need to think about the priors for our \\(\\sigma\\)’s. Since we generally like those to be only weakly regularizing, I suggest we continue on with \\(\\operatorname{Exponential}(1)\\), which puts an expected value at 1, easily allows for values as low as zero, and gently discourages very large values. As to the \\(\\mathbf \\Omega\\) matrix, we’ll stick with the good old \\(\\operatorname{LKJ}(4)\\).\nThus, we can express the full model as\n\\[\n\\begin{align}\ny_{ij} & \\sim \\operatorname{Bernoulli}(p_{ij}) \\\\\n\\operatorname{logit}(p_{ij}) & = \\gamma_{00} + \\gamma_{10} \\text{time}_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij} \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} &\n\\sim \\operatorname{Normal}(\\mathbf 0, \\mathbf D \\mathbf \\Omega \\mathbf D') \\\\\n\\mathbf D & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf\\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\\n\\rho_{10} & 1 \\end{bmatrix} \\\\\n\\gamma_{00} & \\sim \\operatorname{Normal}(-2, 1) \\\\\n\\gamma_{10} & \\sim \\operatorname{Normal}(0.25, 0.25) \\\\\n\\sigma_0, \\sigma_1 & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf \\Omega & \\sim \\operatorname{LKJ}(4).\n\\end{align}\n\\]\nTo fit this model with brms, the big new trick is setting family = bernoulli. By default, brm() will use the logit link.\n\nfit6.18 &lt;- brm(\n  data = dogs,\n  family = bernoulli,\n  y ~ 0 + Intercept + trial + (1 + trial | dog),\n  prior = c(prior(normal(-2, 1), class = b, coef = Intercept),\n            prior(normal(0.25, 0.25), class = b),\n            prior(exponential(1), class = sd),\n            prior(lkj(4), class = cor)),\n  iter = 2000, warmup = 1000, cores = 3, chains = 3,\n  seed = 6,\n  file = \"fits/fit06.18\")\n\nCheck the parameter summary.\n\nprint(fit6.18)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ 0 + Intercept + trial + (1 + trial | dog) \n   Data: dogs (Number of observations: 750) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~dog (Number of levels: 30) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.36      0.25     0.02     0.92 1.00     1057     1766\nsd(trial)                0.09      0.03     0.04     0.14 1.00      811     1150\ncor(Intercept,trial)    -0.07      0.33    -0.66     0.60 1.00      760     1336\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -2.34      0.24    -2.82    -1.86 1.00     3111     1834\ntrial         0.35      0.03     0.29     0.42 1.00     2268     1782\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt can take a while to make sense of the output of a logistic regression model, multilevel or otherwise. Perhaps for now, you might compare the sizes of our posterior means with the priors we used. To me, it’s helpful to see what this model predicts. For that, we’ll want to extract the posterior draws.\n\ndraws &lt;- as_draws_df(fit6.18)\n\nIf we focus on the \\(\\gamma\\)’s, here’s the population trajectory in \\(p\\), across the trials.\n\ndraws %&gt;% \n  select(b_Intercept, b_trial) %&gt;% \n  expand_grid(trial = seq(from = 0, to = 25, by = 0.1)) %&gt;% \n  # the magic happens here\n  mutate(prob = inv_logit_scaled(b_Intercept + b_trial * trial)) %&gt;% \n  \n  ggplot(aes(x = trial, y = prob)) +\n  geom_hline(yintercept = 0:1, color = \"white\") +\n  stat_lineribbon(.width = 0.95, fill = alpha(\"grey67\", 0.5)) +\n  scale_y_continuous(\"probability of success\", breaks = 0:1, limits = 0:1) +\n  labs(title = \"The population-level learning curve\",\n       subtitle = expression(We~get~the~overall~success~probability~over~time~with~logit^{-1}*(gamma[0][0]+gamma[1][0]*italic(trial[j]))*'.')) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIn the mutate() line, note our use of the brms::inv_logit_scaled() function (i.e., the \\(\\operatorname{logit}^{-1}\\) function). That line was the equivalent of \\(p_j = \\operatorname{logit}^{-1}(\\gamma_{00} + \\gamma_{10} \\text{time}_j)\\). Another thing to keep in mind: The conventional Gaussian regression paradigm, particularly when using the Singer and Willet style \\(\\epsilon_i \\sim \\operatorname{N}(0, \\sigma_\\epsilon^2)\\) notation, can give you a sense that the regression models are supposed to be in the metric of the criterion. That is, we expect to see \\(y_{ij}\\) on the \\(y\\)-axis of our trajectory plots. That isn’t the case when we switch to logistic regression. Now, we’re predicting \\(p(y_{ij} = 1)\\), not \\(y_{ij}\\) itself. This is why our trajectory can take on all values between 0 and 1, whereas our criterion \\(y_{ij}\\) can only take on 0’s and 1’s.\nAnother question we might ask of our model is at what point across the trial axis do we cross the threshold of a \\(p = .5\\) success rate? For a simple model, like ours, the formula for that threshold is \\(-\\gamma_{00}/ \\gamma_{10}\\). Here’s how to compute and summarize the threshold with our posterior draws.\n\nthreshold &lt;- draws %&gt;%\n  transmute(threshold = -b_Intercept / b_trial) %&gt;% \n  mean_qi()\n\nthreshold\n\n# A tibble: 1 × 6\n  threshold .lower .upper .width .point .interval\n      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1      6.72   5.76   7.70   0.95 mean   qi       \n\n\nThe population threshold is about 6.7 trials, give or take one. Here’s what that looks like in a plot.\n\n# adjust for plotting\nthreshold &lt;- threshold %&gt;% \n  expand_grid(prob = 0:1)\n\n# wrangle\ndraws %&gt;% \n  select(b_Intercept, b_trial) %&gt;% \n  expand_grid(trial = seq(from = 0, to = 25, by = 0.1)) %&gt;% \n  mutate(prob = inv_logit_scaled(b_Intercept + b_trial * trial)) %&gt;% \n  \n  # plot\n  ggplot(aes(x = trial, y = prob)) +\n  geom_hline(yintercept = 0:1, color = \"white\") +\n  geom_hline(yintercept = 0.5, linetype = 2, color = \"white\") +\n  stat_lineribbon(.width = 0.95, fill = alpha(\"grey67\", 0.5)) +\n  geom_smooth(data = threshold,\n              aes(x = threshold, xmin = .lower, xmax = .upper),\n              stat = \"identity\",\n              fill = \"black\", color = \"black\", alpha = 1/8) +\n  scale_y_continuous(\"probability of success\", breaks = 0:2 / 2, labels = c(\"0\", \".5\", \"1\")) +\n  labs(title = \"The population-level threshold\",\n       subtitle = expression(-gamma[0][0]/gamma[1][0]*\" marks the typical half-way point for learning.\"),\n       x = \"trial\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe plot is a little cumbersome, but hopefully it clarifies the idea.\nNext, we might want to better understand our model at the level of the dogs. As a first step, we might make a plot of the 30 dog-level \\(\\pi\\) parameters.\n\n# wrangle\npi &lt;- rbind(\n  # pi0\n  coef(fit6.18, summary = F)$dog[, , \"Intercept\"],\n  # pi1\n  coef(fit6.18, summary = F)$dog[, , \"trial\"]\n) %&gt;% \n  data.frame() %&gt;% \n  set_names(1:30) %&gt;% \n  mutate(draw = rep(1:3000, times = 2),\n         pi   = rep(c(\"p0\", \"p1\"), each = n() / 2)) %&gt;% \n  pivot_longer(-c(draw, pi)) %&gt;% \n  pivot_wider(names_from = pi, values_from = value) \n\n# plot\npi %&gt;% \n  ggplot(aes(x = p0, y = p1, group = name)) +\n  stat_ellipse(geom = \"polygon\", level = 0.1, alpha = 1/2) +\n  labs(title = \"Dog-level intercepts and slopes\",\n       subtitle = \"Each ellipse marks off the 10% posterior interval.\",\n       x = expression(pi[0][italic(i)]~(log~odds)),\n       y = expression(pi[1][italic(i)]~(log~odds))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe full bivariate posterior for each dog’s \\(\\pi\\)’s is wider than we’ve depicted with our little 10% ellipses. But even those ellipses give you a sense of their distributions better than simple posterior mean points. It can be hard to interpret these, directly, since both the axes are on the log-odds scale. But hopefully you can at least get a sense of two things: There’s more variability across the \\(\\pi_{0i}\\) parameters than across the \\(\\pi_{1i}\\) parameters (look at the ranges in the axes). Also, there is more variability within the \\(\\pi_{0i}\\) parameters than within the \\(\\pi_{1i}\\) parameters (look at the shapes of the ellipses, themselves). This is the essence of the posterior means and standard deviations of the level-2 \\(\\sigma\\) parameters.\n\nposterior_summary(fit6.18)[3:4, 1:2]\n\n                    Estimate  Est.Error\nsd_dog__Intercept 0.35953181 0.25149432\nsd_dog__trial     0.08505759 0.02590135\n\n\nAll this is still quite abstract. It might be helpful if we plotted a handful of the dog-specific trajectories.\n\nnd &lt;- crossing(\n  dog   = subset,\n  trial = seq(from = 0, to = 25, length.out = 100))\n\nfitted(fit6.18, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n\n  ggplot(aes(x = trial)) +\n  geom_hline(yintercept = 0:1, color = \"white\") +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = dogs %&gt;% filter(dog %in% subset),\n             aes(y = y)) +\n  scale_y_continuous(\"y\", breaks = 0:1) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ dog, ncol = 4, labeller = label_both)\n\n\n\n\n\n\n\n\nKeep in mind that whereas the \\(y\\)-axis is on the scale of the data, the trajectories themselves are actually of \\(p_{ij}\\). To my eye, the dog-level trajectories all look very similar. It might be easier to detect the variation across the dogs by way of their thresholds. To compute the dog-level thresholds, we use the formula \\(-\\pi_{0i} / \\pi_{1i}\\).\n\npi %&gt;% \n  mutate(threshold = -p0 / p1) %&gt;% \n  group_by(name) %&gt;% \n  tidybayes::mean_qi(threshold) %&gt;% \n  \n  ggplot(aes(x = threshold, xmin = .lower, xmax = .upper, y = reorder(name, threshold))) +\n  geom_pointrange(size = 1/5) +\n  scale_y_discrete(\"dogs, ranked by threshold\", breaks = NULL) +\n  xlim(0, 25) +\n  labs(title = expression(50*'%'~threshold),\n       subtitle = expression(-pi[0][italic(i)]/pi[1][italic(i)]*\" marks the point where the \"*italic(i)*\"th dog has acquired the skill half way.\"),\n       x = \"trial\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe dog-specific thresholds range from \\(5 \\pm 2\\) to \\(12 \\pm 3.5\\). Here’s what it looks like if we adjust our trajectory plots to include the threshold information.\n\n# compute the dog-level thresholds\nthreshold &lt;- pi %&gt;% \n  mutate(threshold = -p0 / p1) %&gt;% \n  group_by(name) %&gt;% \n  tidybayes::mean_qi(threshold) %&gt;% \n  mutate(dog = name %&gt;% as.double()) %&gt;% \n  select(dog, threshold) %&gt;% \n  filter(dog %in% subset) %&gt;% \n  expand(nesting(dog, threshold),\n         point = 1:3) %&gt;% \n  mutate(trial = ifelse(point == 1, -Inf, threshold),\n         prob  = ifelse(point == 3, -Inf, 0.5))\n\n# go through the usual fitted() steps\nnd &lt;- crossing(\n  dog   = subset,\n  trial = seq(from = 0, to = 25, length.out = 100))\n\nfitted(fit6.18, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n\n  # plot!\n  ggplot(aes(x = trial)) +\n  geom_hline(yintercept = 0:1, color = \"white\") +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_path(data = threshold,\n            aes(y = prob),\n            linetype = 2) +\n  scale_y_continuous(\"success probability\", breaks = 0:2 / 2, labels = c(\"0\", \".5\", \"1\")) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ dog, ncol = 4, labeller = label_both)\n\n\n\n\n\n\n\n\nTo keep things simple, we only focused on threshold posterior means.\n\n\n6.5.5 You may want more\nIf you’d like more practice with logistic regression, or with other aspects of the generalized linear model, here are a few resources to consider:\n\nGelman & Hill (2006) covered the generalized linear model, including logistic regression, in both single-level and multilevel contexts. You can find their supplemental materials at https://www.stat.columbia.edu/~gelman/arm/software/. Heads up: some of their Bayesian model code is getting a dated.\nGelman et al. (2020) is something of a second edition of the first half of Gelman & Hill (2006). This text covered the generalized linear model, but only from a single-level context. You can find supporting materials at https://avehtari.github.io/ROS-Examples/.\nBoth editions of McElreath’s text (2015, 2020) cover the generalized linear model, including logistic regression, from both single-level and multilevel contexts. You can find all kinds of supporting material at https://xcelab.net/rm/statistical-rethinking/.\nI have tidyverse + brms ebook translations of both of McElreath’s texts (Kurz, 2026b, 2026a). I’m also slowly working through Gelman et al. (2020), which you can find on GitHub at https://github.com/ASKurz/Working-through-Regression-and-other-stories.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#session-info",
    "href": "06.html#session-info",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 GGally_2.4.0    brms_2.23.0     Rcpp_1.1.0      patchwork_1.3.2 lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n  [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          mgcv_1.9-3              loo_2.9.0.9000         \n [11] systemfonts_1.3.1       vctrs_0.6.5             reshape2_1.4.5          arrayhelpers_1.1-0      pkgconfig_2.0.3        \n [16] crayon_1.5.3            fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          utf8_1.2.6             \n [21] rmarkdown_2.30          tzdb_0.5.0              ragg_1.5.0              bit_4.6.0               xfun_0.55              \n [26] jsonlite_2.0.0          uuid_1.2-1              parallel_4.5.1          R6_2.6.1                stringi_1.8.7          \n [31] RColorBrewer_1.1-3      StanHeaders_2.36.0.9000 estimability_1.5.1      rstan_2.36.0.9000       knitr_1.51             \n [36] zoo_1.8-14              bayesplot_1.15.0.9000   Matrix_1.7-3            splines_4.5.1           timechange_0.3.0       \n [41] tidyselect_1.2.1        rstudioapi_0.17.1       abind_1.4-8             codetools_0.2-20        curl_7.0.0             \n [46] pkgbuild_1.4.8          lattice_0.22-7          plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1   \n [51] S7_0.2.1                flextable_0.9.10        askpass_1.2.1           posterior_1.6.1.9000    coda_0.19-4.1          \n [56] evaluate_1.0.5          survival_3.8-3          ggstats_0.11.0          RcppParallel_5.1.11-1   ggdist_3.3.3           \n [61] zip_2.3.3               xml2_1.4.0              pillar_1.11.1           tensorA_0.36.2.1        checkmate_2.3.3        \n [66] stats4_4.5.1            distributional_0.5.0    generics_0.1.4          vroom_1.6.6             hms_1.1.4              \n [71] rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4            glue_1.8.0              gdtools_0.4.4          \n [76] emmeans_1.11.2-8        tools_4.5.1             data.table_1.17.8       mvtnorm_1.3-3           grid_4.5.1             \n [81] QuickJSR_1.8.1          nlme_3.1-168            cli_3.6.5               textshaping_1.0.4       officer_0.7.2          \n [86] svUnit_1.0.8            fontBitstreamVera_0.1.1 viridisLite_0.4.2       Brobdingnag_1.2-9       V8_8.0.1               \n [91] gtable_0.3.6            digest_0.6.39           fontquiver_0.2.1        TH.data_1.1-4           htmlwidgets_1.6.4      \n [96] farver_2.1.2            htmltools_0.5.9         lifecycle_1.0.5         fontLiberation_0.1.0    openssl_2.3.4          \n[101] bit64_4.6.0-1           MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "06.html#comments",
    "href": "06.html#comments",
    "title": "6  Modeling Discontinuous and Nonlinear Change",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBürkner, P.-C. (2021). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nHead, R., & Pike, D. (1975). A review of response surface methodology from a biometric point of view. Biometrics. Journal of the International Biometric Society, 31, 803–851.\n\n\nKeiley, M. K., Bates, J. E., Dodge, K. A., & Pettit, G. S. (2000). A cross-domain growth analysis: Externalizing and internalizing behaviors during 8 years of childhood. Journal of Abnormal Child Psychology, 28(2), 161–179. https://doi.org/10.1023/A:1005122814723\n\n\nKurz, A. S. (2026a). Statistical rethinking 2 with brms and the tidyverse (version 0.5.0). https://solomon.quarto.pub/sr2/\n\n\nKurz, A. S. (2026b). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0). https://solomon.quarto.pub/sr/\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nSandberg, D. E., Meyer-Bahlburg, H. F. L., & Yager, T. J. (1991). The Child Behavior Checklist nonclinical standardization samples: Should they be utilized as norms? Journal of the American Academy of Child & Adolescent Psychiatry, 30(1), 124–134. https://doi.org/10.1097/00004583-199101000-00019\n\n\nSchloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., & Larmarange, J. (2021). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modeling Discontinuous and Nonlinear Change</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  Examining the Multilevel Model’s Error Covariance Structure",
    "section": "",
    "text": "7.1 The “standard” specification of the multilevel model for change\nLoad the opposites_pp data (Willett, 1988).\nlibrary(tidyverse)\n\nopposites_pp &lt;- read_csv(\"~/Dropbox/Recoding Applied Longitudinal Data Analysis/data/opposites_pp.csv\")\n\nglimpse(opposites_pp)\n\nRows: 140\nColumns: 6\n$ id   &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 10, 10,…\n$ time &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2…\n$ opp  &lt;dbl&gt; 205, 217, 268, 302, 219, 243, 279, 302, 142, 212, 250, 289, 206, 230, 248, 273, 190, 220, 229, 220, 165, 205, 207, …\n$ cog  &lt;dbl&gt; 137, 137, 137, 137, 123, 123, 123, 123, 129, 129, 129, 129, 125, 125, 125, 125, 81, 81, 81, 81, 110, 110, 110, 110,…\n$ ccog &lt;dbl&gt; 23.5429, 23.5429, 23.5429, 23.5429, 9.5429, 9.5429, 9.5429, 9.5429, 15.5429, 15.5429, 15.5429, 15.5429, 11.5429, 11…\n$ wave &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3…\nWillett (1988) clarified these data were simulated for pedagogical purposes, which will become important later on. For now, here’s our version of Table 7.1.\nopposites_pp %&gt;% \n  mutate(name = str_c(\"opp\", wave)) %&gt;% \n  select(id, name, opp, cog) %&gt;% \n  \n  pivot_wider(names_from = name, values_from = opp) %&gt;% \n  select(id, starts_with(\"opp\"), cog) %&gt;% \n  head(n = 10)\n\n# A tibble: 10 × 6\n      id  opp1  opp2  opp3  opp4   cog\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1   205   217   268   302   137\n 2     2   219   243   279   302   123\n 3     3   142   212   250   289   129\n 4     4   206   230   248   273   125\n 5     5   190   220   229   220    81\n 6     6   165   205   207   263   110\n 7     7   170   182   214   268    99\n 8     8    96   131   159   213   113\n 9     9   138   156   197   200   104\n10    10   216   252   274   298    96\nThe data are composed of 35 people’s scores on a cognitive task, across 4 time points.\nopposites_pp %&gt;% \n  count(id)\n\n# A tibble: 35 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     4\n 2     2     4\n 3     3     4\n 4     4     4\n 5     5     4\n 6     6     4\n 7     7     4\n 8     8     4\n 9     9     4\n10    10     4\n# ℹ 25 more rows\nOur first model will serve as a comparison model for all that follow. We’ll often refer to it as the standard multilevel model for change. You’ll see why in a bit. It follows the form\n\\[\n\\begin{align}\n\\text{opp}_{ij} & = \\pi_{0i} + \\pi_{1i} \\text{time}_{ij} + \\epsilon_{ij}\\\\\n\\pi_{0i} & = \\gamma_{00} + \\gamma_{01} (\\text{cog}_i - \\overline{\\text{cog}}) + \\zeta_{0i}\\\\\n\\pi_{1i} & = \\gamma_{10} + \\gamma_{11} (\\text{cog}_i - \\overline{\\text{cog}}) + \\zeta_{1i} \\\\\n\\epsilon_{ij} & \\stackrel{iid}{\\sim} \\operatorname{Normal}(0, \\sigma_\\epsilon) \\\\\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix} & \\stackrel{iid}{\\sim} \\operatorname{Normal} \\left (\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf D \\mathbf\\Omega \\mathbf D' \\right ) \\\\\n\\mathbf D      & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf \\Omega & = \\begin{bmatrix} 1 & \\rho_{01} \\\\ \\rho_{01} & 1 \\end{bmatrix},\n\\end{align}\n\\]\nwhere the \\((\\text{cog}_i - \\overline{\\text{cog}})\\) notation is meant to indicate we are mean centering the time-invariant covariate \\(\\text{cog}_i\\). In the data, the mean-centered version is saved as ccog.\nopposites_pp %&gt;% \n  filter(wave == 1) %&gt;% \n  \n  ggplot(aes(ccog)) +\n  geom_histogram(binwidth = 5) +\n  theme(panel.grid = element_blank())\nTo keep things simple, we’ll be using brms default priors for all the models in this chapter. Fit the model.\nlibrary(brms)\n\nfit7.1 &lt;- brm(\n  data = opposites_pp, \n  family = gaussian,\n  opp ~ 0 + Intercept + time + ccog + time:ccog + (1 + time | id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.01\")\nCheck the summary.\nprint(fit7.1, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: opp ~ 0 + Intercept + time + ccog + time:ccog + (1 + time | id) \n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 35) \n                    Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsd(Intercept)         36.119     4.933   27.567   46.837 1.001     1249     1827\nsd(time)              10.575     1.808    7.466   14.570 1.000     1289     2344\ncor(Intercept,time)   -0.438     0.167   -0.711   -0.074 1.001     1501     2221\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept  164.282     6.464  151.454  176.704 1.006      828     1301\ntime        26.962     2.093   22.917   31.209 1.003     1558     2290\nccog        -0.144     0.520   -1.179    0.888 1.004     1274     2011\ntime:ccog    0.439     0.170    0.104    0.770 1.001     1781     2433\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma   12.970     1.153   10.947   15.365 1.001     1727     2537\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nIf you’re curious, here’s the summary of our \\(\\sigma\\) parameters transformed to the variance/covariance metric.\nlibrary(tidybayes)\n\nlevels &lt;- c(\"sigma[epsilon]^2\", \"sigma[0]^2\", \"sigma[1]^2\", \"sigma[0][1]\")\n\nsigma &lt;- as_draws_df(fit7.1) %&gt;% \n  transmute(`sigma[0]^2` = sd_id__Intercept^2,\n            `sigma[1]^2` = sd_id__time^2,\n            `sigma[epsilon]^2` = sigma^2,\n            `sigma[0][1]` = sd_id__Intercept * sd_id__time * cor_id__Intercept__time)\n\nsigma %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 4 × 7\n  name             value .lower .upper .width .point .interval\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 sigma[epsilon]^2  166.  120.   236.    0.95 median qi       \n2 sigma[0]^2       1274.  760.  2194.    0.95 median qi       \n3 sigma[1]^2        109.   55.8  212.    0.95 median qi       \n4 sigma[0][1]      -163. -394.   -22.4   0.95 median qi",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Examining the Multilevel Model's Error Covariance Structure</span>"
    ]
  },
  {
    "objectID": "07.html#using-the-composite-model-to-understand-assumptions-about-the-error-covariance-matrix",
    "href": "07.html#using-the-composite-model-to-understand-assumptions-about-the-error-covariance-matrix",
    "title": "7  Examining the Multilevel Model’s Error Covariance Structure",
    "section": "7.2 Using the composite model to understand assumptions about the error covariance matrix",
    "text": "7.2 Using the composite model to understand assumptions about the error covariance matrix\nDropping the terms specifying the distributional assumptions, we can reexpress the model formula, from above, in the composite format as\n\\[\n\\begin{align}\n\\text{opp}_{ij} & = [\\gamma_{00} + \\gamma_{10} \\text{time}_{ij} + \\gamma_{01} (\\text{cog}_i - \\overline{\\text{cog}}) + \\gamma_{11} (\\text{cog}_i - \\overline{\\text{cog}}) \\times \\text{time}_{ij}] \\\\\n& \\;\\;\\; + [\\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij} + \\epsilon_{ij}],\n\\end{align}\n\\]\nwhere we’ve divided up the structural and stochastic components with our use of brackets. We might think of the terms of the stochastic portion as a composite residual, \\(r_{ij} = [\\epsilon_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{time}_{ij}]\\). Thus, we can rewrite the composite equation with the composite residual term \\(r_{ij}\\) as\n\\[\n\\text{opp}_{ij} = [\\gamma_{00} + \\gamma_{10} \\text{time}_{ij} + \\gamma_{01} (\\text{cog}_i - \\overline{\\text{cog}}) + \\gamma_{11} (\\text{cog}_i - \\overline{\\text{cog}}) \\times \\text{time}_{ij}] + r_{ij}.\n\\]\nIf we were willing to presume, as in OLS or single-level Bayesian regression, that all residuals are independent and normally distributed, we could express that in statistical notation as\n\\[\\begin{align}\n\\begin{bmatrix} r_{11} \\\\ r_{12} \\\\ r_{13} \\\\ r_{14} \\\\ r_{21} \\\\ r_{22} \\\\ r_{23} \\\\ r_{24} \\\\ \\vdots \\\\ r_{n1} \\\\ r_{n2} \\\\ r_{n3} \\\\ r_{n4} \\end{bmatrix} & \\sim \\mathcal{N}\n\n\\begin{pmatrix}\n\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix},\n\n\\begin{bmatrix}\n\\sigma_r^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & \\sigma_r^2 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_r^2 & 0 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma_r^2 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n\n0 & 0 & 0 & 0 & \\sigma_r^2 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma_r^2 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\sigma_r^2 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_r^2 & \\dots & 0 & 0 & 0 & 0 \\\\\n\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\\\\n\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & \\sigma_r^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & 0 & \\sigma_r^2 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & \\sigma_r^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & \\sigma_r^2\n\\end{bmatrix}\n\n\\end{pmatrix},\n\\end{align}\\]\nwhere \\(r_{ij}\\) is the \\(i\\)th person’s residual on the \\(j\\)th time point. The variance/covariance matrix \\(\\mathbf \\Sigma\\) is diagonal (i.e., all the off-diagonal elements are 0’s) and homoscedastic (i.e., all the diagonal elements are the same value, \\(\\sigma_r^2\\)).\nThese assumptions are absurd for longitudinal data, which is why we don’t analyze such data with single-level models. If we were to make the less restrictive assumptions that, within people, the residuals were correlated over time and were heteroscedastic, we could express that as\n\\[\\begin{align}\n\\begin{bmatrix} r_{11} \\\\ r_{12} \\\\ r_{13} \\\\ r_{14} \\\\ r_{21} \\\\ r_{22} \\\\ r_{23} \\\\ r_{24} \\\\ \\vdots \\\\ r_{n1} \\\\ r_{n2} \\\\ r_{n3} \\\\ r_{n4} \\end{bmatrix} & \\sim \\mathcal{N}\n\n\\begin{pmatrix}\n\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix},\n\n\\begin{bmatrix}\n\\sigma_{r_1}^2 & \\sigma_{r_1 r_2} & \\sigma_{r_1 r_3} & \\sigma_{r_1 r_4} & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n\\sigma_{r_2 r_1} & \\sigma_{r_2}^2 & \\sigma_{r_2 r_3} & \\sigma_{r_2 r_4} & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n\\sigma_{r_3 r_1} & \\sigma_{r_3 r_2} & \\sigma_{r_3}^2 & \\sigma_{r_3 r_4} & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n\\sigma_{r_4 r_1} & \\sigma_{r_4 r_2} & \\sigma_{r_4 r_3} & \\sigma_{r_4}^2 & 0 & 0 & 0 & 0 & \\dots & 0 & 0 & 0 & 0 \\\\\n\n0 & 0 & 0 & 0 & \\sigma_{r_1}^2 & \\sigma_{r_1 r_2} & \\sigma_{r_1 r_3} & \\sigma_{r_1 r_4} & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{r_2 r_1} & \\sigma_{r_2}^2 & \\sigma_{r_2 r_3} & \\sigma_{r_2 r_4} & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{r_3 r_1} & \\sigma_{r_3 r_2} & \\sigma_{r_3}^2 & \\sigma_{r_3 r_4} & \\dots & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\sigma_{r_4 r_1} & \\sigma_{r_4 r_2} & \\sigma_{r_4 r_3} & \\sigma_{r_4}^2 & \\dots & 0 & 0 & 0 & 0 \\\\\n\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\\\\n\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & \\sigma_{r_1}^2 & \\sigma_{r_1 r_2} & \\sigma_{r_1 r_3} & \\sigma_{r_1 r_4} \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & \\sigma_{r_2 r_1} & \\sigma_{r_2}^2 & \\sigma_{r_2 r_3} & \\sigma_{r_2 r_4} \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & \\sigma_{r_3 r_1} & \\sigma_{r_3 r_2} & \\sigma_{r_3}^2 & \\sigma_{r_3 r_4} \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots & \\sigma_{r_4 r_1} & \\sigma_{r_4 r_2} & \\sigma_{r_4 r_3} & \\sigma_{r_4}^2\n\\end{bmatrix}\n\n\\end{pmatrix}.\n\\end{align}\\]\nthis kind of structure can be called block diagonal, which means the off-diagonal elements are zero between persons, but allowed to be non-zero within persons (i.e., within blocks). The zero elements between person blocks explicate how the residuals are independent between persons. Notice that the variances on the diagonal vary across the four time points (i.e., \\(\\sigma_{r_1}^2, \\dots, \\sigma_{r_4}^2\\)). Yet also notice that the block for one person is identical to the block for all others. Thus, this model allows for heterogeneity across time within persons, but homogeneity between persons.\nWe can express this in the more compact notation,\n\\[\\begin{align}\nr & \\sim \\mathcal{N}\n\\begin{pmatrix} \\mathbf 0,\n\n\\begin{bmatrix}\n\n\\mathbf{\\Sigma}_r & \\mathbf 0 & \\mathbf 0 & \\dots & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf{\\Sigma}_r & \\mathbf 0 & \\dots & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf 0 & \\mathbf{\\Sigma}_r & \\dots & \\mathbf 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf 0 & \\mathbf 0 & \\mathbf 0 & \\mathbf{\\Sigma}_r\n\n\\end{bmatrix}\n\n\\end{pmatrix}, \\;\\;\\; \\text{where} \\\\\n\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_{r_1}^2 & \\sigma_{r_1 r_2} & \\sigma_{r_1 r_3} & \\sigma_{r_1 r_4} \\\\\n  \\sigma_{r_2 r_1} & \\sigma_{r_2}^2 & \\sigma_{r_2 r_3} & \\sigma_{r_2 r_4} \\\\\n  \\sigma_{r_3 r_1} & \\sigma_{r_3 r_2} & \\sigma_{r_3}^2 & \\sigma_{r_3 r_4} \\\\\n  \\sigma_{r_4 r_1} & \\sigma_{r_4 r_2} & \\sigma_{r_4 r_3} & \\sigma_{r_4}^2 \\end{bmatrix}.\n\n\\end{align}\\]\nThe bulk of the rest of the material in this chapter will focus around how different models handle \\(\\mathbf{\\Sigma}_r\\). The standard multilevel growth model has one way. There are many others.\n\n7.2.1 Variance of the composite residual\nUnder the conventional multilevel growth model\n\\[\n\\begin{align}\n\\sigma_{r_j}^2 & = \\operatorname{Var} \\left ( \\epsilon_{ij} + \\zeta_{0i} + \\zeta_{1i} \\text{time}_j \\right ) \\\\\n& = \\sigma_\\epsilon^2 + \\sigma_0^2 + 2 \\sigma_{01} \\text{time}_j + \\sigma_1^2 \\text{time}_j^2.\n\\end{align}\n\\]\nHere’s how to use our posterior samples to compute \\(\\sigma_{r_1}^2, \\dots, \\sigma_{r_4}^2\\).\n\nsigma %&gt;% \n  mutate(iter = 1:n()) %&gt;% \n  expand(nesting(iter, `sigma[epsilon]^2`, `sigma[0]^2`, `sigma[1]^2`, `sigma[0][1]`),\n         time = 0:3) %&gt;% \n  mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * time + `sigma[1]^2` * time^2) %&gt;% \n  mutate(name = str_c(\"sigma[italic(r)[\", time + 1, \"]]^2\")) %&gt;% \n  \n  ggplot(aes(x = r, y = name)) +\n  stat_halfeye(.width = 0.95, size = 1) +\n  scale_x_continuous(\"marginal posterior\", expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  coord_cartesian(ylim = c(1.5, 4.2)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAs is often the case with variance parameters, the posteriors show marked right skew. Here are the numeric summaries.\n\nsigma %&gt;% \n  mutate(iter = 1:n()) %&gt;% \n  expand(nesting(iter, `sigma[epsilon]^2`, `sigma[0]^2`, `sigma[1]^2`, `sigma[0][1]`),\n         time = 0:3) %&gt;% \n  mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * time + `sigma[1]^2` * time^2) %&gt;% \n  mutate(name = str_c(\"sigma[italic(r)[\", time + 1, \"]]^2\")) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(r)\n\n# A tibble: 4 × 7\n  name                      r .lower .upper .width .point .interval\n  &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 sigma[italic(r)[1]]^2 1443.   929.  2365.   0.95 median qi       \n2 sigma[italic(r)[2]]^2 1219.   811.  1979.   0.95 median qi       \n3 sigma[italic(r)[3]]^2 1209.   813.  1995.   0.95 median qi       \n4 sigma[italic(r)[4]]^2 1421.   915.  2431.   0.95 median qi       \n\n\nThough our precise numeric values are different from those in the text, we see the same overall pattern. Using our posterior medians, we can update \\(\\mathbf{\\Sigma}_r\\) to\n\\[\n\\begin{align}\n\\hat{\\mathbf{\\Sigma}}_r & = \\begin{bmatrix}\n  1430 & \\hat{\\sigma}_{r_1 r_2} & \\hat{\\sigma}_{r_1 r_3} & \\hat{\\sigma}_{r_1 r_4} \\\\\n  \\hat{\\sigma}_{r_2 r_1} & 1201 & \\hat{\\sigma}_{r_2 r_3} & \\hat{\\sigma}_{r_2 r_4} \\\\\n  \\hat{\\sigma}_{r_3 r_1} & \\hat{\\sigma}_{r_3 r_2} & 1203 & \\hat{\\sigma}_{r_3 r_4} \\\\\n  \\hat{\\sigma}_{r_4 r_1} & \\hat{\\sigma}_{r_4 r_2} & \\hat{\\sigma}_{r_4 r_3} & 1416 \\end{bmatrix}.\n\\end{align}\n\\]\n\nFor the opposites-naming data, composite residual variance is greatest at the beginning and end of data collection and smaller in between. And, while not outrageously heteroscedastic, this situation is clearly beyond the bland homoscedasticity that we routinely assume for residuals in cross-sectional data. (p. 252)\n\nIf you work through the equation at the beginning of this section–which I am not going to do, here–, you’ll see that the standard multilevel growth model is set up such that the residual variance follows a quadratic function with time. To give a sense, here we plot the expected \\(\\sigma_r^2\\) values over a wider and more continuous range of time values.\n\nset.seed(7)\n\nsigma %&gt;% \n  mutate(iter = 1:n()) %&gt;% \n  slice_sample(n = 50) %&gt;% \n  expand(nesting(iter, `sigma[epsilon]^2`, `sigma[0]^2`, `sigma[1]^2`, `sigma[0][1]`),\n         time = seq(from = -4.2, to = 6.6, length.out = 200)) %&gt;% \n  mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * time + `sigma[1]^2` * time^2) %&gt;% \n  \n  ggplot(aes(x = time, y = r, group = iter)) +\n  geom_line(linewidth = 1/6, alpha = 1/2) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expression(sigma[italic(r)]^2),\n                     expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +\n  labs(subtitle = expression(\"50 posterior draws showing the quadratic shape of \"*sigma[italic(r)[time]]^2)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nSince we have 4,000 posterior draws for all the parameters, we also have 4,000 posterior draws for the quadratic curve. Here we just show 50. The curve is at its minimum at \\(\\text{time} = -(\\sigma_{01} / \\sigma_1^2)\\). Since we have posterior distributions for \\(\\sigma_{01}\\) and \\(\\sigma_1^2\\), we’ll also have a posterior distribution for the minimum point. Here it is.\n\nsigma %&gt;% \n  mutate(minimum = -`sigma[0][1]` / `sigma[1]^2`) %&gt;% \n  \n  ggplot(aes(x = minimum)) +\n  stat_halfeye(.width = 0.95) +\n  scale_x_continuous(\"time\", expand = c(0, 0), limits = c(-4.2, 6.6)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(Minimum~value~(-sigma[0][1]/sigma[1]^2))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf we plug those minimum time values into the equation for \\(\\sigma_{r_\\text{time}}^2\\), we’ll get the posterior distribution for the minimum variance value.\n\nsigma %&gt;% \n  mutate(minimum = -`sigma[0][1]` / `sigma[1]^2`) %&gt;% \n  mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * minimum + `sigma[1]^2` * minimum^2) %&gt;% \n  \n  ggplot(aes(x = r)) +\n  stat_halfeye(.width = 0.95) +\n  scale_x_continuous(expression(sigma[italic(r)[time]]^2), limits = c(0, NA)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = \"Minimum variance\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s the numeric summary.\n\nsigma %&gt;% \n  mutate(minimum = -`sigma[0][1]` / `sigma[1]^2`) %&gt;% \n  mutate(r = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * minimum + `sigma[1]^2` * minimum^2) %&gt;% \n  median_qi(r)\n\n# A tibble: 1 × 6\n      r .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 1147.   773.  1853.   0.95 median qi       \n\n\n\n\n7.2.2 Covariance of the composite residuals\nIn addition to the variances in \\(\\mathbf{\\Sigma}_r\\), we might focus on the off-diagonal covariances, too. We can define the covariance between two time points \\(\\sigma_{r_j, r_{j'}}\\) as\n\\[\n\\sigma_{r_j, r_{j'}} = \\sigma_0^2 + \\sigma_{01} (t_j + t_{j'}) + \\sigma_1^2 t_j t_{j'},\n\\]\nwhere \\(\\sigma_0^2\\), \\(\\sigma_{01}\\) and \\(\\sigma_1^2\\) all have their usual interpretation, and \\(t_j\\) and \\(t_{j'}\\) are the numeric values for whatever variable is used to index time in the model, which is time in the case of fit7.1. Here we compute and plot the marginal posteriors for all \\(4 \\times 4 = 16\\) parameters.\n\n# arrange the panels\nlevels &lt;- c(\n  \"sigma[italic(r)[1]]^2\", \"sigma[italic(r)[1]][italic(r)[2]]\", \"sigma[italic(r)[1]][italic(r)[3]]\", \"sigma[italic(r)[1]][italic(r)[4]]\", \n  \"sigma[italic(r)[2]][italic(r)[1]]\", \"sigma[italic(r)[2]]^2\", \"sigma[italic(r)[2]][italic(r)[3]]\", \"sigma[italic(r)[2]][italic(r)[4]]\", \n  \"sigma[italic(r)[3]][italic(r)[1]]\", \"sigma[italic(r)[3]][italic(r)[2]]\", \"sigma[italic(r)[3]]^2\", \"sigma[italic(r)[3]][italic(r)[4]]\", \n  \"sigma[italic(r)[4]][italic(r)[1]]\", \"sigma[italic(r)[4]][italic(r)[2]]\", \"sigma[italic(r)[4]][italic(r)[3]]\", \"sigma[italic(r)[4]]^2\")\n\n# wrangle\nsigma &lt;- sigma %&gt;% \n  mutate(\n    `sigma[italic(r)[1]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 0 + `sigma[1]^2` * 0^2,\n    `sigma[italic(r)[2]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 1 + `sigma[1]^2` * 1^2,\n    `sigma[italic(r)[3]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 2 + `sigma[1]^2` * 2^2,\n    `sigma[italic(r)[4]]^2` = `sigma[epsilon]^2` + `sigma[0]^2` + 2 * `sigma[0][1]` * 3 + `sigma[1]^2` * 3^2,\n    \n    `sigma[italic(r)[2]][italic(r)[1]]` = `sigma[0]^2` + `sigma[0][1]` * (1 + 0) + `sigma[1]^2` * 1 * 0,\n    `sigma[italic(r)[3]][italic(r)[1]]` = `sigma[0]^2` + `sigma[0][1]` * (2 + 0) + `sigma[1]^2` * 2 * 0,\n    `sigma[italic(r)[4]][italic(r)[1]]` = `sigma[0]^2` + `sigma[0][1]` * (3 + 0) + `sigma[1]^2` * 3 * 0,\n    `sigma[italic(r)[3]][italic(r)[2]]` = `sigma[0]^2` + `sigma[0][1]` * (2 + 1) + `sigma[1]^2` * 2 * 1,\n    `sigma[italic(r)[4]][italic(r)[2]]` = `sigma[0]^2` + `sigma[0][1]` * (3 + 1) + `sigma[1]^2` * 3 * 1,\n    `sigma[italic(r)[4]][italic(r)[3]]` = `sigma[0]^2` + `sigma[0][1]` * (3 + 2) + `sigma[1]^2` * 3 * 2,\n    \n    `sigma[italic(r)[1]][italic(r)[2]]` = `sigma[0]^2` + `sigma[0][1]` * (0 + 1) + `sigma[1]^2` * 0 * 1,\n    `sigma[italic(r)[1]][italic(r)[3]]` = `sigma[0]^2` + `sigma[0][1]` * (0 + 2) + `sigma[1]^2` * 0 * 2,\n    `sigma[italic(r)[2]][italic(r)[3]]` = `sigma[0]^2` + `sigma[0][1]` * (1 + 2) + `sigma[1]^2` * 1 * 2,\n    `sigma[italic(r)[1]][italic(r)[4]]` = `sigma[0]^2` + `sigma[0][1]` * (0 + 3) + `sigma[1]^2` * 0 * 3,\n    `sigma[italic(r)[2]][italic(r)[4]]` = `sigma[0]^2` + `sigma[0][1]` * (1 + 3) + `sigma[1]^2` * 1 * 3,\n    `sigma[italic(r)[3]][italic(r)[4]]` = `sigma[0]^2` + `sigma[0][1]` * (2 + 3) + `sigma[1]^2` * 2 * 3)\n\nsigma %&gt;% \n  select(contains(\"italic\")) %&gt;% \n  pivot_longer(everything()) %&gt;%  \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = 0.95, size = 1) +\n  scale_x_continuous(\"marginal posterior\", expand = expansion(mult = c(0, 0.05))) +\n  scale_y_discrete(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 4000),\n                  ylim = c(0.5, NA)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, labeller = label_parsed)\n\n\n\n\n\n\n\n\nIt might be helpful to reduce the complexity of this plot by focusing on the posterior medians. With a little help from geom_tile() and geom_text(), we’ll make a plot version of the matrix at the top of page 255 in the text.\n\nsigma %&gt;% \n  select(contains(\"italic\")) %&gt;% \n  pivot_longer(everything()) %&gt;%  \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate(label = round(value, digits = 0)) %&gt;% \n  \n  ggplot(aes(x = 0, y = 0)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label)) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, NA)) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Sigma)[italic(r)]*\" for the standard multilevel model for change\")) +\n  theme(legend.text = element_text(hjust = 1)) +\n  facet_wrap(~ name, labeller = label_parsed)\n\n\n\n\n\n\n\n\nAlthough our posterior median values differ a bit from the REML values reported in the text, the overall pattern holds. Hopefully the coloring in the plot helps highlight what Singer and Willett described as a “‘band diagonal’ structure, in which the overall magnitude of the residual covariances tends to decline in diagonal ‘bands’ the further you get from the main diagonal” (p. 255).\nOne of the consequences for this structure is that in cases where both \\(\\sigma_1^2 \\rightarrow 0\\) and \\(\\sigma_{01} \\rightarrow 0\\), the residual covariance matrix becomes compound symmetric, which is:\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_\\epsilon^2 + \\sigma_0^2 & \\sigma_0^2 & \\sigma_0^2 & \\sigma_0^2 \\\\\n  \\sigma_0^2 & \\sigma_\\epsilon^2 + \\sigma_0^2 & \\sigma_0^2 & \\sigma_0^2 \\\\\n  \\sigma_0^2 & \\sigma_0^2 & \\sigma_\\epsilon^2 + \\sigma_0^2 & \\sigma_0^2 \\\\\n  \\sigma_0^2 & \\sigma_0^2 & \\sigma_0^2 & \\sigma_\\epsilon^2 + \\sigma_0^2 \\end{bmatrix}.\n\\end{align}\n\\]\n\nCompound symmetric error covariance structures are particularly common in longitudinal data, especially if the slopes of the change trajectories do not differ much asroc people. Regardless of these special cases, however, the most sensible question to ask of your data is whether the error covariance structure that the “standard” multilevel model for change demands is realistic when applied to data in practice? The answer to this question will determine whether the standard model can be applied ubiquitously, as question we soon address. (pp. 255–256)\n\n\n\n7.2.3 Autocorrelation of the composite residuals\nWe can use the following equation to convert our \\(\\mathbf{\\Sigma}_r\\) into a correlation matrix:\n\\[\\rho_{r_j r_{j^\\prime}} = \\sigma_{r_j r_{j^\\prime}} \\Big / \\sqrt{\\sigma_{r_j}^2 \\sigma_{r_{j^\\prime}}^2}.\\]\nHere we use the formula and plot the posteriors.\n\n# arrange the panels\nlevels &lt;- c(\n  \"sigma[italic(r)[1]]\", \"rho[italic(r)[1]][italic(r)[2]]\", \"rho[italic(r)[1]][italic(r)[3]]\", \"rho[italic(r)[1]][italic(r)[4]]\", \n  \"rho[italic(r)[2]][italic(r)[1]]\", \"sigma[italic(r)[2]]\", \"rho[italic(r)[2]][italic(r)[3]]\", \"rho[italic(r)[2]][italic(r)[4]]\", \n  \"rho[italic(r)[3]][italic(r)[1]]\", \"rho[italic(r)[3]][italic(r)[2]]\", \"sigma[italic(r)[3]]\", \"rho[italic(r)[3]][italic(r)[4]]\", \n  \"rho[italic(r)[4]][italic(r)[1]]\", \"rho[italic(r)[4]][italic(r)[2]]\", \"rho[italic(r)[4]][italic(r)[3]]\", \"sigma[italic(r)[4]]\")\n\nsigma &lt;- sigma %&gt;% \n  select(contains(\"italic\")) %&gt;% \n  mutate(\n    `sigma[italic(r)[1]]` = `sigma[italic(r)[1]]^2` / sqrt(`sigma[italic(r)[1]]^2`^2),\n    `rho[italic(r)[2]][italic(r)[1]]` = `sigma[italic(r)[2]][italic(r)[1]]` / sqrt(`sigma[italic(r)[2]]^2` * `sigma[italic(r)[1]]^2`),\n    `rho[italic(r)[3]][italic(r)[1]]` = `sigma[italic(r)[3]][italic(r)[1]]` / sqrt(`sigma[italic(r)[3]]^2` * `sigma[italic(r)[1]]^2`),\n    `rho[italic(r)[4]][italic(r)[1]]` = `sigma[italic(r)[4]][italic(r)[1]]` / sqrt(`sigma[italic(r)[4]]^2` * `sigma[italic(r)[1]]^2`),\n    \n    `rho[italic(r)[1]][italic(r)[2]]` = `sigma[italic(r)[1]][italic(r)[2]]` / sqrt(`sigma[italic(r)[1]]^2` * `sigma[italic(r)[2]]^2`),\n    `sigma[italic(r)[2]]` = `sigma[italic(r)[2]]^2` / sqrt(`sigma[italic(r)[2]]^2`^2),\n    `rho[italic(r)[3]][italic(r)[2]]` = `sigma[italic(r)[3]][italic(r)[2]]` / sqrt(`sigma[italic(r)[3]]^2` * `sigma[italic(r)[2]]^2`),\n    `rho[italic(r)[4]][italic(r)[2]]` = `sigma[italic(r)[4]][italic(r)[2]]` / sqrt(`sigma[italic(r)[4]]^2` * `sigma[italic(r)[2]]^2`),\n    \n    `rho[italic(r)[1]][italic(r)[3]]` = `sigma[italic(r)[1]][italic(r)[3]]` / sqrt(`sigma[italic(r)[1]]^2` * `sigma[italic(r)[3]]^2`),\n    `rho[italic(r)[2]][italic(r)[3]]` = `sigma[italic(r)[2]][italic(r)[3]]` / sqrt(`sigma[italic(r)[2]]^2` * `sigma[italic(r)[3]]^2`),\n    `sigma[italic(r)[3]]` = `sigma[italic(r)[3]]^2` / sqrt(`sigma[italic(r)[3]]^2`^2),\n    `rho[italic(r)[4]][italic(r)[3]]` = `sigma[italic(r)[4]][italic(r)[3]]` / sqrt(`sigma[italic(r)[4]]^2` * `sigma[italic(r)[3]]^2`),\n    \n    `rho[italic(r)[1]][italic(r)[4]]` = `sigma[italic(r)[1]][italic(r)[4]]` / sqrt(`sigma[italic(r)[1]]^2` * `sigma[italic(r)[4]]^2`),\n    `rho[italic(r)[2]][italic(r)[4]]` = `sigma[italic(r)[2]][italic(r)[4]]` / sqrt(`sigma[italic(r)[2]]^2` * `sigma[italic(r)[4]]^2`),\n    `rho[italic(r)[3]][italic(r)[4]]` = `sigma[italic(r)[3]][italic(r)[4]]` / sqrt(`sigma[italic(r)[3]]^2` * `sigma[italic(r)[4]]^2`),\n    `sigma[italic(r)[4]]` = `sigma[italic(r)[4]]^2` / sqrt(`sigma[italic(r)[4]]^2`^2))\n\nsigma %&gt;% \n  select(`sigma[italic(r)[1]]`:`sigma[italic(r)[4]]`) %&gt;% \n  pivot_longer(everything()) %&gt;%  \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = 0.95, size = 1) +\n  scale_x_continuous(\"marginal posterior\", expand = c(0, 0),\n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\")) +\n  scale_y_discrete(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 1),\n                  ylim = c(0.5, NA)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, labeller = label_parsed)\n\n\n\n\n\n\n\n\nAs before, it might be helpful to reduce the complexity of this plot by focusing on the posterior medians. We’ll make a plot version of the correlation matrix in the middle of page 256 in the text.\n\nsigma %&gt;% \n  select(`sigma[italic(r)[1]]`:`sigma[italic(r)[4]]`) %&gt;% \n  pivot_longer(everything()) %&gt;%  \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate(label = round(value, digits = 2)) %&gt;% \n  \n  ggplot(aes(x = 0, y = 0)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label)) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, 1)) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Omega)[italic(r)]*\" for the standard multilevel model for change\")) +\n  theme(legend.text = element_text(hjust = 1)) +\n  facet_wrap(~ name, labeller = label_parsed)\n\n\n\n\n\n\n\n\nThe correlation matrix has an even more pronounced band-diagonal structure. Thus even though the standard multilevel model of change does not contain an explicit autocorrelation parameter \\(\\rho\\), the model does account for residual autocorrelation. We might take this even further. Notice the parameters \\(\\rho_{r_2, r_1}\\), \\(\\rho_{r_3, r_2}\\), and \\(\\rho_{r_4, r_3}\\) are all autocorrelations of the first order, and further note how similar their posterior medians all are. Here’s a summary of the average of those three parameters, which we’ll just call \\(\\rho\\).\n\nsigma %&gt;% \n  # take the average of the three parameters\n  transmute(rho = (`rho[italic(r)[2]][italic(r)[1]]` + `rho[italic(r)[3]][italic(r)[2]]` + `rho[italic(r)[4]][italic(r)[3]]`) / 3) %&gt;% \n  # summarize\n  median_qi()\n\n# A tibble: 1 × 6\n    rho .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.828  0.740  0.897   0.95 median qi       \n\n\nIf you look at the “estimate” column in Table 7.3 (pp. 258–259), you’ll see the \\(\\hat \\rho\\) values for the autoregressive and heterogeneous-autoregressive models are very similar to the \\(\\hat \\rho\\) posterior we just computed for the standard multilevel model of change. The standard multilevel model of change accounts for autocorrelation, but it does so without an explicit \\(\\rho\\) parameter.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Examining the Multilevel Model's Error Covariance Structure</span>"
    ]
  },
  {
    "objectID": "07.html#postulating-an-alternative-error-covariance-structure",
    "href": "07.html#postulating-an-alternative-error-covariance-structure",
    "title": "7  Examining the Multilevel Model’s Error Covariance Structure",
    "section": "7.3 Postulating an alternative error covariance structure",
    "text": "7.3 Postulating an alternative error covariance structure\nSinger and Willett wrote:\n\nit is easy to specify alternative covariance structures for the composite residual and determine analytically which specification—the “standard” or an alternative—fits best. You already possess the analytic tools and skills needed for this work. (p. 257)\n\nFrequentest R users would likely do this with the nlme package (Pinheiro et al., 2021). Recent additions to brms makes this largely possible, but not completely so. The six error structures listed in this section were:\n\nunstructured,\ncompound symmetric,\nheterogeneous compound symmetric,\nautoregressive,\nheterogeneous autoregressive, and\nToeplitz.\n\nThe Toeplitz structure is not currently available with brms, but we can experiment with the first five. For details, see issue #403 in the brms GitHub repo.\n\n7.3.1 Unstructured error covariance matrix\nModels using an unstructured error covariance matrix include \\(k (k + 1) / 2\\) variance/covariance parameters, where \\(k\\) is the number of time waves in the data. In the case of our opposites_pp data, we can compute the number of parameters as follows.\n\nk &lt;- 4\nk * (k + 1) / 2\n\n[1] 10\n\n\nWe end up with 4 variances and 6 covariances. Following the notation Singer and Willett used in the upper left corner of Table 7.3 (p. 258), we can express this as\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_1^2 & \\sigma_{12} & \\sigma_{13} & \\sigma_{14} \\\\\n  \\sigma_{21} & \\sigma_2^2 & \\sigma_{23} & \\sigma_{24} \\\\\n  \\sigma_{31} & \\sigma_{32} & \\sigma_3^2 & \\sigma_{34} \\\\\n  \\sigma_{41} & \\sigma_{42} & \\sigma_{43} & \\sigma_4^2 \\end{bmatrix}.\n\\end{align}\n\\]\n\nThe great appeal of an unstructured error covariance structure is that it places no restrictions on the structure of \\(\\mathbf{\\Sigma}_r\\). For a given set of fixed effects, its deviance statistic will always be the smallest of any error covariance structure. If you have just a few waves of data, this choice can be attractive. But if you have many waves, it can require an exorbitant number of parameters… [However,] the “standard” model requires only 3 variance components (\\(\\sigma_0^2\\), \\(\\sigma_1^2\\), and \\(\\sigma_\\epsilon^2\\)) and one covariance component, \\(\\sigma_{01}\\). (p. 260)\n\nTo fit the unstructured model with brms, we use the unstr() function. Notice how we’ve dropped the usual (1 + time | id) syntax. Instead, we indicate the data are temporally structured by the time variable by setting time = time within unstr(), and we indicate the data are grouped by id by setting gr = id, also within unstr(). Also, notice we’ve wrapped the entire model formula within the bf() function. The second line within the bf() function has sigma on the left side of the ~ operator, which is something we haven’t seen before. With that line, we have allowed the residual variance \\(\\sigma_\\epsilon\\) to vary across time points. By default, brms will use the log link, to insure the model for \\(\\sigma_{\\epsilon j}\\) will never predict negative variances.\n\nfit7.2 &lt;- brm(\n  data = opposites_pp, \n  family = gaussian,\n  bf(opp ~ 0 + Intercept + time + ccog + time:ccog + unstr(time = time, gr = id),\n     sigma ~ 0 + factor(time)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.02\")\n\nCheck the summary.\n\nprint(fit7.2, digits = 3, robust = T)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: opp ~ 0 + Intercept + time + ccog + time:ccog + unstr(time = time, gr = id) \n         sigma ~ 0 + factor(time)\n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n             Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\ncortime(0,1)    0.751     0.070    0.583    0.862 1.000     2317     2772\ncortime(0,2)    0.659     0.088    0.441    0.801 1.000     2152     2910\ncortime(1,2)    0.807     0.058    0.652    0.897 1.001     2525     2717\ncortime(0,3)    0.336     0.140    0.033    0.576 1.001     2290     2824\ncortime(1,3)    0.637     0.098    0.408    0.787 1.001     3011     3144\ncortime(2,3)    0.735     0.076    0.540    0.848 1.000     2963     3200\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept          165.505     5.751  154.795  177.194 1.001     3098     3063\ntime                26.613     2.018   22.366   30.716 1.000     3125     3018\nccog                -0.109     0.462   -1.005    0.810 1.000     3444     2960\ntime:ccog            0.456     0.163    0.135    0.802 1.000     3233     2746\nsigma_factortime0    3.571     0.117    3.357    3.824 1.002     2587     2530\nsigma_factortime1    3.458     0.113    3.253    3.688 1.001     2123     2406\nsigma_factortime2    3.496     0.106    3.294    3.724 1.002     2196     2410\nsigma_factortime3    3.519     0.114    3.313    3.753 1.000     2817     2771\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere’s a lot of exciting things going on in that output. We’ll start with the bottom 4 rows in the Population-Level Effects section, which contains our the summaries for our \\(\\log (\\sigma_{\\epsilon j})\\) parameters. To get them out of the log metric, we exponentiate. Here’s a quick conversion.\n\nfixef(fit7.2)[5:8, -2] %&gt;% exp()\n\n                  Estimate     Q2.5    Q97.5\nsigma_factortime0 35.73391 28.71447 45.80566\nsigma_factortime1 31.88199 25.87892 39.96905\nsigma_factortime2 33.09534 26.93729 41.44291\nsigma_factortime3 33.87613 27.47317 42.65918\n\n\nNow let’s address the new Correlation Structures section of the print() output. Just as brms decomposes the typical multilevel model level-2 variance/covariance matrix \\(\\mathbf{\\Sigma}\\) as\n\\[\n\\begin{align}\n\\mathbf \\Sigma & = \\mathbf D \\mathbf \\Omega \\mathbf D, \\text{where} \\\\\n\\mathbf D      & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\text{and} \\\\\n\\mathbf \\Omega & = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix},\n\\end{align}\n\\]\nthe same kind of thing happens when we fit a model with an unstructured variance/covariance matrix with the unstr() function. The print() output returned posterior summaries for the elements of the correlation matrix \\(\\mathbf \\Omega\\) in the Correlation Structures section, and it returned posterior summaries for the elements of the diagonal matrix of standard deviations \\(\\mathbf D\\) in the last four rows of the Population-Level Effects section. But notice that instead of \\(2 \\times 2\\) matrices like we got with our conventional growth model fit7.1, both \\(\\mathbf D\\) and \\(\\mathbf \\Omega\\) are now \\(4 \\times 4\\) matrices right out of the gate. Thus if we use the posterior medians from the print() output as our point estimates, we can express the \\(\\hat{\\mathbf \\Sigma}\\) matrix from our unstructured fit7.2 model as\n\\[\n\\begin{align}\n\\mathbf \\Sigma & = \\mathbf D \\mathbf \\Omega \\mathbf D \\\\\n\\hat{\\mathbf D} & = \\begin{bmatrix}\n  35.6 & 0 & 0 & 0 \\\\\n  0 & 31.6  & 0 & 0 \\\\\n  0 & 0 & 33.0 & 0 \\\\\n  0 & 0 & 0 & 33.7\n  \\end{bmatrix} \\\\\n\\hat{\\mathbf \\Omega} & = \\begin{bmatrix}\n  1 & .75 & .66 & .33 \\\\\n  .75 & 1 & .81 & .64 \\\\\n  .66 & .81 & 1 & .73 \\\\\n  .33 & .64 & .73 & 1\n  \\end{bmatrix}.\n\\end{align}\n\\]\nHere’s how to compute and summarize the \\(\\mathbf D\\) and \\(\\mathbf \\Omega\\) parameters with the as_draws_df() output.\n\n# wrangle\nsigma.us &lt;- as_draws_df(fit7.2) %&gt;%\n  transmute(`sigma[1]` = exp(b_sigma_factortime0),\n            `sigma[2]` = exp(b_sigma_factortime1),\n            `sigma[3]` = exp(b_sigma_factortime2),\n            `sigma[4]` = exp(b_sigma_factortime3),\n            \n            `rho[12]` = cortime__0__1,\n            `rho[13]` = cortime__0__2,\n            `rho[14]` = cortime__0__3,\n            `rho[23]` = cortime__1__2,\n            `rho[24]` = cortime__1__3,\n            `rho[34]` = cortime__2__3,\n            \n            `rho[21]` = cortime__0__1, \n            `rho[31]` = cortime__0__2, \n            `rho[32]` = cortime__1__2, \n            `rho[41]` = cortime__0__3, \n            `rho[42]` = cortime__1__3, \n            `rho[43]` = cortime__2__3)\n\n# summarize\nsigma.us %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 16 × 7\n   name     value .lower .upper .width .point .interval\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 rho[12]   0.75   0.58   0.86   0.95 median qi       \n 2 rho[13]   0.66   0.44   0.8    0.95 median qi       \n 3 rho[14]   0.34   0.03   0.58   0.95 median qi       \n 4 rho[21]   0.75   0.58   0.86   0.95 median qi       \n 5 rho[23]   0.81   0.65   0.9    0.95 median qi       \n 6 rho[24]   0.64   0.41   0.79   0.95 median qi       \n 7 rho[31]   0.66   0.44   0.8    0.95 median qi       \n 8 rho[32]   0.81   0.65   0.9    0.95 median qi       \n 9 rho[34]   0.73   0.54   0.85   0.95 median qi       \n10 rho[41]   0.34   0.03   0.58   0.95 median qi       \n11 rho[42]   0.64   0.41   0.79   0.95 median qi       \n12 rho[43]   0.73   0.54   0.85   0.95 median qi       \n13 sigma[1] 35.6   28.7   45.8    0.95 median qi       \n14 sigma[2] 31.8   25.9   40.0    0.95 median qi       \n15 sigma[3] 33.0   26.9   41.4    0.95 median qi       \n16 sigma[4] 33.8   27.5   42.7    0.95 median qi       \n\n\nSince Singer and Willett preferred the variance/covariance parameterization for \\(\\mathbf \\Sigma\\), we’ll practice wrangling the posterior draws to transform our results into that metric, too.\n\n# transform\nsigma.us &lt;- sigma.us %&gt;% \n  transmute(`sigma[1]^2` = `sigma[1]`^2,\n            `sigma[12]` = `sigma[1]` * `sigma[2]` * `rho[12]`,\n            `sigma[13]` = `sigma[1]` * `sigma[3]` * `rho[13]`,\n            `sigma[14]` = `sigma[1]` * `sigma[4]` * `rho[14]`,\n            \n            `sigma[21]` = `sigma[2]` * `sigma[1]` * `rho[21]`,\n            `sigma[2]^2` = `sigma[2]`^2,\n            `sigma[23]` = `sigma[2]` * `sigma[3]` * `rho[23]`,\n            `sigma[24]` = `sigma[2]` * `sigma[4]` * `rho[24]`,\n            \n            `sigma[31]` = `sigma[3]` * `sigma[1]` * `rho[31]`,\n            `sigma[32]` = `sigma[3]` * `sigma[2]` * `rho[32]`,\n            `sigma[3]^2` = `sigma[3]`^2,\n            `sigma[34]` = `sigma[3]` * `sigma[4]` * `rho[34]`,\n            \n            `sigma[41]` = `sigma[4]` * `sigma[1]` * `rho[41]`,\n            `sigma[42]` = `sigma[4]` * `sigma[2]` * `rho[42]`,\n            `sigma[43]` = `sigma[4]` * `sigma[3]` * `rho[43]`,\n            `sigma[4]^2` = `sigma[4]`^2)\n\n# summarize\nsigma.us %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 16 × 7\n   name       value .lower .upper .width .point .interval\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 sigma[1]^2 1264.  825.   2098.   0.95 median qi       \n 2 sigma[12]   840.  503.   1438.   0.95 median qi       \n 3 sigma[13]   765.  429.   1321.   0.95 median qi       \n 4 sigma[14]   393.   40.5   846.   0.95 median qi       \n 5 sigma[2]^2 1008.  670.   1598.   0.95 median qi       \n 6 sigma[21]   840.  503.   1438.   0.95 median qi       \n 7 sigma[23]   841.  520.   1390.   0.95 median qi       \n 8 sigma[24]   677.  367.   1176.   0.95 median qi       \n 9 sigma[3]^2 1087.  726.   1718.   0.95 median qi       \n10 sigma[31]   765.  429.   1321.   0.95 median qi       \n11 sigma[32]   841.  520.   1390.   0.95 median qi       \n12 sigma[34]   806.  484.   1375.   0.95 median qi       \n13 sigma[4]^2 1139.  755.   1820.   0.95 median qi       \n14 sigma[41]   393.   40.5   846.   0.95 median qi       \n15 sigma[42]   677.  367.   1176.   0.95 median qi       \n16 sigma[43]   806.  484.   1375.   0.95 median qi       \n\n\nBut again, I suspect it will be easier to appreciate our posterior \\(\\hat{\\mathbf \\Sigma}\\) in a tile plot. Here’s a summary using the posterior medians, similar to what Singer and Willett reported in the rightmost column of Table 7.3.\n\nlevels &lt;- c(\n  \"sigma[1]^2\", \"sigma[12]\", \"sigma[13]\", \"sigma[14]\", \n  \"sigma[21]\", \"sigma[2]^2\", \"sigma[23]\", \"sigma[24]\", \n  \"sigma[31]\", \"sigma[32]\", \"sigma[3]^2\", \"sigma[34]\", \n  \"sigma[41]\", \"sigma[42]\", \"sigma[43]\", \"sigma[4]^2\")\n\nsigma.us %&gt;% \n  pivot_longer(everything()) %&gt;%  \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate(label = round(value, digits = 0)) %&gt;% \n  \n  ggplot(aes(x = 0, y = 0)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label, color = value &lt; 1),\n            show.legend = F) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, NA)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Sigma)[italic(r)]*\" for the unstructured model\")) +\n  theme(legend.text = element_text(hjust = 1)) +\n  facet_wrap(~ name, labeller = label_parsed)\n\n\n\n\n\n\n\n\nIn case you’re curious, here’s the variance/covariance matrix from the sample data.\n\nfit7.2$data %&gt;% \n  select(id, time, opp) %&gt;% \n  pivot_wider(names_from = time, values_from = opp) %&gt;% \n  select(-id) %&gt;% \n  cov() %&gt;% \n  round(digits = 0)\n\n     0    1    2    3\n0 1309  976  922  563\n1  976 1125 1021  855\n2  922 1021 1292 1081\n3  563  855 1081 1415\n\n\nThe brms default priors are weakly regularizing, particularly the LKJ prior for the correlation matrix \\(\\mathbf \\Omega\\), and I believe this is why the values from our model are systemically lower that the sample statistics. If you find this upsetting, collect more data, which hill help the likelihood dominate the prior.\n\n\n7.3.2 Compound symmetric error covariance matrix\n“A compound symmetric error covariance matrix requires just two parameters, labeled \\(\\sigma^2\\) and \\(\\sigma_1^2\\) in table 7.3” (p. 260). From the table, we see that matrix follows the form\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma^2 + \\sigma_1^2 & \\sigma_1^2 & \\sigma_1^2 & \\sigma_1^2 \\\\\n  \\sigma_1^2 & \\sigma^2 + \\sigma_1^2 & \\sigma_1^2 & \\sigma_1^2 \\\\\n  \\sigma_1^2 & \\sigma_1^2 & \\sigma^2 + \\sigma_1^2 & \\sigma_1^2 \\\\\n  \\sigma_1^2 & \\sigma_1^2 & \\sigma_1^2 & \\sigma^2 + \\sigma_1^2 \\end{bmatrix},\n\\end{align}\n\\]\nwhere \\(\\sigma_1^2\\) does not have the same meaning we’ve become accustomed to (i.e., the level-2 variance in linear change over time). The meaning of \\(\\sigma^2\\) might also be a little opaque. Happily, there’s another way to express this matrix, which is a modification of the heterogeneous compound symmetric matrix we see listed in Table 7.3. That alternative is:\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_\\epsilon^2 & \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\rho \\\\\n  \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 & \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\rho \\\\\n  \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 & \\sigma_\\epsilon^2 \\rho \\\\\n  \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\rho & \\sigma_\\epsilon^2 \\end{bmatrix},\n\\end{align}\n\\]\nwhere the term on the diagonal, \\(\\sigma_\\epsilon^2\\), is the residual variance, which is constrained to equality across all four time points. In all cells in the off-diagonal, we see \\(\\sigma_\\epsilon^2\\) multiplied by \\(\\rho\\). In this parameterization, \\(\\rho\\) is the correlation between time points, and that correlation is constrained to equality across all possible pairs of time points. Although this notation is a little different from the notation used in the text, I believe it will help us interpret our model. As we’ll see, brms uses this alternative parameterization.\nTo fit the compound symmetric model with brms, we use the cosy() function. Notice how like with the unstructured model fit7.2, we’ve dropped the usual (1 + time | id) syntax. Instead, we impose compound symmetry within persons by setting gr = id within cosy().\n\nfit7.3 &lt;- brm(\n  data = opposites_pp,\n  family = gaussian,\n  opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.03\")\n\nCheck the model summary.\n\nprint(fit7.3, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id) \n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\ncosy    0.723     0.059    0.602    0.829 1.001     2590     2657\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept  164.376     5.728  152.970  175.620 1.000     3406     2479\ntime        26.972     1.410   24.149   29.661 1.001     3687     2962\nccog        -0.102     0.472   -1.022    0.815 1.001     3519     3006\ntime:ccog    0.431     0.112    0.214    0.657 1.000     3931     3225\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma   35.528     3.524   29.654   43.530 1.002     2662     2594\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSee that new cosy row? That’s \\(\\rho\\), the residual correlation among the time points. The sigma row on the bottom has it’s typical interpretation, it’s the residual standard deviation, what we typically call \\(\\sigma_\\epsilon\\). Square it and you’ll have what we called \\(\\sigma_\\epsilon^2\\) in the matrix, above. Okay, since our brms model is parameterized differently from what Singer and Willett reported in the text (see Table 7.3, p. 258), we’ll wrangle the posterior draws a bit.\n\nsigma.cs &lt;- as_draws_df(fit7.3) %&gt;%\n  transmute(rho                    = cosy,\n            sigma_e                = sigma,\n            `sigma^2 + sigma[1]^2` = sigma^2) %&gt;%\n  mutate(`sigma[1]^2` = rho * sigma_e^2) %&gt;% \n  mutate(`sigma^2` = `sigma^2 + sigma[1]^2` - `sigma[1]^2`)\n\n# what did we do?\nhead(sigma.cs)\n\n# A tibble: 6 × 5\n    rho sigma_e `sigma^2 + sigma[1]^2` `sigma[1]^2` `sigma^2`\n  &lt;dbl&gt;   &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 0.641    32.5                  1054.         675.      379.\n2 0.649    30.9                   954.         619.      334.\n3 0.684    33.4                  1115.         763.      352.\n4 0.713    38.3                  1463.        1044.      420.\n5 0.807    36.8                  1354.        1093.      261.\n6 0.756    39.4                  1551.        1172.      379.\n\n\nHere’s the numeric summary.\n\nsigma.cs %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 5 × 7\n  name                   value .lower  .upper .width .point .interval\n  &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 rho                     0.73    0.6    0.83   0.95 median qi       \n2 sigma_e                35.2    29.6   43.5    0.95 median qi       \n3 sigma[1]^2            899.    553.  1542.     0.95 median qi       \n4 sigma^2               336.    261.   451.     0.95 median qi       \n5 sigma^2 + sigma[1]^2 1240.    879.  1895.     0.95 median qi       \n\n\nTo simplify, we might pull the posterior medians for \\(\\sigma^2 + \\sigma_1^2\\) and \\(\\sigma_1^2\\). We’ll call them diagonal and off_diagonal, respectively.\n\ndiagonal &lt;- median(sigma.cs$`sigma^2 + sigma[1]^2`)\noff_diagonal &lt;- median(sigma.cs$`sigma[1]^2`)\n\nNow we have them, we can make our colored version of the \\(\\mathbf{\\Sigma}_r\\) Singer and Willett reported in the rightmost column of Table 7.3.\n\ncrossing(row = 1:4,\n         col = factor(1:4)) %&gt;% \n  mutate(value = if_else(row == col, diagonal, off_diagonal)) %&gt;% \n  mutate(label = round(value, digits = 0),\n         col = fct_rev(col)) %&gt;% \n  \n  ggplot(aes(x = row, y = col)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label)) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, NA)) +\n  scale_x_continuous(NULL, breaks = NULL, position = \"top\", expand = c(0, 0)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Sigma)[italic(r)]*\" for the compound symmetric model\")) +\n  theme(legend.text = element_text(hjust = 1))\n\n\n\n\n\n\n\n\n\n\n7.3.3 Heterogeneous compound symmetric error covariance matrix\nNow we extend the compound symmetric matrix by allowing the residual variances to vary across the time waves. Thus, instead of a single \\(\\sigma_\\epsilon^2\\) parameter, we’ll have \\(\\sigma_1^2\\) through \\(\\sigma_4^2\\). However, we still have a single correlation parameter \\(\\rho\\). We can express this as\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho & \\sigma_1 \\sigma_3 \\rho & \\sigma_1 \\sigma_4 \\rho \\\\\n  \\sigma_2 \\sigma_1 \\rho & \\sigma_1^2 & \\sigma_2 \\sigma_3 \\rho & \\sigma_2 \\sigma_4 \\rho \\\\\n  \\sigma_3 \\sigma_1 \\rho & \\sigma_3 \\sigma_2 \\rho & \\sigma_3^2 & \\sigma_3 \\sigma_4 \\rho \\\\\n  \\sigma_4 \\sigma_1 \\rho & \\sigma_4 \\sigma_2 \\rho & \\sigma_4 \\sigma_3 \\rho & \\sigma_4^2 \\end{bmatrix},\n\\end{align}\n\\]\nwhere, even though the correlation is the same in all cells, the covariances will differ because they are based on different combinations of the \\(\\sigma\\) parameters. To fit this model with brms, we will continue to use cosy(gr = id). But now we wrap the entire model formula within the bf() function and allow the residual standard deviations to vary across he waves with the line sigma ~ 0 + factor(time).\n\nfit7.4 &lt;- brm(\n  data = opposites_pp,\n  family = gaussian,\n  bf(opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id),\n     sigma ~ 0 + factor(time)), \n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.04\")\n\nCheck the summary.\n\nprint(fit7.4, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: opp ~ 0 + Intercept + time + ccog + time:ccog + cosy(gr = id) \n         sigma ~ 0 + factor(time)\n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n     Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\ncosy    0.720     0.060    0.592    0.830 1.005     1704     1837\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept          164.415     5.682  153.560  175.795 1.000     3690     2738\ntime                26.905     1.502   23.950   29.849 1.002     3800     2933\nccog                -0.179     0.463   -1.085    0.718 1.002     3251     2619\ntime:ccog            0.440     0.118    0.213    0.673 1.002     3958     2884\nsigma_factortime0    3.641     0.124    3.417    3.902 1.005     2012     2178\nsigma_factortime1    3.494     0.119    3.272    3.742 1.003     1843     2362\nsigma_factortime2    3.529     0.121    3.306    3.774 1.002     2065     2370\nsigma_factortime3    3.593     0.125    3.361    3.853 1.003     1955     2295\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you look at the second row in the output, you’ll see that the brms default was to model \\(\\log(\\sigma_j)\\). Thus, you’ll have to exponentiate those posteriors to get them in their natural metric. Here’s a quick conversion.\n\nfixef(fit7.4)[5:8, -2] %&gt;% exp()\n\n                  Estimate     Q2.5    Q97.5\nsigma_factortime0 38.14764 30.48905 49.51246\nsigma_factortime1 32.91011 26.35654 42.19741\nsigma_factortime2 34.10152 27.28920 43.53938\nsigma_factortime3 36.34854 28.81602 47.13603\n\n\nTo get the marginal posteriors for the full \\(\\mathbf{\\Sigma}_r\\) matrix, we’ll want to work directly with the output from as_draws_df().\n\nsigma.hcs &lt;- as_draws_df(fit7.4) %&gt;%\n  transmute(`sigma[1]`   = exp(b_sigma_factortime0),\n            `sigma[2]`   = exp(b_sigma_factortime1),\n            `sigma[3]`   = exp(b_sigma_factortime2),\n            `sigma[4]`   = exp(b_sigma_factortime3),\n            \n            rho          = cosy,\n            \n            `sigma[1]^2` = exp(b_sigma_factortime0)^2,\n            `sigma[2]^2` = exp(b_sigma_factortime1)^2,\n            `sigma[3]^2` = exp(b_sigma_factortime2)^2,\n            `sigma[4]^2` = exp(b_sigma_factortime3)^2) %&gt;% \n  mutate(`sigma[2]*sigma[1]*rho` = `sigma[2]` * `sigma[1]` * rho,\n         `sigma[3]*sigma[1]*rho` = `sigma[3]` * `sigma[1]` * rho,\n         `sigma[4]*sigma[1]*rho` = `sigma[4]` * `sigma[1]` * rho,\n         \n         `sigma[1]*sigma[2]*rho` = `sigma[1]` * `sigma[2]` * rho,\n         `sigma[3]*sigma[2]*rho` = `sigma[3]` * `sigma[2]` * rho,\n         `sigma[4]*sigma[2]*rho` = `sigma[4]` * `sigma[2]` * rho,\n         \n         `sigma[1]*sigma[3]*rho` = `sigma[1]` * `sigma[3]` * rho,\n         `sigma[2]*sigma[3]*rho` = `sigma[2]` * `sigma[3]` * rho,\n         `sigma[4]*sigma[3]*rho` = `sigma[4]` * `sigma[3]` * rho,\n         \n         `sigma[1]*sigma[4]*rho` = `sigma[1]` * `sigma[4]` * rho,\n         `sigma[2]*sigma[4]*rho` = `sigma[2]` * `sigma[4]` * rho,\n         `sigma[3]*sigma[4]*rho` = `sigma[3]` * `sigma[4]` * rho)\n\n# what did we do?\nglimpse(sigma.hcs)\n\nRows: 4,000\nColumns: 21\n$ `sigma[1]`              &lt;dbl&gt; 35.56208, 35.57836, 33.08933, 40.70314, 30.48914, 32.99203, 37.94815, 34.25566, 35.42878, 35.996…\n$ `sigma[2]`              &lt;dbl&gt; 33.89407, 27.96338, 30.16567, 31.72296, 29.12656, 31.00532, 30.26289, 32.73009, 26.74998, 29.142…\n$ `sigma[3]`              &lt;dbl&gt; 32.12802, 32.03955, 28.60996, 33.40432, 29.84702, 30.76859, 35.32730, 28.68270, 28.11751, 26.272…\n$ `sigma[4]`              &lt;dbl&gt; 35.59806, 32.35704, 35.97292, 35.62483, 35.83509, 35.78706, 38.32063, 30.29754, 29.57775, 29.713…\n$ rho                     &lt;dbl&gt; 0.6875016, 0.6944474, 0.6521933, 0.6996793, 0.6928762, 0.7100510, 0.7489264, 0.5943043, 0.684699…\n$ `sigma[1]^2`            &lt;dbl&gt; 1264.6614, 1265.8195, 1094.9037, 1656.7458, 929.5879, 1088.4742, 1440.0624, 1173.4503, 1255.1984…\n$ `sigma[2]^2`            &lt;dbl&gt; 1148.8083, 781.9504, 909.9677, 1006.3460, 848.3564, 961.3296, 915.8427, 1071.2586, 715.5612, 849…\n$ `sigma[3]^2`            &lt;dbl&gt; 1032.2095, 1026.5327, 818.5296, 1115.8487, 890.8449, 946.7061, 1248.0183, 822.6971, 790.5946, 69…\n$ `sigma[4]^2`            &lt;dbl&gt; 1267.2221, 1046.9782, 1294.0513, 1269.1287, 1284.1537, 1280.7136, 1468.4711, 917.9409, 874.8432,…\n$ `sigma[2]*sigma[1]*rho` &lt;dbl&gt; 828.6758, 690.8995, 650.9944, 903.4427, 615.3044, 726.3313, 860.0828, 666.3285, 648.9028, 699.77…\n$ `sigma[3]*sigma[1]*rho` &lt;dbl&gt; 785.4975, 791.6107, 617.4211, 951.3265, 630.5244, 720.7857, 1004.0152, 583.9306, 682.0767, 630.8…\n$ `sigma[4]*sigma[1]*rho` &lt;dbl&gt; 870.3366, 799.4551, 776.3187, 1014.5648, 757.0235, 838.3485, 1089.0868, 616.8060, 717.4991, 713.…\n$ `sigma[1]*sigma[2]*rho` &lt;dbl&gt; 828.6758, 690.8995, 650.9944, 903.4427, 615.3044, 726.3313, 860.0828, 666.3285, 648.9028, 699.77…\n$ `sigma[3]*sigma[2]*rho` &lt;dbl&gt; 748.6545, 622.1790, 562.8679, 741.4388, 602.3458, 677.3814, 800.6820, 557.9252, 514.9919, 510.74…\n$ `sigma[4]*sigma[2]*rho` &lt;dbl&gt; 829.5143, 628.3445, 707.7259, 790.7251, 723.1915, 787.8648, 868.5249, 589.3366, 541.7371, 577.63…\n$ `sigma[1]*sigma[3]*rho` &lt;dbl&gt; 785.4975, 791.6107, 617.4211, 951.3265, 630.5244, 720.7857, 1004.0152, 583.9306, 682.0767, 630.8…\n$ `sigma[2]*sigma[3]*rho` &lt;dbl&gt; 748.6545, 622.1790, 562.8679, 741.4388, 602.3458, 677.3814, 800.6820, 557.9252, 514.9919, 510.74…\n$ `sigma[4]*sigma[3]*rho` &lt;dbl&gt; 786.2923, 719.9372, 671.2268, 832.6347, 741.0801, 781.8494, 1013.8701, 516.4594, 569.4323, 520.7…\n$ `sigma[1]*sigma[4]*rho` &lt;dbl&gt; 870.3366, 799.4551, 776.3187, 1014.5648, 757.0235, 838.3485, 1089.0868, 616.8060, 717.4991, 713.…\n$ `sigma[2]*sigma[4]*rho` &lt;dbl&gt; 829.5143, 628.3445, 707.7259, 790.7251, 723.1915, 787.8648, 868.5249, 589.3366, 541.7371, 577.63…\n$ `sigma[3]*sigma[4]*rho` &lt;dbl&gt; 786.2923, 719.9372, 671.2268, 832.6347, 741.0801, 781.8494, 1013.8701, 516.4594, 569.4323, 520.7…\n\n\nHere’s the numeric summary.\n\nsigma.hcs %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 21 × 7\n   name                    value .lower  .upper .width .point .interval\n   &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 rho                      0.72   0.59    0.83   0.95 median qi       \n 2 sigma[1]                38.0   30.5    49.5    0.95 median qi       \n 3 sigma[1]*sigma[2]*rho  896.   535.   1609.     0.95 median qi       \n 4 sigma[1]*sigma[3]*rho  930.   556.   1655.     0.95 median qi       \n 5 sigma[1]*sigma[4]*rho  990.   589.   1795.     0.95 median qi       \n 6 sigma[1]^2            1447.   930.   2451.     0.95 median qi       \n 7 sigma[2]                32.8   26.4    42.2    0.95 median qi       \n 8 sigma[2]*sigma[1]*rho  896.   535.   1609.     0.95 median qi       \n 9 sigma[2]*sigma[3]*rho  796.   479.   1425.     0.95 median qi       \n10 sigma[2]*sigma[4]*rho  853.   509.   1517.     0.95 median qi       \n# ℹ 11 more rows\n\n\nThat’s a lot of information to wade through. Here we simplify the picture by making our plot version of the matrix Singer and Willett reported in the rightmost column of Table 7.3.\n\n# arrange the panels\nlevels &lt;- c(\n  \"sigma[1]^2\", \"sigma[1]*sigma[2]*rho\", \"sigma[1]*sigma[3]*rho\", \"sigma[1]*sigma[4]*rho\", \n  \"sigma[2]*sigma[1]*rho\", \"sigma[2]^2\", \"sigma[2]*sigma[3]*rho\", \"sigma[2]*sigma[4]*rho\", \n  \"sigma[3]*sigma[1]*rho\", \"sigma[3]*sigma[2]*rho\", \"sigma[3]^2\", \"sigma[3]*sigma[4]*rho\", \n  \"sigma[4]*sigma[1]*rho\", \"sigma[4]*sigma[2]*rho\", \"sigma[4]*sigma[3]*rho\", \"sigma[4]^2\")\n\n\nsigma.hcs %&gt;% \n  select(`sigma[1]^2`:`sigma[3]*sigma[4]*rho`) %&gt;% \n  pivot_longer(everything()) %&gt;%  \n  mutate(name = factor(name, levels = levels)) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate(label = round(value, digits = 0)) %&gt;% \n  \n  ggplot(aes(x = 0, y = 0)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label)) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, NA)) +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Sigma)[italic(r)]*\" for the heterogeneous compound symmetric model\")) +\n  theme(legend.text = element_text(hjust = 1)) +\n  facet_wrap(~ name, labeller = label_parsed)\n\n\n\n\n\n\n\n\n\n\n7.3.4 Autoregressive error covariance matrix\nThe first-order autoregressive has a strict “band-diagonal” structure governed by two parameters, which Singer and Willett called \\(\\sigma^2\\) and \\(\\rho\\). From Table 7.3 (p. 260), we see that matrix follows the form\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma^2 & \\sigma^2 \\rho & \\sigma^2 \\rho^2 & \\sigma^2 \\rho^3 \\\\\n  \\sigma^2 \\rho & \\sigma^2 & \\sigma^2 \\rho & \\sigma^2 \\rho^2 \\\\\n  \\sigma^2 \\rho^2 & \\sigma^2 \\rho & \\sigma^2 & \\sigma^2 \\rho \\\\\n  \\sigma^2 \\rho^3 & \\sigma^2 \\rho^2 & \\sigma^2 \\rho & \\sigma^2 \\end{bmatrix},\n\\end{align}\n\\]\nwhere \\(\\rho\\) is the correlation of one time point to the one immediately before or after, after conditioning on the liner model. In a similar way, \\(\\rho^2\\) is the correlation between time points with one degree of separation (e.g., time 1 with time 3) and \\(\\rho^3\\) is the correlation between the first and fourth time point. The other parameter, \\(\\sigma^2\\) is the residual variance after conditioning on the linear model.\nOnce can fit this model with brms using a version of the ar() syntax. However, the model will follow a slightly different parameterization, following the form:\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^3 \\\\\n  \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 \\\\\n  \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho \\\\\n  \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^3 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho^2 & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2 \\rho & \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2\n  \\end{bmatrix},\\\\\n\\end{align}\n\\]\nwhere \\(\\sigma_\\epsilon\\) is the residual variance after conditioning on both the linear model AND the autoregressive correlation \\(\\rho\\). It’s not clear to me why brms is parameterized this way, but this is what we’ve got. The main point to get is that what Singer and Willett called \\(\\sigma\\) in their autoregressive model, we’ll have to call \\(\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2}\\). Thus, if you substitute our verbose brms term \\(\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2}\\) for Singer and Willett’s compact term \\(\\sigma\\), you’ll see the hellish matrix above is the same as the much simpler one before it.\nTo fit the first-order autoregressive model with brms, we use the ar() function. As with the last few models, notice how we continue to omit the (1 + time | id) syntax. Instead, we impose the autoregressive structure within persons by setting gr = id within ar(). We also set cov = TRUE.\n\nfit7.5 &lt;- brm(\n  data = opposites_pp,\n  family = gaussian,\n  opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.05\")\n\nCheck the summary.\n\nprint(fit7.5, digits = 3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE) \n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nar[1]    0.816     0.041    0.731    0.888 1.001     4530     3265\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept  164.193     6.037  152.037  176.006 1.000     4923     3328\ntime        27.227     1.856   23.603   30.875 1.000     5013     3664\nccog        -0.039     0.480   -0.998    0.899 1.000     5030     3189\ntime:ccog    0.423     0.152    0.118    0.727 1.000     5223     3356\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma   20.316     1.395   17.797   23.198 1.002     4379     3131\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe ar[1] row in our summary is \\(\\rho\\). As we discussed just before fitting the model, the sigma line is the summary for what I’m calling \\(\\sigma_\\epsilon\\), which is the residual standard deviation after conditioning on both the linear model AND \\(\\rho\\). If we rename the \\(\\sigma^2\\) parameter in the text as \\(\\sigma_\\text{Singer \\& Willett (2003)}^2\\), we can convert our \\(\\sigma_\\epsilon\\) parameter to that metric using the formula\n\\[\\sigma_\\text{Singer \\& Willett (2003)}^2 = \\left (\\sigma_\\epsilon \\Big / \\sqrt{1 - \\rho^2} \\right )^2.\\]\nWith that formula in hand, we’re ready to compute the marginal posteriors for the full \\(\\mathbf{\\Sigma}_r\\) matrix, saving the results as sigma.ar.\n\nsigma.ar &lt;- as_draws_df(fit7.5) %&gt;% \n  mutate(sigma_e = sigma,\n         sigma   = sigma_e / sqrt(1 - `ar[1]`^2)) %&gt;% \n  transmute(rho             = `ar[1]`,\n            `sigma^2`       = sigma^2,\n            `sigma^2 rho`   = sigma^2 * rho,\n            `sigma^2 rho^2` = sigma^2 * rho^2,\n            `sigma^2 rho^3` = sigma^2 * rho^3)\n\nHere’s the numeric summary.\n\nsigma.ar %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 5 × 7\n  name            value .lower  .upper .width .point .interval\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 rho              0.82   0.73    0.89   0.95 median qi       \n2 sigma^2       1252.   890.   1912.     0.95 median qi       \n3 sigma^2 rho   1023.   668.   1664.     0.95 median qi       \n4 sigma^2 rho^2  838.   496.   1471.     0.95 median qi       \n5 sigma^2 rho^3  686.   365.   1298.     0.95 median qi       \n\n\nTo simplify, we might pull the posterior medians for \\(\\sigma^2\\) through \\(\\sigma^2 \\rho^3\\).\n\ns2   &lt;- median(sigma.ar$`sigma^2`)\ns2p  &lt;- median(sigma.ar$`sigma^2 rho`)\ns2p2 &lt;- median(sigma.ar$`sigma^2 rho^2`)\ns2p3 &lt;- median(sigma.ar$`sigma^2 rho^3`)\n\nNow we have them, we can make our colored version of the \\(\\mathbf{\\Sigma}_r\\) Singer and Willett reported in the rightmost column of Table 7.3.\n\ncrossing(row = 1:4,\n         col = factor(1:4)) %&gt;% \n  mutate(value = c(s2, s2p, s2p2, s2p3,\n                   s2p, s2, s2p, s2p2,\n                   s2p2, s2p, s2, s2p,\n                   s2p3, s2p2, s2p, s2)) %&gt;% \n  mutate(label = round(value, digits = 0),\n         col   = fct_rev(col)) %&gt;% \n  \n  ggplot(aes(x = row, y = col)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label)) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, NA)) +\n  scale_x_continuous(NULL, breaks = NULL, position = \"top\", expand = c(0, 0)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Sigma)[italic(r)]*\" for the autoregressive model\")) +\n  theme(legend.text = element_text(hjust = 1))\n\n\n\n\n\n\n\n\nWith this presentation, that strict band-diagonal structure really pops.\n\n\n7.3.5 Heterogeneous autoregressive error covariance matrix\nFor the heterogeneous autoregressive error covariance matrix, we relax the assumption that the variances on the diagonal of the \\(\\mathbf{\\Sigma}_r\\) matrix are constant across waves. From Table 7.3 (p. 260), we see that matrix follows the form\n\\[\n\\begin{align}\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho & \\sigma_1 \\sigma_3 \\rho^2 & \\sigma_1 \\sigma_4 \\rho^3 \\\\\n  \\sigma_2 \\sigma_1 \\rho & \\sigma_2^2 & \\sigma_2 \\sigma_3 \\rho & \\sigma_2 \\sigma_4 \\rho^2 \\\\\n  \\sigma_3 \\sigma_1 \\rho^2 & \\sigma_3 \\sigma_2 \\rho & \\sigma_3^2 & \\sigma_3 \\sigma_4 \\rho \\\\\n  \\sigma_4 \\sigma_1 \\rho^3 & \\sigma_4 \\sigma_2 \\rho^2 & \\sigma_4 \\sigma_3 \\rho & \\sigma_4^2 \\end{bmatrix},\n\\end{align}\n\\]\nwhere, as before, \\(\\rho\\) is the correlation of one time point to the one immediately before or after, after conditioning on the liner model. To fit this model with brms, we continue to use the ar(gr = id, cov = TRUE) syntax. The only adjustment is we now wrap the formula within the bf() function and add a second line for sigma.\n\nfit7.6 &lt;- brm(\n  data = opposites_pp,\n  family = gaussian,\n  bf(opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE),\n     sigma ~ 0 + factor(time)), \n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.06\")\n\nInspect the parameter summary.\n\nprint(fit7.6, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: opp ~ 0 + Intercept + time + ccog + time:ccog + ar(gr = id, cov = TRUE) \n         sigma ~ 0 + factor(time)\n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nar[1]    0.808     0.043    0.715    0.884 1.000     4091     3167\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept          164.639     6.156  152.417  176.771 1.000     4077     3125\ntime                27.139     2.002   23.248   31.070 1.000     4145     3300\nccog                -0.119     0.504   -1.079    0.893 1.002     3979     2731\ntime:ccog            0.428     0.160    0.110    0.738 1.002     4262     3212\nsigma_factortime0    3.066     0.102    2.874    3.279 1.001     3738     3018\nsigma_factortime1    2.967     0.086    2.810    3.143 1.000     3266     2883\nsigma_factortime2    3.012     0.085    2.856    3.191 1.001     3777     3029\nsigma_factortime3    3.026     0.102    2.838    3.238 1.000     3885     3633\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere are summaries for the four \\(\\sigma_\\epsilon\\) posteriors, after exponentiation.\n\nfixef(fit7.6)[5:8, -2] %&gt;% exp()\n\n                  Estimate     Q2.5    Q97.5\nsigma_factortime0 21.44741 17.70577 26.54369\nsigma_factortime1 19.44305 16.60763 23.17069\nsigma_factortime2 20.33684 17.39643 24.31242\nsigma_factortime3 20.62398 17.07822 25.47298\n\n\nExtending our workflow from the last section, here how we might compute the marginal posteriors for the full \\(\\mathbf{\\Sigma}_r\\) matrix, saving the results as sigma.har.\n\nsigma.har &lt;- as_draws_df(fit7.6) %&gt;% \n  mutate(sigma_1e = exp(b_sigma_factortime0),\n         sigma_2e = exp(b_sigma_factortime1),\n         sigma_3e = exp(b_sigma_factortime2),\n         sigma_4e = exp(b_sigma_factortime3)) %&gt;% \n  mutate(sigma_1 = sigma_1e / sqrt(1 - `ar[1]`^2),\n         sigma_2 = sigma_2e / sqrt(1 - `ar[1]`^2),\n         sigma_3 = sigma_3e / sqrt(1 - `ar[1]`^2),\n         sigma_4 = sigma_4e / sqrt(1 - `ar[1]`^2)) %&gt;% \n  transmute(rho = `ar[1]`,\n            \n            `sigma_1^2` = sigma_1^2,\n            `sigma_2^2` = sigma_2^2,\n            `sigma_3^2` = sigma_3^2,\n            `sigma_4^2` = sigma_4^2,\n            \n            `sigma_2 sigma_1 rho`   = sigma_2 * sigma_1 * rho,\n            `sigma_3 sigma_1 rho^2` = sigma_3 * sigma_1 * rho^2,\n            `sigma_4 sigma_1 rho^3` = sigma_4 * sigma_1 * rho^3,\n            `sigma_3 sigma_2 rho`   = sigma_3 * sigma_2 * rho,\n            `sigma_4 sigma_2 rho^2` = sigma_4 * sigma_2 * rho^2,\n            `sigma_4 sigma_3 rho`   = sigma_4 * sigma_3 * rho)\n\nHere’s the numeric summary.\n\nsigma.har %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  median_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 11 × 7\n   name                    value .lower  .upper .width .point .interval\n   &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 rho                      0.81   0.72    0.88   0.95 median qi       \n 2 sigma_1^2             1348.   876.   2204.     0.95 median qi       \n 3 sigma_2 sigma_1 rho    990.   626.   1684.     0.95 median qi       \n 4 sigma_2^2             1111.   724.   1821.     0.95 median qi       \n 5 sigma_3 sigma_1 rho^2  840.   480.   1516.     0.95 median qi       \n 6 sigma_3 sigma_2 rho    940.   569.   1614.     0.95 median qi       \n 7 sigma_3^2             1218.   775.   2042.     0.95 median qi       \n 8 sigma_4 sigma_1 rho^3  691.   362.   1317.     0.95 median qi       \n 9 sigma_4 sigma_2 rho^2  773.   439.   1415.     0.95 median qi       \n10 sigma_4 sigma_3 rho   1002.   614.   1730.     0.95 median qi       \n11 sigma_4^2             1250.   804.   2091      0.95 median qi       \n\n\nAs in the last section, we might pull the posterior medians for \\(\\sigma1^2\\) through \\(\\sigma_4^2\\).\n\ns12   &lt;- median(sigma.har$`sigma_1^2`)\ns22   &lt;- median(sigma.har$`sigma_2^2`)\ns32   &lt;- median(sigma.har$`sigma_3^2`)\ns42   &lt;- median(sigma.har$`sigma_4^2`)\n\ns2s1p  &lt;- median(sigma.har$`sigma_2 sigma_1 rho`)\n\ns3s1p2 &lt;- median(sigma.har$`sigma_3 sigma_1 rho^2`)\ns3s2p  &lt;- median(sigma.har$`sigma_3 sigma_2 rho`)\n\ns4s1p3 &lt;- median(sigma.har$`sigma_4 sigma_1 rho^3`)\ns4s2p2 &lt;- median(sigma.har$`sigma_4 sigma_2 rho^2`)\ns4s3p  &lt;- median(sigma.har$`sigma_4 sigma_3 rho`)\n\nNow we have them, we can make our colored version of the \\(\\mathbf{\\Sigma}_r\\) Singer and Willett reported in the rightmost column of Table 7.3.\n\ncrossing(row = 1:4,\n         col = factor(1:4)) %&gt;% \n  mutate(value = c(s12, s2s1p, s3s1p2, s4s1p3,\n                   s2s1p, s22, s3s2p, s4s2p2,\n                   s3s1p2, s3s2p, s32, s4s3p,\n                   s4s1p3, s4s2p2, s4s3p, s42)) %&gt;% \n  mutate(label = round(value, digits = 0),\n         col   = fct_rev(col)) %&gt;% \n  \n  ggplot(aes(x = row, y = col)) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = label)) +\n  scale_fill_viridis_c(\"posterior\\nmedian\", option = \"A\", limits = c(0, NA)) +\n  scale_x_continuous(NULL, breaks = NULL, position = \"top\", expand = c(0, 0)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) +\n  labs(subtitle = expression(hat(Sigma)[italic(r)]*\" for the heterogeneous autoregressive model\")) +\n  theme(legend.text = element_text(hjust = 1))\n\n\n\n\n\n\n\n\nEven though there’s still a strict band diagonal correlation structure, the heterogeneous variances allow for differences among the covariances within the bands.\n\n\n7.3.6 Toeplitz error covariance matrix\nHandy as it is, brms is not yet set up to fit models with the Toeplitz error covariance matrix, at this time. For details, see brms GitHub issue #403.\n\n\n7.3.7 Does choosing the “correct” error covariance structure really matter?\nNow we have all our models, we might compare them with information criteria, as was done in the text. Here we’ll use the WAIC. First, compute and save the WAIC estimates.\n\nfit7.1 &lt;- add_criterion(fit7.1, criterion = \"waic\")\nfit7.2 &lt;- add_criterion(fit7.2, criterion = \"waic\")\nfit7.3 &lt;- add_criterion(fit7.3, criterion = \"waic\")\nfit7.4 &lt;- add_criterion(fit7.4, criterion = \"waic\")\nfit7.5 &lt;- add_criterion(fit7.5, criterion = \"waic\")\nfit7.6 &lt;- add_criterion(fit7.6, criterion = \"waic\")\n\nNow compare the models using WAIC differences and WAIC weights.\n\nloo_compare(fit7.1, fit7.2, fit7.3, fit7.4, fit7.5, fit7.6, criterion = \"waic\") %&gt;% print(simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit7.1    0.0       0.0  -586.7       6.2         47.1    3.5    1173.5   12.3 \nfit7.5  -20.9       4.1  -607.7       8.5          3.4    0.5    1215.4   17.1 \nfit7.2  -21.5       2.8  -608.2       7.6         14.2    1.7    1216.4   15.3 \nfit7.6  -23.7       4.2  -610.5       8.8          6.7    1.1    1220.9   17.6 \nfit7.3  -39.5       5.9  -626.2       8.1          4.5    0.7    1252.5   16.3 \nfit7.4  -41.4       5.4  -628.1       7.8          7.0    0.8    1256.2   15.6 \n\nmodel_weights(fit7.1, fit7.2, fit7.3, fit7.4, fit7.5, fit7.6, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit7.1 fit7.2 fit7.3 fit7.4 fit7.5 fit7.6 \n     1      0      0      0      0      0 \n\n\nBy both methods of comparison, the standard multilevel model for change was the clear winner.\n\nPerhaps most important, consider how choice of an error covariance structure affects our ability to address our research questions, especially given that it is the fixed effects–and not the variance components–that usually embody these questions. Some might say that refining the error covariance structure for the multilevel model for change is akin to rearranging the deck chairs on the Titanic–it rarely fundamentally changes our parameter estimates. Indeed, regardless of the error structure chosen, estimates of the fixed effects are unbiased and may not be affected much by choices made in the stochastic part of the model (providing that neither the data, nor the error structure, are idiosyncratic). (p. 264, emphasis in the original)\n\nFor more on the idea that researchers generally just care about fixed effects, see the paper by McNeish et al. (2017), On the unnecessary ubiquity of hierarchical linear modeling. Although I can’t disagree with the logic presented by Singer and Willett, or by McNeish and colleagues, I’m uneasy with this perspective for a couple reasons. First, I suspect part of the reason researchers don’t theorize about variances and covariances is because those are difficult metrics for many of us to think about. Happily, brms makes these more accessible by parameterizing them as standard deviations and correlations.\nSecond, in many disciplines, including my own (clinical psychology), multilevel models are still exotic and researchers just aren’t used to thinking in their terms. But I see this as more of a reason to spread the multilevel gospel than to down emphasize variance parameters. In my opinion, which is heavily influenced by McElreath’s, it would be great if someday soon, researchers used multilevel models (longitudinal or otherwise) as the default rather than the exception.\nThird, I actually care about random effects. If you go back and compare the models from this chapter, it was only the multilevel growth model (fit7.1) that assigned person-specific intercepts and slopes. In clinical psychology, this matters! I want a model that allows me to make plots like this:\n\n# 35 person-specific growth trajectories\nnd &lt;- opposites_pp %&gt;% \n  distinct(id, ccog) %&gt;% \n  expand(nesting(id, ccog),\n         time = c(0, 3))\n\nfitted(fit7.1, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = time + 1, y = Estimate, group = id)) +\n  geom_line(linewidth = 1/4, alpha = 2/3) +\n  labs(subtitle = \"35 person-specific growth trajectories\",\n       x = \"day\",\n       y = \"opposites naming task\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n# 35 person-specific intercept and slope parameters\nrbind(coef(fit7.1)$id[, , \"Intercept\"],\n      coef(fit7.1)$id[, , \"time\"]) %&gt;% \n  data.frame() %&gt;% \n  mutate(type = rep(c(\"intercepts\", \"slopes\"), each = n() / 2)) %&gt;% \n  group_by(type) %&gt;% \n  arrange(Estimate) %&gt;% \n  mutate(id = 1:35) %&gt;% \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = id)) +\n  geom_pointrange(linewidth = 1/2, size = 1/10) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = \"35 person-specific intercept and slope parameters\",\n       x = \"marginal posterior\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ type, scales = \"free_x\")\n\n\n\n\n\n\n\n\nOf all the models we fit, only the standard multilevel model for change allows us to drill down all the way to the individual participants and this was accomplished by how it parameterized \\(\\mathbf{\\Sigma}_r\\). Even your substantive theory isn’t built around the subtleties in the \\(4 \\times 4 = 16\\) cells in the \\(\\mathbf{\\Sigma}_r\\), it still matters that our fit7.1 parameterized them by way of \\(\\sigma_\\epsilon\\), \\(\\sigma_0\\), \\(\\sigma_1\\), and \\(\\rho_{01}\\).\nBut Singer and Willett went on:\n\nBut refining our hypotheses about the error covariance structure does affect the precision of estimates of the fixed effects and will therefore impact hypothesis testing and confidence interval construction. (p. 264, emphasis in the original)\n\nI’m going to showcase this in a coefficient plot, rather than the way the authors did in their Table 7.4. First, we’ll want a custom wrangling function. Let’s call it gamma_summary().\n\ngamma_summary &lt;- function(brmsfit) {\n  fixef(brmsfit)[1:4, ] %&gt;% \n    data.frame() %&gt;% \n    mutate(gamma = c(\"gamma[0][0]\", \"gamma[1][0]\", \"gamma[0][1]\", \"gamma[1][1]\"))\n}\n\ngamma_summary(fit7.1)\n\n             Estimate Est.Error        Q2.5       Q97.5       gamma\nIntercept 164.2819042 6.4642477 151.4539370 176.7035537 gamma[0][0]\ntime       26.9621087 2.0933771  22.9170354  31.2093872 gamma[1][0]\nccog       -0.1438951 0.5199258  -1.1788654   0.8879831 gamma[0][1]\ntime:ccog   0.4392846 0.1697978   0.1044915   0.7695013 gamma[1][1]\n\n\nNow wrangle and plot.\n\ntype &lt;- c(\n  \"standard growth model\", \"unstructured\", \"compound symmetric\", \n  \"heterogeneous compound symmetric\", \"autoregressive\", \"heterogeneous autoregressive\")\n\ntibble(fit  = str_c(\"fit7.\", c(1:6)),\n       type = factor(type, levels = type)) %&gt;% \n  mutate(type  = fct_rev(type),\n         gamma = map(fit, ~get(.) %&gt;% gamma_summary())) %&gt;% \n  unnest(gamma) %&gt;% \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = type)) +\n  geom_pointrange(size = 1/5) +\n  scale_x_continuous(\"marginal posterior\", expand = expansion(mult = 0.25)) +\n  labs(subtitle = \"Parameter precision can vary across model types.\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank()) +\n  facet_wrap(~ gamma, scales = \"free_x\", labeller = label_parsed, nrow = 1)\n\n\n\n\n\n\n\n\nYep, the way we parameterize \\(\\mathbf{\\Sigma}_r\\) might have subtle consequences for the widths of our marginal \\(\\gamma\\) posteriors. This can matter a lot when you’re analyzing data from, say, a clinical trial where lives may depend on the results of your analysis. Rearrange the deck chairs on your Titanic with caution, friends. Someone might trip and hurt themselves.\n\n\n7.3.8 Bonus: Did we trade multilevel for multivariate?\nSo far in this chapter, only the first model fit7.1 used the conventional brms multilevel syntax, such as (1 + time | id). The other five models used the unstr(), cosy(), or ar() helper functions, instead. Singer and Willett didn’t exactly come out and say it this way, but in a sense, fit7.1 is the only multilevel model we’ve fit so far in this chapter. Mallinckrod et al. (2008) made the point more clearly:\n\nA simple formulation of the general linear mixed model… can be implemented in which the random effects are not explicitly modeled, but rather are included as part of the marginal covariance matrix… leading them to what could alternatively be described as a multivariate normal model. (p. 306)\n\nThus we might think of all our models after fit7.1 as special kinds of multivariate panel models. To help make this easier to see, let’s first fit a simplified version of the unstructured model, following the form\n\\[\n\\begin{align}\n\\text{opp}_{ij} & \\sim \\operatorname{Normal}(\\mu_j, \\mathbf{\\Sigma}_r) \\\\\n\\mu_j & = \\pi_{0j} \\\\\n\\mathbf{\\Sigma}_r & = \\begin{bmatrix}\n  \\sigma_1^2 & \\sigma_{12} & \\sigma_{13} & \\sigma_{14} \\\\\n  \\sigma_{21} & \\sigma_2^2 & \\sigma_{23} & \\sigma_{24} \\\\\n  \\sigma_{31} & \\sigma_{32} & \\sigma_3^2 & \\sigma_{34} \\\\\n  \\sigma_{41} & \\sigma_{42} & \\sigma_{43} & \\sigma_4^2 \\end{bmatrix},\n\\end{align}\n\\]\nwhere \\(\\pi_{0j}\\) is the intercept, which is estimated separately for each of the 4 \\(j\\) time points, and \\(\\mathbf{\\Sigma}_r\\) is the unstructured variance/covariance matrix. To fit those separate \\(\\pi_{0j}\\) intercepts with brm(), we use the syntax of 0 + factor(time) in the \\(\\mu_j\\) portion of the model. Otherwise, the syntax is much the same as with the first unstructured model fit7.2.\n\nfit7.7 &lt;- brm(\n  data = opposites_pp, \n  family = gaussian,\n  bf(opp ~ 0 + factor(time) + unstr(time = time, gr = id),\n     sigma ~ 0 + factor(time)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.07\")\n\nCheck the parameter summary.\n\nprint(fit7.7, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: opp ~ 0 + factor(time) + unstr(time = time, gr = id) \n         sigma ~ 0 + factor(time)\n   Data: opposites_pp (Number of observations: 140) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n             Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\ncortime(0,1)    0.739     0.077    0.563    0.859 1.001     2354     2259\ncortime(0,2)    0.624     0.098    0.409    0.782 1.001     2037     2187\ncortime(1,2)    0.780     0.065    0.628    0.883 1.000     2782     2943\ncortime(0,3)    0.288     0.144   -0.015    0.545 1.001     2298     2515\ncortime(1,3)    0.585     0.107    0.346    0.761 1.002     3050     2823\ncortime(2,3)    0.735     0.077    0.555    0.857 1.001     3018     2793\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nfactortime0        164.606     6.252  152.389  177.005 1.002     2387     2579\nfactortime1        192.099     5.528  180.967  202.933 1.001     2121     2114\nfactortime2        216.843     5.886  205.079  228.142 1.001     2046     1889\nfactortime3        246.174     6.432  233.158  258.451 1.001     2523     2876\nsigma_factortime0    3.568     0.116    3.351    3.806 1.001     2785     3109\nsigma_factortime1    3.460     0.112    3.251    3.690 1.001     2478     2365\nsigma_factortime2    3.528     0.109    3.321    3.751 1.001     2239     2744\nsigma_factortime3    3.613     0.116    3.404    3.850 1.001     2875     2784\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOnce again, those 4 sigma_ parameters were fit using the log link. Here they are after exponentiation.\n\nfixef(fit7.7)[5:8, -2] %&gt;% exp()\n\n                  Estimate     Q2.5    Q97.5\nsigma_factortime0 35.45656 28.52038 44.97009\nsigma_factortime1 31.82985 25.81269 40.04808\nsigma_factortime2 34.05243 27.69567 42.56658\nsigma_factortime3 37.08273 30.09709 47.00746\n\n\nAll we’ve really done is fit an unconditional multivariate normal model for opp, across the four time points. To see, compare the posterior point estimates in the Population-Level Effects section (or their exponentiated values, above), to the sample means and SD’s for opp over time.\n\nopposites_pp %&gt;% \n  group_by(time) %&gt;% \n  summarise(m = mean(opp),\n            s = sd(opp))\n\n# A tibble: 4 × 3\n   time     m     s\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  164.  36.2\n2     1  192   33.5\n3     2  217.  35.9\n4     3  246.  37.6\n\n\nIn a similar way, the correlations in the Correlation Structures part of the print() output are really just regularized sample correlations. Compare them to the un-regularized Pearson’s correlation estimates.\n\nopposites_pp %&gt;% \n  select(id, time, opp) %&gt;% \n  pivot_wider(names_from = time, values_from = opp) %&gt;% \n  select(-id) %&gt;% \n  cor() %&gt;% \n  round(digits = 3)\n\n      0     1     2     3\n0 1.000 0.804 0.709 0.414\n1 0.804 1.000 0.847 0.678\n2 0.709 0.847 1.000 0.800\n3 0.414 0.678 0.800 1.000\n\n\nThe reason for the regularization, by the way, is because of the default LKJ(1) prior. That prior is more strongly regularizing as you increase the dimensions of the correlation matrix. In the case of a \\(4 \\times 4\\) matrix, the LKJ will definitely push the individual correlations toward zero, particularly in the case of a modestly-sized data set like opposites_pp.\nBut anyway, we can fit basically the same model by explicitly using the brms multivariate syntax, as outlined in Bürkner’s (2022) vignette, Estimating multivariate models with brms. Before we can fit a more conventional multivariate model to the data, we’ll need to convert opposites_pp to the wide format, which we’ll call opposites_wide.\n\nopposites_wide &lt;- opposites_pp %&gt;% \n  mutate(time = str_c(\"t\", time)) %&gt;% \n  select(-wave) %&gt;% \n  pivot_wider(names_from = time, values_from = opp)\n\n# what?\nhead(opposites_wide)\n\n# A tibble: 6 × 7\n     id   cog   ccog    t0    t1    t2    t3\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   137  23.5    205   217   268   302\n2     2   123   9.54   219   243   279   302\n3     3   129  15.5    142   212   250   289\n4     4   125  11.5    206   230   248   273\n5     5    81 -32.5    190   220   229   220\n6     6   110  -3.46   165   205   207   263\n\n\nNow we’re ready to fit a multivariate model to our 4 variables t0 through t3. To do so, we place all 4 variables within the mvbind() function, and we nest the entire model formula within the bf() function. Also notice we are explicitly asking for residual correlations by setting set_rescor(TRUE).\n\nfit7.8 &lt;- brm(\n  data = opposites_wide, \n  family = gaussian,\n  bf(mvbind(t0, t1, t2, t3) ~ 1) +\n    set_rescor(TRUE),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.08\")\n\nReview the model summary.\n\nprint(fit7.8, digits = 3)\n\n Family: MV(gaussian, gaussian, gaussian, gaussian) \n  Links: mu = identity\n         mu = identity\n         mu = identity\n         mu = identity \nFormula: t0 ~ 1 \n         t1 ~ 1 \n         t2 ~ 1 \n         t3 ~ 1 \n   Data: opposites_wide (Number of observations: 35) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nt0_Intercept  164.516     5.807  152.990  176.136 1.001     2339     2617\nt1_Intercept  192.037     5.210  181.789  201.859 1.001     2047     2374\nt2_Intercept  216.789     5.630  205.493  227.683 1.001     1967     2313\nt3_Intercept  246.113     6.292  233.689  258.336 1.000     2565     2718\n\nFurther Distributional Parameters:\n         Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma_t0   35.805     4.109   28.956   45.091 1.000     2540     2826\nsigma_t1   32.240     3.591   26.055   39.884 1.001     2103     2130\nsigma_t2   34.425     3.851   27.758   42.555 1.000     2240     2692\nsigma_t3   37.532     4.569   29.879   47.750 1.001     2610     2746\n\nResidual Correlations: \n              Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nrescor(t0,t1)    0.738     0.078    0.556    0.861 1.000     2183     2371\nrescor(t0,t2)    0.624     0.101    0.397    0.790 1.000     2007     2291\nrescor(t1,t2)    0.784     0.065    0.633    0.886 1.000     2613     3327\nrescor(t0,t3)    0.288     0.147   -0.015    0.549 1.000     2343     2962\nrescor(t1,t3)    0.590     0.105    0.357    0.770 1.000     2879     3158\nrescor(t2,t3)    0.737     0.076    0.564    0.860 1.000     2884     2930\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe results are very similar to those for fit7.7, above, with a few small differences. First, we now have a Residual Correlations section instead of a Correlation Structures. The output of the two is basically the same, though. The various sigma_ parameters are now in their own Family Specific Parameters, instead of in the bottom fo the Population-Level Effects section. You’ll also note the sigma_ parameters are all in their natural metric, rather than the log metric. This is because, by default, brms used the log link for the syntax we used for fit7.7, but used the identity link for the syntax we used for fit7.8.\nEven if you take these small formatting-type differences into account, a careful eye will notice the parameter summaries are still a little different between these two models. The reason is the defualt priors were a little different. Take a look:\n\nfit7.7$prior\n\n                prior    class        coef group resp  dpar nlpar lb ub tag       source\n               (flat)        b                                                   default\n               (flat)        b factortime0                                  (vectorized)\n               (flat)        b factortime1                                  (vectorized)\n               (flat)        b factortime2                                  (vectorized)\n               (flat)        b factortime3                                  (vectorized)\n               (flat)        b                        sigma                      default\n               (flat)        b factortime0            sigma                 (vectorized)\n               (flat)        b factortime1            sigma                 (vectorized)\n               (flat)        b factortime2            sigma                 (vectorized)\n               (flat)        b factortime3            sigma                 (vectorized)\n lkj_corr_cholesky(1) Lcortime                                                   default\n\nfit7.8$prior\n\n                   prior     class coef group resp dpar nlpar lb ub tag  source\n student_t(3, 166, 35.6) Intercept              t0                      default\n   student_t(3, 196, 43) Intercept              t1                      default\n student_t(3, 215, 32.6) Intercept              t2                      default\n student_t(3, 246, 47.4) Intercept              t3                      default\n    lkj_corr_cholesky(1)   Lrescor                                      default\n   student_t(3, 0, 35.6)     sigma              t0             0        default\n     student_t(3, 0, 43)     sigma              t1             0        default\n   student_t(3, 0, 32.6)     sigma              t2             0        default\n   student_t(3, 0, 47.4)     sigma              t3             0        default\n\n\nI’m not going to break down exactly why brms made different default prior choices for these models, but experienced brms users should find these results very predictable, particularly due to our use of the 0 + factor(time) syntax in the fit7.7 model. Know your software defaults, friends. So let’s try fitting these two models one more time, but with a couple adjustments. First, we will explicitly set the priors based on the defaults used by fit7.8. We will manually request the identity link for the sigma_ parameters in fit7.7. We will also increase the post-warmup draws for both models, to help account for MCMC sampling error.\nNow fit the models.\n\n# unstructured\nfit7.7b &lt;- brm(\n  data = opposites_pp, \n  family = brmsfamily(family = \"gaussian\", link = \"identity\", link_sigma = \"identity\"),\n  bf(opp ~ 0 + factor(time) + unstr(time = time, gr = id),\n     sigma ~ 0 + factor(time)),\n  prior = c(prior(student_t(3, 166, 35.6), class = b, coef = factortime0),\n            prior(student_t(3, 196, 43.0), class = b, coef = factortime1),\n            prior(student_t(3, 215, 32.6), class = b, coef = factortime2),\n            prior(student_t(3, 246, 47.4), class = b, coef = factortime3),\n            \n            prior(student_t(3, 0, 35.6), class = b, coef = factortime0, dpar = sigma),\n            prior(student_t(3, 0, 43.0), class = b, coef = factortime1, dpar = sigma),\n            prior(student_t(3, 0, 32.6), class = b, coef = factortime2, dpar = sigma),\n            prior(student_t(3, 0, 47.4), class = b, coef = factortime3, dpar = sigma),\n            \n            prior(lkj(1), class = cortime)),\n  iter = 13500, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.07b\")\n\n# multivariate\nfit7.8b &lt;- brm(\n  data = opposites_wide, \n  family = gaussian,\n  bf(mvbind(t0, t1, t2, t3) ~ 1) +\n    set_rescor(TRUE),\n  prior = c(prior(student_t(3, 166, 35.6), class = Intercept, resp = t0),\n            prior(student_t(3, 196, 43.0), class = Intercept, resp = t1),\n            prior(student_t(3, 215, 32.6), class = Intercept, resp = t2),\n            prior(student_t(3, 246, 47.4), class = Intercept, resp = t3),\n            \n            prior(student_t(3, 0, 35.6), class = sigma, resp = t0),\n            prior(student_t(3, 0, 43.0), class = sigma, resp = t1),\n            prior(student_t(3, 0, 32.6), class = sigma, resp = t2),\n            prior(student_t(3, 0, 47.4), class = sigma, resp = t3),\n            \n            prior(lkj(1), class = rescor)),\n  iter = 13500, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/fit07.08b\")\n\nThis time, let’s just compare the \\(\\mu\\) and \\(\\sigma\\) posteriors in a coefficient plot.\n\n# combine\nbind_rows(\n  as_draws_df(fit7.7b) %&gt;%\n    select(contains(\"factortime\")) %&gt;% \n    set_names(c(str_c(\"mu_\", 0:3), str_c(\"sigma_\", 0:3))),\n  as_draws_df(fit7.8b) %&gt;%\n    select(starts_with(\"b_\"), starts_with(\"sigma_\"))  %&gt;% \n    set_names(c(str_c(\"mu_\", 0:3), str_c(\"sigma_\", 0:3)))\n) %&gt;% \n  # wrangle\n  mutate(fit = rep(c(\"fit7.7b\", \"fit7.8b\"), each = n() / 2)) %&gt;% \n  pivot_longer(-fit) %&gt;% \n  separate(name, into = c(\"par\", \"time\"), sep = \"_\") %&gt;% \n  group_by(fit, par, time) %&gt;% \n  mean_qi(value) %&gt;%\n  \n  # plot!\n  ggplot(aes(x = value, xmin = .lower, xmax = .upper, y = time, group = fit, color = fit)) +\n  geom_pointrange(position = position_dodge(width = -0.5)) +\n  scale_color_viridis_d(NULL, end = 0.5) +\n  xlab(\"posterior\") +\n  facet_wrap(~ par, labeller = label_parsed, scales = \"free_x\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNow the results between the two methods are very similar. If you wanted to, you could make a similar kind of plot for the two versions of the correlation matrix, too. Thus, the various models Singer and Willett introduced in this chapter are special multivariate alternatives to the standard multilevel approach we’ve preferred up to this point. Cool, huh?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Examining the Multilevel Model's Error Covariance Structure</span>"
    ]
  },
  {
    "objectID": "07.html#session-info",
    "href": "07.html#session-info",
    "title": "7  Examining the Multilevel Model’s Error Covariance Structure",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 brms_2.23.0     Rcpp_1.1.0      lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0   dplyr_1.1.4    \n [8] purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] estimability_1.5.1      timechange_0.3.0        lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3         \n[16] magrittr_2.0.4          posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7             tools_4.5.1            \n[21] utf8_1.2.6              knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1    htmlwidgets_1.6.4      \n[26] bit_4.6.0               pkgbuild_1.4.8          curl_7.0.0              plyr_1.8.9              RColorBrewer_1.1-3     \n[31] abind_1.4-8             multcomp_1.4-29         withr_3.0.2             grid_4.5.1              stats4_4.5.1           \n[36] xtable_1.8-4            inline_0.3.21           emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65            \n[41] cli_3.6.5               mvtnorm_1.3-3           rmarkdown_2.30          crayon_1.5.3            generics_0.1.4         \n[46] RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000      \n[51] splines_4.5.1           bayesplot_1.15.0.9000   parallel_4.5.1          matrixStats_1.5.0       vctrs_0.6.5            \n[56] V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          arrayhelpers_1.1-0     \n[61] hms_1.1.4               bit64_4.6.0-1           ggdist_3.3.3            glue_1.8.0              codetools_0.2-20       \n[66] distributional_0.5.0    stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1          \n[71] htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1                vroom_1.6.6             evaluate_1.0.5         \n[76] lattice_0.22-7          backports_1.5.0         rstantools_2.5.0.9000   coda_0.19-4.1           gridExtra_2.3          \n[81] nlme_3.1-168            checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Examining the Multilevel Model's Error Covariance Structure</span>"
    ]
  },
  {
    "objectID": "07.html#comments",
    "href": "07.html#comments",
    "title": "7  Examining the Multilevel Model’s Error Covariance Structure",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBürkner, P.-C. (2022). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nMallinckrod, C. H., Lane, P. W., Schnell, D., Peng, Y., & Mancuso, J. P. (2008). Recommendations for the primary analysis of continuous endpoints in longitudinal clinical trials. Drug Information Journal, 42(4), 303–319. https://doi.org/10.1177/009286150804200402\n\n\nMcNeish, D., Stapleton, L. M., & Silverman, R. D. (2017). On the unnecessary ubiquity of hierarchical linear modeling. Psychological Methods, 22(1), 114. https://doi.org/10.1037/met0000078\n\n\nPinheiro, J., Bates, D., & R-core. (2021). nlme: Linear and nonlinear mixed effects models [Manual]. https://CRAN.R-project.org/package=nlme\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nWillett, J. B. (1988). Chapter 9: Questions and answers in the measurement of change. Review of Research in Education, 15, 345–422. https://doi.org/10.2307/1167368",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Examining the Multilevel Model's Error Covariance Structure</span>"
    ]
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Modeling Change Using Covariance Structure Analysis",
    "section": "",
    "text": "Session info\nTo be fleshed out later\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5         tools_4.5.1       htmltools_0.5.9   rstudioapi_0.17.1\n [8] rmarkdown_2.30    knitr_1.51        jsonlite_2.0.0    xfun_0.55         digest_0.6.39     rlang_1.1.7       evaluate_1.0.5",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Change Using Covariance Structure Analysis</span>"
    ]
  },
  {
    "objectID": "08.html#comments",
    "href": "08.html#comments",
    "title": "8  Modeling Change Using Covariance Structure Analysis",
    "section": "Comments",
    "text": "Comments",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modeling Change Using Covariance Structure Analysis</span>"
    ]
  },
  {
    "objectID": "09.html",
    "href": "09.html",
    "title": "9  A Framework for Investigating Event Occurrence",
    "section": "",
    "text": "9.1 Should you conduct a survival analysis? The “whether” and “when” test",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Framework for Investigating Event Occurrence</span>"
    ]
  },
  {
    "objectID": "09.html#should-you-conduct-a-survival-analysis-the-whether-and-when-test",
    "href": "09.html#should-you-conduct-a-survival-analysis-the-whether-and-when-test",
    "title": "9  A Framework for Investigating Event Occurrence",
    "section": "",
    "text": "To determine whether a research question calls for survival analysis, we find it helpful to apply a simple mnemonic we refer to as “the whether and when test.” If your research questions include either word–whether or when–you probably need to use survival methods. (p. 306, emphasis added)\n\n\n9.1.1 Time to relapse among recently treated alcoholics\nWithin the addictive-behaviors literature, researchers often study if and when participants relapse (i.e., begin using the substance(s) again).\n\n\n9.1.2 Length of stay in teaching\nEducation researchers can use survival analysis to study whether and for how long newly-hired teachers stay in their positions.\n\n\n9.1.3 Age at first suicide ideation\nSuicide is a major health risk and clinical researchers sometimes use survival analysis to whether and when participants have first considered killing themselves.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Framework for Investigating Event Occurrence</span>"
    ]
  },
  {
    "objectID": "09.html#framing-a-research-question-about-event-occurrence",
    "href": "09.html#framing-a-research-question-about-event-occurrence",
    "title": "9  A Framework for Investigating Event Occurrence",
    "section": "9.2 Framing a research question about event occurrence",
    "text": "9.2 Framing a research question about event occurrence\nSurvival analyses share three common characteristics.\n\nEach has a clearly defined:\n\nTarget event, whose occurrence is being studies\nBeginning of time, an initial starting point when no one under study has yet experienced the target event\nMetrics for clocking time, a meaningful scale in which event occurrence is recorded (p. 310, emphasis in the original)\n\n\n\n9.2.1 Defining event occurrence\n“Event occurrence represents an individual’s transition from one ‘state’ to another ‘state’” (p. 310). Though our primary focus will be on binary states (e.g., drinking/abstinent), survival analyses can handle more categories (e.g., whether/when marriages end in divorce or death).\n\n\n9.2.2 Identifying the “beginning of time”\n\nThe “beginning of time” is a moment when everyone in the population occupies one, and only one, of the possible states… Over time, as individuals move from the original state to the next, they experience the target event. The timing of this transition–the distance from the “beginning of time” until the event occurrence–is referred to as the event time.\nTo identify the “beginning of time” in a given study, imagine placing everyone in the population on a time-line, an axis with the “beginning of time” at one end and the last moment when event occurrence could be observed at the other. The goal is to “start the clock” when on one in the population has yet experienced the event but everyone is at least (theoretically) eligible to do so. In the language of survival analysis, you want to start the clock when everyone in the population is at risk of experiencing the event. (pp. 311–312, emphasis in the original)\n\n\n\n9.2.3 Specifying a metric for time\n\nWe distinguish between data recorded in thin precise units and those recorded in thicker intervals by calling the former continuous time and the latter discrete time.\n[Though survival methods can handle both discrete and continuous time,] time should be recorded in the smallest possible units relevant to the process under study. No single metric is universally appropriate, and even different studies of the identical event might use different scales. (p. 313, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Framework for Investigating Event Occurrence</span>"
    ]
  },
  {
    "objectID": "09.html#censoring-how-complete-are-the-data-on-event-occurrence",
    "href": "09.html#censoring-how-complete-are-the-data-on-event-occurrence",
    "title": "9  A Framework for Investigating Event Occurrence",
    "section": "9.3 Censoring: How complete are the data on event occurrence?",
    "text": "9.3 Censoring: How complete are the data on event occurrence?\n\nNo matter when data collection begins, and no matter how long it lasts, some sample members are likely to have unknown event times. Statisticians call this problem censoring and they label the people with the unknown event times censored observations. Because censoring is inevitable–and a fundamental conundrum in the study of event occurrence–we now explore it in detail. (p. 316, emphasis in the original)\n\n\n9.3.1 How and why does censoring arise?\n\nCensoring occurs whenever a researcher does not know an individual’s event time. There are two major reasons for censoring: (1) some individuals will never experience the target event; and (2) others will experience the event, but not during the study’s data collection. Some of these latter individuals will experience the event shortly after data collection ends while others will do so at a much later time. As a practical matter, though, these distinctions matter little because you cannot distinguish among them. That, unfortunately, is the nature of censoring: it prevents you from knowing the very quantity of interest–whether and, if so, when the target event occurs for a subset of the sample. (pp. 316–317, emphasis in the original)\n\n\n\n9.3.2 Different types of censoring\n“Methodologists make two major types of distinctions: first, between non-informative and informative censoring mechanisms, and second, between right- and left-censoring” (p. 318, emphasis in the original).\n\n9.3.2.1 Noninformative versus informative censoring\n\nA noninformative censoring mechanism operates independent of event occurrence and the risk of event occurrence. If censoring is under an investigator’s control, determined in advance by design–as it usually is–then it is noninformative… [Under this mechanism] we can therefore assume that all individuals who remain in the study after the censoring date are representative of everyone who would have remained in the study had censoring not occurred.\nIf censoring occurs because individuals have experienced the event or are likely to do so in the future, the censoring mechanism is informative… Under these circumstances, we can no longer assume that those people who remain in the study after this tie are representative of all individuals who would have remained in the study had censoring not occurred. The noncensored individuals differ systematically from the censored individuals. (pp. 318–319, emphasis in the original)\n\n\n\n9.3.2.2 Right- versus left-censoring\n\nRight-censoring arises when an event time is unknown because event occurrence is not observed. Left-censoring arises when an event time is unknown because the beginning of time is not observed…. Because [right-censoring] is the one typically encountered in practice, and because it is the type for which survival methods were developed, references to censoring, unencumbered by a directional modifier, usually refer to right-censoring.\nHow to left-censored observations arise? Often they arise because researchers have not paid sufficient attention to identifying the beginning of time during the design phase. If the beginning of time is defined well–as that moment when all individuals in the population are eligible to experience the event but none have yet done so–left-censoring can be eliminated….\nLeft-censoring presents challenges not easily addressed even with the most sophisticated of survival methods (Hu & Lawless, 1996). Little progress has been made in this area since Turnbull (1976) offered some basic descriptive approaches and Flinn and Heckman (1982) and Cox and Oakes (1984) offered some directions for fitting models under a restrictive set of assumptions. The most common advice, followed by Fichman, is to set the left-censored spells aside from analysis…. Redefining the beginning of time to coincide with a precipitating event… is often the best way of resolving the otherwise intractable problems that left-censored data pose. Whenever possible, we suggest that researchers consider such a redefinition or otherwise eliminate left-censored data through design. (pp. 319–320, emphasis in the original)\n\n\n\n\n9.3.3 How does censoring affect statistical analysis?\nHere we load the teachers.csv data Singer (1992).\n\nlibrary(tidyverse)\n\nteachers &lt;- read_csv(\"data/teachers.csv\")\n\nglimpse(teachers)\n\nRows: 3,941\nColumns: 3\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32…\n$ t      &lt;dbl&gt; 1, 2, 1, 1, 12, 1, 12, 1, 2, 2, 7, 12, 1, 12, 12, 2, 12, 1, 3, 2, 12, 12, 9, 12, 2, 3, 1, 2, 2, 4, 3, 12, 1, 5, 1…\n$ censor &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,…\n\n\nMake a version of Figure 9.1.\n\nteachers %&gt;% \n  count(censor, t) %&gt;% \n  mutate(censor = if_else(censor == \"0\", \"not censored\", \"censored\")) %&gt;% \n  \n  ggplot(aes(x = t)) +\n  geom_col(aes(y = n)) +\n  geom_text(aes(y = n + 25, label = n)) +\n  scale_x_continuous(\"years\", breaks = 1:12) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~censor, nrow = 2)\n\n\n\n\n\n\n\n\nHere’s a descriptive breakdown of those censored or not.\n\nteachers %&gt;% \n  group_by(censor) %&gt;% \n  summarise(n    = n(),\n            mean = mean(t),\n            sd   = sd(t)) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 2 × 5\n  censor     n  mean    sd percent\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1      0  2207  3.73  2.41    56.0\n2      1  1734  9.60  1.78    44.0\n\n\nWhereas the distribution of the censored occasions is flattish with a bit of a spike at 12, the distribution of the non-censored times has a bit of an exponential look to it. Recall that the exponential distribution is controlled by a single parameter, its rate, and the mean of the exponential distribution is the reciprocal of that rate. If we take the empirical mean and \\(n\\) of the non-censored data and plot those in to the rexp() function, we can simulate exponential data and plot.\n\nset.seed(9)\n\ntibble(years = rexp(n = 2207, rate = 1 / 3.7)) %&gt;% \n  ggplot(aes(x = years)) +\n  geom_histogram(binwidth = 1, boundary = 0) +\n  scale_x_continuous(breaks = 1:12) +\n  coord_cartesian(xlim = c(0, 12)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThat simulation looks pretty similar to our non-censored data. If we stopped there, we might naïvely presume \\(\\operatorname{Exponential}(1/3.7)\\) is a good model for our data. But this would ignore the censored data. One of the solutions researchers have used is\n\nto assign the censored cases the event time they possess at the end of the data collection (e.g., Frank & Keith, 1984). Applying this to our teacher career data (e.g., assigning a career length of 7 years to the 280 teachers censored in the year 7, etc.) yields an estimated mean career duration of 7.5 years. (pp. 322–323)\n\nHere’s what that looks like.\n\nteachers %&gt;% \n  summarise(mean   = mean(t),\n            median = median(t),\n            sd     = sd(t))\n\n# A tibble: 1 × 3\n   mean median    sd\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  6.31      7  3.63\n\n\nI have no idea where the 7.5 value Singer and Willett presented came from. It’s larger than both the mean and the median in the data. But anyway, this method is patently wrong, so it doesn’t matter:\n\nImputing event times for censored cases simply changes all “nonevents” into “events” and further assumes that all these new “events” occur at the earliest time possible–that is, at the moment of censoring. Surely these decisions are most likely wrong. (p. 323)\n\nStay tuned for methods that are better than patently wrong.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Framework for Investigating Event Occurrence</span>"
    ]
  },
  {
    "objectID": "09.html#session-info",
    "href": "09.html#session-info",
    "title": "9  A Framework for Investigating Event Occurrence",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2    \n [8] tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] bit_4.6.0          gtable_0.3.6       jsonlite_2.0.0     crayon_1.5.3       compiler_4.5.1     tidyselect_1.2.1  \n [7] parallel_4.5.1     scales_1.4.0       fastmap_1.2.0      R6_2.6.1           labeling_0.4.3     generics_0.1.4    \n[13] knitr_1.51         htmlwidgets_1.6.4  pillar_1.11.1      RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.7       \n[19] stringi_1.8.7      xfun_0.55          S7_0.2.1           bit64_4.6.0-1      timechange_0.3.0   cli_3.6.5         \n[25] withr_3.0.2        magrittr_2.0.4     digest_0.6.39      grid_4.5.1         vroom_1.6.6        rstudioapi_0.17.1 \n[31] hms_1.1.4          lifecycle_1.0.5    vctrs_0.6.5        evaluate_1.0.5     glue_1.8.0         farver_2.1.2      \n[37] rmarkdown_2.30     tools_4.5.1        pkgconfig_2.0.3    htmltools_0.5.9",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Framework for Investigating Event Occurrence</span>"
    ]
  },
  {
    "objectID": "09.html#comments",
    "href": "09.html#comments",
    "title": "9  A Framework for Investigating Event Occurrence",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nCox, D. R., & Oakes, D. (1984). Analysis of survival data (Vol. 21). CRC Press. https://www.routledge.com/Analysis-of-Survival-Data/Cox-Oakes/p/book/9780412244902\n\n\nFlinn, C. J., & Heckman, J. J. (1982). New methods for analyzing individual event histories. Sociological Methodology, 13, 99–140. https://doi.org/10.2307/270719\n\n\nFrank, A. R., & Keith, T. Z. (1984). Academic abilities of persons entering and remaining in special education. Exceptional Children, 51(1), 76–77. https://eric.ed.gov/?id=EJ306852\n\n\nHu, X. J., & Lawless, J. F. (1996). Estimation from truncated lifetime data with supplementary information on covariates and censoring times. Biometrika, 83(4), 747–761. https://doi.org/10.1093/biomet/83.4.747\n\n\nSinger, J. D. (1992). Are special educators’ career paths special? Results from a 13-year longitudinal study. Exceptional Children, 59(3), 262–279. https://doi.org/10.1177/001440299305900309\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nTurnbull, B. W. (1974). Nonparametric estimation of a survivorship function with doubly censored data. Journal of the American Statistical Association, 69(345), 169–173. https://doi.org/10.1080/01621459.1974.10480146\n\n\nTurnbull, B. W. (1976). The empirical distribution function with arbitrarily grouped, censored and truncated data. Journal of the Royal Statistical Society: Series B (Methodological), 38(3), 290–295. https://doi.org/10.1111/j.2517-6161.1976.tb01597.x",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Framework for Investigating Event Occurrence</span>"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "",
    "text": "10.1 The life table\nTo make a life table as presented in Table 10.1, we need to load the teachers.csv data.\nlibrary(tidyverse)\n\nteachers &lt;- read_csv(\"data/teachers.csv\")\n\nglimpse(teachers)\n\nRows: 3,941\nColumns: 3\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32…\n$ t      &lt;dbl&gt; 1, 2, 1, 1, 12, 1, 12, 1, 2, 2, 7, 12, 1, 12, 12, 2, 12, 1, 3, 2, 12, 12, 9, 12, 2, 3, 1, 2, 2, 4, 3, 12, 1, 5, 1…\n$ censor &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,…\nPerhaps the easiest way to make a life table as presented in Table 10.1 is with help from the survival package (Terry M. Therneau, 2021b; Terry M. Therneau & Grambsch, 2000).\nlibrary(survival)\nHere we’ll use the survfit() function to compute survival curves. Within the survfit() function, we’ll use the Surv() function to make a survival object, which will become the criterion within the model formula. It takes two basic arguments, time and event. With the teachers data, t is time in years. In the data, events are encoded in censor. However, it’s important to understand how the event argument expects the data. From the survival reference manual (Terry M. Therneau, 2021a), we read that event is “the status indicator, normally 0=alive, 1=dead. Other choices are TRUE/FALSE (TRUE = death) or 1/2 (2=death).” Note that whereas within our data censor is coded 0 = event 1 = censored, the event argument expects the opposite. A quick way to solve that is to enter 1 - censor.\nfit10.1 &lt;- survfit(\n  data = teachers,\n  Surv(t, 1 - censor) ~ 1)\nUse the str() function to survey the results.\nfit10.1 %&gt;% str()\n\nList of 17\n $ n        : int 3941\n $ time     : num [1:12] 1 2 3 4 5 6 7 8 9 10 ...\n $ n.risk   : num [1:12] 3941 3485 3101 2742 2447 ...\n $ n.event  : num [1:12] 456 384 359 295 218 184 123 79 53 35 ...\n $ n.censor : num [1:12] 0 0 0 0 0 0 280 307 255 265 ...\n $ surv     : num [1:12] 0.884 0.787 0.696 0.621 0.566 ...\n $ std.err  : num [1:12] 0.00576 0.00829 0.01053 0.01245 0.01396 ...\n $ cumhaz   : num [1:12] 0.116 0.226 0.342 0.449 0.538 ...\n $ std.chaz : num [1:12] 0.00542 0.00781 0.00992 0.01173 0.01319 ...\n $ type     : chr \"right\"\n $ logse    : logi TRUE\n $ conf.int : num 0.95\n $ conf.type: chr \"log\"\n $ lower    : num [1:12] 0.874 0.774 0.682 0.606 0.55 ...\n $ upper    : num [1:12] 0.894 0.8 0.71 0.636 0.581 ...\n $ t0       : num 0\n $ call     : language survfit(formula = Surv(t, 1 - censor) ~ 1, data = teachers)\n - attr(*, \"class\")= chr \"survfit\"\nWe can retrieve the values for the “Year” column from fit10.1$time. The values in the “Time interval” column are a simple transformation from there.\nfit10.1$time\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\nWe can find the values in the “Employed at the beginning of the year” column in fit10.1$n.risk and those in the “Who left during the year” column in fit10.1$n.event.\nfit10.1$n.risk\n\n [1] 3941 3485 3101 2742 2447 2229 2045 1642 1256  948  648  391\n\nfit10.1$n.event\n\n [1] 456 384 359 295 218 184 123  79  53  35  16   5\nWe’ll have to work a little harder to compute the values in the “Censored at the end of the year column.” Here we’ll walk it through in a data frame format.\ndata.frame(n_risk  = fit10.1$n.risk,\n           n_event = fit10.1$n.event) %&gt;% \n  mutate(n_risk_1 = lead(n_risk, default = 0)) %&gt;% \n  mutate(n_censored = n_risk - n_event - n_risk_1)\n\n   n_risk n_event n_risk_1 n_censored\n1    3941     456     3485          0\n2    3485     384     3101          0\n3    3101     359     2742          0\n4    2742     295     2447          0\n5    2447     218     2229          0\n6    2229     184     2045          0\n7    2045     123     1642        280\n8    1642      79     1256        307\n9    1256      53      948        255\n10    948      35      648        265\n11    648      16      391        241\n12    391       5        0        386\nThat is, to get the number of those censored at the end of a given year, you take the number employed at the beginning of that year, subtract the number of those who left (i.e., the number who experienced the “event”), and then subtract the number of those employed at the beginning of the next year. Notice our use of the dplyr::lead() function to get the number employed in the next year (learn more about that function here).\nTo get the values in the “Teachers at the beginning of the year who left during the year” column, which is in a proportion metric, we use division.\nfit10.1$n.event / fit10.1$n.risk\n\n [1] 0.11570667 0.11018651 0.11576911 0.10758570 0.08908868 0.08254823 0.06014670 0.04811206 0.04219745 0.03691983 0.02469136\n[12] 0.01278772\nFinally, to pull the values in the “All teachers still employed at the end of the year” column, we just execute fit10.1$surv.\nfit10.1$surv\n\n [1] 0.8842933 0.7868561 0.6957625 0.6209084 0.5655925 0.5189038 0.4876935 0.4642295 0.4446402 0.4282242 0.4176508 0.4123100\nLet’s put that all together in a tibble.\nmost_rows &lt;- tibble(year = fit10.1$time) %&gt;% \n  mutate(time_int   = str_c(\"[\", year, \", \", year + 1, \")\"), \n         n_employed = fit10.1$n.risk, \n         n_left     = fit10.1$n.event) %&gt;% \n  mutate(n_censored   = n_employed - n_left - lead(n_employed, default = 0),\n         hazard_fun   = n_left / n_employed,\n         survivor_fun = fit10.1$surv)\n\nmost_rows\n\n# A tibble: 12 × 7\n    year time_int n_employed n_left n_censored hazard_fun survivor_fun\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 [1, 2)         3941    456          0     0.116         0.884\n 2     2 [2, 3)         3485    384          0     0.110         0.787\n 3     3 [3, 4)         3101    359          0     0.116         0.696\n 4     4 [4, 5)         2742    295          0     0.108         0.621\n 5     5 [5, 6)         2447    218          0     0.0891        0.566\n 6     6 [6, 7)         2229    184          0     0.0825        0.519\n 7     7 [7, 8)         2045    123        280     0.0601        0.488\n 8     8 [8, 9)         1642     79        307     0.0481        0.464\n 9     9 [9, 10)        1256     53        255     0.0422        0.445\n10    10 [10, 11)        948     35        265     0.0369        0.428\n11    11 [11, 12)        648     16        241     0.0247        0.418\n12    12 [12, 13)        391      5        386     0.0128        0.412\nThe only thing missing from our version of Table 10.1 is we don’t have a row for Year 0. Here’s a quick and dirty way to manually insert those values.\nrow_1 &lt;- tibble(year         = 0, \n                time_int     = \"[0, 1)\", \n                n_employed   = fit10.1$n.risk[1], \n                n_left       = NA, \n                n_censored   = NA, \n                hazard_fun   = NA, \n                survivor_fun = 1)\n\nd &lt;- bind_rows(row_1, most_rows)\n\nd\n\n# A tibble: 13 × 7\n    year time_int n_employed n_left n_censored hazard_fun survivor_fun\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1     0 [0, 1)         3941     NA         NA    NA             1    \n 2     1 [1, 2)         3941    456          0     0.116         0.884\n 3     2 [2, 3)         3485    384          0     0.110         0.787\n 4     3 [3, 4)         3101    359          0     0.116         0.696\n 5     4 [4, 5)         2742    295          0     0.108         0.621\n 6     5 [5, 6)         2447    218          0     0.0891        0.566\n 7     6 [6, 7)         2229    184          0     0.0825        0.519\n 8     7 [7, 8)         2045    123        280     0.0601        0.488\n 9     8 [8, 9)         1642     79        307     0.0481        0.464\n10     9 [9, 10)        1256     53        255     0.0422        0.445\n11    10 [10, 11)        948     35        265     0.0369        0.428\n12    11 [11, 12)        648     16        241     0.0247        0.418\n13    12 [12, 13)        391      5        386     0.0128        0.412\nWe might walk out the notation in our time_int column a bit. Those intervals\nThe values in the n_employed column the risk set, those who are “eligible to experience the event during that interval” (p. 329, emphasis in the original).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#the-life-table",
    "href": "10.html#the-life-table",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "",
    "text": "The fundamental tool for summarizing the sample distribution of event occurrence is the life table. As befits its name, a life table tracks the event histories (the “lives”) of a sample of individuals from the beginning of time (when no one has yet experienced the target event) through the end of data collection. (p. 326, emphasis in the original)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreflect a standard partition of time, in which each interval includes the initial time and excludes the concluding time. Adopting common mathematical notation, [brackets] denote inclusions and (parentheses) denote exclusions. Thus, we bracket each interval’s initial time and place a parenthesis around its concluding time. (p. 328, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#a-framework-for-characterizing-the-distribution-of-discrete-time-event-occurrence-data",
    "href": "10.html#a-framework-for-characterizing-the-distribution-of-discrete-time-event-occurrence-data",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "10.2 A framework for characterizing the distribution of discrete-time event occurrence data",
    "text": "10.2 A framework for characterizing the distribution of discrete-time event occurrence data\n\nThe fundamental quantity used to assess the risk of event occurrence in each discrete time period is known as hazard. Denoted by \\(h(t_{ij})\\), discrete time hazard is the conditional probability that individual \\(i\\) will experience the event time in period \\(j\\), given that he or she did not experience it in any earlier time period. Because hazard represents the risk of the event occurrence in each discrete time period among those people eligible to experience the event (those in the risk set) hazard tells us precisely what we want to know: whether and when events occurs. (p. 330, emphasis in the original)\n\nIf we let \\(T_i\\) stand for the discrete value in time person \\(i\\) experiences the event, we can express the conditional probability the event might occur in the \\(j^\\text{th}\\) interval as\n\\[h(t_{ij}) = \\Pr[T_i = j \\mid T \\geq j].\\]\nThat last part, \\(T \\geq j\\), clarifies the event can only occur once and, therefore, cannot have occurred in any of the prior levels of \\(j\\). More plainly put, imagine the event is death and person \\(i\\) died during the period of \\(T_j = 20\\). In such a case, it’s nonsensical to speak of that \\(i^\\text{th}\\) person’s hazard for the period of \\(T_j = 25\\). They’re already dead.\nAlso, “the discrete-time hazard probabilities expressed as a function of time–labeled \\(h(t_{ij})\\)–is known as the population discrete-time hazard function” (p 330, emphasis in the original). That was expressed in the 6th column in Table 10.1, which we called hazard_fun in our d tibble.\n\nd %&gt;% \n  select(year, hazard_fun)\n\n# A tibble: 13 × 2\n    year hazard_fun\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1     0    NA     \n 2     1     0.116 \n 3     2     0.110 \n 4     3     0.116 \n 5     4     0.108 \n 6     5     0.0891\n 7     6     0.0825\n 8     7     0.0601\n 9     8     0.0481\n10     9     0.0422\n11    10     0.0369\n12    11     0.0247\n13    12     0.0128\n\n\nYou might notice \\(h(t_{ij})\\) is in a proportion metric and it is not cumulative. If you look above in the code, you’ll see we computed that by hazard_fun = n_left / n_employed. More formally and generally, this is an operationalization of\n\\[\\hat h(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j},\\]\nwhere \\(n \\text{ events}_j\\) is the number of individuals who experienced the event in the \\(j^{th}\\) period and \\(n \\text{ at risk}_j\\) is the number within the period who have not (a) already experienced the event and (b) been censored. Also note that by \\(\\hat h(t_{j})\\), we’re indicating we’re talking about the maximum likelihood estimate for \\(h(t_{j})\\). Because no one is at risk during the initial time point, \\(h(t_0)\\) is undefined (i.e., NA). Here we mimic the top panel of Figure 10.1 and plot our \\(\\hat h(t_{j})\\) over time.\n\nd %&gt;% \n  ggplot(aes(x = year, y = hazard_fun)) +\n  geom_line() +\n  scale_x_continuous(\"years in teaching\", breaks = 0:13, limits = c(0, 13)) +\n  scale_y_continuous(expression(\"estimated hazard probability, \"*hat(italic(h))(italic(t[j]))), \n                     breaks = c(0, 0.05, 0.1, 0.15), limits = c(0, 0.15)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n10.2.1 Survivor function\nThe survivor function provides another way of describing the distribution of event occurrence over time. Unlike the hazard function, which assesses the unique risk associated with each time period, the survivor function cumulates these period-by-period risks of event occurrence (or more properly, nonoccurrence) together to assess the probability that a randomly selected individual will survive will not experience the event.\nWe can formally define the survivor function, \\(S(t_{ij})\\), as\n\\[S(t_{ij}) = \\Pr[T &gt; j],\\]\nwhere \\(S\\) is survival as a function of time, \\(t\\). But since our data are finite, we can only have an estimate of the “true” survivor function, which we call \\(\\hat S(t_{ij})\\). Here it is in a plot, our version of the bottom panel of Figure 10.1.\n\nd %&gt;% \n  ggplot(aes(x = year, y = survivor_fun)) +\n  geom_hline(yintercept = 0.5, color = \"white\", linetype = 2) +\n  geom_line() +\n  scale_x_continuous(\"years in teaching\", breaks = 0:13, limits = c(0, 13)) +\n  scale_y_continuous(expression(\"estimated survival probability, \"*hat(italic(S))(italic(t[j]))),\n                     breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n10.2.2 Median lifetime\n\nHaving characterized the distribution of event times using the hazard and survivor functions, we often want to identify the distribution’s center. Were there no censoring, all event times would be known, and we could compute a sample mean. But because of censoring, another estimate of central tendency is preferred: the median lifetime.\nThe estimated median lifetime identifies that value for \\(T\\) for which the value of the estimated survivor function is .5. It is the point in time by which we estimate that half of the sample has experienced the target event, half has not. (p. 337, emphasis in the original)\n\nIf we use filter(), well see our median lifetime rests between years 6 and 7.\n\nd %&gt;% \n  filter(year %in% c(6, 7)) %&gt;%\n  # this just simplifies the output\n  select(year, time_int, survivor_fun)\n\n# A tibble: 2 × 3\n   year time_int survivor_fun\n  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1     6 [6, 7)          0.519\n2     7 [7, 8)          0.488\n\n\nUsing a simple descriptive approach, we’d just say the median lifetime was between years 6 and 7. We could also follow Miller (1981) and linearly interpolate between the two values of \\(S(t_j)\\) bracketing .5. If we let \\(m\\) be the time interval just before the median lifetime, \\(\\hat S(t_m)\\) be the value of the survivor function in that \\(m^\\text{th}\\) interval, and \\(\\hat S(t_{m + 1})\\) be the survival value in the next interval, the can write\n\\[\\text{Estimated median lifetime} = m + \\Bigg [\\frac{\\hat S(t_m) - .5}{\\hat S(t_m) - \\hat S(t_{m + 1})} \\Bigg ] \\big ((m + 1) - m \\big).\\]\nWe can compute that by hand like so.\n\nm        &lt;- 6\nm_plus_1 &lt;- 7\n\nstm &lt;- d %&gt;% \n  filter(year == m) %&gt;% \n  pull(survivor_fun)\n\nstm_plus_1 &lt;- d %&gt;% \n  filter(year == m_plus_1) %&gt;% \n  pull(survivor_fun)\n\n# compute the interpolated median lifetime and save it as `iml`\niml &lt;- m + ((stm - .5) / (stm - stm_plus_1)) * ((m + 1) - m)\niml\n\n[1] 6.605691\n\n\nNow we have the iml value, we can add that information to our version of the lower panel of Figure 10.1.\n\nline &lt;- tibble(\n  year         = c(0, iml, iml),\n  survivor_fun = c(0.5, 0.5, 0))\n\nd %&gt;% \n  ggplot(aes(x = year, y = survivor_fun)) +\n  geom_path(data = line,\n            color = \"white\", linetype = 2) +\n  geom_line() +\n  annotate(geom = \"text\",\n           x = iml, y = 0.55,\n           label = \"All teachers (6.6 years)\",\n           hjust = 0) +\n  scale_x_continuous(\"years in teaching\", breaks = 0:13, limits = c(0, 13)) +\n  scale_y_continuous(expression(\"estimated survival probability, \"*hat(italic(S))(italic(t[j]))),\n                     breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe can compute the estimates for the 5- and 10-year survival rates as a direct algebraic transformation of the survival function from those years.\n\nd %&gt;% \n  filter(year %in% c(5, 10)) %&gt;% \n  select(year, survivor_fun) %&gt;% \n  mutate(`survival rate (%)` = (100 * survivor_fun) %&gt;% round(digits = 0))\n\n# A tibble: 2 × 3\n   year survivor_fun `survival rate (%)`\n  &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n1     5        0.566                  57\n2    10        0.428                  43",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#developing-intuition-about-hazard-functions-survivor-functions-and-median-lifetimes",
    "href": "10.html#developing-intuition-about-hazard-functions-survivor-functions-and-median-lifetimes",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "10.3 Developing intuition about hazard functions, survivor functions, and median lifetimes",
    "text": "10.3 Developing intuition about hazard functions, survivor functions, and median lifetimes\n\nDeveloping intuition about these sample statistics requires exposure to estimates computed from a wide range of studies. To jump-start this process, we review results from four studies that differ across three salient dimensions–the type of event investigated, the metric used to record discrete time, and most important, the underlying profile of risk–and discuss how we would examine, and describe, the estimated hazard functions, survivor functions, and median lifetimes. (p. 339)\n\nHere we load the four relevant data sets.\n\ncocaine  &lt;- read_csv(\"data/cocaine_relapse.csv\")\nsex      &lt;- read_csv(\"data/firstsex.csv\")\nsuicide  &lt;- read_csv(\"data/suicide.csv\")\ncongress &lt;- read_csv(\"data/congress.csv\")\n\n# glimpse(cocaine)\n# glimpse(sex)\n# glimpse(suicide)\n# glimpse(congress)\n\nWe have a lot of leg work in front of use before we can recreate Figure 10.2. First, we’ll feed each of the four data sets into the survfit() function.\n\nfit10.2 &lt;- survfit(\n  data = cocaine,\n  Surv(week, 1 - censor) ~ 1)\n\nfit10.3 &lt;- survfit(\n  data = sex,\n  Surv(time, 1 - censor) ~ 1)\n\nfit10.4 &lt;- survfit(\n  data = suicide,\n  Surv(time, 1 - censor) ~ 1)\n\nfit10.5 &lt;- survfit(\n  data = congress,\n  Surv(time, 1 - censor) ~ 1)\n\nGiven the four fits all follow the same basic form and given our end point is to make the same basic plots for each, we can substantially streamline our code by making a series of custom functions. For our first custom function, make_lt(), we’ll save the general steps for making life tables for each data set.\n\nmake_lt &lt;- function(fit) {\n  \n  # arrange the lt data for all rows but the first\n  most_rows &lt;- tibble(time = fit$time) %&gt;% \n    mutate(time_int = str_c(\"[\", time, \", \", time + 1, \")\"), \n           n_risk   = fit$n.risk, \n           n_event  = fit$n.event) %&gt;% \n    mutate(hazard_fun   = n_event / n_risk,\n           survivor_fun = fit$surv)\n  \n  # define the values for t = 2 and t = 1\n  time_1 &lt;- fit$time[1]\n  time_0 &lt;- time_1 - 1\n  \n  # define the values for the row for which t = 1\n  row_1 &lt;- tibble(\n    time         = time_0, \n    time_int     = str_c(\"[\", time_0, \", \", time_1, \")\"),\n    n_risk       = fit$n.risk[1],\n    n_event      = NA,\n    hazard_fun   = NA, \n    survivor_fun = 1)\n  \n  # make the full life table\n  bind_rows(row_1, most_rows)\n}\n\nUse make_lt() to make the four life tables.\n\nlt_cocaine  &lt;- make_lt(fit10.2)\nlt_sex      &lt;- make_lt(fit10.3)\nlt_suicide  &lt;- make_lt(fit10.4)\nlt_congress &lt;- make_lt(fit10.5)\n\nYou’ll note that the four survival-curve plots in Figure 10.2 all show the median lifetime using the interpolation method. Here we’ll save the necessary steps to compute that for each model as the make_iml() function.\n\nmake_iml &lt;- function(lt) {\n  \n  # lt is a generic name for a life table of the \n  # kind we made with our `make_lt()` function\n  \n  # determine the mth row\n  lt_m &lt;- lt %&gt;% \n    filter(survivor_fun &gt; .5) %&gt;% \n    slice(n())\n  \n  # determine the row for m + 1\n  lt_m1 &lt;- lt %&gt;% \n    filter(survivor_fun &lt; .5) %&gt;% \n    slice(1)\n  \n  # pull the value for m\n  m  &lt;- pull(lt_m, time)\n  \n  # pull the two survival function values\n  stm  &lt;- pull(lt_m, survivor_fun)\n  stm1 &lt;- pull(lt_m1, survivor_fun)\n  \n  # plug the values into Equation 10.6 (page 338)\n  m + ((stm - .5) / (stm - stm1)) * ((m + 1) - m)\n}\n\nIf you want, you can use make_iml() directly like this.\n\nmake_iml(lt_cocaine)\n\n[1] 7.5\n\n\nHowever, our approach will be to wrap it in another function, line_tbl(), with which we will save the coordinates necessary for marking off the median lifetimes and them save them in a tibble.\n\nline_tbl &lt;- function(lt) {\n  iml &lt;- make_iml(lt)\n  tibble(time         = c(lt[1, 1] %&gt;% pull(), iml, iml),\n         survivor_fun = c(0.5, 0.5, 0))\n}\n\nIt works like this.\n\nline_tbl(lt_cocaine)\n\n# A tibble: 3 × 2\n   time survivor_fun\n  &lt;dbl&gt;        &lt;dbl&gt;\n1   0            0.5\n2   7.5          0.5\n3   7.5          0  \n\n\nIf you look closely at the hazard function plots in the left column of Figure 10.2, you’ll note they share many common settings (e.g., the basic shape, the label of the \\(y\\)-axis). But there are several parameters we’ll need to set custom settings for. To my eye, those are:\n\nthe data;\nthe \\(x\\)-axis label, break points, and limits; and\nthe \\(y\\)-axis break points, and limits.\n\nWith our custom h_plot() function, we’ll leave those parameters free while keeping all the other ggplot2 parameters the same.\n\nh_plot &lt;- function(data = data, \n                   xlab = xlab, xbreaks = xbreaks, xlimits = xlimits,\n                   ybreaks = ybreaks, ylimits = ylimits) {\n  ggplot(data = data,\n         mapping = aes(x = time, y = hazard_fun)) +\n    geom_line() +\n    scale_x_continuous(xlab, breaks = xbreaks, limits = xlimits) +\n    scale_y_continuous(expression(widehat(italic(h(t)))), \n                       breaks = ybreaks, limits = ylimits) +\n    theme(panel.grid = element_blank())\n}\n\nNow we’ll make a similar custom plotting function, s_plot(), for the hazard function plots on the right column of Figure 10.2.\n\ns_plot &lt;- function(data = data, xlab = xlab, xbreaks = xbreaks, xlimits = xlimits) {\n  \n  # compute the interpolated median life value\n  iml &lt;- make_iml(data)\n  \n  # make the imp line values\n  line &lt;- data %&gt;% \n    line_tbl()\n  \n  ggplot(data = data,\n         mapping = aes(x = time, y = survivor_fun)) +\n    geom_path(data = line,\n              color = \"white\", linetype = 2) +\n    geom_line() +\n    annotate(geom = \"text\",\n             x = iml, y = 0.6,\n             label = str_c(\"widehat(ML)==\", iml %&gt;% round(1)),\n             size = 3, hjust = 0, parse = T) +\n    scale_x_continuous(xlab, breaks = xbreaks, limits = xlimits) +\n    scale_y_continuous(expression(widehat(italic(S(t)))),\n                       breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n    theme(panel.grid = element_blank())\n}\n\nNow we make the eight subplots in bulk, naming them p1, p2, and so on.\n\n# cocaine\np1 &lt;- lt_cocaine %&gt;% \n  h_plot(xlab = \"Weeks after release\", \n         xbreaks = 0:12, xlimits = c(0, 12),\n         ybreaks = c(0, 0.05, 0.1, 0.15), ylimits = c(0, 0.15))\n\np2 &lt;- lt_cocaine %&gt;% \n  s_plot(xlab = \"Weeks after release\", \n         xbreaks = 0:12, xlimits = c(0, 12))\n\n# sex\np3 &lt;- lt_sex %&gt;% \n  h_plot(xlab = \"Grade\", \n         xbreaks = 6:12, xlimits = c(6, 12),\n         ybreaks = 0:3 / 10, ylimits = c(0, 0.325))\n\np4 &lt;- lt_sex %&gt;% \n  s_plot(xlab = \"Grade\", \n         xbreaks = 6:12, xlimits = c(6, 12))\n\n# suicide\np5 &lt;- lt_suicide %&gt;% \n  h_plot(xlab = \"Age\", \n         xbreaks = 1:9 * 2 + 3, xlimits = c(5, 22),\n         ybreaks = c(0, 0.05, 0.1, 0.15), ylimits = c(0, 0.16))\n\np6 &lt;- lt_suicide %&gt;% \n  s_plot(xlab = \"Age\", \n         xbreaks = 1:9 * 2 + 3, xlimits = c(5, 22))\n\n# congress\np7 &lt;- lt_congress %&gt;% \n  h_plot(xlab = \"Terms in office\", \n         xbreaks = 0:8, xlimits = c(0, 8),\n         ybreaks = 0:3 / 10, ylimits = c(0, 0.3))\n\np8 &lt;- lt_congress %&gt;% \n  s_plot(xlab = \"Terms in office\", \n         xbreaks = 0:8, xlimits = c(0, 8))\n\nNow we’ll use some functions and syntax from the patchwork package to combine the subplots and make Figure 10.2.\n\nlibrary(patchwork)\n\np12 &lt;- (p1 + p2) + plot_annotation(title = \"A\") & theme(plot.margin = margin(0, 5.5, 0, 5.5))\np34 &lt;- (p3 + p4) + plot_annotation(title = \"B\") & theme(plot.margin = margin(0, 5.5, 0, 5.5))\np56 &lt;- (p5 + p6) + plot_annotation(title = \"C\") & theme(plot.margin = margin(0, 5.5, 0, 5.5))\np78 &lt;- (p7 + p8) + plot_annotation(title = \"D\") & theme(plot.margin = margin(0, 5.5, 0, 5.5))\n\n(wrap_elements(p12) /\n  wrap_elements(p34) /\n  wrap_elements(p56) /\n  wrap_elements(p78))\n\n\n\n\n\n\n\n\nBoom! Looks like a dream.\n\n10.3.1 Identifying periods of high and low risk using hazard functions\nIt can be useful to evaluate hazard functions based on whether they are monotonic (i.e., have a single distinctive peak and single distinctive trough) and nonmonotonic (i.e., have multiple distinctive peaks or troughs). Globally speaking, the hazard functions for rows A and B are monotonic and the remaining two are nonmonotonic.\nSinger and Willett remarked “monotonically increasing hazard functions are common when studying events that are ultimately inevitable (or near universal)…. [However,] nonmonotonic hazard functions, like those in Panels C and D, generally arise in studies of long duration” (p. 342).\nHowever, when risk is constant over time, hazard functions will not have peaks or troughs.\n\n\n10.3.2 Survivor functions as a context for evaluating the magnitude of hazard\nUnlike with hazard functions, all survivor functions decrease or stay constant over time. They are monotonic (i.e., they never switch direction, they never increase). From the text (p. 344), we learn three ways hazard functions relate to survival functions:\n\nWhen hazard is high, the survivor function drops rapidly.\nWhen hazard is low, the survivor function drops slowly.\nWhen hazard is zero, the survivor function remains unchanged.\n\n\n\n10.3.3 Strengths and limitations of estimated median lifetimes\n\nWhen examining a median lifetime, we find it helpful to remember three important limitations on its interpretation. First, it identifies only an “average” event time; it tells us little about the distribution of even times and is relatively insensitive to extreme values. Second, the median lifetime is not necessarily a moment when the target event is especially likely to occur…. Third, the median lifetime reveals little about the distribution of risk over time; identical median lifetimes can result from dramatically different survivor and hazard functions. (pp. 345–346, emphasis in the original)\n\nWithout access to Singer and Willett’s hypothetical data, we’re not in a good position to recreate their Figure 10.3. Even the good folks at IDRE gave up on that one.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#quantifying-the-effects-of-sampling-variation",
    "href": "10.html#quantifying-the-effects-of-sampling-variation",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "10.4 Quantifying the effects of sampling variation",
    "text": "10.4 Quantifying the effects of sampling variation\nWe can quantify the uncertainty in the estimates with standard errors.\n\n10.4.1 The standard error of the estimated hazard probabilities\nThe formula for the frequentist standard errors for the hazard probabilities follows the form\n\\[se \\left (\\hat h(t_j) \\right) = \\sqrt{\\frac{\\hat h(t_j) \\left (1 - \\hat h(t_j) \\right)}{n \\text{ at risk}_j}}.\\]\nWe can express that equation R code to recreate the first four columns of Table 10.2. We’ll be pulling much of the information from fit10.1. But to show our work within a tibble format, we’ll be adding a column after \\(n_j\\). Our additional n_event column will contain the information pulled from fit10.1$n.event, which we’ll use to compute the \\(\\hat h(t_j)\\).\n\nse_h_hat &lt;- tibble(year    = fit10.1$time,\n                   n_j     = fit10.1$n.risk,\n                   n_event = fit10.1$n.event) %&gt;% \n  mutate(h_hat = n_event / n_j) %&gt;% \n  mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j))\n\nse_h_hat\n\n# A tibble: 12 × 5\n    year   n_j n_event  h_hat se_h_hat\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1     1  3941     456 0.116   0.00510\n 2     2  3485     384 0.110   0.00530\n 3     3  3101     359 0.116   0.00575\n 4     4  2742     295 0.108   0.00592\n 5     5  2447     218 0.0891  0.00576\n 6     6  2229     184 0.0825  0.00583\n 7     7  2045     123 0.0601  0.00526\n 8     8  1642      79 0.0481  0.00528\n 9     9  1256      53 0.0422  0.00567\n10    10   948      35 0.0369  0.00612\n11    11   648      16 0.0247  0.00610\n12    12   391       5 0.0128  0.00568\n\n\nAs in the text, our standard errors are pretty small. To get a better sense, here they are in a rug plot.\n\nse_h_hat %&gt;% \n  ggplot(aes(x = se_h_hat)) +\n  geom_rug(length = unit(0.25, \"in\")) +\n  scale_x_continuous(expression(italic(se)(hat(italic(h))(italic(t[j])))), \n                     limits = c(0.004, 0.007)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nStandard errors for discrete hazards probabilities share a property with those for other probabilities: they are less certain (i.e., larger) for probability values near .5 and increasingly certain (i.e., smaller) for probability values approaching 0 and 1. To give a sense of that, here are the corresponding \\(se \\big (\\hat h(t_j) \\big)\\) for a series of \\(\\hat h(t_j)\\) values ranging from 0 to 1, with \\(n_j\\) held constant at 1,000.\n\ntibble(n_j   = 1000,\n       h_hat = seq(from = 0, to = 1, by = 0.01)) %&gt;% \n  mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j)) %&gt;% \n  \n  ggplot(aes(x = h_hat, y = se_h_hat)) +\n  geom_point() +\n  labs(x = expression(hat(italic(h))(italic(t[j]))),\n       y = expression(italic(se)(hat(italic(h))(italic(t[j]))))) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAlso, as the size of the risk set, \\(n_j\\), influences the standard errors in the typical way. All things equal, a larger \\(n\\) will make for a smaller \\(se\\). To give a sense, here’s the same basic plot from above, but this time with \\(n_j = 100, 1{,}000, \\text{ and } 10{,}000\\).\n\ncrossing(n_j   = c(100, 1000, 10000),\n         h_hat = seq(from = 0, to = 1, by = 0.01)) %&gt;% \n  mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j),\n         n_j      = str_c(\"italic(n[j])==\", n_j)) %&gt;% \n  \n  ggplot(aes(x = h_hat, y = se_h_hat)) +\n  geom_point() +\n  labs(x = expression(hat(italic(h))(italic(t[j]))),\n       y = expression(italic(se)(hat(italic(h))(italic(t[j]))))) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank()) +\n  facet_wrap(~ n_j, nrow = 1, labeller = label_parsed)\n\n\n\n\n\n\n\n\n\n\n10.4.2 Standard error of the estimated survival probabilities\nComputing the frequentist standard errors for estimated survival probabilities is more difficult because these are the products of (1 - hazard) for the current and all previous survival probabilities. Computing them is such a pain, Singer and Willett recommend you rely on Greenwood’s (1926) approximation. This follows the form\n\\[se \\big (\\hat S(t_j) \\big) = \\hat S(t_j)  \\sqrt{\\frac{\\hat h(t_1)}{n_1 \\big (1 - \\hat h(t_1) \\big)} + \\frac{\\hat h(t_2)}{n_2 \\big (1 - \\hat h(t_2) \\big)} + \\cdots + \\frac{\\hat h(t_j)}{n_j \\big (1 - \\hat h(t_j) \\big)}}.\\]\nHere we put the formula to work and finish our version of Table 10.2. For the sake of sanity, we’re simply calling our “Term under the square root sign” column term. Note our use of the cumsum() function.\n\n# suspend scientific notation\noptions(scipen = 999)\n\ntibble(year    = fit10.1$time,\n       n_j     = fit10.1$n.risk,\n       n_event = fit10.1$n.event) %&gt;% \n  mutate(h_hat = n_event / n_j) %&gt;% \n  mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j),\n         s_hat    = fit10.1$surv,\n         term     = cumsum(h_hat / (n_j * (1 - h_hat)))) %&gt;% \n  mutate(se_s_hat = s_hat * sqrt(term),\n         std.err  = fit10.1$std.err)\n\n# A tibble: 12 × 9\n    year   n_j n_event  h_hat se_h_hat s_hat      term se_s_hat std.err\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1  3941     456 0.116   0.00510 0.884 0.0000332  0.00510 0.00576\n 2     2  3485     384 0.110   0.00530 0.787 0.0000687  0.00652 0.00829\n 3     3  3101     359 0.116   0.00575 0.696 0.000111   0.00733 0.0105 \n 4     4  2742     295 0.108   0.00592 0.621 0.000155   0.00773 0.0124 \n 5     5  2447     218 0.0891  0.00576 0.566 0.000195   0.00790 0.0140 \n 6     6  2229     184 0.0825  0.00583 0.519 0.000235   0.00796 0.0153 \n 7     7  2045     123 0.0601  0.00526 0.488 0.000267   0.00796 0.0163 \n 8     8  1642      79 0.0481  0.00528 0.464 0.000297   0.00800 0.0172 \n 9     9  1256      53 0.0422  0.00567 0.445 0.000332   0.00811 0.0182 \n10    10   948      35 0.0369  0.00612 0.428 0.000373   0.00827 0.0193 \n11    11   648      16 0.0247  0.00610 0.418 0.000412   0.00848 0.0203 \n12    12   391       5 0.0128  0.00568 0.412 0.000445   0.00870 0.0211 \n\n\nFor comparisson, we also added the \\(se \\big (\\hat S(t_j) \\big)\\) values coputed by the survivor package in the final column, std.err.\n\ntibble(year    = fit10.1$time,\n       n_j     = fit10.1$n.risk,\n       n_event = fit10.1$n.event) %&gt;% \n  mutate(h_hat = n_event / n_j) %&gt;% \n  mutate(se_h_hat = sqrt((h_hat * (1 - h_hat)) / n_j),\n         s_hat    = fit10.1$surv,\n         term     = cumsum(h_hat / (n_j * (1 - h_hat)))) %&gt;% \n  mutate(Greenwood = s_hat * sqrt(term),\n         `survival package`  = fit10.1$std.err) %&gt;% \n  pivot_longer(Greenwood:`survival package`) %&gt;% \n  mutate(name = factor(name,\n                       levels = c(\"survival package\", \"Greenwood\"))) %&gt;% \n  \n  ggplot(aes(x = year, y = value, color = name)) +\n  geom_point(size = 4) +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.55) +\n  scale_x_continuous(breaks = 1:12) +\n  scale_y_continuous(expression(italic(se)), limits = c(0, 0.025)) +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.125, 0.9),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIt’s out of my expertise to comment on which should we should trust more. But Singer and Willett did note that “as an approximation, Greenwood’s formula is accurate only asymptotically” (p. 351).\n\n# turn scientific notation back on (R default)\noptions(scipen = 0)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#a-simple-and-useful-strategy-for-constructing-the-life-table",
    "href": "10.html#a-simple-and-useful-strategy-for-constructing-the-life-table",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "10.5 A simple and useful strategy for constructing the life table",
    "text": "10.5 A simple and useful strategy for constructing the life table\n\nHow can you construct a life table for your data set? For preliminary analyses, it is easy to use the prepackaged routines available in the major statistical packages. If you choose this approach, be sure to check whether your package allows you to: (1) select the partition of time; and (2) ignore any actuarial corrections invoked due to continuous-time assumptions (that do not hold in discrete time). When event times have been measured using a discrete-time scale, actuarial corrections (discussed in chapter13) are inappropriate. Although most packages clearly document the algorithm being used, we suggest that you double-check by comparing results with one or two estimates computed by hand. (p. 351, emphasis in the original)\n\nFor more information about the methods we’ve been using via the survival package, browse through the documentation listed on the CRAN page, https://CRAN.R-project.org/package=survival, with a particular emphasis on the reference manual and A package for survival analysis in R (Terry M. Therneau, 2021c). But back to the text:\n\nDespite the simplicity of preprogrammed algorithms, [Singer and Willett] prefer an alternative approach for life table construction. This approach requires construction of a person-period data set, much like the period-period data set used for growth modeling. Once you create the person-period data set, you can compute descriptive statistics sing any standard cross-tabulation routine. (p. 351)\n\n\n10.5.1 The person-period data set\nHere is the person-level data set displayed in Figure 10.4; it’s just a subset of the teachers data.\n\nteachers %&gt;% \n  filter(id %in% c(20, 126, 129))\n\n# A tibble: 3 × 3\n     id     t censor\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    20     3      0\n2   126    12      0\n3   129    12      1\n\n\nYou can transform the person-level survival data set into the person-period variant shown on the right panel of Figure 10.4 with a workflow like this.\n\nteachers_pp &lt;- teachers %&gt;% \n  uncount(weights = t) %&gt;% \n  group_by(id) %&gt;% \n  mutate(period = 1:n()) %&gt;% \n  mutate(event = if_else(period == max(period) & censor == 0, 1, 0)) %&gt;% \n  select(-censor) %&gt;% \n  ungroup()\n\nteachers_pp %&gt;% \n  filter(id %in% c(20, 126, 129))\n\n# A tibble: 27 × 3\n      id period event\n   &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt;\n 1    20      1     0\n 2    20      2     0\n 3    20      3     1\n 4   126      1     0\n 5   126      2     0\n 6   126      3     0\n 7   126      4     0\n 8   126      5     0\n 9   126      6     0\n10   126      7     0\n# ℹ 17 more rows\n\n\nYou don’t necessarily need to use ungroup() at the end, but it’s probably a good idea. Anyway, note how the information previously contained in the censor column has been transformed to the event column, which is coded 0 = no event, 1 = event. With this coding, we know a participant has been censored when event == 0 on their max(period) row.\nWe can count the number of teachers in the sample like this.\n\nteachers %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  3941\n\n\nTo get a sense of the difference in the data structures, here are the number of rows for the original person-level teachers data and for our person-period transformation.\n\n# person-level\nteachers %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  3941\n\n# person-period\nteachers_pp %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 24875\n\n\nHere is the breakdown of the number of rows in the person-period teachers data for which event == 1 or event == 0.\n\nteachers_pp %&gt;% \n  count(event)\n\n# A tibble: 2 × 2\n  event     n\n  &lt;dbl&gt; &lt;int&gt;\n1     0 22668\n2     1  2207\n\n\n\n\n10.5.2 Using the person-period data set to construct the life table\n“All the life table’s essential elements can be computed through cross-tabulation of PERIOD and EVENT in the person-period data set” (p. 354, emphasis in the original). Here’s how we might use the tidyverse do that with teachers_pp.\n\nteachers_lt &lt;- teachers_pp %&gt;% \n  # change the coding for `event` in anticipation of the final format\n  mutate(event = str_c(\"event = \", event)) %&gt;% \n  group_by(period) %&gt;% \n  count(event) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = event,\n              values_from = n) %&gt;% \n  mutate(total = `event = 0` + `event = 1`) %&gt;% \n  mutate(prop_e_1 = (`event = 1` / total) %&gt;% round(digits = 4))\n\nteachers_lt\n\n# A tibble: 12 × 5\n   period `event = 0` `event = 1` total prop_e_1\n    &lt;int&gt;       &lt;int&gt;       &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1      1        3485         456  3941   0.116 \n 2      2        3101         384  3485   0.110 \n 3      3        2742         359  3101   0.116 \n 4      4        2447         295  2742   0.108 \n 5      5        2229         218  2447   0.0891\n 6      6        2045         184  2229   0.0825\n 7      7        1922         123  2045   0.0601\n 8      8        1563          79  1642   0.0481\n 9      9        1203          53  1256   0.0422\n10     10         913          35   948   0.0369\n11     11         632          16   648   0.0247\n12     12         386           5   391   0.0128\n\n\nHere are the totals Singer and Willett displayed in the bottom row.\n\nteachers_lt %&gt;% \n  pivot_longer(`event = 0`:total) %&gt;% \n  group_by(name) %&gt;% \n  summarise(total = sum(value)) %&gt;% \n  pivot_wider(names_from = name,\n              values_from = total)\n\n# A tibble: 1 × 3\n  `event = 0` `event = 1` total\n        &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n1       22668        2207 24875\n\n\n\nThe ability to construct a life table using the person-period data set provides a simple strategy for conducting the descriptive analyses outlined in this chapter. This strategy yields appropriate statistics regardless of the amount, or pattern, of censoring. Perhaps even more important, the person-period data set is the fundamental tool for fitting discrete-time hazard models to data, using methods that we describe in the next chapter. (p. 356)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#bonus-fit-the-discrete-time-hazard-models-with-brms",
    "href": "10.html#bonus-fit-the-discrete-time-hazard-models-with-brms",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "10.6 Bonus: Fit the discrete-time hazard models with brms",
    "text": "10.6 Bonus: Fit the discrete-time hazard models with brms\nThe frequentists aren’t the only ones who can discrete-time hazard models. Bayesians can get in on the fun, too. The first step is to decide on an appropriate likelihood function. Why? Because Bayes’ formula requires that we define the likelihood and the priors. As all the parameters in the model are seen through the lens of the likelihood, it’s important we consider it with care.\nHappily, the exercises in the last section did a great job preparing us for the task. In Table 10.3, Singer and Willett hand computed the discrete hazards (i.e., the values in the “Proportion EVENT = 1” column) by dividing the valued in the “EVENT = 1” column by those in the “Total” column. Discrete hazards are proportions. Proportions have two important characteristics; they are continuous and necessarily range between 0 and 1. You know what else has those characteristics? Probabilities.\nSo far in this text, we have primarily focused on models using the Gaussian likelihood. Though it’s a workhorse, the Gaussian is inappropriate for modeling proportions/probabilities. Good old Gauss is great at modeling unbounded continuous data, but it can fail miserably when working with bounded data and our proportions/probabilities are most certainly bounded. The binomial likelihood, however, is well-suited for handling probabilities. Imagine you have data that can take on values of 0 and 1, such as failures/successes, no’s/yesses, fails/passes, and no-events/events. If you sum up all the 1’s and divide them by the total cases, you get a proportion/probability. The simple binomial model takes just that kind of data–the number of 1’s and the number of total cases. The formula for the binomial likelihood is\n\\[\\Pr (z \\mid n, p) = \\frac{n!}{z!(n - z)!} p^z (1 - p)^{n - z},\\]\nwhere \\(z\\) is the number of cases for which the value is 1, \\(n\\) is the total number of cases, and \\(p\\) is the probability of a 1 across cases. As the data provide us with \\(z\\) and \\(n\\), we end up estimating \\(p\\). If we’re willing to use what’s called a link function, we can estimate \\(p\\) with a linear model with any number of predictors. When fitting binomial regression models, you can take your choice among several link functions, the most popular of which is the logit. This will be our approach. As you may have guesses, using the logit link to fit a binomial model is often termed logistic regression. Welcome to the generalized linear model.\nLet’s fire up brms.\n\nlibrary(brms)\n\nBefore we fit the model, it will make our lives easier if we redefine period as a factor and rename our event = 1 column as event. We defined period as a factor because we want to fit a model with discrete time. Renaming event = 1 column as event just makes it easier on the brm() function to read the variable.\n\nteachers_lt &lt;- teachers_lt %&gt;% \n  mutate(period = factor(period),\n         event  = `event = 1`)\n\nWith this formulation, event is our \\(z\\) term and total is our \\(n\\) term. We’re estimating \\(p\\). Behold the mode syntax.\n\nfit10.6 &lt;- brm(\n  data = teachers_lt,\n  family = binomial,\n  event | trials(total) ~ 0 + period,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 1, iter = 2000, warmup = 1000,\n  seed = 10,\n  file = \"fits/fit10.06\")\n\nCheck the summary.\n\nprint(fit10.6)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(total) ~ 0 + period \n   Data: teachers_lt (Number of observations: 12) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nperiod1     -2.03      0.05    -2.13    -1.94 1.00     8232     2802\nperiod2     -2.09      0.06    -2.20    -1.98 1.00     9299     2913\nperiod3     -2.03      0.06    -2.15    -1.93 1.00     9240     3350\nperiod4     -2.12      0.06    -2.24    -1.99 1.00     9405     2531\nperiod5     -2.32      0.07    -2.46    -2.19 1.00     8022     3216\nperiod6     -2.41      0.08    -2.57    -2.26 1.00     8555     3045\nperiod7     -2.75      0.09    -2.94    -2.57 1.00     8481     3040\nperiod8     -2.99      0.12    -3.22    -2.77 1.00     8966     2761\nperiod9     -3.13      0.14    -3.40    -2.86 1.00     7762     3253\nperiod10    -3.27      0.18    -3.62    -2.94 1.00     8575     2718\nperiod11    -3.69      0.26    -4.22    -3.23 1.00     9301     2599\nperiod12    -4.38      0.45    -5.37    -3.59 1.00     9029     3007\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause we used the 0 + Intercept syntax in the presence of a factor predictor, period, we suppressed the default intercept. Instead, we have separate “intercepts” for each of the 12 levels period. Because we used the conventional logit link, the parameters are all on the log-odds scale. Happily, we can use the plogis() function to convert them back into the probability metric. Here’s a quick and dirty conversion using the output from fixef().\n\nfixef(fit10.6) %&gt;% plogis()\n\n           Estimate Est.Error        Q2.5      Q97.5\nperiod1  0.11567890 0.5123075 0.105988479 0.12576996\nperiod2  0.11018973 0.5138348 0.099876961 0.12118738\nperiod3  0.11566288 0.5142310 0.104437289 0.12723862\nperiod4  0.10758428 0.5154676 0.096217529 0.11983230\nperiod5  0.08908959 0.5172146 0.078527309 0.10050792\nperiod6  0.08237622 0.5198561 0.071012977 0.09471206\nperiod7  0.05997496 0.5237001 0.050374975 0.07100347\nperiod8  0.04802106 0.5290276 0.038328790 0.05916089\nperiod9  0.04200683 0.5343985 0.032231454 0.05404363\nperiod10 0.03673341 0.5439678 0.025979648 0.05035764\nperiod11 0.02429261 0.5635033 0.014530393 0.03822677\nperiod12 0.01231631 0.6112087 0.004651289 0.02696934\n\n\nCompare these Estimate values with the values in the “Estimated hazard probability” column from Table 10.2 in the text (p. 349). They are very close. We can go further and look at these hazard estimates in a plot. We’ll use the tidybayes::stat_lineribbon() function to plot their posterior means atop their 50% and 95% intervals.\n\nlibrary(tidybayes)\n\nas_draws_df(fit10.6) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  mutate_all(plogis) %&gt;% \n  set_names(1:12) %&gt;% \n  pivot_longer(everything(),\n               names_to = \"period\",\n               values_to = \"hazard\") %&gt;% \n  mutate(period = period %&gt;% as.double()) %&gt;% \n  \n  ggplot(aes(x = period, y = hazard)) +\n  stat_lineribbon(.width = c(0.5, 0.95), size = 1/3) +\n  # add the hazard estimates from `survival::survfit()`\n  geom_point(data = tibble(period = fit10.1$time,\n                           hazard = fit10.1$n.event / fit10.1$n.risk),\n             aes(y = hazard),\n             size = 2, color = \"violetred1\") +\n  scale_fill_manual(\"CI\", values = c(\"grey75\", \"grey60\")) +\n  scale_x_continuous(breaks = 1:12) +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.925, 0.825),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFor comparison sake, those violet dots in the foreground are the hazard estimates from the frequentist survival::survfit() function. Turns out our Bayesian results are very similar to the frequentist ones. Hopefully this isn’t a surprise. There was a lot of data and we used fairly weak priors. For simple models under those conditions, frequentist and Bayesian results should be pretty close.\nRemember that \\(se \\big (\\hat h(t_j) \\big)\\) formula from back in section 10.4.1? That doesn’t quite apply to the posterior standard deviations from our Bayesian model. Even so, our posterior \\(\\textit{SD}\\)s will be very similar to the ML standard errors. Let’s compare those in a plot, too.\n\nas_draws_df(fit10.6) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  mutate_all(plogis) %&gt;% \n  set_names(1:12) %&gt;% \n  pivot_longer(everything(),\n               names_to = \"period\",\n               values_to = \"hazard\") %&gt;% \n  mutate(period = period %&gt;% as.double()) %&gt;% \n  group_by(period) %&gt;% \n  summarise(sd = sd(hazard)) %&gt;% \n  bind_cols(se_h_hat %&gt;% select(se_h_hat)) %&gt;% \n  pivot_longer(-period) %&gt;% \n  mutate(name = factor(name,\n                       levels = c(\"sd\", \"se_h_hat\"),\n                       labels = c(\"Bayesian\", \"ML\"))) %&gt;% \n  \n  ggplot(aes(x = period, y = value, color = name)) +\n  geom_point(size = 3, position = position_dodge(width = 0.25)) +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.55) +\n  scale_x_continuous(breaks = 1:12) +\n  scale_y_continuous(expression(italic(se)), limits = c(0, 0.01)) +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.09, 0.9),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nMan those are close.\nWe can also use our brms output to depict the survivor function. On page 337 in the text (Equation 10.5), Singer and Willett demonstrated how to define the survivor function in terms of the hazard function. It follows the form\n\\[\\hat S (t_j) = [1 - \\hat h (t_j)][1 - \\hat h (t_{j - 1})][1 - \\hat h (t_{j - 2})] \\dots [1 - \\hat h (t_1)].\\]\nIn words, “each year’s estimated survival probability is the successive product of the complement of the estimated hazard function probabilities across this and all previous years” (p. 337, emphasis in the original). Here’s how you might do that with the output from as_draws_df().\n\ndraws &lt;- as_draws_df(fit10.6) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  # transform the hazards from the log-odds metric to probabilities\n  mutate_all(plogis) %&gt;% \n  set_names(str_c(\"h\", 1:12)) %&gt;% \n  # take the \"complement\" of each hazard\n  mutate_all(~1 - .) %&gt;% \n  # apply Equation 10.5\n  transmute(s0  = 1, \n            s1  = h1, \n            s2  = h1 * h2, \n            s3  = h1 * h2 * h3, \n            s4  = h1 * h2 * h3 * h4, \n            s5  = h1 * h2 * h3 * h4 * h5, \n            s6  = h1 * h2 * h3 * h4 * h5 * h6, \n            s7  = h1 * h2 * h3 * h4 * h5 * h6 * h7, \n            s8  = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8, \n            s9  = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9, \n            s10 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10, \n            s11 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10 * h11, \n            s12 = h1 * h2 * h3 * h4 * h5 * h6 * h7 * h8 * h9 * h10 * h11 * h12)\n\nglimpse(draws)\n\nRows: 4,000\nColumns: 13\n$ s0  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ s1  &lt;dbl&gt; 0.8882725, 0.8841842, 0.8870165, 0.8906272, 0.8801260, 0.8808762, 0.8877767, 0.8809166, 0.8876376, 0.8810157, 0.8888…\n$ s2  &lt;dbl&gt; 0.7995095, 0.7774375, 0.7912860, 0.7868829, 0.7908395, 0.7832934, 0.7903921, 0.7841811, 0.7899500, 0.7838445, 0.7896…\n$ s3  &lt;dbl&gt; 0.7028917, 0.6808228, 0.7015976, 0.6929662, 0.7017946, 0.6869828, 0.7084219, 0.6943937, 0.6949710, 0.6913329, 0.6993…\n$ s4  &lt;dbl&gt; 0.6213355, 0.6097996, 0.6252030, 0.6174389, 0.6270114, 0.6093450, 0.6338153, 0.6189226, 0.6220228, 0.6166871, 0.6242…\n$ s5  &lt;dbl&gt; 0.5610498, 0.5585496, 0.5724598, 0.5556052, 0.5765991, 0.5591931, 0.5739501, 0.5614475, 0.5688041, 0.5591788, 0.5701…\n$ s6  &lt;dbl&gt; 0.5138439, 0.5084096, 0.5309647, 0.5129127, 0.5250917, 0.5137151, 0.5248989, 0.5104266, 0.5270480, 0.5091437, 0.5273…\n$ s7  &lt;dbl&gt; 0.4816660, 0.4780778, 0.4981472, 0.4848866, 0.4926044, 0.4857342, 0.4901806, 0.4768854, 0.4980448, 0.4760355, 0.4978…\n$ s8  &lt;dbl&gt; 0.4627847, 0.4513930, 0.4807182, 0.4594040, 0.4705295, 0.4622705, 0.4658327, 0.4520261, 0.4762691, 0.4511294, 0.4757…\n$ s9  &lt;dbl&gt; 0.4434304, 0.4293679, 0.4605806, 0.4387918, 0.4521570, 0.4401244, 0.4495158, 0.4296258, 0.4589467, 0.4296229, 0.4582…\n$ s10 &lt;dbl&gt; 0.4299888, 0.4165077, 0.4411670, 0.4245981, 0.4335712, 0.4265166, 0.4325786, 0.4127737, 0.4410256, 0.4138769, 0.4418…\n$ s11 &lt;dbl&gt; 0.4192832, 0.4051306, 0.4302408, 0.4135297, 0.4234692, 0.4144529, 0.4225917, 0.4016406, 0.4303435, 0.4033449, 0.4312…\n$ s12 &lt;dbl&gt; 0.4158431, 0.3967256, 0.4257802, 0.4102225, 0.4161995, 0.4122905, 0.4127486, 0.3958909, 0.4265116, 0.3970272, 0.4270…\n\n\nWe’ll learn how to simplify that syntax with help from the cumprod() function in the next chapter. Now we have our survival estimates, we can make our Bayesian version of the lower panel of Figure 10.1.\n\n# this will help us depict the median lifetime\nline &lt;- tibble(period   = c(0, iml, iml),\n               survival = c(0.5, 0.5, 0))\n\n# wrangle\ndraws %&gt;% \n  pivot_longer(everything(), values_to = \"survival\") %&gt;% \n  mutate(period = str_remove(name, \"s\") %&gt;% as.double()) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = period, y = survival)) +\n  geom_path(data = line,\n            color = \"white\", linetype = 2) +\n  stat_lineribbon(.width = 0.95, size = 1/3, fill = \"grey75\") +\n  # add the survival estimates from `survival::survfit()`\n  geom_point(data = tibble(period   = fit10.1$time,\n                           survival = fit10.1$surv),\n             size = 2, color = \"violetred1\") +\n  scale_x_continuous(\"years in teaching\", breaks = 0:13, limits = c(0, 13)) +\n  scale_y_continuous(expression(\"estimated survival probability, \"*hat(italic(S))(italic(t[j]))),\n                     breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLike we did with our hazard plot above, we superimposed the frequentist estimates as violet dots atop the Bayesian posterior means and intervals. Because of how narrow the posteriors were, we only showed the 95% intervals, here. As with the hazard estimates, our Bayesian survival estimates are very close to the ones we computed with ML.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#session-info",
    "href": "10.html#session-info",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 brms_2.23.0     Rcpp_1.1.0      patchwork_1.3.2 survival_3.8-3  lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] timechange_0.3.0        estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 magrittr_2.0.4         \n[16] posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7             tools_4.5.1             utf8_1.2.6             \n[21] knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1    htmlwidgets_1.6.4       curl_7.0.0             \n[26] bit_4.6.0               pkgbuild_1.4.8          plyr_1.8.9              RColorBrewer_1.1-3      abind_1.4-8            \n[31] multcomp_1.4-29         withr_3.0.2             grid_4.5.1              stats4_4.5.1            xtable_1.8-4           \n[36] inline_0.3.21           emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65             cli_3.6.5              \n[41] mvtnorm_1.3-3           rmarkdown_2.30          crayon_1.5.3            generics_0.1.4          RcppParallel_5.1.11-1  \n[46] rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000       splines_4.5.1          \n[51] bayesplot_1.15.0.9000   parallel_4.5.1          matrixStats_1.5.0       vctrs_0.6.5             V8_8.0.1               \n[56] Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          arrayhelpers_1.1-0      hms_1.1.4              \n[61] bit64_4.6.0-1           ggdist_3.3.3            glue_1.8.0              codetools_0.2-20        distributional_0.5.0   \n[66] stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1           htmltools_0.5.9        \n[71] Brobdingnag_1.2-9       R6_2.6.1                vroom_1.6.6             evaluate_1.0.5          lattice_0.22-7         \n[76] backports_1.5.0         rstantools_2.5.0.9000   gridExtra_2.3           coda_0.19-4.1           nlme_3.1-168           \n[81] checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "10.html#comments",
    "href": "10.html#comments",
    "title": "10  Describing Discrete-Time Event Occurrence Data",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nGreenwood, M. (1926). The natural duration of cancer. Reports on Public Health and Medical Subjects, 33, 1–26.\n\n\nMiller, R. G. (1981). Survival analysis. John Wiley & Sons.\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nTherneau, Terry M. (2021a). survival reference manual, Version 3.2-10. https://CRAN.R-project.org/package=survival/survival.pdf\n\n\nTherneau, Terry M. (2021b). survival: Survival analysis [Manual]. https://github.com/therneau/survival\n\n\nTherneau, Terry M. (2021c). A package for survival analysis in R. https://CRAN.R-project.org/package=survival/vignettes/survival.pdf\n\n\nTherneau, Terry M., & Grambsch, P. M. (2000). Modeling survival data: Extending the Cox model. Springer. https://link.springer.com/book/10.1007/978-1-4757-3294-8",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Describing Discrete-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "11.html",
    "href": "11.html",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "",
    "text": "11.1 Toward a statistical model for discrete-time hazard\nTime to load Capaldi, Crosby, and Stoolmiller’s (1996) firstsex.csv data.\nlibrary(tidyverse)\n\nsex &lt;- read_csv(\"data/firstsex.csv\")\n\nglimpse(sex)\n\nRows: 180\nColumns: 5\n$ id     &lt;dbl&gt; 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, …\n$ time   &lt;dbl&gt; 9, 12, 12, 12, 11, 9, 12, 11, 12, 11, 12, 11, 9, 12, 10, 12, 7, 12, 10, 12, 11, 12, 11, 12, 11, 12, 11, 9, 12, 11…\n$ censor &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,…\n$ pt     &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,…\n$ pas    &lt;dbl&gt; 1.9788670, -0.5454916, -1.4049800, 0.9741806, -0.6356313, -0.2428857, -0.8685001, 0.4535947, 0.8017636, -0.745946…\nHere are the cases broken down by time and censor status.\nsex %&gt;% \n  count(time, censor)\n\n# A tibble: 7 × 3\n   time censor     n\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1     7      0    15\n2     8      0     7\n3     9      0    24\n4    10      0    29\n5    11      0    25\n6    12      0    26\n7    12      1    54\nSince these data show no censoring before the final time point, it is straightforward to follow along with the text (p. 358) and compute the percent who had already had sex by the 12th grade.\nsex %&gt;% \n  count(censor) %&gt;% \n  mutate(percent = 100 * (n / sum(n)))\n\n# A tibble: 2 × 3\n  censor     n percent\n   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1      0   126      70\n2      1    54      30\nHere we break the data down by our central predictor, pt, which is coded “0 for boys who lived with both biological parents” and “1 for boys who experienced one or more parenting transitions” before the 7th grade.\nsex %&gt;% \n  count(pt) %&gt;% \n  mutate(percent = 100 * (n / sum(n)))\n\n# A tibble: 2 × 3\n     pt     n percent\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     0    72      40\n2     1   108      60",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#toward-a-statistical-model-for-discrete-time-hazard",
    "href": "11.html#toward-a-statistical-model-for-discrete-time-hazard",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "",
    "text": "11.1.1 Plots of within-group hazard functions and survivor functions\n\nPlots of sample hazard functions and survivor functions estimates separately for groups distinguished by their predictor values are invaluable exploratory tools. If a predictor is categorical, like PT, construction of these displays is straightforward. If a predictor is continuous, you should just temporarily categorize its values for plotting purposes. (pp. 358–359, emphasis in the original)\n\nTo make our version of the descriptive plots in Figure 11.1, we’ll need to first load the survival package.\n\nlibrary(survival)\n\nfit11.1 will be of the cases for which pt ==  0 and fit11.2 will be of the cases for which pt ==  1. With fit11.3, we use all cases regardless of pt status.\n\nfit11.1 &lt;- survfit(\n  data = sex %&gt;% filter(pt == 0),\n  Surv(time, 1 - censor) ~ 1)\n\nfit11.2 &lt;- survfit(\n  data = sex %&gt;% filter(pt == 1),\n  Surv(time, 1 - censor) ~ 1)\n\nfit11.3 &lt;- survfit(\n  data = sex,\n  Surv(time, 1 - censor) ~ 1)\n\nBefore we plot the results, it might be handy to arrange the fit results in life tables. We can streamline that code with the custom make_lt() function from last chapter.\n\nmake_lt &lt;- function(fit) {\n  \n  # arrange the lt data for all rows but the first\n  most_rows &lt;- tibble(time = fit$time) %&gt;% \n    mutate(time_int = str_c(\"[\", time, \", \", time + 1, \")\"), \n           n_risk   = fit$n.risk, \n           n_event  = fit$n.event) %&gt;% \n    mutate(n_censored   = n_risk - n_event - lead(n_risk, default = 0),\n           hazard_fun   = n_event / n_risk,\n           survivor_fun = fit$surv)\n  \n  # define the values for t = 2 and t = 1\n  time_1 &lt;- fit$time[1]\n  time_0 &lt;- time_1 - 1\n  \n  # define the values for the row for which t = 1\n  row_1 &lt;- tibble(\n    time         = time_0, \n    time_int     = str_c(\"[\", time_0, \", \", time_1, \")\"),\n    n_risk       = fit$n.risk[1],\n    n_event      = NA,\n    n_censored   = NA,\n    hazard_fun   = NA, \n    survivor_fun = 1)\n  \n  # make the full life table\n  bind_rows(row_1, most_rows)\n}\n\nWe’ll use make_lt() separately for each fit, stack the results from the first on top of those from the second, and add a pt column to index the rows. This will be our version of Table 11.1 (p. 360).\n\nlt &lt;- bind_rows(make_lt(fit11.1), make_lt(fit11.2), make_lt(fit11.3)) %&gt;% \n  mutate(pt = factor(rep(c(\"pt = 0\", \"pt = 1\", \"overall\"), each = n() / 3))) %&gt;% \n  select(pt, everything())\n\nlt\n\n# A tibble: 21 × 8\n   pt      time time_int n_risk n_event n_censored hazard_fun survivor_fun\n   &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 pt = 0     6 [6, 7)       72      NA         NA    NA             1    \n 2 pt = 0     7 [7, 8)       72       2          0     0.0278        0.972\n 3 pt = 0     8 [8, 9)       70       2          0     0.0286        0.944\n 4 pt = 0     9 [9, 10)      68       8          0     0.118         0.833\n 5 pt = 0    10 [10, 11)     60       8          0     0.133         0.722\n 6 pt = 0    11 [11, 12)     52      10          0     0.192         0.583\n 7 pt = 0    12 [12, 13)     42       8         34     0.190         0.472\n 8 pt = 1     6 [6, 7)      108      NA         NA    NA             1    \n 9 pt = 1     7 [7, 8)      108      13          0     0.120         0.880\n10 pt = 1     8 [8, 9)       95       5          0     0.0526        0.833\n# ℹ 11 more rows\n\n\nHere is the code for the top panel of Figure 11.1.\n\np1 &lt;- lt %&gt;% \n  filter(pt != \"overall\") %&gt;% \n  \n  ggplot(aes(x = time, y = hazard_fun, color = pt, group = pt)) +\n  geom_line() +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.5) +\n  scale_x_continuous(\"grade\", breaks = 6:12, limits = c(6, 12)) +\n  scale_y_continuous(\"estimated hazard probability\", \n                     limits = c(0, 0.5)) +\n  theme(panel.grid = element_blank())\n\nNow make the plot for the bottom panel.\n\np2 &lt;- lt %&gt;% \n  filter(pt != \"overall\") %&gt;% \n  \n  ggplot(aes(x = time, y = survivor_fun, color = pt, group = pt)) +\n  geom_hline(yintercept = 0.5, color = \"white\", linetype = 2) +\n  geom_line() +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.5) +\n  scale_x_continuous(\"grade\", breaks = 6:12, limits = c(6, 12)) +\n  scale_y_continuous(\"estimated survival probability\",\n                     breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\nCombine the two ggplot2 objects with patchwork syntax to make our version of Figure 11.1.\n\nlibrary(patchwork)\n\n(p1 / p2) +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nOn page 361, Singer and Willett compared the hazard probabilities at grades 8 and 11 for boys in the two pt groups. We can make that comparison with filter().\n\nlt %&gt;% \n  filter(time %in% c(8, 11) &\n           pt != \"overall\") %&gt;% \n  select(pt:time, hazard_fun)\n\n# A tibble: 4 × 3\n  pt      time hazard_fun\n  &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 pt = 0     8     0.0286\n2 pt = 0    11     0.192 \n3 pt = 1     8     0.0526\n4 pt = 1    11     0.283 \n\n\nCompare the two groups on the hazard probabilities at grade 9.\n\nlt %&gt;% \n  filter(time == 9 &\n           pt != \"overall\") %&gt;% \n  select(pt:time, hazard_fun)\n\n# A tibble: 2 × 3\n  pt      time hazard_fun\n  &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 pt = 0     9      0.118\n2 pt = 1     9      0.178\n\n\nNow compare them on their hazard probabilities in grade 12.\n\nlt %&gt;% \n  filter(time == 12 &\n           pt != \"overall\") %&gt;% \n  select(pt:time, hazard_fun)\n\n# A tibble: 2 × 3\n  pt      time hazard_fun\n  &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 pt = 0    12      0.190\n2 pt = 1    12      0.474\n\n\nAt the top of page 362, Singer and Willett compared the percentages of boys who were virgins at grades 9 and 12, by pt status. Those percentages are straight algebraic transformations of the corresponding survival function values.\n\nlt %&gt;% \n  filter(time %in% c(9, 12) &\n           pt != \"overall\") %&gt;% \n  select(pt:time, survivor_fun) %&gt;% \n  mutate(percent_virgins = (100 * survivor_fun) %&gt;% round(digits = 1))\n\n# A tibble: 4 × 4\n  pt      time survivor_fun percent_virgins\n  &lt;fct&gt;  &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1 pt = 0     9        0.833            83.3\n2 pt = 0    12        0.472            47.2\n3 pt = 1     9        0.685            68.5\n4 pt = 1    12        0.185            18.5\n\n\nNow let’s finish off by computing the interpolated median lifetime values for each with our custom make_iml() function.\n\nmake_iml &lt;- function(lt) {\n  \n  # lt is a generic name for a life table of the \n  # kind we made with our `make_lt()` function\n  \n  # determine the mth row\n  lt_m &lt;- lt %&gt;% \n    filter(survivor_fun &gt; .5) %&gt;% \n    slice(n())\n  \n  # determine the row for m + 1\n  lt_m1 &lt;- lt %&gt;% \n    filter(survivor_fun &lt; .5) %&gt;% \n    slice(1)\n  \n  # pull the value for m\n  m  &lt;- pull(lt_m, time)\n  \n  # pull the two survival function values\n  stm  &lt;- pull(lt_m, survivor_fun)\n  stm1 &lt;- pull(lt_m1, survivor_fun)\n  \n  # plug the values into Equation 10.6 (page 338)\n  m + ((stm - .5) / (stm - stm1)) * ((m + 1) - m)\n}\n\nPut make_iml() to work.\n\nmake_iml(lt %&gt;% filter(pt == \"pt = 0\"))\n\n[1] 11.75\n\nmake_iml(lt %&gt;% filter(pt == \"pt = 1\"))\n\n[1] 9.952381\n\n\n\n\n11.1.2 What kind of statistical model do these graphs suggest?\n\nTo postulate a statistical model to represent the relationship between the population discrete-time hazard function and predictors, we must deal with two complications apparent in these displays. One is that any hypothesized model must describe the shape of the entire discrete-time hazard function over time, not just its value in any one period, in much the same way that a multilevel model for change characterizes the shape of entire individual growth trajectories over time. A second complication is that, as a conditional probability, the value of discrete-time hazard must lie between 0 and 1. Any reasonable statistical model for hazard must recognize this constraint, precluding the occurrence of theoretically impossible values. (p. 362, emphasis in the original)\n\n\n11.1.2.1 The bounded nature of hazard\nA conventional way to handle the bounded nature of probabilities is transform the scale of the data. Cox (1972) recommended either the odds and log-odds (i.e., logit) transformations. Given a probability, \\(p\\), we compute the odds as\n\\[\\text{odds} = \\frac{p}{1 - p}.\\]\nLog-odds is a minor extension; you simply take the log of the odds, which we can formally express as\n\\[\\text{log-odds} = \\log \\left (\\frac{p}{1 - p} \\right ).\\]\nTo make the conversions easy, we’ll define1 a couple convenience functions: odds() and log_odds().\n1 R already has a built-in function to convert probabilities to the log-odds scale. Somewhat confusingly, it’s called qlogis(). You can learn more by executing ?qlogis or by browsing through this great blog post by Roman Cheplyaka. It’s generally a good idea to stick to the functions in base R rather than make your own, like we did earlier in this chapter (see this twitter thread). Since the name of qlogis() isn’t the easiest to remember, you can always execute something like log_odds &lt;- qlogis or logit &lt;- qlogis at the beginning of your scripts and then use one of those as a thin wrapper for qlogis().\nodds &lt;- function(p) {\n  p / (1 - p)\n}\n\nlog_odds &lt;- function(p) {\n  log(p / (1 - p))\n}\n\nHere’s how they work.\n\ntibble(p = seq(from = 0, to = 1, by = 0.1)) %&gt;% \n  mutate(odds     = odds(p),\n         log_odds = log_odds(p))\n\n# A tibble: 11 × 3\n       p    odds log_odds\n   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1   0     0     -Inf    \n 2   0.1   0.111   -2.20 \n 3   0.2   0.25    -1.39 \n 4   0.3   0.429   -0.847\n 5   0.4   0.667   -0.405\n 6   0.5   1        0    \n 7   0.6   1.5      0.405\n 8   0.7   2.33     0.847\n 9   0.8   4        1.39 \n10   0.9   9        2.20 \n11   1   Inf      Inf    \n\n\nBefore we make our version of Figure 11.2, it might be instructive to see how odds and log-odds compare to probabilities in a plot. Here we’ll compare them to probabilities ranging from .01 to .99.\n\ntibble(p = seq(from = 0.01, to = 0.99, by = 0.01)) %&gt;% \n  mutate(odds        = odds(p),\n         `log(odds)` = log_odds(p)) %&gt;% \n  pivot_longer(-p) %&gt;% \n  mutate(name = factor(name,\n                       levels = c(\"odds\", \"log(odds)\"))) %&gt;% \n  \n  ggplot(aes(x = p, y = value)) +\n  geom_line() +\n  labs(x = \"probability\",\n       y = \"transformed scale\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nOdds are bounded to values of zero and above and have an inflection at 1. Log-odds are unbounded and have an inflection point at 0. Here we’ll save the odds and log-odds for our hazard functions within the lt life table.\n\nlt &lt;- lt %&gt;% \n  mutate(odds     = odds(hazard_fun),\n         log_odds = log_odds(hazard_fun)) \n\nlt\n\n# A tibble: 21 × 10\n   pt      time time_int n_risk n_event n_censored hazard_fun survivor_fun    odds log_odds\n   &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 pt = 0     6 [6, 7)       72      NA         NA    NA             1     NA         NA   \n 2 pt = 0     7 [7, 8)       72       2          0     0.0278        0.972  0.0286    -3.56\n 3 pt = 0     8 [8, 9)       70       2          0     0.0286        0.944  0.0294    -3.53\n 4 pt = 0     9 [9, 10)      68       8          0     0.118         0.833  0.133     -2.01\n 5 pt = 0    10 [10, 11)     60       8          0     0.133         0.722  0.154     -1.87\n 6 pt = 0    11 [11, 12)     52      10          0     0.192         0.583  0.238     -1.44\n 7 pt = 0    12 [12, 13)     42       8         34     0.190         0.472  0.235     -1.45\n 8 pt = 1     6 [6, 7)      108      NA         NA    NA             1     NA         NA   \n 9 pt = 1     7 [7, 8)      108      13          0     0.120         0.880  0.137     -1.99\n10 pt = 1     8 [8, 9)       95       5          0     0.0526        0.833  0.0556    -2.89\n# ℹ 11 more rows\n\n\nWe’re ready to make and combine the subplots for our version of Figure 11.2.\n\n# hazard\np1 &lt;- lt %&gt;% \n  filter(pt != \"overall\") %&gt;% \n  \n  ggplot(aes(x = time, y = hazard_fun, color = pt, group = pt)) +\n  geom_line() +\n  scale_y_continuous(NULL, breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  labs(subtitle = \"Estimated hazard\") +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.1, 0.825))\n\n# odds\np2 &lt;- lt %&gt;% \n  filter(pt != \"overall\") %&gt;% \n  \n  ggplot(aes(x = time, y = odds, color = pt, group = pt)) +\n  geom_line() +\n  scale_y_continuous(NULL, breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  labs(subtitle = \"Estimated odds\") +\n  theme(legend.position = \"none\")\n\n# log-odds\np3 &lt;- lt %&gt;% \n  filter(pt != \"overall\") %&gt;% \n  \n  ggplot(aes(x = time, y = log_odds, color = pt, group = pt)) +\n  geom_line() +\n  scale_y_continuous(NULL, limits = c(-4, 0)) +\n  labs(subtitle = \"Estimated logit(hazard)\") +\n  theme(legend.position = \"none\")\n\n(p1 / p2 / p3 ) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.5) &\n  scale_x_continuous(\"grade\", breaks = 6:12, limits = c(6, 12)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n11.1.2.2 What statistical model could have generated these sample data?\nWith the survival models from the prior sections, we were lazy and just used the survival package. But recall from the end of the last chapter that we can fit the analogous models brms using the binomial likelihood. This subsection is a great place to practice those some more. The fitted lines Singer and Willett displayed in Figure 11.3 can all be reproduced with binomial regression. However, the sex data are not in a convenient form to fit those models. Just like we did in last chapter, we’ll want to take a two-step process whereby we first convert the data to the long (i.e., person-period) format and then summarize. Happily, we can accomplish that first step by uploading the data in the firstsex_pp.csv file, which are already in the long format.\n\nsex_pp &lt;- read_csv(\"data/firstsex_pp.csv\")\n\nglimpse(sex_pp)\n\nRows: 822\nColumns: 11\n$ id     &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, 9, 9, 9, 9, 10, 10, …\n$ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8, 9, 7, 8, 9, 10, 11…\n$ event  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d7     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ d8     &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,…\n$ d9     &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ d10    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ d11    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ d12    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ pt     &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pas    &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -1.40498…\n\n\nNow we’ll compute the desired summary values and wrangle a bit.\n\nsex_aggregated &lt;- sex_pp %&gt;% \n  mutate(event = if_else(event == 1, \"event\", \"no_event\")) %&gt;% \n  group_by(period) %&gt;% \n  count(event, pt) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = event,\n              values_from = n) %&gt;% \n  mutate(total         = event + no_event,\n         period_center = period - mean(period),\n         peroid_factor = factor(period),\n         pt            = factor(pt))\n\nsex_aggregated\n\n# A tibble: 12 × 7\n   period pt    event no_event total period_center peroid_factor\n    &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;    &lt;int&gt; &lt;int&gt;         &lt;dbl&gt; &lt;fct&gt;        \n 1      7 0         2       70    72          -2.5 7            \n 2      7 1        13       95   108          -2.5 7            \n 3      8 0         2       68    70          -1.5 8            \n 4      8 1         5       90    95          -1.5 8            \n 5      9 0         8       60    68          -0.5 9            \n 6      9 1        16       74    90          -0.5 9            \n 7     10 0         8       52    60           0.5 10           \n 8     10 1        21       53    74           0.5 10           \n 9     11 0        10       42    52           1.5 11           \n10     11 1        15       38    53           1.5 11           \n11     12 0         8       34    42           2.5 12           \n12     12 1        18       20    38           2.5 12           \n\n\nNote how we saved the grade values in three columns:\n\nperiod has them as continuous values, which will be hand for plotting;\nperiod_center has them as mean-centered continuous values, which will make fitting the linear model in the middle panel easier; and\nperiod_factor has them saved as a factor, which will help us fit the model in the bottom panel.\n\nFire up brms.\n\nlibrary(brms)\n\nBefore we fit the models, it might be good to acknowledge we’re jumping ahead of the authors, a bit. Singer and Willett didn’t discuss fitting discrete time hazard models until section 11.3.2. Sure, their focus was on the frequentist approach using maximum likelihood. But the point still stands. If these model fitting details feel a bit rushed, they are.\nAny anxious feelings aside, now fit the three binomial models. We continue to use weakly-regularizing priors for each.\n\n# top panel\nfit11.4 &lt;- brm(\n  data = sex_aggregated,\n  family = binomial,\n  event | trials(total) ~ 0 + pt,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.04\")\n\n# middle panel\nfit11.5 &lt;- brm(\n  data = sex_aggregated,\n  family = binomial,\n  event | trials(total) ~ 0 + pt + period_center,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.05\")\n\n# bottom panel\nfit11.6 &lt;- brm(\n  data = sex_aggregated,\n  family = binomial,\n  event | trials(total) ~ 0 + pt + peroid_factor,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.06\")\n\nCheck the model summaries.\n\nprint(fit11.4)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(total) ~ 0 + pt \n   Data: sex_aggregated (Number of observations: 12) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\npt0    -2.15      0.17    -2.51    -1.82 1.00     2933     2316\npt1    -1.44      0.12    -1.67    -1.21 1.00     3416     2883\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit11.5)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(total) ~ 0 + pt + period_center \n   Data: sex_aggregated (Number of observations: 12) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\npt0              -2.23      0.18    -2.60    -1.90 1.00     3173     3053\npt1              -1.35      0.13    -1.60    -1.10 1.00     3518     2753\nperiod_center     0.43      0.07     0.30     0.56 1.00     3692     3246\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit11.6)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(total) ~ 0 + pt + peroid_factor \n   Data: sex_aggregated (Number of observations: 12) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\npt0                -3.00      0.31    -3.63    -2.42 1.00     1228     1889\npt1                -2.11      0.27    -2.66    -1.61 1.00     1063     1382\nperoid_factor8     -0.77      0.47    -1.73     0.10 1.00     1736     1819\nperoid_factor9      0.68      0.34     0.04     1.36 1.00     1306     1972\nperoid_factor10     1.15      0.34     0.51     1.84 1.00     1339     1608\nperoid_factor11     1.31      0.35     0.65     2.01 1.00     1441     2110\nperoid_factor12     1.79      0.36     1.10     2.50 1.00     1318     2129\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can extract the fitted values and their summaries for each row in the data with fitted(). To get them in the log-odds metric, we need to set scale = \"linear\". Here’s a quick example with fit11.4.\n\nfitted(fit11.4, scale = \"linear\")\n\n       Estimate Est.Error      Q2.5     Q97.5\n [1,] -2.154803 0.1727112 -2.512909 -1.824280\n [2,] -1.438563 0.1180980 -1.667146 -1.208921\n [3,] -2.154803 0.1727112 -2.512909 -1.824280\n [4,] -1.438563 0.1180980 -1.667146 -1.208921\n [5,] -2.154803 0.1727112 -2.512909 -1.824280\n [6,] -1.438563 0.1180980 -1.667146 -1.208921\n [7,] -2.154803 0.1727112 -2.512909 -1.824280\n [8,] -1.438563 0.1180980 -1.667146 -1.208921\n [9,] -2.154803 0.1727112 -2.512909 -1.824280\n[10,] -1.438563 0.1180980 -1.667146 -1.208921\n[11,] -2.154803 0.1727112 -2.512909 -1.824280\n[12,] -1.438563 0.1180980 -1.667146 -1.208921\n\n\nIf we convert that output to a data frame, tack on the original data values, and wrangle a bit, we’ll be in good shape to make the top panel of Figure 11.3. Below we’ll do that for each of the three panels, saving them as p1, p2, and p3.\n\n# logit(hazard) is horizontal with time\np1 &lt;- fitted(fit11.4, scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(sex_aggregated) %&gt;% \n  mutate(pt = str_c(\"pt = \", pt)) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/4) +\n  geom_line(aes(y = Estimate),\n            alpha = 1/2) +\n  geom_point(aes(y = log_odds(event / total))) +\n  scale_y_continuous(NULL, limits = c(-4, 0)) +\n  labs(subtitle = \"logit(hazard) is horizontal with time\") +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.1, 0.825))\n\n# logit(hazard) is linear with time\np2 &lt;- fitted(fit11.5, scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(sex_aggregated) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/4) +\n  geom_line(aes(y = Estimate),\n            alpha = 1/2) +\n  geom_point(aes(y = log_odds(event / total))) +\n  labs(subtitle = \"logit(hazard) is linear with time\",\n       y = \"logit(hazard)\") +\n  coord_cartesian(ylim = c(-4, 0)) +\n  theme(legend.position = \"none\")\n\n# logit(hazard) is completely general with time\np3 &lt;- fitted(fit11.6, scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(sex_aggregated) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/4) +\n  geom_line(aes(y = Estimate),\n            alpha = 1/2) +\n  geom_point(aes(y = log_odds(event / total))) +\n  labs(subtitle = \"logit(hazard) is completely general with time\",\n       y = NULL) +\n  coord_cartesian(ylim = c(-4, 0)) +\n  theme(legend.position = \"none\")\n\nNow combine the plots with patchwork syntax.\n\n(p1 / p2 / p3) &\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_x_continuous(\"Grade\", breaks = 6:12, limits = c(6, 12)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIn addition to the posterior means (i.e., our analogues to the fitted values in the text), we added the 95% Bayesian intervals to give a better sense of the uncertainty in each model. Singer and Willett mused the unconstrained model (fit6) was a better fit to the data than the other two. We can quantify that with a LOO comparison.\n\nfit11.4 &lt;- add_criterion(fit11.4, criterion = \"loo\")\nfit11.5 &lt;- add_criterion(fit11.5, criterion = \"loo\")\nfit11.6 &lt;- add_criterion(fit11.6, criterion = \"loo\")\n\nloo_compare(fit11.4, fit11.5, fit11.6) %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nfit11.6   0.0       0.0   -31.2      1.7         4.7   0.6     62.5   3.3   \nfit11.5  -1.2       3.4   -32.4      3.3         3.7   1.3     64.8   6.7   \nfit11.4 -28.1      10.5   -59.4     10.4         9.3   2.6    118.7  20.7   \n\n\nHere are the LOO weights.\n\nmodel_weights(fit11.4, fit11.5, fit11.6, weights = \"loo\") %&gt;% round(digits = 3)\n\nfit11.4 fit11.5 fit11.6 \n  0.000   0.238   0.762",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#formal-representation-of-the-population-discrete-time-hazard-model",
    "href": "11.html#formal-representation-of-the-population-discrete-time-hazard-model",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "11.2 Formal representation of the population discrete-time hazard model",
    "text": "11.2 Formal representation of the population discrete-time hazard model\nEarlier equations for the hazard function omitted substantive predictors. Now consider the case where \\(X_{1ij}, X_{2ij}, \\dots , X_{Pij}\\) stand for the \\(P\\) predictors which may or may not vary across individuals \\(i\\) and time periods \\(j\\). Thus \\(x_{pij}\\) is the value for the \\(i^\\text{th}\\) individual on the \\(p^\\text{th}\\) variable during the \\(j^\\text{th}\\) period. We can use this to define the conditional hazard function as\n\\[h(t_{ij}) = \\Pr [T_i = j \\mid T \\geq j \\text{ and } X_{1ij} = x_{1ij}, X_{2ij} = x_{2ij}, \\dots , X_{Pij} = x_{pij}].\\]\nBuilding further and keeping the baseline shape of the discrete hazard function flexible, we want a method that allows each of the \\(j\\) time periods to have its own value. Imagine a set of \\(J\\) dummy variables, \\(D_1, D_2, \\dots, D_J\\), marking off each of the time periods. For example, say \\(J = 6\\), we could depict this in a tibble like so.\n\ntibble(period = 1:6) %&gt;% \n  mutate(d1 = if_else(period == 1, 1, 0),\n         d2 = if_else(period == 2, 1, 0),\n         d3 = if_else(period == 3, 1, 0),\n         d4 = if_else(period == 4, 1, 0),\n         d5 = if_else(period == 5, 1, 0),\n         d6 = if_else(period == 6, 1, 0))\n\n# A tibble: 6 × 7\n  period    d1    d2    d3    d4    d5    d6\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     1     0     0     0     0     0\n2      2     0     1     0     0     0     0\n3      3     0     0     1     0     0     0\n4      4     0     0     0     1     0     0\n5      5     0     0     0     0     1     0\n6      6     0     0     0     0     0     1\n\n\nIf we were to use a set of dummies of this kind in a model, we would omit the conventional regression intercept, replacing it with the \\(J\\) dummies. Now presume we’re fitting a hazard model using the logit link, \\(\\operatorname{logit} h(t_{ij})\\). We can express the discrete conditional hazard model with a general functional form with respect to time as\n\\[\\operatorname{logit} h(t_{ij}) = [\\alpha_1 D_{1ij} + \\alpha_2 D_{2ij} + \\cdots + \\alpha_J D_{Jij}] + [\\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\cdots + \\beta_P X_{Pij}],\\]\nwhere the \\(\\alpha\\) parameters are the \\(J\\) time-period dummies and the \\(\\beta\\) parameters are for other time-varying or time-invariant predictors. This is just the type of model we used to fit fit116. For that model, the basic equation was\n\\[\\operatorname{logit} h(t_{ij}) = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ],\\]\nwhere the only substantive predictor was the time-invariant pt. However, that formula could be a little misleading. Recall the formula:\n\nfit11.6$formula\n\nevent | trials(total) ~ 0 + pt + peroid_factor \n\n\nWe suppressed the default regression intercept with the ~ 0 + syntax and the only two predictors were pt and peroid_factor. Both were saved as factor variables. Functionally, that’s why period_factor worked as \\(\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}\\), a series of 5 dummy variables with no reference category. The same basic thing goes for pt. Because pt was a factor used in a model formula with no conventional intercept, it acted as if it was a series of 2 dummy variables with no reference category. Thus, we might rewrite the model equation for fit6 as\n\\[\\operatorname{logit} h(t_{ij}) = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_0 \\text{PT}_{0i} + \\beta_1 \\text{PT}_{1i} ].\\]\nAnd since we’re practicing fitting these models as Bayesians, the fit6 equation with a fuller expression of the likelihood and the priors looks like\n\\[\n\\begin{align}\n\\text{event}_{ij} & = \\operatorname{Binomial}(n = \\text{trials}_{ij}, p_{ij}) \\\\\n\\operatorname{logit} (p_{ij}) & =  [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_0 \\text{PT}_{0i} + \\beta_1 \\text{PT}_{1i} ] \\\\\n\\alpha_7, \\alpha_8, \\dots, \\alpha_{12} & \\sim \\operatorname{Normal}(0, 4) \\\\\n\\beta_0 \\text{ and } \\beta_1 & \\sim \\operatorname{Normal}(0, 4),\n\\end{align}\n\\]\nwhere we’re describing the model in terms of the criterion, event, rather than in terms of \\(h(t_{ij})\\). And what is the criterion, event? It’s a vector of counts. The binomial likelihood allows us to model vectors of counts in terms of the number of trials, as indexed by our trials vector, and the (conditional) probability of a “1” in a given trial. In this context, \\(h(t_{ij}) = p_{ij}\\).\n\n11.2.1 What do the parameters represent?\nGiven our factor coding of pt, our two submodels for the equations in the last section are\n\\[\n\\begin{align}\n\\text{when PT = 0: } \\operatorname{logit} h(t_j) & = [\\alpha_7 D_7 + \\alpha_8 D_8 + \\cdots + \\alpha_{12} D_{12}] + \\beta_0 \\\\\n\\text{when PT = 1: } \\operatorname{logit} h(t_j) & = [\\alpha_7 D_7 + \\alpha_8 D_8 + \\cdots + \\alpha_{12} D_{12}] + \\beta_1,\n\\end{align}\n\\]\nwhere we used Singer and Willett’s simplified notation and dropped all the \\(i\\) subscripts and most of the \\(j\\) subscripts.\n\n\n11.2.2 An alternative representation of the model\nIn the previous sections, we expressed the model in terms of the logit of the criterion or the \\(p\\) parameter of the likelihood. Another strategy is the express the criterion (or \\(p\\)) in its natural metric and put the nonlinear portion on the right side of the equation. If we consider the generic discrete conditional hazard function, that would follow the form\n\\[\nh(t_{ij}) = \\frac{1}{1 + e^{-([\\alpha_1 D_{1ij} + \\alpha_2 D_{2ij} + \\cdots + \\alpha_J D_{Jij}] + [\\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\cdots + \\beta_P X_{Pij}])}}.\n\\]\nThis is just a particular kind of logistic regression model. It also clarifies that “by specifying a linear relationship between predictors and logit hazard we imply a nonlinear relationship between predictors and raw hazard” (p. 377, emphasis in the original). We can explore what that might look like with our version of Figure 11.4. Here we continue to use fit6, but this time we’ll save the output from fitted() before plotting.\n\nf &lt;- fitted(fit11.6, scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(sex_aggregated)\n\nf\n\n     Estimate Est.Error       Q2.5      Q97.5 period pt event no_event total period_center peroid_factor\n1  -2.9961977 0.3118918 -3.6299955 -2.4188144      7  0     2       70    72          -2.5             7\n2  -2.1106221 0.2696068 -2.6604562 -1.6051558      7  1    13       95   108          -2.5             7\n3  -3.7686230 0.4335907 -4.6929008 -2.9779488      8  0     2       68    70          -1.5             8\n4  -2.8830474 0.3974622 -3.7203998 -2.1929817      8  1     5       90    95          -1.5             8\n5  -2.3134865 0.2740091 -2.8577350 -1.7856641      9  0     8       60    68          -0.5             9\n6  -1.4279109 0.2251447 -1.8869407 -0.9980089      9  1    16       74    90          -0.5             9\n7  -1.8495091 0.2655533 -2.3770859 -1.3344318     10  0     8       52    60           0.5            10\n8  -0.9639334 0.2247747 -1.4134571 -0.5362926     10  1    21       53    74           0.5            10\n9  -1.6840227 0.2738422 -2.2347843 -1.1460772     11  0    10       42    52           1.5            11\n10 -0.7984471 0.2498367 -1.3011791 -0.3130270     11  1    15       38    53           1.5            11\n11 -1.2032530 0.2792854 -1.7707307 -0.6776080     12  0     8       34    42           2.5            12\n12 -0.3176774 0.2589108 -0.8189017  0.1767758     12  1    18       20    38           2.5            12\n\n\nMake the subplots.\n\n# logit(hazard)\np1 &lt;- f %&gt;% \n  mutate(pt = str_c(\"pt = \", pt)) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = Estimate)) +\n  labs(subtitle = \"logit(hazard)\",\n       y = NULL) +\n  coord_cartesian(ylim = c(-4, 0)) +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.1, 0.825))\n\n# odds\np2 &lt;- f %&gt;% \n  mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = exp) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = Estimate)) +\n  labs(subtitle = \"odds\",\n       y = NULL) +\n  coord_cartesian(ylim = c(0, 0.8)) +\n  theme(legend.position = \"none\")\n\n# hazard\np3 &lt;- f %&gt;% \n  mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = plogis) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = Estimate)) +\n  labs(subtitle = \"hazard (i.e., probability)\",\n       y = NULL) +\n  coord_cartesian(ylim = c(0, 0.5)) +\n  theme(legend.position = \"none\")\n\nCombine the subplots with patchwork.\n\n(p1 / p2 / p3) &\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_x_continuous(\"Grade\", breaks = 6:12, limits = c(6, 12)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe mutate_at() conversions we made for p2 and p3 were based on the guidelines in Table 11.2. Those were:\n\ntibble(`original scale` = c(\"logit\", \"odds\", \"logit\"),\n       `desired scale`  = c(\"odds\", \"probability\", \"probability\"),\n       transformation   = c(\"exp(logit)\", \"odds / (1 + odds)\", \"1 / (1 + exp(-1 * logit))\")) %&gt;% \n  flextable::flextable() %&gt;% \n  flextable::width(width = c(1.5, 1.5, 2))\n\noriginal scaledesired scaletransformationlogitoddsexp(logit)oddsprobabilityodds / (1 + odds)logitprobability1 / (1 + exp(-1 * logit))\n\n\nWe accomplished the transformation in the bottom row with the plogis() function.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#fitting-a-discrete-time-hazard-model-to-data",
    "href": "11.html#fitting-a-discrete-time-hazard-model-to-data",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "11.3 Fitting a discrete-time hazard model to data",
    "text": "11.3 Fitting a discrete-time hazard model to data\nAs Singer and Willett wrote, “with data collected on a random sample of individuals from a target population, you can easily fit a discrete-time hazard model, estimate its parameters using maximum likelihood methods, and evaluate goodness-of-fit” (pp. 378–379. As we’ve already demonstrated, you can fit them with Bayesian software, too. Though we’ll be focusing on brms, you might also want to check out the rstanarm package, about which you can learn more from Brilleman, Elci, Novik, and Wolfe’s (2020) preprint, Bayesian survival analysis Using the rstanarm R package, Brilleman’s (2019) vignette, Estimating survival (time-to-event) models with rstanarm, and the Survival models in rstanarm thread in the Stan forums.\n\n11.3.1 Adding predictors to the person-period data set\nAt the beginning of section 11.1.2.2, we already loaded a version of the person-period data including the discrete-time dummies. It has our substantive predictors pt and pas, too. We saved it as sex_pp. Here’s a glimpse().\n\nsex_pp %&gt;% \n  glimpse()\n\nRows: 822\nColumns: 11\n$ id     &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, 9, 9, 9, 9, 10, 10, …\n$ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8, 9, 7, 8, 9, 10, 11…\n$ event  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d7     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ d8     &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,…\n$ d9     &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ d10    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ d11    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ d12    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ pt     &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pas    &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -1.40498…\n\n\n\n\n11.3.2 Maximum likelihood estimates [and Bayesian posteriors] for the discrete-time hazard model\nWe’re not going to walk through all the foundational equations for the likelihood and log-likelihood functions (Equations 11.11 through 11.13). For our purposes, just note that “it turns out that the standard logistic regression routines widely available in all major statistical packages, when applied appropriately in the person-period data set, actually provide estimates of the parameters of the discrete-time hazard model” (p. 383, emphasis in the original). Happily, this is what we’ve been doing. Bayesian logistic regression via the binomial likelihood has been our approach. And since we’re Bayesians, the same caveat applies to survival models as applied to the other longitudinal models we fit in earlier chapters. We’re not just maximizing likelihoods, here. Bayes’s formula requires us to multiply the likelihood by the prior.\n\\[\n\\underbrace{p(\\theta \\mid d)}_\\text{posterior} \\propto \\underbrace{p(d \\mid \\theta)}_\\text{likelihood} \\; \\underbrace{p(\\theta)}_\\text{prior}\n\\]\n\n\n11.3.3 Fitting the discrete-time hazard model to data\nIn one sense, fitting discrete-hazard models with Bayesian logistic regression is old hat, for us. We’ve been doing that since the end of last chapter. But one thing I haven’t clarified is, up to this point, we have been using the aggregated binomial format. To show what I mean, we might look at the data we used for our last model, fit11.6.\n\nsex_aggregated\n\n# A tibble: 12 × 7\n   period pt    event no_event total period_center peroid_factor\n    &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;    &lt;int&gt; &lt;int&gt;         &lt;dbl&gt; &lt;fct&gt;        \n 1      7 0         2       70    72          -2.5 7            \n 2      7 1        13       95   108          -2.5 7            \n 3      8 0         2       68    70          -1.5 8            \n 4      8 1         5       90    95          -1.5 8            \n 5      9 0         8       60    68          -0.5 9            \n 6      9 1        16       74    90          -0.5 9            \n 7     10 0         8       52    60           0.5 10           \n 8     10 1        21       53    74           0.5 10           \n 9     11 0        10       42    52           1.5 11           \n10     11 1        15       38    53           1.5 11           \n11     12 0         8       34    42           2.5 12           \n12     12 1        18       20    38           2.5 12           \n\n\nNow recall the formula for the binomial likelihood from the end of last chapter:\n\\[\\Pr (z \\mid n, p) = \\frac{n!}{z!(n - z)!} p^z (1 - p)^{n - z},\\]\nwhere \\(z\\) is the number of cases for which the value is 1, \\(n\\) is the total number of cases, and \\(p\\) is the constant chance of a 1 across cases. We refer to binomial data as aggregated with \\(n &gt; 1\\). Our \\(n\\) vector in the sex_aggregated, total, ranged from 38 to 108. Accordingly, our \\(z\\) vector, event, was always some value equal or lower to that in the same row for total.\nThe person-period data, sex_pp, contain the same information but in a different format. Instead, each event cell only takes on a value of 0 or 1 (i.e., \\(n = 1\\)). If you were to sum up all the values in the total column of the sex_aggregated data, you’d return 822.\n\nsex_aggregated %&gt;% \n  summarise(sum = sum(total))\n\n# A tibble: 1 × 1\n    sum\n  &lt;int&gt;\n1   822\n\n\nThis is also the total number of rows in the sex_pp data.\n\nsex_pp %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   822\n\n\nIt’s also the case that when \\(n = 1\\), the right side of the equation for the binomial function reduces to\n\\[p^z (1 - p)^{1 - z}.\\]\nWhether you are working with aggregated or un-aggregated data, both are suited to fit logistic regression models with the binomial likelihood. Just specify the necessary information in the model syntax. For brms, the primary difference is how you use the trials() function. When we fit our logistic regression models using the aggregated data, we specified trials(total), which informed the brm() function what the \\(n\\) values were. In the case of unaggregated binomial data, we can just state trials(1). Each cell is the outcome \\(z\\) for a single trial.\nBefore we fit the models, we might talk a bit about priors. When we fit the first model of this kind at the end of Chapter 10, we just used prior(normal(0, 4), class = b) without comment. Recall we’re modeling probabilities in the log-odds space. In Section 11.1.2.1 we used a plot to compare probability values to their log-odds counterparts. Let’s take a more focused look.\n\ntibble(log_odds = -8:8) %&gt;% \n  mutate(p = plogis(log_odds)) %&gt;% \n  \n  ggplot(aes(x = log_odds, y = p)) +\n  geom_hline(yintercept = 0:5 / 5, color = \"white\") +\n  geom_point() +\n  scale_x_continuous(breaks = -8:8) +\n  scale_y_continuous(breaks = 0:5 / 5) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWhen \\(\\operatorname{log-odds} p = 0\\), \\(p = .5\\). Once \\(\\operatorname{log-odds} p\\) approaches the \\(\\mp 4\\) neighborhood, the corresponding values for \\(p\\) asymptote at the boundaries \\([0, 1]\\). By using a \\(\\operatorname{Normal} (0, 4)\\) prior for \\(\\operatorname{log-odds} p\\), we’re putting bulk of the prior mass in the \\(\\operatorname{log-odds} p\\) space between, say, -8 and 8. In the absence of other information, this might be a good place to start. A little further down, we’ll reexamine this set-up. For now, here’s how to use brm() to fit Models A through D from page 385.\n\nlibrary(brms)\n\n# model a\nfit11.7 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.07\")\n\n# model b\nfit11.8 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.08\")\n\n# model c\nfit11.9 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pas,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.09\")\n\n# model d\nfit11.10 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.10\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#interpreting-parameter-estimates",
    "href": "11.html#interpreting-parameter-estimates",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "11.4 Interpreting parameter estimates",
    "text": "11.4 Interpreting parameter estimates\nHere are the model summaries in bulk.\n\nprint(fit11.7)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 \n   Data: sex_pp (Number of observations: 822) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd7     -2.42      0.27    -2.99    -1.90 1.00     5474     2708\nd8     -3.16      0.39    -3.99    -2.45 1.00     5084     2689\nd9     -1.73      0.22    -2.18    -1.32 1.00     5905     2946\nd10    -1.30      0.21    -1.72    -0.90 1.00     6003     2873\nd11    -1.18      0.24    -1.67    -0.73 1.00     5527     2820\nd12    -0.74      0.24    -1.21    -0.27 1.00     5473     2634\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit11.8)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt \n   Data: sex_pp (Number of observations: 822) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd7     -3.00      0.32    -3.67    -2.41 1.00     3002     2651\nd8     -3.72      0.43    -4.60    -2.94 1.00     3904     2855\nd9     -2.28      0.27    -2.83    -1.76 1.00     2858     2526\nd10    -1.82      0.26    -2.33    -1.32 1.00     2847     2679\nd11    -1.66      0.27    -2.19    -1.15 1.00     3142     2901\nd12    -1.17      0.27    -1.71    -0.64 1.00     3426     2801\npt      0.86      0.21     0.44     1.27 1.00     1836     2479\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit11.9)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pas \n   Data: sex_pp (Number of observations: 822) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd7     -2.48      0.27    -3.04    -1.95 1.00     6119     2828\nd8     -3.20      0.39    -4.04    -2.50 1.00     5649     2934\nd9     -1.75      0.23    -2.21    -1.31 1.00     5914     2947\nd10    -1.30      0.21    -1.72    -0.88 1.00     6462     3165\nd11    -1.15      0.24    -1.62    -0.68 1.00     6668     2811\nd12    -0.65      0.24    -1.15    -0.19 1.00     6184     3042\npas     0.44      0.11     0.22     0.67 1.00     6467     2951\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit11.10)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas \n   Data: sex_pp (Number of observations: 822) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd7     -2.92      0.32    -3.57    -2.32 1.00     3002     2698\nd8     -3.62      0.42    -4.49    -2.85 1.00     3622     2913\nd9     -2.16      0.28    -2.73    -1.64 1.00     2697     3005\nd10    -1.70      0.26    -2.21    -1.20 1.00     2686     2709\nd11    -1.52      0.28    -2.09    -1.01 1.00     2838     2869\nd12    -1.01      0.28    -1.58    -0.48 1.00     2461     2688\npt      0.65      0.23     0.19     1.12 1.00     1579     2498\npas     0.30      0.13     0.05     0.55 1.00     3899     2889\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAlthough the text distinguishes between \\(\\alpha\\) and \\(\\beta\\) parameters (i.e., intercept and slope parameters, respectively), our brms output makes no such distinction. These are all of class = b, population-level \\(\\beta\\) parameters.\nWhen viewed in bulk, all those print() calls yield a lot of output. We can arrange the parameter summaries similar to those in Table 11.3 with a little tricky wrangling.\n\ntibble(model = str_c(\"model \", letters[1:4]),\n       fit   = str_c(\"fit11.\", 7:10)) %&gt;% \n  mutate(f = map(fit, ~ get(.) %&gt;% \n                   fixef() %&gt;% \n                   data.frame() %&gt;% \n                   rownames_to_column(\"parameter\"))) %&gt;% \n  unnest(f) %&gt;% \n  mutate(e_sd  = str_c(round(Estimate, digits = 2), \" (\", round(Est.Error, digits = 2), \")\")) %&gt;% \n  select(model, parameter, e_sd) %&gt;% \n  pivot_wider(names_from = model, values_from = e_sd) %&gt;% \n  flextable::flextable() %&gt;% \n  flextable::width(width = 1)\n\nparametermodel amodel bmodel cmodel dd7-2.42 (0.27)-3 (0.32)-2.48 (0.27)-2.92 (0.32)d8-3.16 (0.39)-3.72 (0.43)-3.2 (0.39)-3.62 (0.42)d9-1.73 (0.22)-2.28 (0.27)-1.75 (0.23)-2.16 (0.28)d10-1.3 (0.21)-1.82 (0.26)-1.3 (0.21)-1.7 (0.26)d11-1.18 (0.24)-1.66 (0.27)-1.15 (0.24)-1.52 (0.28)d12-0.74 (0.24)-1.17 (0.27)-0.65 (0.24)-1.01 (0.28)pt0.86 (0.21)0.65 (0.23)pas0.44 (0.11)0.3 (0.13)\n\n\n\n11.4.1 The time indicators\n\nAs a group, the \\(\\hat \\alpha\\)s are [Bayesian] estimates for the baseline logit hazard function. The amount and direction of variation in their values describe the shape of this function and tell us whether risk increases, decreases, or remains steady over time. (p. 387)\n\nA coefficient plot might help us get a sense of that across the four models.\n\ntibble(model = str_c(\"model \", letters[1:4]),\n       fit   = str_c(\"fit11.\", 7:10)) %&gt;% \n  mutate(f = map(fit, ~ get(.) %&gt;% \n                   fixef() %&gt;% \n                   data.frame() %&gt;% \n                   rownames_to_column(\"parameter\"))) %&gt;% \n  unnest(f) %&gt;% \n  filter(str_detect(parameter, \"d\")) %&gt;% \n  mutate(parameter = factor(str_remove(parameter, \"b_\"), \n                            levels = str_c(\"d\", 12:7))) %&gt;%\n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = parameter)) +\n  geom_pointrange(size = 1/4) +\n  labs(x = \"posterior (log-odds scale)\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank()) +\n  facet_wrap(~ model, nrow = 1)\n\n\n\n\n\n\n\n\n“The fairly steady increase over time in the magnitude of the \\(\\hat \\alpha\\)s in each model [in the coefficient plot] shows that, in this sample of boys, the risk of first intercourse increases over time” (p. 387). When comparing the \\(\\hat \\alpha\\)s across models, it’s important to recall that the presence/absence of substantive covariates means each model has a different baseline group.\nBecause they were in the log-odds scale, the model output and our coefficient plot can be difficult to interpret. With the plogis(), we can convert the \\(\\hat \\alpha\\)s to the hazard (i.e., probability) metric.\n\ntibble(model = str_c(\"model \", letters[1:4]),\n       fit   = str_c(\"fit11.\", 7:10)) %&gt;% \n  mutate(f = map(fit, ~ get(.) %&gt;% \n                   fixef() %&gt;% \n                   data.frame() %&gt;% \n                   rownames_to_column(\"parameter\"))) %&gt;% \n  unnest(f) %&gt;% \n  filter(str_detect(parameter, \"d\")) %&gt;% \n  mutate(parameter = factor(str_remove(parameter, \"b_\"), \n                            levels = str_c(\"d\", 12:7))) %&gt;%\n  mutate_at(vars(Estimate:Q97.5), .funs = plogis) %&gt;% \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = parameter)) +\n  geom_pointrange(size = 1/4) +\n  labs(x = \"posterior (hazard scale)\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank()) +\n  facet_wrap(~ model, nrow = 1)\n\n\n\n\n\n\n\n\nBuilding further, here’s our version of Table 11.4.\n\nfixef(fit11.7) %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(\"predictor\") %&gt;% \n  mutate(`time period` = str_remove(predictor, \"d\") %&gt;% as.double()) %&gt;% \n  select(`time period`, predictor, Estimate) %&gt;% \n  mutate(`fitted odds`   = exp(Estimate),\n         `fitted hazard` = plogis(Estimate)) %&gt;% \n  mutate_if(is.double, round, digits = 4) %&gt;% \n  flextable::flextable() %&gt;% \n  flextable::width(width = 1)\n\ntime periodpredictorEstimatefitted oddsfitted hazard7d7-2.42150.08880.08168d8-3.15550.04260.04099d9-1.73440.17650.150010d10-1.29750.27320.214611d11-1.17530.30870.235912d12-0.74020.47700.3230\n\n\n\n\n11.4.2 Dichotomous substantive predictors\nHere’s the summary for pt from fit11.8 (i.e., Model B).\n\nfixef(fit11.8)[\"pt\", ]\n\n Estimate Est.Error      Q2.5     Q97.5 \n0.8579369 0.2140275 0.4417184 1.2704125 \n\n\nIf we take the anti-log (i.e., exponentiate) of that coefficient, we’ll return an odds ratio. Here’s the conversion with just the posterior mean.\n\nfixef(fit11.8)[\"pt\", 1] %&gt;% exp()\n\n[1] 2.35829\n\n\nTo get a better sense of the conversion, here it is in a plot.\n\nlibrary(tidybayes)\n\nposterior_samples(fit11.8) %&gt;% \n  transmute(`log-odds`     = b_pt,\n            `hazard ratio` = exp(b_pt)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name, levels = c(\"log-odds\", \"hazard ratio\"))) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"marginal posterior for pt\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\nThis tells us that, in every grade, the estimated odds of first intercourse are nearly two and one half times higher for boys who experienced a parenting transition in comparison to boys raised with both biological parents. In substantive terms, an odds ratio of this magnitude represents a substantial, and potentially important, effect. (p. 398)\n\nTo reframe the odds ratio in terms of the other group (i.e., pt == 0), take the reciprocal.\n\n1 / exp(fixef(fit11.8)[7, 1])\n\n[1] 0.424036\n\n\n“This tells us that the estimated odds of first intercourse for boys who did not experience a parenting transition are approximately 40% of the odds for boys who did. These complimentary ways of reporting effect sizes are equivalent” (p. 389)\n\n\n11.4.3 Continuous substantive predictors\nHere’s the conditional effect of pas from fit11.9 (i.e., Model C).\n\nfixef(fit11.9)[\"pas\", ]\n\n Estimate Est.Error      Q2.5     Q97.5 \n0.4448054 0.1147100 0.2195005 0.6668125 \n\n\nTo understand pas, our measure of parental antisocial behavior, it will help to look at its range.\n\nrange(sex_pp$pas)\n\n[1] -1.716180  2.781413\n\n\nExponentiating (i.e., taking the anti-log) the posterior of a continuous predictor is a legitimate way to convert it to a hazard ratio.\n\nas_draws_df(fit11.9) %&gt;% \n  transmute(`log-odds`     = b_pas,\n            `hazard ratio` = exp(b_pas)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name, levels = c(\"log-odds\", \"hazard ratio\"))) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"marginal posterior for pas\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nHere’s how to compute the hazard ratio for a 2-unit difference in pas.\n\nexp(fixef(fit11.9)[7, 1] * 2)\n\n[1] 2.434182\n\n\nHere’s what that looks like in a plot.\n\nas_draws_df(fit11.9) %&gt;% \n  transmute(`log-odds`                   = b_pas,\n            `hazard ratio`               = exp(b_pas),\n            `hr for a 2-unit difference` = exp(b_pas * 2)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name = factor(name,\n                       levels = c(\"log-odds\", \"hazard ratio\", \"hr for a 2-unit difference\"))) %&gt;%\n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"marginal posterior for pas\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n11.4.4 Polytomous substantive predictors\nUnfortunately, neither the sex nor the sex_pp data sets contain the polytomous version of pt as Singer described in this section. However, we can simulate a similar set of dummy variables which will allow us to trace the basic steps in Singer and Willett’s workflow.\nSince we’ve been working with the sex_pp data for the past few models, we’ll continue using it here. However, this creates a minor challenge. What we want to do is use the sample() function to randomly assign cases with values of 1, 2, or 3 conditional on whether pt == 0. The catch is, we need to make sure that random value is constant for each case. Our solution will be to first nest the data such that each case only has one row. Here’s what that looks like.\n\nset.seed(11)\n\nsex_pp &lt;- sex_pp %&gt;% \n  nest(data = c(period, event, d7, d8, d9, d10, d11, d12, pas)) %&gt;% \n  mutate(random = sample(1:3, size = n(), replace = T)) %&gt;% \n  mutate(pt_cat = ifelse(pt == 0, pt, random)) %&gt;% \n  mutate(pt1 = ifelse(pt_cat == 1, 1, 0),\n         pt2 = ifelse(pt_cat == 2, 1, 0),\n         pt3 = ifelse(pt_cat == 3, 1, 0)) %&gt;% \n  select(id, pt, random, pt_cat:pt3, data)\n\nsex_pp %&gt;% \n  head()\n\n# A tibble: 6 × 8\n     id    pt random pt_cat   pt1   pt2   pt3 data            \n  &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;          \n1     1     0      2      0     0     0     0 &lt;tibble [3 × 9]&gt;\n2     2     1      2      2     0     1     0 &lt;tibble [6 × 9]&gt;\n3     3     0      1      0     0     0     0 &lt;tibble [6 × 9]&gt;\n4     5     1      1      1     1     0     0 &lt;tibble [6 × 9]&gt;\n5     6     0      1      0     0     0     0 &lt;tibble [5 × 9]&gt;\n6     7     1      2      2     0     1     0 &lt;tibble [3 × 9]&gt;\n\n\nHere are the number of cases for each of the four levels of our pseudovariable pt_cat.\n\nsex_pp %&gt;% \n  count(pt_cat)\n\n# A tibble: 4 × 2\n  pt_cat     n\n   &lt;dbl&gt; &lt;int&gt;\n1      0    72\n2      1    41\n3      2    30\n4      3    37\n\n\nOur breakdown isn’t exactly the same as the one in the text (p. 391), but we’re in the ballpark. Before we’re ready to fit our next model, we’ll need to unnest() the data, which will transform sex_pp back into the familiar long format.\n\nsex_pp &lt;- sex_pp %&gt;% \n  unnest(data)\n\nBefore we fit the model with the pt* dummies, let’s backup and talk about priors, again. When we fit the last four models, our discussion about priors for \\(p\\) focused on the posterior implications for those parameters in the log-odds space. Things get odd when we consider the implications in the probability space. To demonstrate, let’s simulate from \\(\\operatorname{Normal}(0, 4)\\) and see what it looks like when we transform the draws back into the probability metric.\n\nset.seed(11)\n\ntibble(log_odds = rnorm(1e6, mean = 0, sd = 4)) %&gt;% \n  mutate(p = plogis(log_odds)) %&gt;% \n  ggplot(aes(x = p)) +\n  geom_histogram(bins = 50) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nPerhaps unexpectedly, our log-odds \\(\\operatorname{Normal}(0, 4)\\) prior ended up bunching up the prior mass at the boundaries. Depending on what we want, this may or may not make sense. If we want to regularize the coefficients toward zero in the probability space, something closer to \\(\\operatorname{Normal}(0, 1)\\) would be a better idea. Here’s a look at what happens when we compare three simulations in that range.\n\nset.seed(11)\n\ntibble(sd = c(2, 1.5, 1)) %&gt;% \n  mutate(log_odds = map(sd, ~rnorm(1e6, mean = 0, sd = .))) %&gt;% \n  unnest(log_odds) %&gt;% \n  mutate(sd = str_c(\"sd = \", sd),\n         p  = plogis(log_odds)) %&gt;% \n\n  ggplot(aes(x = p)) +\n  geom_histogram(bins = 50) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ sd)\n\n\n\n\n\n\n\n\nIt looks like the log-odds \\(\\operatorname{Normal}(0, 1)\\) gently regularizes \\(p\\) toward .5, but still allows for stronger values. This might be a good prior to use for our substantive covariates, what Singer and Willett referred to as the \\(\\beta\\) parameters. But I don’t know that this makes sense for our \\(\\alpha\\) parameters. If you’ve been following along with the model output, the life tables, and so on, you’ll have noticed those tend to drift toward the lower end of the probability range. Regularizing toward \\(p = .5\\) might not be a good idea. In the absence of good substantive or statistical theory, perhaps it’s best to use a flat prior. The log-odds \\(\\operatorname{Normal} (0, 1.5)\\) prior is nearly flat on the probability space, but it does still push the mass away from the boundaries.\nMaybe we can come up with something better. What if we simulated a large number of draws from the \\(\\operatorname{Uniform}(0, 1)\\) distribution, converted those draws to the log-odds metric, and fit a simple Student’s \\(t\\) model? If we wanted to stay within the Student-\\(t\\) family of priors, of which the normal is a special case, that would give us a sense of the what prior values would approximate a uniform distribution on the probability scale.\n\nset.seed(11)\n\ndat &lt;- tibble(p = runif(1e5, 0, 1)) %&gt;% \n  mutate(g = log_odds(p)) \n\nfit11.11 &lt;- brm(\n  data = dat,\n  family = student,\n  g ~ 1,\n  chains = 4, cores = 4,\n  file = \"fits/fit11.11\")\n\nCheck the summary.\n\nprint(fit11.11)\n\n Family: student \n  Links: mu = identity \nFormula: g ~ 1 \n   Data: dat (Number of observations: 100000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.01    -0.01     0.02 1.00     2702     2539\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.57      0.01     1.55     1.58 1.00     2573     2810\nnu        7.61      0.18     7.29     7.95 1.00     2147     2359\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we can reverse the process. Here’s what it would look like if we simulated from the Student \\(t\\)-distribution based on those posterior means and then converted the results into the probability metric.\n\nset.seed(11)\n\ntibble(g = rt(1e6, df = 7.61) * 1.57) %&gt;% \n  mutate(p = plogis(g)) %&gt;% \n  \n  ggplot(aes(x = p)) +\n  geom_histogram(bins = 50) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNow here’s what it might look like if fit the next model with the \\(\\operatorname{Normal}(0, 1)\\) prior for the \\(\\beta\\) parameters and \\(\\text{Student-} t (7.61, 0, 1.57)\\) prior for the \\(\\alpha\\) parameters.\n\nfit11.12 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt1 + pt2 + pt3,\n  prior = c(prior(student_t(7.61, 0, 1.57), class = b), \n            prior(normal(0, 1), class = b, coef = pt1), \n            prior(normal(0, 1), class = b, coef = pt2), \n            prior(normal(0, 1), class = b, coef = pt3)),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.12\")\n\nHere is the model summary.\n\nprint(fit11.12)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt1 + pt2 + pt3 \n   Data: sex_pp (Number of observations: 822) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd7     -2.83      0.30    -3.43    -2.26 1.00     3834     3297\nd8     -3.50      0.40    -4.34    -2.78 1.00     5135     2881\nd9     -2.13      0.26    -2.65    -1.64 1.00     3509     2972\nd10    -1.67      0.24    -2.15    -1.20 1.00     3704     3301\nd11    -1.50      0.26    -2.03    -1.01 1.00     4463     3192\nd12    -1.03      0.26    -1.57    -0.53 1.00     4586     2975\npt1     0.47      0.25    -0.02     0.95 1.00     3001     3249\npt2     0.51      0.27    -0.02     1.05 1.00     3283     3075\npt3     1.01      0.26     0.50     1.52 1.00     3252     2975\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can use the tidy() function and a few lines of wrangling code to make a version of the table in the middle of page 391.\n\nfixef(fit11.12)[7:9, ] %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(\"predictor\") %&gt;% \n  mutate(`odds ratio` = exp(Estimate)) %&gt;% \n  select(predictor, Estimate, `odds ratio`) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n  predictor Estimate odds ratio\n1       pt1    0.470      1.599\n2       pt2    0.514      1.672\n3       pt3    1.013      2.753\n\n\nBecause our data did not include the original values for pt1 through pt3, the results in our table will not match those in the text. We did get pretty close, though, eh? Hopefully this gives a sense of the workflow.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#displaying-fitted-hazard-and-survivor-functions",
    "href": "11.html#displaying-fitted-hazard-and-survivor-functions",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "11.5 Displaying fitted hazard and survivor functions",
    "text": "11.5 Displaying fitted hazard and survivor functions\nThis will be an extension of what we’ve already been doing.\n\n11.5.1 A strategy for a single categorical substantive predictor\nWe can make our version of Table 11.5 like so. To reduce clutter, we will use abbreviated column names.\n\ntibble(time  = 7:12,\n       alpha = fixef(fit11.8)[1:6, 1],\n       beta  = fixef(fit11.8)[7, 1]) %&gt;% \n  mutate(lh0 = alpha,\n         lh1 = alpha + beta) %&gt;% \n  mutate(h0 = plogis(lh0),\n         h1 = plogis(lh1)) %&gt;% \n  mutate(s0 = cumprod(1 - h0),\n         s1 = cumprod(1 - h1)) %&gt;% \n  # this just simplifies the output\n  mutate_if(is.double, round, digits = 4)\n\n# A tibble: 6 × 9\n   time alpha  beta   lh0    lh1     h0     h1    s0    s1\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     7 -3.00 0.858 -3.00 -2.15  0.0472 0.105  0.953 0.895\n2     8 -3.72 0.858 -3.72 -2.87  0.0236 0.0538 0.930 0.847\n3     9 -2.28 0.858 -2.28 -1.42  0.0926 0.194  0.844 0.683\n4    10 -1.82 0.858 -1.82 -0.964 0.139  0.276  0.727 0.494\n5    11 -1.66 0.858 -1.66 -0.801 0.160  0.310  0.610 0.341\n6    12 -1.17 0.858 -1.17 -0.317 0.236  0.421  0.466 0.197\n\n\nFor the alpha and beta columns, we just subset the values from fixef(). The two logit-hazard columns, lh0 and lh1, were simple algebraic transformations of alpha and beta, respectively. To make the two hazard columns, h0 and h1, we applied the plogis() function to lh0 and lh1, respectively. To make the two survival columns, s0 and s1, we applied the cumprod() function to one minus the two hazard columns. Note how all this is based off of the posterior means. There’s enough going on with Table 11.5 that it makes sense to ignore uncertainty But when we’re ready to go beyond table glancing and actually make a plot, we will go beyond posterior means and reintroduce the uncertainty in the model.\nTwo of these plots are quite similar to two of the subplots from Figure 11.4, back in Section 11.2.1. But recall that though those plots were based on fit11.6, which was based on the aggregated data, the plots we are about to make will be based on fit11.8, the analogous model based on the disaggregated person-period data. Regardless of whether the logistic regression model is based on aggregated data, the post-processing approach will involve the fitted() function. However, the specifics of how we use fitted() will differ. For the disaggregated data used to fit fit11.8, here is how we might define the newdata, pump it through the model via fitted(), and wrangle.\n\nnd &lt;- crossing(pt     = 0:1,\n               period = 7:12) %&gt;% \n  mutate(d7  = if_else(period == 7, 1, 0),\n         d8  = if_else(period == 8, 1, 0),\n         d9  = if_else(period == 9, 1, 0),\n         d10 = if_else(period == 10, 1, 0),\n         d11 = if_else(period == 11, 1, 0),\n         d12 = if_else(period == 12, 1, 0))\n\nf &lt;- fitted(fit11.8,\n            newdata = nd,\n            scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(pt = str_c(\"pt = \", pt))\n\nf\n\n     Estimate Est.Error       Q2.5      Q97.5     pt period d7 d8 d9 d10 d11 d12\n1  -3.0045085 0.3201316 -3.6661894 -2.4085317 pt = 0      7  1  0  0   0   0   0\n2  -3.7244765 0.4282015 -4.5958307 -2.9358676 pt = 0      8  0  1  0   0   0   0\n3  -2.2817925 0.2727385 -2.8325954 -1.7567166 pt = 0      9  0  0  1   0   0   0\n4  -1.8217178 0.2570652 -2.3291953 -1.3223914 pt = 0     10  0  0  0   1   0   0\n5  -1.6593222 0.2666743 -2.1899975 -1.1501632 pt = 0     11  0  0  0   0   1   0\n6  -1.1747822 0.2724716 -1.7132102 -0.6392948 pt = 0     12  0  0  0   0   0   1\n7  -2.1465716 0.2799461 -2.7126277 -1.6286540 pt = 1      7  1  0  0   0   0   0\n8  -2.8665396 0.3968174 -3.6929531 -2.1443914 pt = 1      8  0  1  0   0   0   0\n9  -1.4238557 0.2384241 -1.9046141 -0.9652484 pt = 1      9  0  0  1   0   0   0\n10 -0.9637809 0.2227195 -1.4271826 -0.5471339 pt = 1     10  0  0  0   1   0   0\n11 -0.8013854 0.2477223 -1.2949851 -0.3102280 pt = 1     11  0  0  0   0   1   0\n12 -0.3168453 0.2650743 -0.8483419  0.1979386 pt = 1     12  0  0  0   0   0   1\n\n\nHere we make and save the upper two panels of Figure 11.6.\n\n# logit(hazard)\np1 &lt;- f %&gt;% \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = Estimate)) +\n  labs(subtitle = \"fitted logit(hazard)\",\n       y = NULL) +\n  coord_cartesian(ylim = c(-4, 0)) +\n  theme(legend.background = element_rect(fill = \"transparent\"),\n        legend.key = element_rect(color = \"grey92\"),\n        legend.position = c(0.1, 0.825))\n\n# hazard\np2 &lt;- f %&gt;% \n  mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = plogis) %&gt;% \n  \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = Estimate)) +\n  labs(subtitle = \"fitted hazard\",\n       y = NULL) +\n  coord_cartesian(ylim = c(0, 0.5)) +\n  theme(legend.position = \"none\")\n\nBefore we’re ready to make the last panel, we’ll redo our fitted() work, this time including predicted values for grade 6.\n\nnd &lt;- crossing(pt     = 0:1,\n               period = 6:12) %&gt;% \n  mutate(d6  = if_else(period == 6, 1, 0),\n         d7  = if_else(period == 7, 1, 0),\n         d8  = if_else(period == 8, 1, 0),\n         d9  = if_else(period == 9, 1, 0),\n         d10 = if_else(period == 10, 1, 0),\n         d11 = if_else(period == 11, 1, 0),\n         d12 = if_else(period == 12, 1, 0))\n\nf &lt;- fitted(fit11.8, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(pt = str_c(\"pt = \", pt))\n\nf\n\n     Estimate  Est.Error        Q2.5      Q97.5     pt period d6 d7 d8 d9 d10 d11 d12\n1  0.50000000 0.00000000 0.500000000 0.50000000 pt = 0      6  1  0  0  0   0   0   0\n2  0.04929972 0.01480780 0.024936027 0.08252443 pt = 0      7  0  1  0  0   0   0   0\n3  0.02554916 0.01034991 0.009992967 0.05040871 pt = 0      8  0  0  1  0   0   0   0\n4  0.09517391 0.02337014 0.055587998 0.14720204 pt = 0      9  0  0  0  1   0   0   0\n5  0.14205156 0.03097250 0.088733711 0.21042071 pt = 0     10  0  0  0  0   1   0   0\n6  0.16306147 0.03612953 0.100652317 0.24045928 pt = 0     11  0  0  0  0   0   1   0\n7  0.23945289 0.04902283 0.152747807 0.34540597 pt = 0     12  0  0  0  0   0   0   1\n8  0.70032482 0.04456270 0.608668421 0.78081336 pt = 1      6  1  0  0  0   0   0   0\n9  0.10751164 0.02628804 0.062232325 0.16401483 pt = 1      7  0  1  0  0   0   0   0\n10 0.05734801 0.02073893 0.024293529 0.10485649 pt = 1      8  0  0  1  0   0   0   0\n11 0.19674112 0.03728895 0.129587139 0.27582862 pt = 1      9  0  0  0  1   0   0   0\n12 0.27831049 0.04415757 0.193538052 0.36652961 pt = 1     10  0  0  0  0   1   0   0\n13 0.31217660 0.05245849 0.215010245 0.42305909 pt = 1     11  0  0  0  0   0   1   0\n14 0.42275310 0.06353559 0.299780810 0.54932372 pt = 1     12  0  0  0  0   0   0   1\n\n\nThe values for grade 6 (i.e., those for when d6 == 1) are nonsensical. The main reason we included d6 in the fitted results and in the nd data is so we’d have the slots in our f object. In the code block below, we’ll fill those slots with the appropriate values (0) and then convert the hazard summaries to the survival (i.e., cumulative probability) metric.\n\nf &lt;- f %&gt;% \n  mutate(Estimate = if_else(period == 6, 0, Estimate),\n         Q2.5     = if_else(period == 6, 0, Q2.5),\n         Q97.5    = if_else(period == 6, 0, Q97.5)) %&gt;% \n  group_by(pt) %&gt;% \n  mutate(s       = cumprod(1 - Estimate),\n         s_lower = cumprod(1 - Q2.5),\n         s_upper = cumprod(1 - Q97.5)) %&gt;% \n  select(pt:d12, s:s_upper)\n\nf %&gt;% glimpse()\n\nRows: 14\nColumns: 12\nGroups: pt [2]\n$ pt      &lt;chr&gt; \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 1\", \"pt = 1\", \"pt = 1\", \"pt = 1\", \"p…\n$ period  &lt;int&gt; 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 10, 11, 12\n$ d6      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0\n$ d7      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0\n$ d8      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0\n$ d9      &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0\n$ d10     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0\n$ d11     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0\n$ d12     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1\n$ s       &lt;dbl&gt; 1.0000000, 0.9507003, 0.9264107, 0.8382406, 0.7191672, 0.6018987, 0.4577723, 1.0000000, 0.8924884, 0.8413059, 0.…\n$ s_lower &lt;dbl&gt; 1.0000000, 0.9750640, 0.9653202, 0.9116600, 0.8307650, 0.7471466, 0.6330216, 1.0000000, 0.9377677, 0.9149860, 0.…\n$ s_upper &lt;dbl&gt; 1.0000000, 0.9174756, 0.8712268, 0.7429804, 0.5866420, 0.4455785, 0.2916730, 1.0000000, 0.8359852, 0.7483267, 0.…\n\n\nMake and save the final panel.\n\n# save the interpolated median lifetime values\nimls &lt;- c(make_iml(lt %&gt;% filter(pt == \"pt = 0\")), make_iml(lt %&gt;% filter(pt == \"pt = 1\")))\n\n# hazard\np3 &lt;- f %&gt;% \n  ggplot(aes(x = period, group = pt,\n             fill = pt, color = pt)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  geom_segment(x = imls[1], xend = imls[1],\n               y = -Inf, yend = 0.5,\n               color = \"white\", linetype = 2) +\n  geom_segment(x = imls[2], xend = imls[2],\n               y = -Inf, yend = 0.5,\n               color = \"white\", linetype = 2) +\n  geom_ribbon(aes(ymin = s_lower, ymax = s_upper),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = s)) + \n  scale_y_continuous(NULL, breaks = c(0, 0.5, 1)) +\n  labs(subtitle = \"fitted survival probability\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme(legend.position = \"none\")\n\nCombine the subplots to finish off our version of Figure 11.6.\n\n(p1 / p2 / p3) &\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_x_continuous(\"Grade\", breaks = 6:12, limits = c(6, 12)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere is the breakdown of what percentage of boys will still be virgins at grades 9 and 12, based on pt status, as indicated by fit11.8.\n\nf %&gt;% \n  filter(period %in% c(9, 12)) %&gt;% \n  mutate_if(is.double, ~ (. * 100) %&gt;% round(digits = 0)) %&gt;% \n  mutate(`percent virgins` = str_c(s, \" [\", s_lower, \", \", s_upper, \"]\")) %&gt;% \n  select(period, pt, `percent virgins`) %&gt;% \n  arrange(period)\n\n# A tibble: 4 × 3\n# Groups:   pt [2]\n  period pt     `percent virgins`\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;            \n1      9 pt = 0 84 [91, 74]      \n2      9 pt = 1 68 [80, 54]      \n3     12 pt = 0 46 [63, 29]      \n4     12 pt = 1 19 [35, 9]       \n\n\n\n\n11.5.2 Extending this strategy to multiple predictors (some of which are continuous)\n\nIt is easy to display fitted hazard and survivor functions for model involving multiple predictor by extending these ideas in a straightforward manner. Instead of plotting one fitted function for each predictor value, select several prototypical predictor values (using strategies presented in section 4.5.3 and plot fitted functions for combinations of these values. (p. 394, emphasis in the original)\n\nWe’ll be focusing on fit11.10, which includes both pt and sas as substantive predictors. pt only takes two values, 0 and 1. For pas, we’ll use the conventional -1, 0, and 1. Here’s the fitted()-related code.\n\nnd &lt;- crossing(pt  = 0:1,\n               pas = -1:1) %&gt;% \n  expand(nesting(pt, pas),\n         period = 6:12) %&gt;% \n  mutate(d6  = if_else(period == 6, 1, 0),\n         d7  = if_else(period == 7, 1, 0),\n         d8  = if_else(period == 8, 1, 0),\n         d9  = if_else(period == 9, 1, 0),\n         d10 = if_else(period == 10, 1, 0),\n         d11 = if_else(period == 11, 1, 0),\n         d12 = if_else(period == 12, 1, 0))\n\nf &lt;- fitted(fit11.10, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nhead(f)\n\n    Estimate   Est.Error        Q2.5      Q97.5 pt pas period d6 d7 d8 d9 d10 d11 d12\n1 0.42551332 0.030772069 0.366453944 0.48718611  0  -1      6  1  0  0  0   0   0   0\n2 0.04040092 0.012828155 0.020460045 0.07007207  0  -1      7  0  1  0  0   0   0   0\n3 0.02123362 0.008811553 0.008034598 0.04190023  0  -1      8  0  0  1  0   0   0   0\n4 0.08137512 0.021006064 0.045742638 0.12755480  0  -1      9  0  0  0  1   0   0   0\n5 0.12186778 0.028481788 0.073149687 0.18529576  0  -1     10  0  0  0  0   1   0   0\n6 0.14237510 0.034274264 0.083828430 0.21582173  0  -1     11  0  0  0  0   0   1   0\n\n\nMake the two subplots.\n\n# logit(hazard)\np1 &lt;- f %&gt;% \n  mutate(pt  = str_c(\"pt = \", pt),\n         pas = str_c(\"pas = \", pas)) %&gt;% \n  mutate(pas = factor(pas,\n                      levels = str_c(\"pas = \", 1:-1))) %&gt;% \n  filter(period &gt; 6) %&gt;% \n  \n  ggplot(aes(x = period, group = pas,\n             fill = pas, color = pas)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = Estimate)) +\n  labs(subtitle = \"fitted logit(hazard)\",\n       y = NULL) +\n  coord_cartesian(ylim = c(0, 0.5)) +\n  facet_wrap(~ pt)\n\n# hazard\np2 &lt;- f %&gt;% \n  mutate(Estimate = if_else(period == 6, 0, Estimate),\n         Q2.5     = if_else(period == 6, 0, Q2.5),\n         Q97.5    = if_else(period == 6, 0, Q97.5)) %&gt;% \n  mutate(pt  = str_c(\"pt = \", pt),\n         pas = str_c(\"pas = \", pas)) %&gt;% \n  mutate(pas = factor(pas,\n                      levels = str_c(\"pas = \", 1:-1))) %&gt;% \n  group_by(pt, pas) %&gt;% \n  mutate(s       = cumprod(1 - Estimate),\n         s_lower = cumprod(1 - Q2.5),\n         s_upper = cumprod(1 - Q97.5)) %&gt;% \n  \n  ggplot(aes(x = period, group = pas,\n             fill = pas, color = pas)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  geom_ribbon(aes(ymin = s_lower, ymax = s_upper),\n              linewidth = 0, alpha = 1/6) +\n  geom_line(aes(y = s)) +\n  scale_y_continuous(NULL, breaks = c(0, 0.5, 1)) +\n  labs(subtitle = \"fitted survival probability\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ pt)\n\nCombine the subplots to make our version of Figure 11.7.\n\n((p1 / p2) &\n  scale_fill_viridis_d(NULL, option = \"D\", end = 0.8, direction = -1) &\n  scale_color_viridis_d(NULL, option = \"D\", end = 0.8, direction = -1) &\n  scale_x_continuous(\"Grade\", breaks = 6:12, limits = c(6, 12)) &\n  theme(panel.grid = element_blank())) +\n   plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nHere we departed from the text a bit by separating the subplots by pt status. They’re already cluttered enough as is.\n\n\n11.5.3 Two cautions when interpreting fitted hazard and survivor functions\nBeware of inferring statistical interaction of a substantive predictor and time when examining plots if fitted hazard and survivor functions. The root of this difficulty is in our use of a link function.\n\nBecause the model expresses the linear effect of the predictor on logit hazard, you cannot draw a conclusion about the stability of an effect using graphs plotted on a raw hazard scale. In fact, the logic works in the opposite direction. If the size of the gap between fitted hazard functions is constant over time, [the] effect of the predictor must vary over time! (pp. 396-397, emphasis in the original)\n\nAlso, please don’t confuse plots of fitted values with descriptive sample-based plots. Hopefully our inclusion of 95% intervals helps prevent this.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#comparing-models-using-deviance-statistics-and-information-criteria",
    "href": "11.html#comparing-models-using-deviance-statistics-and-information-criteria",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "11.6 Comparing models using deviance statistics and information criteria",
    "text": "11.6 Comparing models using deviance statistics and information criteria\n\nWe now introduce two important questions that we usually address before interpreting parameters and displaying results: Which of the alternative models fits better: Might a predictor’s observed effect be the result of nothing more than sampling variation? (p. 397)\n\nMuch of the material in this section will be a refresher from the material we covered in Section 4.6.\n\n11.6.1 The deviance statistic\nThe log-likelihood, LL, is\n\na summary statistic routinely output (in some form) by any program that provides ML estimates. As discussed in section 4.6, its relative magnitude across a series of models fit to the same set of data can be informative (although its absolute magnitude is not). The larger the LL statistic, the better the fit. (pp. 397–398)\n\nNote that in some form part. Frequentist software typically returns the LL for a given model as a single value. As we learned way back in Section 4.6, we can use the log_lik() function to get the LL information from our brms fits. However, form the brms reference manual we discover log_lik() returns an “S x N matrix containing the pointwise log-likelihood samples, where S is the number of samples and N is the number of observations in the data” (p. 112). Using fit11.7 as a test case, here’s what that looks like.\n\nlog_lik(fit11.7) %&gt;% \n  str()\n\n num [1:4000, 1:822] -0.1086 -0.0709 -0.1042 -0.0756 -0.0781 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nTo compute the LL for each HMC iteration, you sum across the rows. Deviance is that sum multiplied by -2. Here’s that in a tibble.\n\nll &lt;- fit11.7 %&gt;%\n  log_lik() %&gt;%\n  as_tibble(.name_repair = ~ str_c(\"c\", 1:822)) %&gt;%\n  mutate(ll = rowSums(.)) %&gt;% \n  mutate(deviance = -2 * ll) %&gt;% \n  select(ll, deviance, everything())\n\nll\n\n# A tibble: 4,000 × 824\n      ll deviance      c1      c2    c3      c4      c5     c6     c7     c8     c9     c10     c11    c12    c13    c14    c15\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -329.     658. -0.109  -0.0455 -1.98 -0.109  -0.0455 -0.149 -0.253 -0.219 -0.255 -0.109  -0.0455 -0.149 -0.253 -0.219 -0.255\n 2 -330.     661. -0.0709 -0.0295 -2.11 -0.0709 -0.0295 -0.130 -0.284 -0.182 -0.297 -0.0709 -0.0295 -0.130 -0.284 -0.182 -0.297\n 3 -331.     662. -0.104  -0.0605 -1.68 -0.104  -0.0605 -0.206 -0.176 -0.395 -0.455 -0.104  -0.0605 -0.206 -0.176 -0.395 -0.455\n 4 -329.     658. -0.0756 -0.0328 -1.97 -0.0756 -0.0328 -0.151 -0.333 -0.199 -0.366 -0.0756 -0.0328 -0.151 -0.333 -0.199 -0.366\n 5 -327.     655. -0.0781 -0.0504 -1.95 -0.0781 -0.0504 -0.154 -0.307 -0.249 -0.346 -0.0781 -0.0504 -0.154 -0.307 -0.249 -0.346\n 6 -328.     657. -0.0611 -0.0425 -1.89 -0.0611 -0.0425 -0.163 -0.299 -0.229 -0.310 -0.0611 -0.0425 -0.163 -0.299 -0.229 -0.310\n 7 -331.     662. -0.0841 -0.0342 -1.92 -0.0841 -0.0342 -0.159 -0.385 -0.235 -0.298 -0.0841 -0.0342 -0.159 -0.385 -0.235 -0.298\n 8 -329.     657. -0.114  -0.0306 -1.69 -0.114  -0.0306 -0.204 -0.209 -0.244 -0.313 -0.114  -0.0306 -0.204 -0.209 -0.244 -0.313\n 9 -328.     655. -0.0679 -0.0407 -2.05 -0.0679 -0.0407 -0.138 -0.259 -0.336 -0.434 -0.0679 -0.0407 -0.138 -0.259 -0.336 -0.434\n10 -329.     658. -0.0616 -0.0749 -2.10 -0.0616 -0.0749 -0.131 -0.284 -0.279 -0.396 -0.0616 -0.0749 -0.131 -0.284 -0.279 -0.396\n# ℹ 3,990 more rows\n# ℹ 807 more variables: c16 &lt;dbl&gt;, c17 &lt;dbl&gt;, c18 &lt;dbl&gt;, c19 &lt;dbl&gt;, c20 &lt;dbl&gt;, c21 &lt;dbl&gt;, c22 &lt;dbl&gt;, c23 &lt;dbl&gt;, c24 &lt;dbl&gt;,\n#   c25 &lt;dbl&gt;, c26 &lt;dbl&gt;, c27 &lt;dbl&gt;, c28 &lt;dbl&gt;, c29 &lt;dbl&gt;, c30 &lt;dbl&gt;, c31 &lt;dbl&gt;, c32 &lt;dbl&gt;, c33 &lt;dbl&gt;, c34 &lt;dbl&gt;, c35 &lt;dbl&gt;,\n#   c36 &lt;dbl&gt;, c37 &lt;dbl&gt;, c38 &lt;dbl&gt;, c39 &lt;dbl&gt;, c40 &lt;dbl&gt;, c41 &lt;dbl&gt;, c42 &lt;dbl&gt;, c43 &lt;dbl&gt;, c44 &lt;dbl&gt;, c45 &lt;dbl&gt;, c46 &lt;dbl&gt;,\n#   c47 &lt;dbl&gt;, c48 &lt;dbl&gt;, c49 &lt;dbl&gt;, c50 &lt;dbl&gt;, c51 &lt;dbl&gt;, c52 &lt;dbl&gt;, c53 &lt;dbl&gt;, c54 &lt;dbl&gt;, c55 &lt;dbl&gt;, c56 &lt;dbl&gt;, c57 &lt;dbl&gt;,\n#   c58 &lt;dbl&gt;, c59 &lt;dbl&gt;, c60 &lt;dbl&gt;, c61 &lt;dbl&gt;, c62 &lt;dbl&gt;, c63 &lt;dbl&gt;, c64 &lt;dbl&gt;, c65 &lt;dbl&gt;, c66 &lt;dbl&gt;, c67 &lt;dbl&gt;, c68 &lt;dbl&gt;,\n#   c69 &lt;dbl&gt;, c70 &lt;dbl&gt;, c71 &lt;dbl&gt;, c72 &lt;dbl&gt;, c73 &lt;dbl&gt;, c74 &lt;dbl&gt;, c75 &lt;dbl&gt;, c76 &lt;dbl&gt;, c77 &lt;dbl&gt;, c78 &lt;dbl&gt;, c79 &lt;dbl&gt;, …\n\n\nSince we have distributions for the LL and deviance, we may as well visualize them in a plot.\n\nll %&gt;%\n  pivot_longer(ll:deviance) %&gt;% \n  mutate(name = factor(name, levels = c(\"ll\", \"deviance\"))) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nHere’s how to compute the LL and deviance distributions for each of our four models, fit11.7 through fit11.10, in bulk.\n\nll &lt;- tibble(model = str_c(\"model \", letters[1:4]),\n             name  = str_c(\"fit11.\", 7:10)) %&gt;% \n  mutate(fit = map(name, get)) %&gt;% \n  mutate(ll = map(fit, ~log_lik(.) %&gt;% data.frame() %&gt;% transmute(ll = rowSums(.)))) %&gt;% \n  select(-fit) %&gt;% \n  unnest(ll) %&gt;% \n  mutate(deviance = -2 * ll)\n\nll %&gt;% \n  glimpse()\n\nRows: 16,000\nColumns: 4\n$ model    &lt;chr&gt; \"model a\", \"model a\", \"model a\", \"model a\", \"model a\", \"model a\", \"model a\", \"model a\", \"model a\", \"model a\", \"…\n$ name     &lt;chr&gt; \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"fit11.7\", \"…\n$ ll       &lt;dbl&gt; -329.1751, -330.4153, -330.9811, -329.1648, -327.3291, -328.4891, -330.8290, -328.6211, -327.5347, -328.9812, -…\n$ deviance &lt;dbl&gt; 658.3502, 660.8305, 661.9622, 658.3296, 654.6582, 656.9782, 661.6579, 657.2422, 655.0694, 657.9625, 658.3806, 6…\n\n\nNow plot the LL and deviance distributions for each.\n\nll %&gt;%\n  pivot_longer(ll:deviance,\n               names_to = \"statistic\") %&gt;% \n  mutate(statistic = factor(statistic, levels = c(\"ll\", \"deviance\"))) %&gt;% \n  \n  ggplot(aes(x = value, y = model)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95, normalize = \"panels\") +\n  labs(x = NULL,\n       y = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ statistic, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\n\n11.6.2 Deviance-based hypothesis tests for individual predictors\nSinger and Willett wrote: “Comparing deviance statistics for pairs of nested models that differ only by a single substantive predictor permits evaluation of the ‘statistical significance’ of that predictor” (p. 399). I’m just not going to appeal to null-hypothesis significance testing in this project and, as an extension, I am not going to appeal to tests using the \\(\\chi^2\\) distribution. But sure, you could take our deviance distributions and compare them with difference distributions. Singer and Willett made four deviance comparisons in this section. Here’s what that might look like using our deviance distributions.\n\nll %&gt;% \n  select(model, deviance) %&gt;% \n  mutate(iter = rep(1:4000, times = 4)) %&gt;% \n  pivot_wider(names_from = model,\n              values_from = deviance) %&gt;% \n  mutate(`a - b` = `model a` - `model b`,\n         `a - c` = `model a` - `model c`,\n         `c - d` = `model c` - `model d`,\n         `b - d` = `model b` - `model d`) %&gt;% \n  pivot_longer(contains(\"-\")) %&gt;% \n  \n  ggplot(aes(x = value, y = name)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95, normalize = \"panels\") +\n  labs(x = \"deviance difference distribution\",\n       y = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBut really, like no one does this with Bayesian models. If you think you have a good theoretical reason to use this approach, do not cite this project as a justification. I do not endorse it.\n\n\n11.6.3 Deviance-based hypothesis tests for groups of predictors\nWe won’t be doing this.\n\n\n11.6.4 Comparing nonnested models using [WAIC and LOO]\nNow we return to our preferred methods for model comparison. Use the add_criterion() function to compute the WAIC and LOO and add their output to the model fits.\n\nfit11.7  &lt;- add_criterion(fit11.7, criterion = c(\"loo\", \"waic\"))\nfit11.8  &lt;- add_criterion(fit11.8, criterion = c(\"loo\", \"waic\"))\nfit11.9  &lt;- add_criterion(fit11.9, criterion = c(\"loo\", \"waic\"))\nfit11.10 &lt;- add_criterion(fit11.10, criterion = c(\"loo\", \"waic\"))\n\nFirst compare Models B and C (i.e., fit11.8 and fit11.9, respectively).\n\nloo_compare(fit11.8, fit11.9, criterion = \"loo\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit11.8    0.0       0.0  -324.5     17.6         7.1    0.6    649.0   35.2  \nfit11.9   -1.3       4.6  -325.7     17.6         7.2    0.6    651.5   35.1  \n\nloo_compare(fit11.8, fit11.9, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit11.8    0.0       0.0  -324.5      17.6          7.1    0.6     648.9   35.2 \nfit11.9   -1.3       4.6  -325.7      17.6          7.1    0.6     651.5   35.1 \n\n\nIn a head-to-head comparison, Model B is a little better than Model C. However, the standard error for their difference score is about three times as large as the difference itself. This is not a difference I would write home about. Now compare Models A through D.\n\nloo_compare(fit11.7, fit11.8, fit11.9, fit11.10, criterion = \"loo\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit11.10    0.0       0.0  -322.7     17.6         8.2    0.6    645.5   35.2  \nfit11.8    -1.7       2.5  -324.5     17.6         7.1    0.6    649.0   35.2  \nfit11.9    -3.0       2.8  -325.7     17.6         7.2    0.6    651.5   35.1  \nfit11.7    -9.4       4.8  -332.2     17.6         6.2    0.5    664.4   35.3  \n\nloo_compare(fit11.7, fit11.8, fit11.9, fit11.10, criterion = \"waic\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit11.10    0.0       0.0  -322.7      17.6          8.2    0.6     645.5   35.2 \nfit11.8    -1.7       2.5  -324.5      17.6          7.1    0.6     648.9   35.2 \nfit11.9    -3.0       2.8  -325.7      17.6          7.1    0.6     651.5   35.1 \nfit11.7    -9.5       4.8  -332.2      17.6          6.2    0.5     664.4   35.3 \n\n\nModel D (i.e., fit11.10, the full model) has the best (i.e., lowest) WAIC and LOO estimates. However, the standard errors for its difference scores with the other models is on the large side, particularly for Models B and C. So sure, adding either pt or sas to the model helps a bit and adding them both helps a little more, but neither predictor is a huge winner when you take that model complexity penalty into account.\nAs discussed earlier, we can also compare the models using weights. Here we’ll use the WAIC, LOO, and stacking weights to compare all four models.\n\nmodel_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = \"loo\") %&gt;% round(digits = 3)\n\n fit11.7  fit11.8  fit11.9 fit11.10 \n   0.000    0.143    0.041    0.816 \n\nmodel_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = \"waic\") %&gt;% round(digits = 3)\n\n fit11.7  fit11.8  fit11.9 fit11.10 \n   0.000    0.143    0.041    0.816 \n\nmodel_weights(fit11.7, fit11.8, fit11.9, fit11.10, weights = \"stacking\") %&gt;% round(digits = 3)\n\n fit11.7  fit11.8  fit11.9 fit11.10 \n   0.000    0.352    0.259    0.389 \n\n\nModel D has the best showing across the three weighting schemes.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#statistical-inference-using-uncertainty-in-the-bayesian-posterior",
    "href": "11.html#statistical-inference-using-uncertainty-in-the-bayesian-posterior",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "11.7 Statistical inference using [uncertainty in the Bayesian posterior]",
    "text": "11.7 Statistical inference using [uncertainty in the Bayesian posterior]\nI generally take a model-based approach to Bayesian statistics and I prefer to scrutinize marginal posteriors, consider effect sizes, and use graphical depictions of my models (e.g., posterior predictive checks) over hypothesis testing. Further extending that approach, here, puts us at further odds with the content in the test. In addition, the authors spent some time discussing the asymptotic properties of ML standard errors. Our Bayesian approach is not based on asymptotic theory and we just don’t need to concern ourselves with whether our marginal posteriors are Gaussian. They often are, which is nice. But we summarize our posteriors with percentile-based 95% intervals, we are not presuming they are symmetric or Gaussian.\n\n11.7.1 The Wald chi-square statistic\nThis will not be our approach. On page 404, Singer and Willett wrote: “The logistic regression analysis routines in all major statistical packages routinely output asymptotic standard errors.” This comment presumes we’re focusing on frequentist packages. Our rough analogue to frequentist standard errors is our Bayesian posterior standard deviations. The authors focused on the two substantive predictors from Model D (i.e., fit11.10). Here’s another look at the brms summary.\n\nprint(fit11.10)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas \n   Data: sex_pp (Number of observations: 822) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd7     -2.92      0.32    -3.57    -2.32 1.00     3002     2698\nd8     -3.62      0.42    -4.49    -2.85 1.00     3622     2913\nd9     -2.16      0.28    -2.73    -1.64 1.00     2697     3005\nd10    -1.70      0.26    -2.21    -1.20 1.00     2686     2709\nd11    -1.52      0.28    -2.09    -1.01 1.00     2838     2869\nd12    -1.01      0.28    -1.58    -0.48 1.00     2461     2688\npt      0.65      0.23     0.19     1.12 1.00     1579     2498\npas     0.30      0.13     0.05     0.55 1.00     3899     2889\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRecall that the second column for our ‘Population-Level Effects’, ‘Est.Error’, contains the standard deviation for each dimension of the posteriors listed (i.e., for each parameter ranging from d7 to pas). This is similar, but distinct from, the frequentist standard error. Instead of focusing on \\(p\\)-values connected to standard errors, why not look at the marginal posteriors directly?\n\ndraws &lt;- as_draws_df(fit11.10)\n\ndraws %&gt;% \n  pivot_longer(b_pt:b_pas) %&gt;% \n  \n  ggplot(aes(x = value, y = name, fill = stat(x &gt; 0))) +\n  stat_slab() +\n  scale_fill_manual(values = c(\"blue3\", \"red3\")) +\n  labs(x = \"marginal posterior\",\n       y = NULL) +\n  coord_cartesian(ylim = c(1.5, 2)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf we’d like to keep with the NHST perspective, zero is not a particularly credible value for either parameter. But neither are negative values in generals. In terms of uncertainty, look how much wider the posterior for pt is when compared with pas. And don’t forget that these are on the log-odds scale.\nLooking at those densities might lead one to ask, Exactly what proportion of the posterior draws for each is zero or below? You can compute that like this.\n\ndraws %&gt;% \n  pivot_longer(b_pt:b_pas) %&gt;% \n  group_by(name) %&gt;% \n  summarise(`percent zero or below` = 100 * mean(value &lt;= 0))\n\n# A tibble: 2 × 2\n  name  `percent zero or below`\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 b_pas                    0.85\n2 b_pt                     0.3 \n\n\nLess that 1% of the draws were zero or below for each.\n\n\n11.7.2 [Asymmetric credible intervals] for parameters and odds ratios\nWhether we use percentile-based credible intervals, as we typically do, or use highest posterior density intervals, neither depends on asymptotic theory nor do they depend on the posterior standard deviation. That is, our Bayesian intervals do not presume the marginal posteriors are Gaussian. Let’s look back at the summary output for fit11.10, this time using the fixef() function.\n\nfixef(fit11.10)\n\n      Estimate Est.Error       Q2.5      Q97.5\nd7  -2.9158998 0.3175476 -3.5694292 -2.3224360\nd8  -3.6152021 0.4244056 -4.4859984 -2.8510084\nd9  -2.1558972 0.2775011 -2.7335041 -1.6361895\nd10 -1.7001798 0.2592930 -2.2134526 -1.1950339\nd11 -1.5227911 0.2753575 -2.0883454 -1.0105407\nd12 -1.0114062 0.2824609 -1.5767367 -0.4808470\npt   0.6463053 0.2340488  0.1931138  1.1227183\npas  0.3013619 0.1263270  0.0512668  0.5474599\n\n\nWe find the lower- and upper-limits for our percentile-based Bayesian credible intervals in the last two columns. If you’d like HDIs instead, use the convenience functions from tidybayes.\n\ndraws %&gt;% \n  pivot_longer(b_pt:b_pas) %&gt;% \n  group_by(name) %&gt;% \n  mean_hdi(value)\n\n# A tibble: 2 × 7\n  name  value .lower .upper .width .point .interval\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_pas 0.301 0.0414  0.534   0.95 mean   hdi      \n2 b_pt  0.646 0.147   1.07    0.95 mean   hdi      \n\n\nWe can exponentiate our measures of central tendency (e.g., posterior means) and posterior intervals to transform them out of the log-odds metric an into the odds-ratio metric. Here are the results working directly with fixef().\n\nfixef(fit11.10)[c(\"pt\", \"pas\"), -2] %&gt;% exp()\n\n    Estimate     Q2.5    Q97.5\npt  1.908477 1.213021 3.073197\npas 1.351698 1.052604 1.728856\n\n\nKeep in mind that fixating on just the 95% intervals is a little NHST-centric. Since we have entire posterior distributions to summarize, we might consider other intervals. Here we use another graphical approach by using tidybayes_statintervalh() to mark off the 10, 30, 50, 70, and 90% intervals for both substantive predictors. Both are in the odds-ratio metric.\n\ndraws %&gt;% \n  pivot_longer(b_pt:b_pas) %&gt;% \n  mutate(`odds ratio` = exp(value)) %&gt;% \n  \n  ggplot(aes(x = `odds ratio`, y = name)) +\n  stat_interval(size = 5, .width = seq(from = 0.1, to = 0.9, by = 0.2)) +\n  scale_color_grey(\"CI level:\", start = 0.8, end = 0.2) +\n  scale_x_continuous(breaks = 1:3) +\n  ylab(NULL) +\n  coord_cartesian(xlim = c(1, 3)) +\n  theme(legend.position = \"top\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe frequentist 95% confidence intervals are asymmetric when expressed in the odds-ratio metric and so are our various Bayesian intervals. However, the asymmetry in our Bayesian intervals is less noteworthy because there was no explicit assumption of symmetry when they were in the log-odds metric.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#session-info",
    "href": "11.html#session-info",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 brms_2.23.0     Rcpp_1.1.0      patchwork_1.3.2 survival_3.8-3  lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          loo_2.9.0.9000          systemfonts_1.3.1      \n[11] vctrs_0.6.5             reshape2_1.4.5          arrayhelpers_1.1-0      pkgconfig_2.0.3         crayon_1.5.3           \n[16] fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          utf8_1.2.6              rmarkdown_2.30         \n[21] tzdb_0.5.0              ragg_1.5.0              bit_4.6.0               xfun_0.55               jsonlite_2.0.0         \n[26] uuid_1.2-1              parallel_4.5.1          R6_2.6.1                stringi_1.8.7           RColorBrewer_1.1-3     \n[31] StanHeaders_2.36.0.9000 estimability_1.5.1      rstan_2.36.0.9000       knitr_1.51              zoo_1.8-14             \n[36] bayesplot_1.15.0.9000   Matrix_1.7-3            splines_4.5.1           timechange_0.3.0        tidyselect_1.2.1       \n[41] rstudioapi_0.17.1       abind_1.4-8             codetools_0.2-20        curl_7.0.0              pkgbuild_1.4.8         \n[46] lattice_0.22-7          plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1    S7_0.2.1               \n[51] flextable_0.9.10        askpass_1.2.1           posterior_1.6.1.9000    coda_0.19-4.1           evaluate_1.0.5         \n[56] RcppParallel_5.1.11-1   ggdist_3.3.3            zip_2.3.3               xml2_1.4.0              pillar_1.11.1          \n[61] tensorA_0.36.2.1        checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0    generics_0.1.4         \n[66] vroom_1.6.6             hms_1.1.4               rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4           \n[71] glue_1.8.0              gdtools_0.4.4           emmeans_1.11.2-8        tools_4.5.1             data.table_1.17.8      \n[76] mvtnorm_1.3-3           grid_4.5.1              QuickJSR_1.8.1          nlme_3.1-168            cli_3.6.5              \n[81] textshaping_1.0.4       officer_0.7.2           svUnit_1.0.8            fontBitstreamVera_0.1.1 viridisLite_0.4.2      \n[86] Brobdingnag_1.2-9       V8_8.0.1                gtable_0.3.6            digest_0.6.39           fontquiver_0.2.1       \n[91] TH.data_1.1-4           htmlwidgets_1.6.4       farver_2.1.2            htmltools_0.5.9         lifecycle_1.0.5        \n[96] fontLiberation_0.1.0    openssl_2.3.4           bit64_4.6.0-1           MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "11.html#comments",
    "href": "11.html#comments",
    "title": "11  Fitting Basic Discrete-Time Hazard Models",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBrilleman, S. (2019). Estimating survival (time-to-event) models with rstanarm. https://github.com/stan-dev/rstanarm/blob/feature/frailty-models/vignettes/surv.Rmd\n\n\nBrilleman, S. L., Elci, E. M., Novik, J. B., & Wolfe, R. (2020). Bayesian survival analysis using the rstanarm R package. https://arxiv.org/abs/2002.09633\n\n\nCapaldi, D. M., Crosby, L., & Stoolmiller, M. (1996). Predicting the timing of first sexual intercourse for at-risk adolescent males. Child Development, 67(2), 344–359. https://doi.org/10.2307/1131818\n\n\nCox, D. R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society: Series B (Methodological), 34(2), 187–202. https://doi.org/10.1111/j.2517-6161.1972.tb00899.x\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting Basic Discrete-Time Hazard Models</span>"
    ]
  },
  {
    "objectID": "12.html",
    "href": "12.html",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "",
    "text": "12.1 Alternative specification for the “main effect” of TIME\nIn the next page, Singer and Willett listed three circumstances under which we might consider alternatives to the completely general approach to time. They were\nIn the subsections to follow, we will explore each in turn.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#alternative-specification-for-the-main-effect-of-time",
    "href": "12.html#alternative-specification-for-the-main-effect-of-time",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "",
    "text": "Use of a completely general specification for TIME [as explored in the last chapter] is an analytic decision, not an integral feature of the model. Nothing about the model or its estimation requires adoption of this, or any other, particular specification for TIME (p. 409, emphasis in the original)\n\n\n\nin studies with many discrete time periods,\nwhen hazard is expected to be near zero in some time periods, and\nwhen some time periods have small risk sets.\n\n\n\n12.1.1 An ordered series of polynomial specifications for TIME\nLoad the Gamse and Conger’s (1997) tenure_pp.csv data.\n\nlibrary(tidyverse)\n\ntenure_pp &lt;- read_csv(\"data/tenure_pp.csv\") %&gt;% \n  # convert the column names to lower case\n  rename_all(tolower)\n\nglimpse(tenure_pp)\n\nRows: 1,474\nColumns: 12\n$ id     &lt;dbl&gt; 111, 111, 111, 111, 111, 211, 211, 211, 211, 211, 211, 311, 311, 311, 311, 311, 311, 311, 311, 411, 411, 411, 411…\n$ period &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1,…\n$ event  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,…\n$ d1     &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ d2     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ d3     &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,…\n$ d4     &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ d5     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ d6     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ d7     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d8     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d9     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nLet’s confirm these data are composed of the records of \\(n = 260\\) early-career academics.\n\ntenure_pp %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   260\n\n\nHere’s a way to count how many cases were censored.\n\ntenure_pp %&gt;% \n  group_by(id) %&gt;% \n  arrange(desc(period)) %&gt;% \n  slice(1) %&gt;%\n  ungroup() %&gt;% \n  count(event) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 2 × 3\n  event     n percent\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     0    94    36.2\n2     1   166    63.8\n\n\nLet’s fire up brms.\n\nlibrary(brms)\n\nAs discussed in the prose and displayed in Tables 12.1 and 12.2, we will fit seven models, ranging from a constant (i.e., intercept only) model to a general (i.e., discrete factor) model. In the last chapter, we discussed how one can fit a general model with a series of \\(J\\) dummies or equivalently with the time variable, period in these data, set as a factor. Here we’ll do both. In preparation, we’ll make a period_f version of period.\n\ntenure_pp &lt;- tenure_pp %&gt;% \n  mutate(period_f = factor(period))\n\nNow fit the models.\n\n# constant\nfit12.1 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 1,\n  prior(normal(0, 4), class = Intercept),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.01\")\n\n# linear\nfit12.2 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + period,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.02\")\n\n# quadratic\nfit12.3 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + period + I(period^2),\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.03\")\n\n# cubic\nfit12.4 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3),\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.04\")\n\n# fourth order\nfit12.5 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3) + I(period^4),\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  control = list(max_treedepth = 12),\n  seed = 12,\n  file = \"fits/fit12.05\")\n\n# fifth order\nfit12.6 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + period + I(period^2) + I(period^3) + I(period^4) + I(period^5),\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  control = list(max_treedepth = 14),\n  seed = 12,\n  init = 0,\n  file = \"fits/fit12.06\")\n\n# general\nfit12.7 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.07\")\n\n# general with `factor(period)`\nfit12.8 &lt;- brm(\n  data = tenure_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + period_f,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.08\")\n\nBefore we compare the models with information criteria, it might be handy to look at the hazard functions for each. A relatively quick way is with the conditional_effects() function.\n\np2 &lt;- plot(conditional_effects(fit12.2), plot = F)[[1]] + ggtitle(\"linear\")\np3 &lt;- plot(conditional_effects(fit12.3), plot = F)[[1]] + ggtitle(\"quadratic\")\np4 &lt;- plot(conditional_effects(fit12.4), plot = F)[[1]] + ggtitle(\"cubic\")\np5 &lt;- plot(conditional_effects(fit12.5), plot = F)[[1]] + ggtitle(\"fourth order\")\np6 &lt;- plot(conditional_effects(fit12.6), plot = F)[[1]] + ggtitle(\"fifth order\")\np7 &lt;- plot(conditional_effects(fit12.8), \n           cat_args = list(size = 3/2), \n           plot = F)[[1]] + ggtitle(\"general\")\n\nBecause it contains no predictors, we cannot use conditional_effects() to make a plot for the constant model (i.e., fit12.1). We’ll have to do that by hand.\n\np1 &lt;- tibble(period = 1:9) %&gt;% \n  ggplot(aes(x = period)) +\n  geom_ribbon(aes(ymin = fixef(fit12.1)[, 3] %&gt;% plogis(),\n                  ymax = fixef(fit12.1)[, 4] %&gt;% plogis()),\n              alpha = 1/5) +\n  geom_line(aes(y = fixef(fit12.1)[, 1] %&gt;% plogis()),\n            linewidth = 1, color = \"blue1\") +\n  ggtitle(\"constant\") +\n  ylab(\"event | trials(1)\")\n\nNow combine and format the subplots with patchwork.\n\nlibrary(patchwork)\n\n(((p1 + p2 + p3 + p4 + p5 + p6) & scale_x_continuous(breaks = 1:9)) + p7) &\n  coord_cartesian(ylim = c(0, 0.5)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe are going to depart from Singer and Willett and no longer entertain using deviance for Bayesian model comparison. But we will compare then using the WAIC and the LOO.\n\nfit12.1 &lt;- add_criterion(fit12.1, criterion = c(\"loo\", \"waic\"))\nfit12.2 &lt;- add_criterion(fit12.2, criterion = c(\"loo\", \"waic\"))\nfit12.3 &lt;- add_criterion(fit12.3, criterion = c(\"loo\", \"waic\"))\nfit12.4 &lt;- add_criterion(fit12.4, criterion = c(\"loo\", \"waic\"))\nfit12.5 &lt;- add_criterion(fit12.5, criterion = c(\"loo\", \"waic\"))\nfit12.6 &lt;- add_criterion(fit12.6, criterion = c(\"loo\", \"waic\"))\nfit12.7 &lt;- add_criterion(fit12.7, criterion = c(\"loo\", \"waic\"))\nfit12.8 &lt;- add_criterion(fit12.8, criterion = c(\"loo\", \"waic\"))\n\nBefore comparing the models in bulk, as in Table 12.2, let’s confirm that whether we use the dummy variable method (fit12.7) or the factor variable method (fit12.8), the results for the general model are the same.\n\nloo_compare(fit12.7, fit12.8, criterion = \"loo\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.7    0.0       0.0  -424.9     22.5         9.3    1.1    849.9   44.9  \nfit12.8    0.0       0.0  -424.9     22.5         9.3    1.1    849.9   44.9  \n\nloo_compare(fit12.7, fit12.8, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit12.7    0.0       0.0  -424.9      22.5          9.2    1.1     849.7   44.9 \nfit12.8    0.0       0.0  -424.9      22.5          9.2    1.1     849.7   44.9 \n\n\nYep, both WAIC and LOO confirm both methods are equivalent. One of the main reasons we used the factor method, here, was because it made it easier to plot the results with conditional_effects(). But with the following model comparisons, we’ll focus on fit12.1 through fit12.7.\n\nloo_compare(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, criterion = \"loo\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.4    0.0       0.0  -420.5     22.1         3.9    0.5    841.1   44.1  \nfit12.3   -0.6       2.2  -421.1     22.0         3.0    0.4    842.3   44.1  \nfit12.5   -0.6       0.8  -421.2     22.2         4.7    0.6    842.4   44.4  \nfit12.6   -1.2       0.8  -421.8     22.2         5.3    0.6    843.5   44.4  \nfit12.7   -4.4       1.5  -424.9     22.5         9.3    1.1    849.9   44.9  \nfit12.2  -15.0       5.9  -435.5     22.1         1.7    0.1    871.1   44.1  \nfit12.1  -99.2      13.2  -519.8     25.2         1.0    0.1   1039.6   50.3  \n\nloo_compare(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, criterion = \"waic\") %&gt;% \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit12.4    0.0       0.0  -420.5      22.1          3.9    0.5     841.1   44.1 \nfit12.3   -0.6       2.2  -421.1      22.0          3.0    0.4     842.3   44.1 \nfit12.5   -0.6       0.8  -421.2      22.2          4.7    0.6     842.3   44.4 \nfit12.6   -1.2       0.8  -421.7      22.2          5.3    0.6     843.5   44.4 \nfit12.7   -4.3       1.5  -424.9      22.5          9.2    1.1     849.7   44.9 \nfit12.2  -15.0       5.9  -435.5      22.1          1.7    0.1     871.1   44.1 \nfit12.1  -99.3      13.2  -519.8      25.2          1.0    0.1    1039.6   50.3 \n\n\nOur results are very similar to those in the AIC column in Table 12.2. Just for kicks and giggles, here are the model weights based on the LOO, WAIC, and stacking method.\n\nmodel_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = \"loo\") %&gt;% \n  round(digits = 3)\n\nfit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 \n  0.000   0.000   0.230   0.419   0.222   0.123   0.005 \n\nmodel_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = \"waic\") %&gt;% \n  round(digits = 3)\n\nfit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 \n  0.000   0.000   0.228   0.420   0.223   0.124   0.005 \n\nmodel_weights(fit12.1, fit12.2, fit12.3, fit12.4, fit12.5, fit12.6, fit12.7, weights = \"stacking\") %&gt;% \n  round(digits = 3)\n\nfit12.1 fit12.2 fit12.3 fit12.4 fit12.5 fit12.6 fit12.7 \n  0.018   0.000   0.445   0.536   0.000   0.000   0.000 \n\n\nAcross the comparison methods, the overall pattern is the cubic model (fit12.4) is marginally better than the rest, but that both the quadratic and fourth-order models were quite close. Unlike when you use model deviance, the parsimony corrections used in the information-criteria-based methods all suggest the general model is overfit.\n\nBefore considering whether these differences in [information criteria] are sufficient to warrant use of an alternative specification for TIME, let us examine the corresponding fitted logit hazard functions. Doing so not only highlights the behavior of logit hazard, it also offers a graphical means of comparing the fit of competing specifications. (p. 413, emphasis in the original)\n\nIn preparation for our Figure 12.1, we’ll make a custom function called make_fitted(), which will streamline some of the data wrangling code.\n\nmake_fitted &lt;- function(fit, scale, ...) {\n  fitted(fit,\n         newdata = nd,\n         scale = scale,\n         ...) %&gt;% \n    data.frame() %&gt;% \n    bind_cols(nd)\n}\n\nIn addition to taking different brms fit objects as input, make_fitted() will allow us to adjust the scale of the output. As you’ll see, we will need to work with to settings in the coming plots. For our first batch of code, we’ll use scale = \"linear\". Because the newdata for our version of the general model (i.e., fit12.8) requires the predictor to be called period_f and the rest of the models require the predictor to be named period, we’ll make and save the results for the former first, redefine our newdata, apply make_fitted() to the rest of the models, and then combine them all. Here it is in one fell swoop.\n\nnd &lt;- tibble(period_f = 1:9)\n\nf &lt;- make_fitted(fit12.8, scale = \"linear\") %&gt;% rename(period = period_f)\n\n# this will simplify the `mutate()` code below\nmodels &lt;- c(\"constant\", \"linear\", \"quadratic\", \"cubic\", \"general\")\n\nnd &lt;- tibble(period = 1:9)\n\nf &lt;- bind_rows(\n  make_fitted(fit12.1, scale = \"linear\"),  # constant\n  make_fitted(fit12.2, scale = \"linear\"),  # linear\n  make_fitted(fit12.3, scale = \"linear\"),  # quadratic\n  make_fitted(fit12.4, scale = \"linear\"),  # cubic\n  f) %&gt;%                                   # general\n  mutate(model = factor(rep(models, each = 9),\n                        levels = models))\n\n# what have we done?\nglimpse(f)\n\nRows: 45\nColumns: 6\n$ Estimate  &lt;dbl&gt; -2.0707275, -2.0707275, -2.0707275, -2.0707275, -2.0707275, -2.0707275, -2.0707275, -2.0707275, -2.0707275, -3…\n$ Est.Error &lt;dbl&gt; 0.08229748, 0.08229748, 0.08229748, 0.08229748, 0.08229748, 0.08229748, 0.08229748, 0.08229748, 0.08229748, 0.…\n$ Q2.5      &lt;dbl&gt; -2.23457104, -2.23457104, -2.23457104, -2.23457104, -2.23457104, -2.23457104, -2.23457104, -2.23457104, -2.234…\n$ Q97.5     &lt;dbl&gt; -1.90267898, -1.90267898, -1.90267898, -1.90267898, -1.90267898, -1.90267898, -1.90267898, -1.90267898, -1.902…\n$ period    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1,…\n$ model     &lt;fct&gt; constant, constant, constant, constant, constant, constant, constant, constant, constant, linear, linear, line…\n\n\nNow we’re in good shape to make and save our version of the top panel of Figure 12.1.\n\np1 &lt;- f %&gt;% \n  ggplot(aes(x = period, y = Estimate, color = model)) +\n  geom_line() +\n  scale_color_viridis_d(option = \"A\", direction = -1) +\n  ylab(\"Fitted logit(hazard)\") +\n  coord_cartesian(ylim = -c(6, 0)) +\n  theme(panel.grid = element_blank())\n\nThe bottom two panels require we redo our make_fitted() code from above, this time setting scale = \"response\".\n\nnd &lt;- tibble(period_f = 1:9)\n\nf &lt;- make_fitted(fit12.8, scale = \"response\") %&gt;% rename(period = period_f)\n\nnd &lt;- tibble(period = 1:9)\n\nf &lt;- bind_rows(\n  make_fitted(fit12.1, scale = \"response\"),  # constant\n  make_fitted(fit12.2, scale = \"response\"),  # linear\n  make_fitted(fit12.3, scale = \"response\"),  # quadratic\n  make_fitted(fit12.4, scale = \"response\"),  # cubic\n  f) %&gt;%                                     # general\n  mutate(model = factor(rep(models, each = 9),\n                        levels = models))\n\nNow make and save the bottom left panel for Figure 12.1.\n\np2 &lt;- f %&gt;% \n  filter(model %in% c(\"quadratic\", \"general\")) %&gt;% \n  \n  ggplot(aes(x = period, y = Estimate, color = model)) +\n  geom_line() +\n  scale_color_viridis_d(option = \"A\", end = 0.5, direction = -1) +\n  ylab(\"Fitted hazard\") +\n  coord_cartesian(ylim = c(0, 0.4)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\nOur f data will also work to make the final bottom right panel for the figure, but we’ll need to convert the Estimate values from the hazard metric to the survival-probability metric. In addition, we will need to add in values for when period = 0. Here we wrangle, plot, and save in one block.\n\nnew_rows &lt;- tibble(\n  Estimate = 0,\n  period   = 0,\n  model    = factor(c(\"quadratic\", \"general\"),\n                    levels = models))\n\np3 &lt;- f %&gt;% \n  filter(model %in% c(\"quadratic\", \"general\")) %&gt;% \n  select(Estimate, period, model) %&gt;% \n  # add the `new_rows` data\n  bind_rows(new_rows) %&gt;%\n  arrange(model, period) %&gt;%\n  group_by(model) %&gt;% \n  # convert hazards to survival probabilities\n  mutate(Estimate = cumprod(1 - Estimate)) %&gt;%\n  \n  # plot!\n  ggplot(aes(x = period, y = Estimate, color = model)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  geom_line() +\n  scale_color_viridis_d(option = \"A\", end = 0.5, direction = -1) +\n  scale_y_continuous(\"Fitted survival probability\", breaks = c(0, 0.5, 1)) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\nNow combine the subplots and format a little with patchwork to make our version of Figure 12.1.\n\np1 + p2 + p3 + \n  plot_layout(guides = \"collect\") &\n  scale_x_continuous(\"Years after hire\", breaks = 0:9, limits = c(0, 9))\n\n\n\n\n\n\n\n\nInstead of the two-row layout in the text, it seemed simpler to arrange the panels all in one row. This way we can let the full version of the line-color legend serve for all three panels.\n\n\n12.1.2 Criteria for comparing alternative specification\n\nThe decline in the deviance statistic across models indicates that fit improves with increasing complexity of the temporal specification. To evaluate the magnitude of this decline, we must also account for the increased number of parameters in the model. You should not adopt a more complex specification if it fits no better than a simpler one. But if an alternative specification is (nearly) as good as the most general one, it may be “good enough.” At the same time, we would not want an alternative that performs measurably worse than we know we can do. (p. 415)\n\nWe won’t be comparing deviances with \\(\\chi^2\\) tests, here. As to information criteria, we got ahead of the authors a bit and presented those comparisons in the last section. Although our use of the WAIC and the LOO is similar to Singer and Willett’s use of the AIC and BIC in that they yield no formal hypothesis test in the form of a \\(p\\)-value, their estimates and difference scores do come with standard errors.\nIn the middle of page 416, the authors focused on comparing the constant and linear models, the linear and quadratic models, and the quadratic and general models. The LOO and WAIC estimates for each were near identical. For the sake of simplicity, here are those three focused comparisons using the LOO.\n\n# the constant and linear models\nl1 &lt;- loo_compare(fit12.1, fit12.2, criterion = \"loo\")\n\n# the linear and quadratic models\nl2 &lt;- loo_compare(fit12.2, fit12.3, criterion = \"loo\")\n\n# the quadratic and general models\nl3 &lt;- loo_compare(fit12.3, fit12.7, criterion = \"loo\")\n\nl1 %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.2    0.0       0.0  -435.5     22.1         1.7    0.1    871.1   44.1  \nfit12.1  -84.3      11.7  -519.8     25.2         1.0    0.1   1039.6   50.3  \n\nl2 %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.3    0.0       0.0  -421.1     22.0         3.0    0.4    842.3   44.1  \nfit12.2  -14.4       5.5  -435.5     22.1         1.7    0.1    871.1   44.1  \n\nl3 %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.3    0.0       0.0  -421.1     22.0         3.0    0.4    842.3   44.1  \nfit12.7   -3.8       2.4  -424.9     22.5         9.3    1.1    849.9   44.9  \n\n\nIf we presume the LOO differences follow a normal distribution, we can use their point estimates and standard errors to plot those distributions using simulated data from good old rnorm().\n\nlibrary(tidybayes)\n\nn &lt;- 1e6\nmodels &lt;- c(\"linear - constant\", \"quadratic - linear\", \"quadratic - general\")\nset.seed(12)\n\n# wrangle\ntibble(loo_difference = c(rnorm(n, mean = l1[2, 1] * -2, sd = l1[2, 2] * 2),\n                          rnorm(n, mean = l2[2, 1] * -2, sd = l2[2, 2] * 2),\n                          rnorm(n, mean = l3[2, 1] * -2, sd = l3[2, 2] * 2))) %&gt;% \n  mutate(models = factor(rep(models, each = n),\n                         levels = models)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = loo_difference)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"LOO-difference simulations based on 1,000,000 draws\",\n       x = \"difference distribution\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ models, scales = \"free\")\n\n\n\n\n\n\n\n\nThe LOO difference for the linear and constant models is decisive. The difference for the quadratic and linear models is fairly large on the information-criteria scale, but the uncertainty in that distribution is fairly large relative to its location (i.e., its mean), which might temper overly-strong conclusions about how much better the quadratic was compared to the linear. The comparison between the quadratic and the general produced a simulation with a modest location and rather large uncertainty relative to the magnitude of that location. All in all, “all signs point to the superiority of the quadratic specification, which fits nearly as well as the general mode, but with fewer parameters” (p. 416).\nIn the next page, Singer and Willett briefly focused on comparing the cubic and quadratic models. Here are their LOO and WAIC caparisons.\n\nloo_compare(fit12.3, fit12.4, criterion = \"loo\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.4    0.0       0.0  -420.5     22.1         3.9    0.5    841.1   44.1  \nfit12.3   -0.6       2.2  -421.1     22.0         3.0    0.4    842.3   44.1  \n\nloo_compare(fit12.3, fit12.4, criterion = \"waic\") %&gt;% print(simplify = F)\n\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit12.4    0.0       0.0  -420.5      22.1          3.9    0.5     841.1   44.1 \nfit12.3   -0.6       2.2  -421.1      22.0          3.0    0.4     842.3   44.1 \n\n\nIn the text, the results for the AIC and BIC differed. Our LOO and WAIC results both agree with the AIC: the cubic model has a slightly lower LOO and WAIC estimate compared to the quadratic. However, the standard errors for the formal difference score are about twice the size of that difference and the absolute magnitude of the difference is rather small to begin with. Here’s what it looks like if we compare them using LOO weights, WAIC weights, and stacking weights.\n\nmodel_weights(fit12.3, fit12.4, weights = \"loo\") %&gt;% round(digits = 3)\n\nfit12.3 fit12.4 \n  0.354   0.646 \n\nmodel_weights(fit12.3, fit12.4, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit12.3 fit12.4 \n  0.352   0.648 \n\nmodel_weights(fit12.3, fit12.4, weights = \"stacking\") %&gt;% round(digits = 3)\n\nfit12.3 fit12.4 \n  0.374   0.626 \n\n\nAcross all three weight comparisons, there was a modest edge for the quadratic model. But back to Singer and Willett:\n\nAlthough decision rules cannot substitute for judgment, intuition, and common sense, we nevertheless conclude by offering two guidelines for selecting among alternative specifications:\n\nIf a smooth specification works nearly as well as the completely general one, appreciably better than all simpler ones, and no worse than all more complex ones, consider adopting it.\nIf no smooth specifications meet these criteria, retain the completely general specification.\n\nIf this decision process leads you to a polynomial specification, then you can interpret the model’s parameters easily, as we [will discuss in a bit]. (p. 417, emphasis in the original)\n\nBefore moving on, we might point out that our Bayesian brms-based framework offers a different option: model averaging. We plot the hazard and survival curves based on weighted averages of multiple models. The weights can be based on various criteria. One approach would be to use the model weights from the LOO or the WAIC. As an example, here we use the LOO weights for the quadratic and cubic models.\n\nnd &lt;- tibble(period = 1:9)\n\npp &lt;- pp_average(\n  fit12.3, fit12.4,\n  weights = \"loo\",\n  newdata = nd,\n  method = \"pp_expect\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd)\n\nThe pp_average() function works much like fitted() or predict(). If you input models and perhaps newdata, it will return estimates that are the weighted averages of the specified models. With the weights = \"loo\" argument, we indicated our desired weights were those from the LOO, just as we computed earlier with the model_weights() function. With the method = \"pp_expect\" argument, we indicated we wanted fitted values like we would get from fitted().\nHere we plot the results in terms of hazard and survival.\n\n# hazard\np1 &lt;- pp %&gt;% \n  ggplot(aes(x = period)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5) +\n  geom_line(aes(y = Estimate)) +\n  scale_x_continuous(\"Years after hire\", breaks = 0:9, limits = c(0, 9)) +\n  ylab(\"hazard\") +\n  theme(panel.grid = element_blank())\n\n# survival\np2 &lt;- pp %&gt;% \n  select(-Est.Error) %&gt;% \n  bind_rows(tibble(Estimate = 0, Q2.5 = 0, Q97.5 = 0, period = 0)) %&gt;% \n  arrange(period) %&gt;% \n  mutate_at(vars(Estimate:Q97.5), .funs = ~ cumprod(1 - .)) %&gt;% \n  \n  ggplot(aes(x = period)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5) +\n  geom_line(aes(y = Estimate)) +\n  scale_x_continuous(\"Years after hire\", breaks = 0:9) +\n  scale_y_continuous(\"survival\", breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\n# combine\n(p1 | p2) + \n  plot_annotation(title = \"Behold the fitted hazard and survival curves based on a weighted\\naverage of the quadratic and linear models!\")\n\n\n\n\n\n\n\n\n\n\n12.1.3 Interpreting parameters from linear, quadratic, and cubic specifications\n“One advantage of a polynomial specification is that you can often interpret its parameters directly” (p. 417). For the polynomial models in this section, Singer and Willett used the \\(TIME - c\\) specification for period where \\(c\\) is a centering constant. They used \\(c = 5\\). Before we can refit our polynomial models with this parameterization, we’ll want to make a new period variable with this centering. We’ll call it period_5.\n\ntenure_pp &lt;- tenure_pp %&gt;% \n  mutate(period_5 = period - 5)\n\n# how do the two `period` variables compare?\ntenure_pp %&gt;% \n  distinct(period, period_5)\n\n# A tibble: 9 × 2\n  period period_5\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1       -4\n2      2       -3\n3      3       -2\n4      4       -1\n5      5        0\n6      6        1\n7      7        2\n8      8        3\n9      9        4\n\n\nNow refit the quadratic model using period_5.\n\nfit12.9 &lt;- update(\n  fit12.3,\n  newdata = tenure_pp,\n  event | trials(1) ~ 0 + Intercept + period_5 + I(period_5^2),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.09\")\n\nCheck the model summary.\n\nprint(fit12.9)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ Intercept + period_5 + I(period_5^2) - 1 \n   Data: tenure_pp (Number of observations: 1474) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      -1.41      0.11    -1.63    -1.20 1.00     2408     2602\nperiod_5        0.61      0.06     0.50     0.74 1.00     2188     2668\nIperiod_5E2    -0.13      0.03    -0.18    -0.08 1.00     2159     2247\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFocusing just on the posterior means, this yields the following formula for the quadratic discrete-time hazard model: \\(\\operatorname{logit} \\hat h (t_j) = \\;\\) -1.4129 \\(\\text{one } +\\) 0.6123\\((\\text{time}_j - 5)\\) -0.1279\\((\\text{time}_j - 5)^2\\).\nSinger and Willett used the term “flipover point” for the point at which the quadratic function reaches its peak or trough. If we let \\(c\\) be the centering constant for time variable, \\(\\alpha_1\\) be the linear coefficient for time, and \\(\\alpha_2\\) be the quadratic coefficient for time, we define the flipover point as\n\\[\\text{flipover point} = [c - 1/2 (\\alpha_1 / \\alpha_2)].\\]\nHere’s what that looks like using the posterior samples.\n\nas_draws_df(fit12.9) %&gt;% \n  transmute(c  = 5,\n            a1 = b_period_5,\n            a2 = b_Iperiod_5E2) %&gt;% \n  mutate(`flipover point` = c - 0.5 * (a1 / a2)) %&gt;% \n  \n  ggplot(aes(x = `flipover point`)) +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  scale_x_continuous(breaks = 7:12) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nJust as each parameter has a posterior distribution, the flipover point, which is a function of two of the parameters, also has a posterior distribution. To understand what this flipover distribution means, it might be helpful to look at it in another way. For that, we’ll employ fitted().\n\nnd &lt;- tibble(period = seq(from = 0, to = 12, by = 0.1)) %&gt;% \n  mutate(period_5 = period - 5)\n\nf &lt;- fitted(fit12.9,\n            newdata = nd,\n            summary = F,\n            scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  pivot_longer(everything()) %&gt;% \n  bind_cols(expand(nd,\n                   iter = 1:4000,\n                   nesting(period, period_5)))\n\nf %&gt;% \n  glimpse()\n\nRows: 484,000\nColumns: 5\n$ name     &lt;chr&gt; \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14\", \"X15\", \"X16\", \"X17\", \"…\n$ value    &lt;dbl&gt; -8.876525, -8.652573, -8.431700, -8.213907, -7.999193, -7.787559, -7.579005, -7.373530, -7.171135, -6.971820, -…\n$ iter     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ period   &lt;dbl&gt; 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2…\n$ period_5 &lt;dbl&gt; -5.0, -4.9, -4.8, -4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4.0, -3.9, -3.8, -3.7, -3.6, -3.5, -3.4, -3.3, -3.…\n\n\nNow make a logit hazard spaghetti plot.\n\nf %&gt;% \n  # how many lines would you like?\n  filter(iter &lt;= 30) %&gt;% \n  \n  ggplot(aes(x = period, y = value, group = iter)) +\n  geom_line(alpha = 1/2) +\n  ylab(\"logit hazard\") +\n  coord_cartesian(xlim = c(0, 11),\n                  ylim = c(-5, 0)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo keep the plot manageable, we filtered to just the first 30 posterior draws. Each was depicted with its own hazard line. Note how each of those hazard lines peaks at a different point along the \\(x\\)-axis. Most peak somewhere around 7.4. Some take on notably higher values.\nNow fit a cubic model using period_5, \\((TIME_j - 5)\\), as the measure of time.\n\nfit12.10 &lt;- update(\n  fit12.4,\n  newdata = tenure_pp,\n  event | trials(1) ~ 0 + Intercept + period_5 + I(period_5^2) + I(period_5^3),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.10\")\n\nCheck the model summary.\n\nprint(fit12.10)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ Intercept + period_5 + I(period_5^2) + I(period_5^3) - 1 \n   Data: tenure_pp (Number of observations: 1474) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      -1.45      0.12    -1.68    -1.22 1.00     2435     2389\nperiod_5        0.75      0.10     0.56     0.94 1.00     1899     1977\nIperiod_5E2    -0.11      0.03    -0.17    -0.07 1.00     2300     2313\nIperiod_5E3    -0.02      0.01    -0.04     0.00 1.00     1941     2060\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFocusing again on just the posterior means, this yields the following formula for the cubic discrete-time hazard model: \\(\\operatorname{logit} \\hat h (t_j) =\\;\\) -1.4531 \\(\\text{one } +\\) 0.7460\\((\\text{time}_j - 5)\\) -0.1142\\((\\text{time}_j - 5)^2\\) -0.0178\\((\\text{time}_j - 5)^3\\). Now if we let \\(c\\), \\(\\alpha_1\\) and \\(\\alpha_2\\) retain their meanings from before and further let \\(\\alpha_3\\) stand for the cubic term for time, we can define the two flipover points in the cubic logit hazard model as\n\\[\\text{flipover points} = c + \\frac{-\\alpha_2 \\pm \\sqrt{\\alpha_2^2 - 3 \\alpha_1 \\alpha_3}}{3 \\alpha_3}.\\]\nDo you see that \\(\\pm\\) sign in the numerator? That’s what gives us the two points. Now apply the formula to fit12.10 and plot.\n\n# extract the posterior draws\ndraws &lt;- as_draws_df(fit12.10) %&gt;% \n  transmute(c  = 5,\n            a1 = b_period_5,\n            a2 = b_Iperiod_5E2,\n            a3 = b_Iperiod_5E3)\n\n# flipover point with \"+\" in the numerator\np1 &lt;- draws %&gt;% \n  mutate(`flipover point 1` = c + (- a2 + sqrt(a2^2 - 3 * a1 * a3)) / (3 * a3)) %&gt;% \n  filter(!is.na(`flipover point 1`)) %&gt;% \n  filter(`flipover point 1` &gt; -50 & `flipover point 1` &lt; 50) %&gt;% \n  \n  ggplot(aes(x = `flipover point 1`)) +\n  stat_halfeyeh(.width = c(0.5, 0.95)) +\n  annotate(geom = \"text\",\n           x = -30, y = 0.85,\n           label = \"italic(c)+frac(-alpha[2]+sqrt(alpha[2]^2-3*alpha[1]*alpha[3]), 3*alpha[3])\",\n           hjust = 0, family = \"Times\", parse = T) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(-30, 20))\n\n# flipover point with \"-\" in the numerator\np2 &lt;- draws %&gt;% \n  mutate(`flipover point 2` = c + (- a2 - sqrt(a2^2 - 3 * a1 * a3)) / (3 * a3)) %&gt;% \n  filter(!is.na(`flipover point 2`)) %&gt;% \n  \n  ggplot(aes(x = `flipover point 2`)) +\n  stat_halfeyeh(.width = c(0.5, 0.95)) +\n  annotate(geom = \"text\",\n           x = 8.2, y = 0.85,\n           label = \"italic(c)+frac(-alpha[2]-sqrt(alpha[2]^2-3*alpha[1]*alpha[3]), 3*alpha[3])\",\n           hjust = 0, family = \"Times\", parse = T) +\n  scale_y_continuous(NULL, breaks = NULL)\n\n# combine!\n(p1 | p2) & theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe plot on the right looks similar to the flipover plot from fit12.9. But look at the massive uncertainty in the flipover point in the plot on the left. If you play around with the code, you’ll see the \\(x\\)-axis extends far beyond the boundaries in the plot. Another spaghetti plot might help show what’s going on.\n\n# redifine the `newdata`\nnd &lt;- tibble(period = seq(from = -8, to = 11, by = 0.1)) %&gt;% \n  mutate(period_5 = period - 5)\n\n# employ `fitted()` and wrangle\nf &lt;- fitted(fit12.10,\n            newdata = nd,\n            summary = F,\n            scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  pivot_longer(everything()) %&gt;% \n  bind_cols(expand(nd,\n                   iter = 1:4000,\n                   nesting(period, period_5)))\n\n# plot!\nf %&gt;% \n  filter(iter &lt;= 30) %&gt;% \n  ggplot(aes(x = period, y = value, group = iter)) +\n  geom_line(alpha = 1/2) +\n  ylab(\"logit hazard\") +\n  coord_cartesian(xlim = c(-7, 10),\n                  ylim = c(-13, 0)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nOn the region to the right of 0 on the \\(x\\)-axis, the plot looks a lot like the one for the quadratic model. But look at how wildly the lines fan out on the left side of 0. Since that’s the region where we find the second flipover point, all that uncertainty got baked into its marginal posterior. Just because I think it looks cool, here’s a version of that plot with lines corresponding to all 4,000 posterior draws.\n\nf %&gt;% \n  ggplot(aes(x = period, y = value, group = iter)) +\n  geom_line(alpha = 1/10, linewidth = 1/10) +\n  ylab(\"logit hazard\") +\n  coord_cartesian(xlim = c(-7, 10),\n                  ylim = c(-13, 0)) +\n  theme(panel.grid = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#using-the-complementary-log-log-link-to-specify-a-discrete-time-hazard-model",
    "href": "12.html#using-the-complementary-log-log-link-to-specify-a-discrete-time-hazard-model",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "12.2 Using the complementary log-log link to specify a discrete-time hazard model",
    "text": "12.2 Using the complementary log-log link to specify a discrete-time hazard model\nSo far we’ve been using the\n\nlogit transformation [which] represented a natural choice because it allowed us to: (1) specify the model using familiar terminology; (2) use widely available software for estimation; and (3) exploit interpretative strategies with which many empirical researchers are comfortable.\nJust like the choice of a completely general specification for the main effect of TIME, use of a logit link is an analytic decision. Nothing about the way in which the model is postulated or fit requires the adoption of this, or any other, particular link function. (p. 419–420, emphasis in the original)\n\nThe complimentary log-log transformation–clog-log for short–is a widely-used alternative. It follows the form\n\\[\\operatorname{clog-log} = \\log \\big (- \\log (1 - p) \\big),\\]\nwhere \\(p\\) is a probability value. In words, “while the logit transformation yields the logarithm of the odds of event occurrence, the clog-log transformation yields the logarithm of the negated logarithm of the probability of event nonoccurrence” (p. 420, emphasis in the original).\n\n12.2.1 The clog-log transformation: When and why it is useful\nHere’s our version of Figure 12.2, where we compare the logit and clog-log transformations.\n\n# simulate the data\ntibble(p = seq(from = 0.00001, to = 0.99999, length.out = 1e4)) %&gt;% \n  mutate(logit   = log(p / (1 - p)),\n         cloglog = log(-log(1 - p))) %&gt;% \n  pivot_longer(-p) %&gt;% \n  mutate(name = factor(name,\n                       levels = c(\"logit\", \"cloglog\"),\n                       labels = c(\"Logit\", \"Complementary log-log\"))) %&gt;% \n  \n  # plot\n  ggplot(aes(x = p, y = value, color = name)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) +\n  scale_y_continuous(\"transformed hazard probability\", \n                     breaks = -3:3 * 5, limits = c(-15, 15)) +\n  xlab(\"hazard probability\") +\n  theme(legend.background = element_rect(fill = \"grey92\"),\n        legend.key = element_rect(fill = \"grey92\", color = \"grey92\"),\n        legend.position = c(0.25, 0.85),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBoth transformations extend to the full \\(-\\infty\\) to \\(\\infty\\) parameter space. But whereas the logit is symmetric around zero and has a memorable point corresponding to \\(p = .5\\) (i.e., 0), the clog-log is asymmetric and has a less-intuitive point corresponding to \\(p = .5\\) (i.e., -0.3665129). Though somewhat odd, the advantage of the clog-log is\n\nit provides a discrete-time statistical model for hazard that has a built-in proportional hazards assumption, and not a proportional odds assumption (as in the case of the logit link). This would be completely unremarkable except for one thing: it provides a conceptual parallelism between the clog-log discrete-time hazard model and the models that we will ultimately describe for continuous-time survival analysis. (p. 421, emphasis in the original)\n\n\n\n12.2.2 A discrete-time hazard model using the complementary log-log link\nSinger and Willett (p. 422):\n\nAny discrete-time hazard model postulated using a logit link can be rewritten using a clog-log link, simply by substituting transformations of the outcome. For example, we can write a general discrete-time hazard model for \\(J\\) time periods and \\(P\\) substantive predictors as:\n\n\\[\\begin{align}\n\\operatorname{clog-log} h(t_j) & = [\\alpha_1 D_1 + \\alpha_2 D_2 + \\cdots + \\alpha_J D_J] \\\\\n& \\;\\; + [\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_P X_P].\n\\end{align}\\]\nNow reload the firstsex_pp.csv data to test this baby out.\n\nsex_pp &lt;- read_csv(\"data/firstsex_pp.csv\")\n\nglimpse(sex_pp)\n\nRows: 822\nColumns: 11\n$ id     &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, 9, 9, 9, 9, 10, 10, …\n$ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8, 9, 7, 8, 9, 10, 11…\n$ event  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d7     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ d8     &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,…\n$ d9     &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ d10    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ d11    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ d12    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ pt     &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pas    &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -1.40498…\n\n\nOur next task is to recreate Figure 12.3, which shows the “sample hazard functions for the grade of first intercourse data displayed on different scales,” namely the logit and clog-log (p. 423). The key word here is “sample,” meaning we’re not fitting full models. For our version of the corresponding figure from the last chapter (i.e., Figure 11.2 from page 363), we computed the sample logit hazard functions with life tables based on the output from the survival::survfit() function. I am not aware that you can get hazards in the clog-log scale from the survfit() function. The folks at IDRE solved the problem by fitting four maximum likelihood models using the glm() function (see here). We’ll follow a similar approach, but with weakly-regularizing priors using the brms::brm() function.\n\n## logit\n# pt == 0\nfit12.11 &lt;- brm(\n  data = sex_pp %&gt;% filter(pt == 0),\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.11\")\n\n# pt == 1\nfit12.12 &lt;- brm(\n  data = sex_pp %&gt;% filter(pt == 1),\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.12\")\n\n## clog-log\n# pt == 0\nfit12.13 &lt;- brm(\n  data = sex_pp %&gt;% filter(pt == 0),\n  family = binomial(link = cloglog),\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.13\")\n\n# pt == 1\nfit12.14 &lt;- brm(\n  data = sex_pp %&gt;% filter(pt == 1),\n  family = binomial(link = cloglog),\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.14\")\n\nThe primary line of code that distinguishes fit12.11 and fit12.12 from fit12.13 and fit12.14 is family = binomial(link = cloglog). With the family argument, we switched out the default logit link for the clog-log link. Before we can make our version of Figure 12.5, we will redefine our nd data for fitted(), pump our four fit objects into our custom make_fitted() function from earlier, and wrangle a little.\n\nnd &lt;- tibble(period = 7:12) %&gt;% \n  mutate(d7  = if_else(period == 7, 1, 0),\n         d8  = if_else(period == 8, 1, 0),\n         d9  = if_else(period == 9, 1, 0),\n         d10 = if_else(period == 10, 1, 0),\n         d11 = if_else(period == 11, 1, 0),\n         d12 = if_else(period == 12, 1, 0))\n\nf &lt;- bind_rows(\n  make_fitted(fit12.11, scale = \"linear\"),\n  make_fitted(fit12.12, scale = \"linear\"),\n  make_fitted(fit12.13, scale = \"linear\"),\n  make_fitted(fit12.14, scale = \"linear\")) %&gt;% \n  mutate(pt   = rep(str_c(\"pt = \", c(0:1, 0:1)), each = n() / 4),\n         link = rep(c(\"logit\", \"clog-log\"), each = n() / 2))\n\nf %&gt;% glimpse()\n\nRows: 24\nColumns: 13\n$ Estimate  &lt;dbl&gt; -3.6706944, -3.6442317, -2.0511472, -1.9086582, -1.4590605, -1.4814170, -2.0152550, -2.9357434, -1.5528577, -0…\n$ Est.Error &lt;dbl&gt; 0.7397893, 0.7660662, 0.3899299, 0.3818780, 0.3641419, 0.4019352, 0.3012705, 0.4743357, 0.2787832, 0.2581887, …\n$ Q2.5      &lt;dbl&gt; -5.3449928, -5.4421979, -2.8616739, -2.7133347, -2.2004954, -2.3294931, -2.6244237, -3.9581043, -2.1051317, -1…\n$ Q97.5     &lt;dbl&gt; -2.444115432, -2.375655323, -1.340400685, -1.212138640, -0.796115364, -0.730152523, -1.460835990, -2.083148560…\n$ period    &lt;int&gt; 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12\n$ d7        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0\n$ d8        &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0\n$ d9        &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0\n$ d10       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0\n$ d11       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0\n$ d12       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1\n$ pt        &lt;chr&gt; \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 0\", \"pt = 1\", \"pt = 1\", \"pt = 1\", \"pt = 1\", \"pt = 1\", …\n$ link      &lt;chr&gt; \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"logit\", \"c…\n\n\nMake our version of Figure 12.3.\n\nf %&gt;% \n  ggplot(aes(x = period, group = interaction(pt, link),\n             color = pt)) +\n  geom_line(aes(y = Estimate, linetype = link)) +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) +\n  scale_x_continuous(\"grade\", breaks = 6:12, limits = c(6, 12)) +\n  ylab(\"transformed hazard probability\") +\n  coord_cartesian(ylim = c(-4, 0)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nOur next step is to fit proper models to the data. First, we will refit fit11.8 from last chapter. Then we’ll fit the clog-log analogue to the same model. The two models, respectively, follow the form\n\\[\n\\begin{align}\n\\operatorname{logit} h(t_{ij}) & = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ] \\; \\text{and} \\\\\n\\operatorname{clog-log} h(t_{ij}) & = [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_i ].\n\\end{align}\n\\]\nFit the models.\n\n# logit\nfit11.8 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.08\")\n\n# clog-log\nfit12.15 &lt;- brm(\n  data = sex_pp,\n  family = binomial(link = cloglog),\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.15\")\n\nBoth models used the binomial likelihood. They only differed in which link function we used to transform the data. To further explicate, we can more fully write out our Bayesian clog-log model as\n\\[\n\\begin{align}\n\\text{event}_{ij} & = \\operatorname{Binomial}(n = 1, p_{ij}) \\\\\n\\operatorname{clog-log} (p_{ij}) & =  [\\alpha_7 D_{7ij} + \\alpha_8 D_{8ij} + \\cdots + \\alpha_{12} D_{12ij}] + [\\beta_1 \\text{PT}_{1i} ] \\\\\n\\alpha_7, \\alpha_8, ..., \\alpha_{12} & \\sim \\operatorname{Normal}(0, 4) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 4).\n\\end{align}\n\\]\nHere are the posterior means for fit11.8 and fit12.15, as depicted in the firs three columns of Table 12.3.\n\npars &lt;- bind_rows(fixef(fit11.8)  %&gt;% data.frame() %&gt;% rownames_to_column(\"par\"),\n                  fixef(fit12.15) %&gt;% data.frame() %&gt;% rownames_to_column(\"par\")) %&gt;% \n  mutate(link = rep(c(\"logit\", \"clog-log\"), each = n() / 2),\n         par  = factor(par, levels = c(str_c(\"d\", 7:12), \"pt\")))\n\npars %&gt;%\n  select(par, link, Estimate) %&gt;% \n  pivot_wider(names_from = link,\n              values_from = Estimate) %&gt;% \n  select(par, `clog-log`, logit) %&gt;% \n  mutate_if(is.double, round, digits = 4)\n\n# A tibble: 7 × 3\n  par   `clog-log`  logit\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 d7        -2.99  -3.00 \n2 d8        -3.70  -3.72 \n3 d9        -2.33  -2.28 \n4 d10       -1.90  -1.82 \n5 d11       -1.77  -1.66 \n6 d12       -1.35  -1.17 \n7 pt         0.770  0.858\n\n\nThey might be easier to compare in a coefficient plot.\n\npars %&gt;% \n  ggplot(aes(x = link, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_pointrange() +\n  labs(x = NULL, \n       y = \"transformed hazard\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(hjust = 0),\n        panel.grid = element_blank()) +\n  facet_wrap(~ par, ncol = 1)\n\n\n\n\n\n\n\n\nInstead of comparing them with deviance values, we will compare the two models using Bayesian information criteria. For simplicity, we’ll focus on the LOO.\n\nfit12.15 &lt;- add_criterion(fit12.15, criterion = c(\"loo\", \"waic\"))\n\nloo_compare(fit12.15, fit11.8, criterion = \"loo\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit12.15    0.0       0.0  -324.3     17.7         7.0    0.6    648.6   35.3  \nfit11.8    -0.2       0.2  -324.5     17.6         7.1    0.6    649.0   35.2  \n\nmodel_weights(fit12.15, fit11.8, weights = \"loo\") %&gt;% round(digits = 3)\n\nfit12.15  fit11.8 \n   0.545    0.455 \n\n\nFrom a LOO perspective, they’re basically the same. The parameter summaries are also quite similar between the two models. A coefficient plot might make it easy to see. “Numerical similarity is common when fitting identical models with alternate link functions (and net risks of event occurrence are low) and suggests that choice of a link function should depend on other considerations” (p. 423).\nWe already know from the last chapter that we can convert logits to probabilities with the function\n\\[p = \\frac{1}{1 + e^{- \\text{logit}}}.\\]\nThe relevant inverse transformation for the clog-log link is\n\\[p = 1 - e^{\\left (- e^{( \\text{clog-log})} \\right)}.\\]\nNow use those formulas to convert our Estimate values into the hazard metric, as shown in the last two columns of Table 12.3.\n\npars %&gt;% \n  filter(par != \"pt\") %&gt;% \n  mutate(Estimate = if_else(str_detect(link, \"logit\"),\n                            1 / (1 + exp(-1 * Estimate)),\n                            1 - exp(-exp(Estimate)))) %&gt;%\n  select(par, link, Estimate) %&gt;% \n  pivot_wider(names_from = link,\n              values_from = Estimate) %&gt;% \n  select(par, `clog-log`, logit) %&gt;% \n  mutate(`clog-log - logit` = `clog-log` - logit) %&gt;% \n  mutate_if(is.double, round, digits = 4)\n\n# A tibble: 6 × 4\n  par   `clog-log`  logit `clog-log - logit`\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt;\n1 d7        0.0492 0.0472             0.002 \n2 d8        0.0244 0.0236             0.0009\n3 d9        0.093  0.0926             0.0004\n4 d10       0.138  0.139             -0.0008\n5 d11       0.157  0.160             -0.0032\n6 d12       0.228  0.236             -0.0082\n\n\nFor kicks, we threw in a column of their differences. The clog-log - logit column highlights how\n\nin general, fitted hazard functions from models estimated with both link functions will be indistinguishable unless hazard is high, once again suggesting that the quality of the estimates does not provide a rationale for selecting one of these link functions over the other. (p. 424)\n\nNow focus on the pt parameter for both models.\n\nIn both cases, we antilog [i.e., exponentiate] parameter estimates, but whereas an antilogged [i.e., exponentiated] coefficient from a model with a logit link is an odds ratio, an antilogged coefficient from a model with a clog-log link is a hazard ratio. (p. 424)\n\nHere’s that in a plot.\n\n# left\np1 &lt;- as_draws_df(fit11.8) %&gt;% \n  ggplot(aes(x = b_pt %&gt;% exp())) +\n  stat_halfeye(.width = c(0.5, 0.95), na.rm = T) +\n  scale_x_continuous(\"b_pt (exponentiated)\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit11.8 (logit link)\",\n       subtitle = \"Exponentiating this parameter yields an odds ratio.\") +\n  coord_cartesian(xlim = c(1, 5)) +\n  theme(panel.grid = element_blank())\n\n# right\np2 &lt;- as_draws_df(fit12.15) %&gt;% \n  ggplot(aes(x = b_pt %&gt;% exp())) +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  scale_x_continuous(\"b_pt (exponentiated)\", limits = c(1, 5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit12.15 (clog-log link)\",\n       subtitle = \"Exponentiating this parameter yields a hazard ratio.\") +\n  theme(panel.grid = element_blank())\n\n# combine\np1 | p2\n\n\n\n\n\n\n\n\nFor the clog-log model, then,\n\nin every grade from 7th to 12th, we estimate the hazard of first intercourse for boys who experienced a parenting transition to be [about] 2.2 times the hazard for their peers raised with both biological parents. This interpretation contrasts with that from the model with a logit link, which suggests that the odds of first intercourse are [about] 2.4 times the odds for boys who experienced a parenting transition. (pp. 424–425, emphasis in the original)\n\n\n\n12.2.3 Choosing between logit and clog-log links for discrete-time hazard models\n\nThe primary advantage of the clog-log link is that in invoking a proportional hazards assumption it yields a direct analog to the continuous time hazard model…. If you believe that the underlying metric for time is truly continuous and that the only reason you observe discretized values is due to measurement difficulties, a model specified with a clog-log link has much to recommend it….\n[Yet,] if data are collected in truly discrete time, the clog-log specification has no particular advantage. As N. Beck (1999); Beck, Katz, and Tucker (1998); and Sueyoshi (1995) argue, the proportional hazards assumption is no more sacred than the proportional odds assumption, and while consistency across models is noble, so, too, is simplicity (which decreases the chances of mistake). (p. 426)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#time-varying-predictors",
    "href": "12.html#time-varying-predictors",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "12.3 Time-varying predictors",
    "text": "12.3 Time-varying predictors\n“Discrete-time survival analysis adopts naturally to the inclusion of time-varying predictors. Because models are fit using a person-period data set, a time-varying predictor simply takes on its appropriate value for each person in each period” (p. 427).\n\n12.3.1 Assumptions underlying a model with time-varying predictors\nLoad the depression data from Wheaton, Rozell, and Hall (1997).\n\ndepression_pp &lt;- read_csv(\"data/depression_pp.csv\") %&gt;% \n  # convert the column names to lower case\n  rename_all(tolower)\n\nglimpse(depression_pp)\n\nRows: 36,997\nColumns: 22\n$ id        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,…\n$ onset     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ age       &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34…\n$ censor    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ censage   &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34…\n$ aged      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ female    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ nsibs     &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2,…\n$ sibs12    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ sibs34    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sibs56    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ sibs78    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sibs9plus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ bigfamily &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ period    &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, …\n$ event     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pd        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pdnow     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ one       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age_18    &lt;dbl&gt; -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ age_18sq  &lt;dbl&gt; 196, 169, 144, 121, 100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 1…\n$ age_18cub &lt;dbl&gt; -2744, -2197, -1728, -1331, -1000, -729, -512, -343, -216, -125, -64, -27, -8, -1, 0, 1, 8, 27, 64, 125, 216, …\n\n\nHere is the participant age range.\n\nrange(depression_pp$age)\n\n[1] 17 57\n\n\nWe might count how many participants experienced a parental divorce, pd, like this.\n\ndepression_pp %&gt;% \n  group_by(id) %&gt;% \n  summarise(pd = if_else(sum(pd) &gt; 0, 1, 0)) %&gt;% \n  ungroup() %&gt;% \n  count(pd) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 2 × 3\n     pd     n percent\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     0  1248    89.6\n2     1   145    10.4\n\n\nHere we fit the discrete hazard model based on a quadratic treatment of \\(\\text{age} - 18\\) with and the time-varying predictor pd and without/with the time-invariant predictor female.\n\n# 16.3771 minutes\nfit12.16 &lt;- brm(\n  data = depression_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.16\")\n\n# 20.49633 minutes\nfit12.17 &lt;- brm(\n  data = depression_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.17\")\n\nCheck the summaries.\n\nprint(fit12.16)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd \n   Data: depression_pp (Number of observations: 36997) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -4.25      0.07    -4.40    -4.11 1.00     1695     2016\nage_18        0.06      0.01     0.04     0.08 1.00     1690     2160\nIage_18E2    -0.01      0.00    -0.01    -0.01 1.00     2743     2942\nIage_18E3     0.00      0.00     0.00     0.00 1.00     2287     2843\npd            0.42      0.16     0.08     0.73 1.00     2187     1952\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit12.17)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female \n   Data: depression_pp (Number of observations: 36997) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -4.59      0.11    -4.80    -4.38 1.00     1696     2080\nage_18        0.06      0.01     0.04     0.08 1.00     1568     1848\nIage_18E2    -0.01      0.00    -0.01    -0.01 1.00     3259     3022\nIage_18E3     0.00      0.00     0.00     0.00 1.00     2284     2521\npd            0.41      0.16     0.08     0.72 1.00     2404     1984\nfemale        0.54      0.11     0.33     0.75 1.00     1583     2038\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBefore we make our version of Figure 12.4, we’ll need to make an aggregated version of the data, which will allow us to replicate the dots and plus signs. We’ll use the same basic wrangling steps from when we made sex_aggregated from back in Chapter 11.\n\ndepression_aggregated &lt;- depression_pp %&gt;% \n  mutate(event = if_else(event == 1, \"event\", \"no_event\")) %&gt;% \n  group_by(age_18) %&gt;% \n  count(event, pd) %&gt;% \n  ungroup() %&gt;% \n  complete(age_18, event, pd) %&gt;% \n  pivot_wider(names_from = event,\n              values_from = n) %&gt;% \n  mutate(pd    = factor(str_c(\"pd = \", pd), levels = str_c(\"pd = \", 1:0)),\n         age   = age_18 + 18,\n         total = event + no_event) %&gt;% \n  mutate(proportion = event / total) %&gt;% \n  mutate(logit = log(proportion / (1 - proportion)))\n\ndepression_aggregated\n\n# A tibble: 72 × 8\n   age_18 pd     event no_event   age total proportion logit\n    &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1    -14 pd = 0     1     1353     4  1354   0.000739 -7.21\n 2    -14 pd = 1    NA       39     4    NA  NA        NA   \n 3    -13 pd = 0    NA     1344     5    NA  NA        NA   \n 4    -13 pd = 1    NA       48     5    NA  NA        NA   \n 5    -12 pd = 0     4     1333     6  1337   0.00299  -5.81\n 6    -12 pd = 1     1       54     6    55   0.0182   -3.99\n 7    -11 pd = 0     5     1322     7  1327   0.00377  -5.58\n 8    -11 pd = 1    NA       60     7    NA  NA        NA   \n 9    -10 pd = 0     2     1313     8  1315   0.00152  -6.49\n10    -10 pd = 1     1       66     8    67   0.0149   -4.19\n# ℹ 62 more rows\n\n\nNow make Figure 12.4.\n\nnd &lt;- crossing(age_18 = -14:21,\n               pd     = 0:1) %&gt;% \n  mutate(age = age_18 + 18)\n  \n# hazard (top panel)\np1 &lt;- fitted(fit12.16, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(pd = factor(str_c(\"pd = \", pd),\n                     levels = str_c(\"pd = \", 1:0))) %&gt;% \n  \n  ggplot(aes(x = age, group = pd, color = pd)) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = depression_aggregated,\n             aes(y = proportion, shape = pd),\n             show.legend = F) +\n  scale_x_continuous(NULL, breaks = NULL, limits = c(0, 40)) +\n  scale_y_continuous(\"proportion experiencing event\", limits = c(0, 0.06))\n\n# logit(hazard) (top panel)\np2 &lt;- fitted(fit12.16,\n             newdata = nd,\n             scale = \"linear\") %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(pd = factor(str_c(\"pd = \", pd),\n                     levels = str_c(\"pd = \", 1:0))) %&gt;% \n  \n  ggplot(aes(x = age, group = pd, color = pd)) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = depression_aggregated,\n             aes(y = logit, shape = pd),\n             show.legend = F) +\n  scale_x_continuous(breaks = 0:8 * 5, limits = c(0, 40)) +\n  scale_y_continuous(\"logit(proportion experiencing event)\", limits = c(-8, -2))\n\n# combine\n(\n  (p1 / p2) & \n  scale_shape_manual(values = c(3, 16)) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) &\n  theme(panel.grid = element_blank())\n) +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n12.3.2 Interpreting and displaying time-varying predictors’ effects\nWe jumped the gun a little, but to repeat:\n\nyou can fit a discrete-time hazard model with time-varying predictors using exactly the same strategies presented in chapter 11. In the person-period data set, use a logistic regression routine to regress the event indicator on variables representing the main effect of TIME and the desired predictors. (pp. 434–435, emphasis in the original)\n\nFor example, here is the statistical formula we used when we fit fit12.17:\n\\[\n\\begin{align}\n\\text{event}_{ij}             & = \\operatorname{Binomial}(n = 1, p_{ij}) \\\\\n\\operatorname{logit} (p_{ij}) & = [\\alpha_0 + \\alpha_1 (\\text{age}_{ij} - 18) + \\alpha_2 (\\text{age}_{ij} - 18)^2 + \\alpha_3 (\\text{age}_{ij} - 18)^3] \\\\\n                              & \\; \\; + [\\beta_1 \\text{pd}_{ij} + \\beta_2 \\text{female}_{i}] \\\\\n\\alpha_0, ..., \\alpha_3       & \\sim \\operatorname{Normal}(0, 4) \\\\\n\\beta_1 \\text{ and } \\beta_2  & \\sim \\operatorname{Normal}(0, 4),\n\\end{align}\n\\]\nwhere \\(\\operatorname{logit} (p_{ij}) = \\operatorname{logit} \\hat h(t_{ij})\\). If you’d like to compare our results for those displayed by Singer and Willett in Equation 12.8, here are our posterior means.\n\nfixef(fit12.17)[, 1] %&gt;% round(digits = 4)\n\nIntercept    age_18 Iage_18E2 Iage_18E3        pd    female \n  -4.5858    0.0599   -0.0074    0.0002    0.4082    0.5403 \n\n\nThe values are pretty close. Though we won’t compare fit12.16 and fit12.17 using deviance values, we will use information criteria.\n\nfit12.16 &lt;- add_criterion(fit12.16, criterion = \"waic\")\nfit12.17 &lt;- add_criterion(fit12.17, criterion = \"waic\")\n\nloo_compare(fit12.16, fit12.17, criterion = \"waic\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit12.17     0.0       0.0 -2075.7      86.0          6.0     0.4    4151.3   172.0\nfit12.16   -11.9       5.0 -2087.6      86.4          4.9     0.4    4175.2   172.8\n\nmodel_weights(fit12.16, fit12.17, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit12.16 fit12.17 \n       0        1 \n\n\nBased on both the WAIC difference and the WAIC weights, fit12.17 is the clear favorite. From that model, Here’s a plot of the anti-logged (i.e., exponentiated) posteriors for pd and female, which yields their odds ratios.\n\nas_draws_df(fit12.17) %&gt;% \n  pivot_longer(b_pd:b_female) %&gt;% \n  mutate(`odds ratio` = exp(value)) %&gt;% \n  \n  ggplot(aes(x = `odds ratio`, y = name)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"xy\") +\n  ylab(NULL) +\n  coord_cartesian(ylim = c(1.4, 2.4)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere is our version of Figure 12.5.\n\nnd &lt;- crossing(female = 0:1,\n               pd     = 0:1) %&gt;% \n  expand_grid(age_18 = -14:21) %&gt;% \n  mutate(age = age_18 + 18)\n  \nf &lt;- fitted(fit12.17, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(sex = if_else(female == 0, \"male\", \"female\"),\n         pd  = factor(str_c(\"pd = \", pd),\n                      levels = str_c(\"pd = \", 1:0))) \n\n# hazard (top panel)\np1 &lt;- f %&gt;% \n  ggplot(aes(x = age, group = pd, color = pd, fill = pd)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5, linewidth = 0) +\n  geom_line(aes(y = Estimate)) +\n  scale_x_continuous(NULL, breaks = NULL, limits = c(0, 40)) +\n  scale_y_continuous(\"fitted hazard\", limits = c(0, 0.04))\n\n# survival (bottom panel)\np2 &lt;- f %&gt;%\n  group_by(sex, pd) %&gt;% \n  mutate_at(vars(Estimate, Q2.5, Q97.5), .funs = ~cumprod(1 - .)) %&gt;% \n  \n  ggplot(aes(x = age, group = pd, color = pd, fill = pd)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5, linewidth = 0) +\n  geom_line(aes(y = Estimate)) +\n  scale_x_continuous(breaks = 0:8 * 5, limits = c(0, 40)) +\n  scale_y_continuous(\"fitted survival probability\", limits = c(0, 1)) +\n  theme(strip.background.x = element_blank(),\n        strip.text.x = element_blank())\n\n# combine\n(\n  (p1 / p2) & \n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6) &\n  theme(panel.grid = element_blank()) &\n  facet_wrap(~ sex)\n) +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nNote how with this many age levels, our discrete-time models are beginning to look like continuous-time models.\n\n\n12.3.3 Two caveats: The problems of state and rate dependence\n\nA time-varying predictor is state-dependent if its values at time \\(t_j\\) are affected by an individual’s state (event occurrence status) at time \\(t_j\\): \\(EVENT_{ij}\\). A time-varying predictor is rate-dependent if its values at time \\(t_j\\) are affected by the individuals value of hazard (the “rate”) at time \\(t_j\\): \\(h (t_{ij})\\). (p. 440, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#the-linear-additivity-assumption-uncovering-violations-and-simple-solutions",
    "href": "12.html#the-linear-additivity-assumption-uncovering-violations-and-simple-solutions",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "12.4 The linear additivity assumption: Uncovering violations and simple solutions",
    "text": "12.4 The linear additivity assumption: Uncovering violations and simple solutions\n\nBecause the focus on hazard causes you to analyze group level summaries, model violations can be more difficult to discern [than in other kinds of regression models]. We therefore devote this section to introducing practical strategies for diagnosing and correcting violations of the linear additivity assumption. (p. 443)\n\n\n12.4.1 Interactions between substantive predictors\n\nWe do not advocate fishing expeditions. Open searches for interactions can be counterproductive, leading to the discovery of many “effects” that are little more than sampling variation. But there are at least two circumstances when a guided search for interactions is crucial:\n\nWhen theory (or common sense!) suggests that two (or more) predictors will interact in the prediction of the outcome. If you hypothesize the existence of interactions a priori, your search will be targeted and efficient.\nWhen examining the effects of “question” predictor(s), variables whose effects you intend to emphasize in your report. You need to be certain that these predictors’ effects do not differ according ot levels of other important predictors, lest you misinterpret your major findings.\n\nWith this in mind, we now demonstrate how to (1) explore your data for the possibility of statistical interactions; and (2) include the additional appropriate terms when necessary. (p. 444, emphasis in the original)\n\nLoad the data from Keiley and Martin (2002).1\n1 In their reference section, Singer and Willett indicated this was a manuscript submitted for publication. To my knowledge, it was never published.\nfirstarrest_pp &lt;- read_csv(\"data/firstarrest_pp.csv\") %&gt;% \n  # convert the column names to lower case\n  rename_all(tolower)\n\nglimpse(firstarrest_pp)\n\nRows: 15,834\nColumns: 19\n$ id     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ time   &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1…\n$ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ abused &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ black  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ablack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d8     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d9     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ d10    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ d11    &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ d12    &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ d13    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ d14    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ d15    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ d16    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ d17    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d18    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ period &lt;dbl&gt; 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 8, …\n$ event  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nThe total \\(N\\) is 1553.\n\nfirstarrest_pp %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1553\n\n\n\\(n = 887\\) were abused.\n\nfirstarrest_pp %&gt;% \n  filter(abused == 1) %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   887\n\n\n\\(n = 342\\) were arrested between the ages of 8 and 18.\n\nfirstarrest_pp %&gt;% \n  filter(censor == 0) %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   342\n\n\nSince the two focal predictors in this section with be black and abused, here are the counts broken down by both.\n\nfirstarrest_pp %&gt;% \n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  count(black, abused) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 4 × 4\n  black abused     n percent\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     0      0   434    27.9\n2     0      1   605    39.0\n3     1      0   232    14.9\n4     1      1   282    18.2\n\n\nHere’s how to hand compute the values for the sample logit(hazard) functions depicted in the top panels of Figure 12.6.\n\n# wrangle\nfirstarrest_pp %&gt;% \n  mutate(event = if_else(event == 1, \"event\", \"no_event\")) %&gt;% \n  group_by(period)  %&gt;% \n  count(event, black, abused) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = event,\n              values_from = n) %&gt;% \n  drop_na(event) %&gt;% \n  mutate(total  = event + no_event,\n         logit  = log(event / total / (1 - event / total)),\n         race   = factor(ifelse(black == 1, \"Black\", \"White\"),\n                         levels = c(\"White\", \"Black\")),\n         abused = ifelse(abused == 1, \"abused\", \"not abused\") %&gt;% factor()) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = period, y = logit)) +\n  geom_line(aes(color = abused, group = abused)) +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_x_continuous(\"age\", breaks = 7:19, limits = c(7, 19)) +\n  scale_y_continuous(\"sample logit(hazard)\", limits = c(-7, -2)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ race)\n\n\n\n\n\n\n\n\nFor kicks and giggles, it might informative to fit a series of subset Bayesian models to make similar versions of those sample hazard functions. Before we fit the models, let’s make our lives easier and make a factor version of our time variable, period.\n\nfirstarrest_pp &lt;- firstarrest_pp %&gt;% \n  mutate(period_f = factor(period))\n\nFit the four subset models.\n\n# white, not abused\nfit12.18 &lt;- brm(\n  data = firstarrest_pp %&gt;% filter(black == 0 & abused == 0),\n  family = binomial,\n  event | trials(1) ~ 0 + period_f,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.18\")\n\n# white, abused\nfit12.19 &lt;- update(\n  fit12.18,\n  newdata = firstarrest_pp %&gt;% filter(black == 0 & abused == 1),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.19\")\n\n# black, not abused\nfit12.20 &lt;- update(\n  fit12.18,\n  newdata = firstarrest_pp %&gt;% filter(black == 1 & abused == 0),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.20\")\n\n# black, abused\nfit12.21 &lt;- update(\n  fit12.18,\n  newdata = firstarrest_pp %&gt;% filter(black == 1 & abused == 1),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.21\")\n\nWe might use fitted() via our custom make_fitted() function to perform some of the pre-plotting computation and wrangling.\n\nnd &lt;- tibble(period_f = 8:18)\n\n# this will simplify the `mutate()` code below\nmodels &lt;- c(\"constant\", \"linear\", \"quadratic\", \"cubic\", \"general\")\n\nf &lt;- bind_rows(\n  make_fitted(fit12.18, scale = \"linear\"),      # white, not abused\n  make_fitted(fit12.19, scale = \"linear\"),      # white, abused\n  make_fitted(fit12.20, scale = \"linear\"),      # black, not abused\n  make_fitted(fit12.21, scale = \"linear\")) %&gt;%  # black, abused\n  mutate(race  = factor(rep(c(\"White\", \"Black\"), each = n() / 2),\n                        levels = c(\"White\", \"Black\")),\n         abuse = rep(c(\"not abused\", \"abused\", \"not abused\", \"abused\"), each = n() / 4))\n\n# what have we done?\nglimpse(f)\n\nRows: 44\nColumns: 7\n$ Estimate  &lt;dbl&gt; -6.099248, -7.792390, -5.430479, -4.484542, -7.799931, -4.127774, -3.844096, -3.041197, -3.663457, -3.644977, …\n$ Est.Error &lt;dbl&gt; 0.9356986, 1.7693566, 0.7074858, 0.4496668, 1.8320148, 0.3864176, 0.3437310, 0.2384953, 0.3143522, 0.3274626, …\n$ Q2.5      &lt;dbl&gt; -8.195173, -11.848842, -7.065758, -5.438153, -12.216967, -4.966447, -4.554072, -3.531641, -4.327254, -4.347736…\n$ Q97.5     &lt;dbl&gt; -4.613263, -5.187907, -4.278036, -3.678533, -5.147071, -3.439456, -3.212598, -2.600326, -3.098999, -3.045191, …\n$ period_f  &lt;int&gt; 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 13, 14, …\n$ race      &lt;fct&gt; White, White, White, White, White, White, White, White, White, White, White, White, White, White, White, White…\n$ abuse     &lt;chr&gt; \"not abused\", \"not abused\", \"not abused\", \"not abused\", \"not abused\", \"not abused\", \"not abused\", \"not abused\"…\n\n\nNow plot.\n\nf %&gt;% \n  ggplot(aes(x = period_f, y = Estimate, group = abuse, color = abuse)) +\n  geom_line() +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_x_continuous(\"age\", breaks = 7:19, limits = c(7, 19)) +\n  ylab(\"sample logit(hazard)\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ race)\n\n\n\n\n\n\n\n\nNotice how this looks different from the version of the plot, above, in that there are five points with very low values on the \\(y\\)-axis. Did you notice the drop_na(event) line in the code when we computed the sample logit(hazard) values by hand? Those points with missing values in the data are what caused those very low log(hazard) estimates in the models. Next we’ll fit the primary statistical models with both black and abused, without and with their interaction term, based on the full data set. You’ll see that when we use all cases, those odd low logit(hazard) values go away.\n\nfit12.22 &lt;- brm(\n  data = firstarrest_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d8 + d9 + d10  + d11  + d12  + d13  + d14  + d15  + d16  + d17  + d18 + abused + black,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.22\")\n\nfit12.23 &lt;- update(\n  fit12.22,\n  newdata = firstarrest_pp,\n  event | trials(1) ~ 0 + d8 + d9 + d10  + d11  + d12  + d13  + d14  + d15  + d16  + d17  + d18 + abused + black + abused:black,\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.23\")\n\nLet’s compare the models with information criteria.\n\nfit12.22 &lt;- add_criterion(fit12.22, criterion = \"waic\")\nfit12.23 &lt;- add_criterion(fit12.23, criterion = \"waic\")\n\nloo_compare(fit12.22, fit12.23, criterion = \"waic\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit12.23     0.0       0.0 -1538.7      65.3         13.8     1.1    3077.3   130.6\nfit12.22    -1.2       2.2 -1539.8      65.3         13.0     1.0    3079.7   130.7\n\nmodel_weights(fit12.22, fit12.23, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit12.22 fit12.23 \n   0.237    0.763 \n\n\nOur results diverge a bit from those in the text. For the deviance test on page 446, Singer and Willett reported a difference of \\(4.05 \\; (p &lt; .05, \\textit{df} = 1)\\) in favor of the full model (i.e., fit12.23). Though both our WAIC difference score and the WAIC weights favor fit12.23, it’s by a hair.\n“As in linear (or logistic) regression, we interpret interaction effects by simultaneously considering all the constituent parameters, for the cross-product term and its main-effect components” (p. 446, emphasis in the original). Rather than the point-estimate table Singer and Willett displayed at the bottom of the page, we’ll present the full posterior distributions odds ratios in a faceted plot.\n\nas_draws_df(fit12.23) %&gt;%\n  expand_grid(abused = 0:1,\n              black  = 0:1) %&gt;% \n  mutate(`odds ratio` = exp(b_abused * abused + b_black * black  + `b_abused:black` * abused * black),\n         abused       = str_c(\"abused = \", abused),\n         black        = str_c(\"black = \", black)) %&gt;% \n  \n  ggplot(aes(x = `odds ratio`)) +\n  stat_halfeye(.width = 0.95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_grid(abused ~ black)\n\n\n\n\n\n\n\n\nHere’s our version of the lower panel of Figure 12.6.\n\n# define the `newdata`\nnd &lt;- firstarrest_pp %&gt;% \n  distinct(abused, black, d8, d9, d10, d11, d12, d13, d14, d15, d16, d17, d18, period)\n\n# use `fitted()` and wrangle\nmake_fitted(fit12.23, scale = \"linear\") %&gt;% \n  mutate(abused = ifelse(abused == 1, \"abused\", \"not abused\") %&gt;% factor(),\n         sex    = factor(ifelse(black == 1, \"Black\", \"White\"),\n                         levels = c(\"White\", \"Black\"))) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5,\n             fill = abused, color = abused)) +\n  geom_ribbon(alpha = 1/5, linewidth = 0) +\n  geom_line() +\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_x_continuous(\"age\", breaks = 7:19, limits = c(7, 19)) +\n  ylab(\"fitted logit(hazard)\") +\n  coord_cartesian(ylim = c(-8, -2)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ sex)\n\n\n\n\n\n\n\n\n\n\n12.4.2 Nonlinear effects\n\nThere are two general strategies for exploring the linearity assumption. The simplest approach–although somewhat limited–is to fit additional models, replacing the raw predictor with a re-expressed version. Although the additional models also invoke a linearity constraint, use of re-expressed predictors guarantees that the effects represent nonlinear relationships for the raw predictors. The ladder of power (section 6.2.1) provides a dizzying array of options. The second approach is to categorize each continuous variable into a small number of groups, create a series of dummy variables representing group membership, and visually examine the pattern of parameter estimates for consecutive dummies to deduce the appropriate functional form. If the pattern is linear, retain the predictor in its raw state; if not, explore an alternative specification.\nAs the first approach is straightforward, we illustrate the second, using the depression onset data presented in section 12.3. (pp. 447–448, emphasis in the original)\n\nHere’s another look at those depression_pp data.\n\ndepression_pp %&gt;% \n  glimpse()\n\nRows: 36,997\nColumns: 22\n$ id        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,…\n$ onset     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ age       &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34…\n$ censor    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ censage   &lt;dbl&gt; 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34…\n$ aged      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ female    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ nsibs     &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2,…\n$ sibs12    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ sibs34    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sibs56    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ sibs78    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sibs9plus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ bigfamily &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ period    &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, …\n$ event     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pd        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pdnow     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ one       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age_18    &lt;dbl&gt; -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ age_18sq  &lt;dbl&gt; 196, 169, 144, 121, 100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 1…\n$ age_18cub &lt;dbl&gt; -2744, -2197, -1728, -1331, -1000, -729, -512, -343, -216, -125, -64, -27, -8, -1, 0, 1, 8, 27, 64, 125, 216, …\n\n\nOur three models will be cubic with respect to our time variable, age_18. The focal predictor whose form (non)linear form we’re interested in is nsibs. Singer and Willett’s first model, Model A (see Table 12.4, p. 449), treated nsibs as linear. Fit the model with brms.\n\n# model a (linear)  \n# 21.363 minutes\nfit12.24 &lt;- brm(\n  data = depression_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + nsibs,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.24\")\n\nCheck the model summary.\n\nprint(fit12.24)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + nsibs \n   Data: depression_pp (Number of observations: 36997) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -4.36      0.12    -4.60    -4.13 1.00     1742     2119\nage_18        0.06      0.01     0.04     0.08 1.00     3278     2629\nIage_18E2    -0.01      0.00    -0.01    -0.01 1.00     4216     3472\nIage_18E3     0.00      0.00     0.00     0.00 1.00     3917     3251\npd            0.35      0.17     0.03     0.66 1.00     1156     1401\nfemale        0.56      0.11     0.36     0.78 1.00     2080     1934\nnsibs        -0.08      0.02    -0.13    -0.04 1.00     2779     2654\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we invert the antilog of nsibs and summarize the posterior with median_qi().\n\nas_draws_df(fit12.24) %&gt;% \n  median_qi(1 / exp(b_nsibs)) %&gt;% \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 1 × 6\n  `1/exp(b_nsibs)` .lower .upper .width .point .interval\n             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1             1.09   1.04   1.14   0.95 median qi       \n\n\nSinger and Willett described nsibs as “highly skewed” (p. 448). Let’s take a look.\n\ndepression_pp %&gt;% \n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  \n  ggplot(aes(x = nsibs)) +\n  geom_bar() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nYep, sure is. Here’s a way to save Singer and Willett’s discretized version of nsibs and then break the cases down in terms of \\(n\\) and percentage, which matches nicely with the numbers at the bottom of page 449.\n\ndepression_pp &lt;- depression_pp %&gt;% \n  mutate(nsibs_cat = case_when(\n    nsibs == 0         ~ \"0\",\n    nsibs %in% c(1, 2) ~ \"1 or 2\",\n    nsibs %in% c(3, 4) ~ \"3 or 4\",\n    nsibs %in% c(5, 6) ~ \"5 or 6\",\n    nsibs %in% c(7, 8) ~ \"7 or 8\",\n    nsibs &gt;= 9         ~ \"9 or more\"\n  )\n  )\n\ndepression_pp %&gt;% \n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  count(nsibs_cat) %&gt;% \n  mutate(percent = (100 * n / sum(n)) %&gt;% round(digits = 1))\n\n# A tibble: 6 × 3\n  nsibs_cat     n percent\n  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 0            98     7  \n2 1 or 2      672    48.2\n3 3 or 4      330    23.7\n4 5 or 6      159    11.4\n5 7 or 8       72     5.2\n6 9 or more    62     4.5\n\n\nNow fit our version of Model B, in which we replace the original linear predictor nsibs with a series of dummies: sibs12, sibs34,…, sibs9plus.\n\n# model b (nonlinear)  \n# 17.33117  minutes\nfit12.25 &lt;- brm(\n  data = depression_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + sibs12 + sibs34 + sibs56 + sibs78 + sibs9plus,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.25\")\n\nHere’s the model summary.\n\nprint(fit12.25)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + sibs12 + sibs34 + sibs56 + sibs78 + sibs9plus \n   Data: depression_pp (Number of observations: 36997) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -4.51      0.21    -4.92    -4.13 1.01     1027     1673\nage_18        0.06      0.01     0.04     0.09 1.00     2649     2767\nIage_18E2    -0.01      0.00    -0.01    -0.01 1.00     5833     2903\nIage_18E3     0.00      0.00     0.00     0.00 1.00     3667     3528\npd            0.37      0.16     0.05     0.68 1.00     2762     2328\nfemale        0.56      0.11     0.35     0.77 1.00     2543     2582\nsibs12        0.03      0.19    -0.33     0.41 1.00     1060     1579\nsibs34        0.02      0.21    -0.37     0.43 1.00     1079     1357\nsibs56       -0.50      0.25    -0.97    -0.00 1.00     1315     1772\nsibs78       -0.81      0.35    -1.52    -0.17 1.00     1935     2748\nsibs9plus    -0.69      0.34    -1.39    -0.06 1.00     1607     1949\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt might be easier to compare the various sibs* coefficients with a coefficient plot.\n\nas_draws_df(fit12.25) %&gt;% \n  pivot_longer(contains(\"sibs\")) %&gt;% \n  mutate(name = str_remove(name, \"b_\")) %&gt;% \n  \n  ggplot(aes(x = value, y = name)) +\n  stat_interval(size = 5, .width = c(0.1, 0.5, 0.9)) +\n  scale_color_grey(\"CI level:\", start = 0.8, end = 0.2) +\n  labs(x = \"sibs coeficients\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere is the breakdown by bigfamily.\n\ndepression_pp %&gt;% \n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  count(bigfamily) %&gt;% \n  mutate(percent = (100 * n / sum(n)) %&gt;% round(digits = 2))\n\n# A tibble: 2 × 3\n  bigfamily     n percent\n      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1         0  1100    79.0\n2         1   293    21.0\n\n\nNow fit our version of Model C, the dichotomized bigfamily model.\n\n# model c (dichotomized)  \n# 24.89283 minutes\nfit12.26 &lt;- brm(\n  data = depression_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + bigfamily,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.26\")\n\nCheck the model summary.\n\nprint(fit12.26)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + Intercept + age_18 + I(age_18^2) + I(age_18^3) + pd + female + bigfamily \n   Data: depression_pp (Number of observations: 36997) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -4.48      0.11    -4.69    -4.28 1.00     1629     2161\nage_18        0.06      0.01     0.04     0.08 1.00     2424     2465\nIage_18E2    -0.01      0.00    -0.01    -0.01 1.00     3872     2473\nIage_18E3     0.00      0.00     0.00     0.00 1.00     3251     2696\npd            0.36      0.17     0.02     0.67 1.00     3036     2072\nfemale        0.56      0.11     0.35     0.78 1.00     2151     2362\nbigfamily    -0.62      0.15    -0.91    -0.33 1.00     2485     2412\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe summaries look very similar to the values in the rightmost column of Table 12.4 in the text. Here is a look at the antilog of our bigfamily coefficient.\n\n# just to make the x-axis pretty\nbreaks &lt;- c(exp(fixef(fit12.26)[\"bigfamily\", c(1, 3:4)]), 1) %&gt;% as.vector()\nlabels &lt;- c(exp(fixef(fit12.26)[\"bigfamily\", c(1, 3:4)]), 1) %&gt;% round(digits = 3) %&gt;% as.vector()\n\n# plot!\nas_draws_df(fit12.26) %&gt;% \n  ggplot(aes(x = exp(b_bigfamily))) +\n  geom_vline(xintercept = 1, color = \"white\") +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  scale_x_continuous(\"bigfamily odds ratio\", breaks = breaks, labels = labels) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNow compute the WAIC estimates for fit12.24 through fit12.26 and compare them by their WAIC differences and WAIC weights.\n\nfit12.24 &lt;- add_criterion(fit12.24, criterion = \"waic\")\nfit12.25 &lt;- add_criterion(fit12.25, criterion = \"waic\")\nfit12.26 &lt;- add_criterion(fit12.26, criterion = \"waic\")\n\nloo_compare(fit12.24, fit12.25, fit12.26, criterion = \"waic\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit12.26     0.0       0.0 -2066.5      85.6          7.0     0.5    4132.9   171.3\nfit12.24    -2.8       3.0 -2069.2      85.8          7.1     0.5    4138.5   171.6\nfit12.25    -3.5       1.1 -2070.0      85.9         11.0     0.7    4140.0   171.7\n\nmodel_weights(fit12.24, fit12.25, fit12.26, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit12.24 fit12.25 fit12.26 \n   0.058    0.027    0.915 \n\n\nOur WAIC estimates are very similar to the AIC estimates presented in Table 12.4. By both the differences and the weights, fit12.16 (Model C) is the best among the three. Though its rank is decisive with the weights, it’s less impressive if you compare the se_diff values with the elpd_diff values. Those suggest there’s a lot of uncertainty in our WAIC differences.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#the-proportionality-assumption-uncovering-violations-and-simple-solutions",
    "href": "12.html#the-proportionality-assumption-uncovering-violations-and-simple-solutions",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "12.5 The proportionality assumption: Uncovering violations and simple solutions",
    "text": "12.5 The proportionality assumption: Uncovering violations and simple solutions\n\nAll the discrete hazard models postulated so far invoke another common, but restrictive assumption: that each predictor has an identical effect in every time period under study. This constraint, known as the proportionality assumption, stipulates that a predictor’s effect does not depend on the respondent’s duration in the initial state….\nYet is it not possible, even likely, that the effects of some predictors will vary over time? (p. 451, emphasis in the original)\n\n\n12.5.1 Discrete-time hazard models that do not invoke a proportionality assumption\n“There are dozens of ways of violating the proportionality assumption” (p. 452). We see three such examples in panels B through D in Figure 12.7. Here’s our version of the plot.\n\np1 &lt;- crossing(z = 0:1,\n               x = 1:8) %&gt;% \n  mutate(y = -2.1 + -0.2 * (x - 1) + 1.1 * z,\n         z = factor(z)) %&gt;% \n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line(aes(linewidth = z))\n\np2 &lt;- crossing(z = 0:1,\n               x = 1:8) %&gt;% \n  mutate(y = -4.8 + 0.28 * (x - 1) + 0.01 * z + 0.25 * x * z,\n         z = factor(z)) %&gt;% \n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line(aes(linewidth = z))\n\np3 &lt;- crossing(z = 0:1,\n               x = 1:8) %&gt;% \n  mutate(y = -4.8 + 0.25 * (x - 1) + 4.3 * z + -0.5 * x * z,\n         z = factor(z)) %&gt;%\n\n  ggplot(aes(x = x, y = y)) +\n  geom_line(aes(linewidth = z))\n\np4 &lt;- crossing(z1 = 0:1,\n               x  = 1:8) %&gt;% \n  mutate(z2 = rep(0:1, times = n() / 2),\n         y  = -2.8 + -0.2 * (x - 1) + 1.8 * z1 + -1.4 * z1 * z2,\n         z1 = factor(z1)) %&gt;% \n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line(aes(linewidth = z1))\n\n# combine\n(\n  (p1 + p2 + p3 + p4) + \n  plot_annotation(tag_levels = \"A\") \n) &\n  scale_linewidth_manual(values = c(1, 1/2)) &\n  scale_x_continuous(\"time period\", breaks = 0:8, limits = c(0, 8)) &\n  scale_y_continuous(\"logit hazard\", limits = c(-5, -0.5)) &\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIn case is wasn’t clear, I just winged it on the equations for the y values in each subplot. If you’d like to annotate the subplots with the arrows and \\(\\beta\\) labels as depicted in the original Figure 12.7, consider it a homework exercise.\n\n\n12.5.2 Investigating the proportionality assumption in practice\nLoad the data from Graham’s (1997) dissertation.\n\nmathdropout_pp &lt;- read_csv(\"data/mathdropout_pp.csv\") %&gt;% \n  # convert the column names to lower case\n  rename_all(tolower)\n\nglimpse(mathdropout_pp)\n\nRows: 9,558\nColumns: 19\n$ id     &lt;dbl&gt; 201303, 201303, 201304, 201304, 201304, 201305, 201305, 201311, 201311, 201311, 201311, 201311, 201316, 201316, 2…\n$ lastpd &lt;dbl&gt; 2, 2, 3, 3, 3, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 1, 5, 5, 5, 5, 5, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,…\n$ female &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,…\n$ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,…\n$ one    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hs11   &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,…\n$ hs12   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,…\n$ coll1  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,…\n$ coll2  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ coll3  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ fhs11  &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ fhs12  &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ fcoll1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ fcoll2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ fcoll3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ period &lt;dbl&gt; 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,…\n$ event  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ ltime  &lt;dbl&gt; 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 0, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,…\n$ fltime &lt;dbl&gt; 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0,…\n\n\nThe data are composed of \\(n = 3{,}790\\) high school students.\n\nmathdropout_pp %&gt;% \n  distinct(id) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  3790\n\n\nHere is the division by female.\n\nmathdropout_pp %&gt;% \n  distinct(id, female) %&gt;% \n  count(female) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 2 × 3\n  female     n percent\n   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1      0  1875    49.5\n2      1  1915    50.5\n\n\nSinger and Willett also wrote: “only 93 men and 39 women took a mathematics class in each of the next five terms” (p. 456). I think this is a typo. Here’s the break down by censor and female at the fifth time period (period == 5).\n\nmathdropout_pp %&gt;% \n  filter(period == 5) %&gt;% \n  count(censor, female)\n\n# A tibble: 4 × 3\n  censor female     n\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1      0      0    39\n2      0      1    39\n3      1      0    93\n4      1      1    51\n\n\nFitting the three models displayed in Table 12.5 is a mild extension from our previous models. From a brm() perspective, there’s nothing new, here.\n\nfit12.27 &lt;- brm(\n  data = mathdropout_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.27\")\n\nfit12.28 &lt;- brm(\n  data = mathdropout_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.28\")\n\nfit12.29 &lt;- brm(\n  data = mathdropout_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female + fltime,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 12,\n  file = \"fits/fit12.29\")\n\nHeads up about the formula for fit12.28 (Model B). Many of the examples in the text and the corresponding data sets we’ve been working with included pre-computed interaction terms. We have been quietly ignoring those and making our interactions by hand in our formula arguments. Here we went ahead with the text and just used event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3. If you wanted a more verbose version of that code, we could have specified either\n\nevent | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female:hs11 + female:hs12 + female:coll1 + female:coll2 + female:coll3 or\nevent | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female:(hs11 + hs12 + coll1 + coll2 + coll3).\n\nAll three return the same results. Anyway, let’s check the model summaries.\n\nprint(fit12.27)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female \n   Data: mathdropout_pp (Number of observations: 9558) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nhs11      -2.13      0.06    -2.24    -2.02 1.00     3837     2896\nhs12      -0.94      0.05    -1.04    -0.85 1.00     3308     2845\ncoll1     -1.45      0.06    -1.57    -1.33 1.00     3747     3002\ncoll2     -0.62      0.07    -0.76    -0.47 1.01     4750     3125\ncoll3     -0.77      0.14    -1.07    -0.49 1.00     5386     2680\nfemale     0.38      0.05     0.28     0.48 1.00     2546     2761\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit12.28)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + fhs11 + fhs12 + fcoll1 + fcoll2 + fcoll3 \n   Data: mathdropout_pp (Number of observations: 9558) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nhs11      -2.01      0.07    -2.15    -1.87 1.00     3900     2832\nhs12      -0.96      0.06    -1.08    -0.85 1.00     4355     2809\ncoll1     -1.48      0.08    -1.65    -1.32 1.00     4052     3383\ncoll2     -0.71      0.10    -0.90    -0.52 1.00     3976     2809\ncoll3     -0.88      0.19    -1.24    -0.52 1.00     4318     3182\nfhs11      0.16      0.10    -0.03     0.35 1.00     3627     2439\nfhs12      0.42      0.08     0.26     0.57 1.00     3875     3157\nfcoll1     0.44      0.12     0.22     0.67 1.00     4092     3386\nfcoll2     0.57      0.14     0.29     0.86 1.00     4135     3059\nfcoll3     0.60      0.28     0.05     1.14 1.00     4362     3415\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(fit12.29)\n\n Family: binomial \n  Links: mu = logit \nFormula: event | trials(1) ~ 0 + hs11 + hs12 + coll1 + coll2 + coll3 + female + fltime \n   Data: mathdropout_pp (Number of observations: 9558) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nhs11      -2.05      0.06    -2.18    -1.92 1.00     2543     2852\nhs12      -0.92      0.05    -1.02    -0.83 1.00     3539     3104\ncoll1     -1.50      0.07    -1.63    -1.37 1.00     3470     2698\ncoll2     -0.72      0.08    -0.89    -0.56 1.00     3224     2950\ncoll3     -0.92      0.15    -1.22    -0.62 1.00     3167     2658\nfemale     0.23      0.08     0.08     0.38 1.00     2216     2421\nfltime     0.12      0.05     0.03     0.21 1.00     2119     2364\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe results are similar to those in Table 12.5. Based on fit12.27 (Model A), here is the posterior for the odds ratio for female.\n\nas_draws_df(fit12.27) %&gt;% \n  ggplot(aes(x = exp(b_female))) +\n  stat_halfeye(.width = c(0.5, 0.95)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"odds ratio for female\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWith regard to the interaction terms for fit12.28 (Model B), Singer and Willett remarked: “Notice how the estimates rise over time” (p. 457). It might be easiest to observe that in a plot.\n\nas_draws_df(fit12.28) %&gt;% \n  pivot_longer(starts_with(\"b_f\")) %&gt;% \n  mutate(name = factor(str_remove(name, \"b_\"),\n                       levels = c(str_c(\"fhs\", 11:12), str_c(\"fcoll\", 1:3)))) %&gt;%\n  \n  ggplot(aes(x = value, y = name)) +\n  stat_interval(size = 5, .width = c(0.1, 0.5, 0.9)) +\n  scale_color_grey(\"CI level:\", start = 0.8, end = 0.2) +\n  labs(x = \"interaction terms\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nMake and save the subplots for Figure 12.8.\n\n# Within-group sample hazard functions\np1 &lt;- mathdropout_pp %&gt;% \n  mutate(event = if_else(event == 1, \"event\", \"no_event\")) %&gt;% \n  group_by(period) %&gt;% \n  count(event, female) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = event,\n              values_from = n) %&gt;% \n  mutate(total = event + no_event,\n         logit = log(event / total / (1 - event / total))) %&gt;% \n  mutate(female = factor(female,\n                         levels = 1:0,\n                         labels = c(\"F\", \"M\"))) %&gt;% \n  \n  # plot\n  ggplot(aes(x = period, y = logit)) +\n  geom_line(aes(color = female),\n            show.legend = F) +\n  scale_y_continuous(\"sample logit(hazard)\", breaks = -2:0) +\n  labs(subtitle = \"Within-group sample hazard functions\")\n\n# Model A: Main effect of female\nnd &lt;- mathdropout_pp %&gt;% \n  distinct(female, period, hs11, hs12, coll1, coll2, coll3)\n\np2 &lt;- make_fitted(fit12.27, scale = \"linear\") %&gt;% \n  mutate(female = factor(female,\n                         levels = 1:0,\n                         labels = c(\"F\", \"M\"))) %&gt;% \n  \n  # plot\n  ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5,\n             fill = female, color = female)) +\n  geom_ribbon(alpha = 1/5, linewidth = 0) +\n  geom_line() +\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_y_continuous(\"fitted logit(hazard)\", breaks = -2:0) +\n  labs(subtitle = \"Model A: Main effect of female\")\n\n# Model B: Completely general\\ninteraction between female and time\nnd &lt;- mathdropout_pp %&gt;% \n  distinct(female, period, hs11, hs12, coll1, coll2, coll3, fhs11, fhs12, fcoll1, fcoll2, fcoll3)\n\np3 &lt;- make_fitted(fit12.28, scale = \"linear\") %&gt;% \n  mutate(female = factor(female,\n                         levels = 1:0,\n                         labels = c(\"F\", \"M\"))) %&gt;% \n  \n  # plot\n  ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5,\n             fill = female, color = female)) +\n  geom_ribbon(alpha = 1/5, linewidth = 0) +\n  geom_line() +\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_y_continuous(\"fitted logit(hazard)\", breaks = -2:0) +\n  labs(subtitle = \"Model B: Completely general\\ninteraction between female and time\")\n\n# Model C: Interaction\\nbetween female and time\nnd &lt;- mathdropout_pp %&gt;% \n  distinct(female, period, hs11, hs12, coll1, coll2, coll3, fltime)\n\np4 &lt;- make_fitted(fit12.29, scale = \"linear\") %&gt;% \n  mutate(female = factor(female,\n                         levels = 1:0,\n                         labels = c(\"F\", \"M\"))) %&gt;% \n  \n  # plot\n  ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5,\n             fill = female, color = female)) +\n  geom_ribbon(alpha = 1/5, linewidth = 0) +\n  geom_line() +\n  scale_fill_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) +\n  scale_y_continuous(\"fitted logit(hazard)\", breaks = -2:0) +\n  labs(subtitle = \"Model C: Interaction\\nbetween female and time\") \n\nNow combine the subplots, augment them in bulk, and return our version of Figure 12.8.\n\n(p1 + p2 + p3 + p4 + plot_layout(guides = \"collect\")) &\n  scale_color_viridis_d(NULL, option = \"A\", end = 0.6, direction = -1) &\n  scale_x_continuous(\"term\", breaks = 1:5,\n                     labels = c(\"HS 11\", \"HS 12\", \"C 1\", \"C 2\", \"C 3\")) &\n  coord_cartesian(ylim = c(-2.3, 0)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nCompute the information criteria for the three models. Since these were a bit faster to compute than for some of the earlier models in this chapter, we’ll go ahead and compute both the LOO and the WAIC.\n\nfit12.27 &lt;- add_criterion(fit12.27, criterion = c(\"loo\", \"waic\"))\nfit12.28 &lt;- add_criterion(fit12.28, criterion = c(\"loo\", \"waic\"))\nfit12.29 &lt;- add_criterion(fit12.29, criterion = c(\"loo\", \"waic\"))\n\nNow compare the models with information criteria differences and weights.\n\nloo_compare(fit12.27, fit12.28, fit12.29, criterion = \"loo\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic   se_looic\nfit12.29     0.0       0.0 -4905.7     51.3         6.8     0.1   9811.5   102.6 \nfit12.28    -2.4       1.2 -4908.1     51.3        10.0     0.2   9816.2   102.6 \nfit12.27    -2.4       2.6 -4908.2     51.2         6.0     0.1   9816.3   102.5 \n\nloo_compare(fit12.27, fit12.28, fit12.29, criterion = \"waic\") %&gt;% print(simplify = F)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic\nfit12.29     0.0       0.0 -4905.7      51.3          6.8     0.1    9811.5   102.6\nfit12.28    -2.4       1.2 -4908.1      51.3         10.0     0.2    9816.2   102.6\nfit12.27    -2.4       2.6 -4908.2      51.2          6.0     0.1    9816.3   102.5\n\nmodel_weights(fit12.27, fit12.28, fit12.29, weights = \"loo\") %&gt;% round(digits = 3)\n\nfit12.27 fit12.28 fit12.29 \n   0.075    0.078    0.847 \n\nmodel_weights(fit12.27, fit12.28, fit12.29, weights = \"waic\") %&gt;% round(digits = 3)\n\nfit12.27 fit12.28 fit12.29 \n   0.075    0.078    0.847 \n\n\nYou’ll note both the LOO and WAIC estimates are very close to those displayed in Table 12.5. When we take their standard errors into account, fit12.29 (Model C) is marginally better than the other two models. fit12.29 also took most of the LOO and WAIC weights.\n\nHow do we interpret the gender differential implied by Model C? Because we have centered TIME at 1, the coefficient for FEMALE (0.2275) estimates the differential in time period 1, which here is 11th grade. Antilogging yields 1.26, which leads us to estimate that in 11th grade, the odds of ending one’s mathematics course-taking career are 26% higher for females. (p. 460, emphasis in the original)\n\nLet’s examine those results with our Bayesian fit.\n\nas_draws_df(fit12.29) %&gt;% \n  mutate(`log odds` = b_female,\n         `odds ratio` = exp(b_female)) %&gt;% \n  pivot_longer(contains(\"odds\")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"the effects of female in two metrics\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nNow examine the odds ratio for three different educational periods.\n\nas_draws_df(fit12.29) %&gt;% \n  mutate(`12th grade`              = exp(b_female + b_fltime),\n         `1st semester of college` = exp(b_female + 2 * b_fltime),\n         `3rd semester of college` = exp(b_female + 4 * b_fltime)) %&gt;% \n  pivot_longer(contains(\" \")) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"The interaction effect in different periods\",\n       x = \"odds ratio\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#the-no-unobserved-heterogeneity-assumption-no-simple-solution",
    "href": "12.html#the-no-unobserved-heterogeneity-assumption-no-simple-solution",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "12.6 The no unobserved heterogeneity assumption: No simple solution",
    "text": "12.6 The no unobserved heterogeneity assumption: No simple solution\n\nAll the hazard models discussed in this book–both the discrete-time models we are discussing now and the continuous-time models we will soon introduce–impose an additional assumption to which we have alluded: the assumption of no unobserved heterogeneity. Every model assumes that the population hazard function for individual \\(i\\) depends only on his or her predictor values. Any pair of individuals who share identical predictor profiles will have identical population functions….\nMany data sets will not conform to this assumption. As in the multilevel model for change (and regular regression for that matter), pairs of individuals who share predictor profiles are very likely to have different outcomes….\nUnobserved heterogeneity can have serious consequences. In their classic (1985) paper, Vaupel and Yaskin elegantly demonstrate what they call “heterogeneity’s ruses”–that ability of unobserved heterogeneity to create the misimpression that a hazard function follows a particular form, when in fact it may not….\nIs it possible to fit a hazard model that accounts for unobserved heterogeneity? As you might expect, doing so requires that we have either additional data (for example, data on repeated events within individuals) or that we invoke other–perhaps less tenable–assumptions about the distribution of event time errors (Aalen, 1988; Heckman & Singer, 1984; Mare, 1994; Scheike & Jensen, 1997; Vaupel et al., 1979). As a result, most empirical researchers–and we–proceed ahead, if not ignoring the problem, at least not addressing it. In the remainder of this book, we assume that all heterogeneity is observed and attributable to the predictors included in our models.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#residual-analysis",
    "href": "12.html#residual-analysis",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "12.7 Residual analysis",
    "text": "12.7 Residual analysis\n\nBefore concluding that your model is sound, you should ascertain how well it performs for individual cases. As in regular regression, we [can] address this question by examining residuals.\nResiduals compare–usually through subtraction–an outcome’s “observed” value to its model-based “expected” value. For a discrete-time hazard model, a simple difference will not suffice because each person has not a single outcome but a set of outcomes–one for each time period when he or she was at risk. This suggests the need for a residual defined at the person-period level. A further complication is that the observed outcome in every time period has a value of either 0 or 1 while its expected value—the predicted hazard probability—lies between these extremes. (p. 463, emphasis in the original)\n\nStarting on page 464, Singer and Willett illustrated a residual analysis using Model D from Table 11.3. In the last chapter, we called that fit11.10. Here it is, again.\n\nfit11.10 &lt;- brm(\n  data = sex_pp,\n  family = binomial,\n  event | trials(1) ~ 0 + d7 + d8 + d9 + d10 + d11 + d12 + pt + pas,\n  prior(normal(0, 4), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 11,\n  file = \"fits/fit11.10\")\n\nWith brms, users can extract the residuals of a brm() fit with the residuals() function.\n\nresiduals(fit11.10) %&gt;% \n  str()\n\n num [1:822, 1:4] -0.092 -0.0495 0.8285 -0.0857 -0.046 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n\n\nThe output is similar to what we get from fitted(). We have a numeric array of 822 rows and 4 columns. There are 822 rows because that is the number of rows in the original data set with which we fit the model, sex_pp.\n\nsex_pp %&gt;% \n  glimpse()\n\nRows: 822\nColumns: 11\n$ id     &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 9, 9, 9, 9, 9, 9, 10, 10, …\n$ period &lt;dbl&gt; 7, 8, 9, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 7, 8, 9, 7, 8, 9, 10, 11…\n$ event  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ d7     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ d8     &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,…\n$ d9     &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ d10    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ d11    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ d12    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ pt     &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pas    &lt;dbl&gt; 1.9788670, 1.9788670, 1.9788670, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -0.5454916, -1.40498…\n\n\nJust as we often express the uncertainty in our Bayesian models with parameter summaries from the posterior, we also express the uncertainty of our residuals. Thus, the four columns returned by the residuals() function are the familiar summary columns of Estimate, Est.Error, Q2.5, and Q97.5. On page 465, Singer and Willett showcased the deviance residuals for eight participants. We’re going to diverge from them a little. In the plot, below, we’ll look at the residuals for the first 10 cases in the data, by period.\n\nresiduals(fit11.10) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(sex_pp) %&gt;%\n  mutate(event  = factor(event),\n         period = factor(str_c(\"period \", period),\n                         levels = str_c(\"period \", 7:12))) %&gt;% \n  # reduce the number of cases\n  filter(id &lt; 11) %&gt;% \n  \n  ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = event)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_pointrange(size = 1/10) +\n  scale_color_viridis_d(option = \"A\", end = 0.6) +\n  scale_x_continuous(breaks = 1:10, labels = rep(c(1, \"\", 10), times = c(1, 8, 1))) +\n  ylab(\"residual\") +\n  theme(legend.position = \"top\", \n        panel.grid = element_blank()) +\n  facet_wrap(~ period, nrow = 1)\n\n\n\n\n\n\n\n\nAs is often the case in coefficient plots, the dots are the posterior means and the intersecting lines are the percentile-based 95% intervals. In the sex_pp data, the event variable encodes when participants experience the event within a given time range. Hopefully the color coding highlights how with hazard models, the residuals are always positive when the criterion variable is a 1 and always negative when the criterion is 0.\nNow we have a bit of a handle on the output from residuals(), let’s plot in bulk.\n\nresiduals(fit11.10) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(sex_pp) %&gt;% \n  mutate(event = factor(event)) %&gt;% \n  \n  ggplot(aes(x = id, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = event)) +\n  geom_hline(yintercept = 0, color = \"white\") +\n  geom_pointrange(size = 1/10, alpha = 1/2) +\n  scale_color_viridis_d(option = \"A\", end = 0.6) +\n  ylab(\"residual\") +\n  theme(legend.position = \"top\", \n        panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nThis plot is our analogue to the top portion of Singer and Willett’s Figure 12.9. But whereas we plotted our residual summaries, they plotted the expected values of their deviance residuals. In contrast with the material on page 464, I am not going to discuss deviance residuals or the sums of the squared deviance residuals. Our brms workflow offers an alternative. Before we offer our alternative, we might focus on deviance residuals for just a moment:\n\nDeviance residuals are so named because, when squared, they represent an individual’s contribution to the deviance statistic for that time period. The sum of the squared deviance residuals across all the records in a person-period data set yields the deviance statistic for the specified model….\nThe absolute value of a deviance residual indicates how well the model fits that person’s data for that period. Large absolute values identify person-period records whose outcomes are poorly predicted. (p. 464)\n\nBack in Chapter 11, we computed the LOO for fit11.10. Let’s take a look at that.\n\nloo(fit11.10)\n\n\nComputed from 4000 by 822 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -322.7 17.6\np_loo         8.2  0.6\nlooic       645.5 35.2\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.8]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nNotice the part of the output that read “All Pareto k estimates are good (k &lt; 0.5).” We can pull those Pareto k estimates like so.\n\nloo(fit11.10)$diagnostics %&gt;% \n  data.frame() %&gt;% \n  glimpse()\n\nRows: 822\nColumns: 3\n$ pareto_k &lt;dbl&gt; 0.001821041, 0.073595944, 0.057567080, 0.148331279, -0.067743335, 0.036297195, -0.019416998, -0.047523633, -0.1…\n$ n_eff    &lt;dbl&gt; 2563.846, 3119.138, 2033.457, 5894.616, 5346.409, 4782.019, 4518.137, 5409.648, 4593.521, 4778.690, 4833.054, 4…\n$ r_eff    &lt;dbl&gt; 0.6420657, 0.7803870, 0.5732927, 1.4746762, 1.3370490, 1.1976025, 1.1333685, 1.3600917, 1.1618036, 1.1948692, 1…\n\n\nWe formatted the output for convenience. Notice there are 822 rows–one for each case in the data. Almost like a computational byproduct, brms returned pareto_k and n_eff values when we computed the LOO estimates for the model. Our focus will be on the pareto_k column. Here are those pareto_k values in a plot.\n\nloo(fit11.10)$diagnostics %&gt;% \n  data.frame() %&gt;% \n  # attach the `id` values\n  bind_cols(sex_pp) %&gt;% \n  \n  ggplot(aes(x = id, y = pareto_k)) +\n  geom_point(alpha = 3/4) + \n  geom_text(data = . %&gt;% filter(pareto_k &gt; .2),\n            aes(x = id + 2, label = id),\n            size = 3, hjust = 0) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo learn the technical details about pareto_k, check out Vehtari, Gelman, and Gabry’s (2017) Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC or Vehtari, Simpson, Gelman, Yao, and Gabry’s (2021) Pareto smoothed importance sampling. In short, we can use pareto_k values to flag cases that were overly-influential on the model in a way that’s a little like Singer and Willett’s deviance residuals. As pointed out in the loo reference manual (Gabry, 2020), the makers of the loo package warn against pareto_k values when they get much larger than \\(0.5\\). We should be a little worried by values that exceed the \\(0.7\\) threshold and it’s very likely a problem when they get larger than \\(1\\). In this case, they’re all below \\(0.4\\) and all is good.\nTo learn more about pareto_k values and what the loo package can do for you, check out Vehtari and Gabry’s (2020) vignette, Using the loo package.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#session-info",
    "href": "12.html#session-info",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 patchwork_1.3.2 brms_2.23.0     Rcpp_1.1.0      lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0  \n [8] dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] estimability_1.5.1      timechange_0.3.0        lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3         \n[16] magrittr_2.0.4          posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7             tools_4.5.1            \n[21] utf8_1.2.6              knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1    htmlwidgets_1.6.4      \n[26] bit_4.6.0               pkgbuild_1.4.8          curl_7.0.0              plyr_1.8.9              RColorBrewer_1.1-3     \n[31] multcomp_1.4-29         abind_1.4-8             withr_3.0.2             grid_4.5.1              stats4_4.5.1           \n[36] xtable_1.8-4            inline_0.3.21           emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65            \n[41] cli_3.6.5               mvtnorm_1.3-3           rmarkdown_2.30          crayon_1.5.3            generics_0.1.4         \n[46] RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000      \n[51] splines_4.5.1           bayesplot_1.15.0.9000   parallel_4.5.1          matrixStats_1.5.0       vctrs_0.6.5            \n[56] V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          arrayhelpers_1.1-0     \n[61] hms_1.1.4               bit64_4.6.0-1           ggdist_3.3.3            glue_1.8.0              codetools_0.2-20       \n[66] distributional_0.5.0    stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1          \n[71] htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1                vroom_1.6.6             evaluate_1.0.5         \n[76] lattice_0.22-7          backports_1.5.0         rstantools_2.5.0.9000   coda_0.19-4.1           gridExtra_2.3          \n[81] nlme_3.1-168            checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "12.html#comments",
    "href": "12.html#comments",
    "title": "12  Extending the Discrete-Time Hazard Model",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAalen, O. O. (1988). Heterogeneity in survival analysis. Statistics in Medicine, 7(11), 1121–1137. https://doi.org/10.1002/sim.4780071105\n\n\nBeck, N. (1999). Modelling space and time: The event history approach. In E. Scarbrough & E. Tanenbaum (Eds.), Research strategies in social science: A guide to new approaches. Oxford University Press. https://doi.org/10.1093/0198292376.001.0001\n\n\nBeck, Nathaniel, Katz, J. N., & Tucker, R. (1998). Taking time seriously: Time-series-cross-section analysis with a binary dependent variable. American Journal of Political Science, 42(4), 1260–1288. https://doi.org/10.2307/2991857\n\n\nGabry, J. (2020). loo reference manual, Version 2.4.1. https://CRAN.R-project.org/package=loo/loo.pdf\n\n\nGamse, B. C., & Conger, D. (1997). An evaluation of the Spencer post-doctoral dissertation program. Abt Associates.\n\n\nGraham, S. E. (1997). The exodus from mathematics: When and why? [PhD thesis]. Harvard Graduate School of Education.\n\n\nHeckman, J., & Singer, B. S. (Eds.). (1984). Longitudinal analysis of labor market data. Cambridge University Press. https://doi.org/10.1017/CCOL0521304539\n\n\nKeiley, M. K., & Martin, N. C. (2002). Child abuse, neglect, and juvenile delinquency: How “new” statistical approaches can inform our understanding of “old” questions—A reanalysis of Widom, 1989. Manuscript Submitted for Publication.\n\n\nMare, R. D. (1994). Discrete-time bivariate hazards with unobserved heterogeneity: A partially observed contingency table approach. Sociological Methodology, 341–383. https://doi.org/10.2307/270987\n\n\nScheike, T. H., & Jensen, T. K. (1997). A discrete survival model with random effects: An application to time to pregnancy. Biometrics. Journal of the International Biometric Society, 318–329. https://doi.org/10.2307/2533117\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nSueyoshi, G. T. (1995). A class of binary response models for grouped duration data. Journal of Applied Econometrics, 10(4), 411–431. https://doi.org/10.1002/jae.3950100406\n\n\nVaupel, J. W., Manton, K. G., & Stallard, E. (1979). The impact of heterogeneity in individual frailty on the dynamics of mortality. Demography, 16(3), 439–454. https://doi.org/10.2307/2061224\n\n\nVaupel, J. W., & Yashin, A. I. (1985). Heterogeneity’s ruses: Some surprising effects of selection on population dynamics. The American Statistician, 39(3), 176–185. https://doi.org/10.1080/00031305.1985.10479424\n\n\nVehtari, A., & Gabry, J. (2020). Using the loo package (version \\(&gt;\\)= 2.0.0). https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4\n\n\nVehtari, A., Simpson, D., Gelman, A., Yao, Y., & Gabry, J. (2021). Pareto smoothed importance sampling. https://arxiv.org/abs/1507.02646\n\n\nWheaton, B., Roszell, P., & Hall, K. (1997). The impact of twenty childhood and adult traumatic stressors on the risk of psychiatric disorder. In I. H. Gotlib & B. Wheaton (Eds.), Stress and adversity over the life course: Trajectories and turning points (pp. 50–72). Cambridge University Press. https://doi.org/10.1017/CBO9780511527623",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Extending the Discrete-Time Hazard Model</span>"
    ]
  },
  {
    "objectID": "13.html",
    "href": "13.html",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "",
    "text": "13.1 A framework for characterizing the distribution of continuous-time event data",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#a-framework-for-characterizing-the-distribution-of-continuous-time-event-data",
    "href": "13.html#a-framework-for-characterizing-the-distribution-of-continuous-time-event-data",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "",
    "text": "Variables measured with greater precision contain more information than those measured with less precision… Finer distinctions, as long as they can be made reliably, lead to more subtle interpretations and more powerful analyses.\nUnfortunately, a switch from discrete- to continuous-time survival analysis is not as trivial as you might hope. In discrete time, the definition of the hazard function is intuitive, its values are easily estimated, and simple graphic displays can illuminate its behavior. In continuous time, although the survivor function is easily defined and estimated, the hazard function is not. As explained below, we must revise its definition and develop new methods for its estimation and exploration. (p. 469)\n\n\n13.1.1 Salient features of continuous-time event occurrence data\n\nBecause continuous time is infinitely divisible, the distribution of event times displays two highly salient properties:\n\nThe probability of observing any particular event time is infinitesimally small. In continuous time, the probability that an event will occur at any specific instant approaches 0. The probability may nor reach 0, but as time’s divisions become finer and finer, it becomes smaller and smaller.\nThe probability that two or more individuals will share the same event time is also infinitesimally small. If the probability of event occurrence at each instant is infinitesimally small, the probability of cooccurrence (a tie) must be smaller still. (p. 470, emphasis in the original)\n\n\nLoad the horn honking data from Diekmann, Jungbauer-Gans, Krassing, and Lorenz (1996).\n\nlibrary(tidyverse)\n\nhonking &lt;- read_csv(\"data/honking.csv\") %&gt;%\n  # make all names lower case\n  rename_all(str_to_lower) %&gt;% \n  mutate(censor_1 = abs(censor - 1))\n\nglimpse(honking)\n\nRows: 57\nColumns: 4\n$ id       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, …\n$ seconds  &lt;dbl&gt; 2.88, 4.63, 2.36, 2.68, 2.50, 4.30, 1.86, 4.01, 1.41, 9.59, 4.44, 3.14, 2.83, 12.29, 4.96, 5.88, 2.12, 6.03, 4.…\n$ censor   &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, …\n$ censor_1 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, …\n\n\nHere’s a quick way to arrange the seconds values and censor status of each case in a similar way to how they appear in Table 13.1.\n\nhonking %&gt;% \n  arrange(seconds) %&gt;% \n  transmute(seconds = ifelse(censor == 0, seconds, str_c(seconds, \"*\")))\n\n# A tibble: 57 × 1\n   seconds\n   &lt;chr&gt;  \n 1 1.41   \n 2 1.41*  \n 3 1.51   \n 4 1.67   \n 5 1.68   \n 6 1.86   \n 7 2.12   \n 8 2.19   \n 9 2.36*  \n10 2.48   \n# ℹ 47 more rows\n\n\nFor kicks, here’s a tile-plot version of Table 13.1.\n\nhonking %&gt;% \n  arrange(seconds) %&gt;% \n  # `formatC()` allows us to retain the trailing zeros when converting the numbers to text\n  mutate(text = formatC(seconds, digits = 2, format = \"f\")) %&gt;% \n  mutate(time = ifelse(censor == 0, text, str_c(text, \"*\")),\n         row  = c(rep(1:6, times = 9), 1:3),\n         col  = rep(1:10, times = c(rep(6, times = 9), 3))) %&gt;% \n  \n  ggplot(aes(x = col, y = row)) +\n  geom_tile(aes(fill = seconds)) +\n  geom_text(aes(label = time, color = seconds &lt; 10)) +\n  scale_fill_viridis_c(option = \"B\", limits = c(0, NA)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  scale_y_reverse() +\n  labs(subtitle = \"Table 13.1: Known and censored (*) event times for 57 motorists blocked by another\\nautomobile (reaction times are recorded to the nearest hundredth of a second)\") +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n13.1.2 The survivor function\n“In continuous time, the survival probability for individual \\(i\\) at time \\(t_j\\) is the probability that his or her event time, \\(T_i\\) will exceed \\(t_j\\)” (p. 472). This follows the equation\n\\[S(t_{ij}) = \\Pr [T_i &gt; t_j].\\]\nHeads up: When Singer and Willett “do not distinguish individuals on the basis of predictors, [they] remove the subscript \\(i\\), letting \\(S(t_j)\\) represent the survivor function for a randomly selected member of the population” (p. 472).\n\n\n13.1.3 The hazard function\n\nThe hazard function assesses the risk–at a particular moment–that an individual who has not yet done so will experience the target event. In discrete time, the moments are time periods, which allows us to express hazard as a conditional probability. In continuous time, the moments are the infinite numbers of infinitesimally small instants of time that exist within any finite time period, a change that requires us to alter our definition. (pp. 472–473, emphasis in the original)\n\nSinger and Willett the went on to demonstrate the notion of “infinitesimally small instants of time” by dividing a year into days, hours, minutes, and seconds. Here’s how we might use R to practice dividing up a year into smaller and smaller units.\n\nyear    &lt;- 1\ndays    &lt;- 365\nhours   &lt;- 24\nminutes &lt;- 60\nseconds &lt;- 60\n\nyear * days\n\n[1] 365\n\nyear * days * hours\n\n[1] 8760\n\nyear * days * hours * minutes\n\n[1] 525600\n\nyear * days * hours * minutes * seconds\n\n[1] 31536000\n\n\nBuilding, we define the continuous-time hazard function as\n\\[h(t_{ij}) = \\text{limit as } \\Delta t \\rightarrow 0 \\left \\{ \\frac{\\Pr[T_i \\text{ is in the interval } (t_j, t_j + \\Delta t) | T_i \\geq t_j]}{\\Delta t} \\right \\},\\]\nwhere \\([t_j, t_j + \\Delta t)\\) is the \\(j\\)th time interval and “the opening phrase ‘\\(\\text{limit as } \\Delta t \\rightarrow 0\\)’ indicates that we evaluate the conditional probability in brackets as the interval width modes closer and closer to 0” (p. 474).\n\nBecause the definitions of hazard differ in continuous and discrete time, their interpretations differ as well. Most important, continuous-time hazard is not a probability. Instead, it is a rate, assessing the conditional probability of event occurrence per unit of time. No matter how tempted you might be to use the nomenclature of probability to describe rates in continuous time, please resist the urge. Rates and probabilities are not the same, and so the interpretive language is not interchangeable. (p. 474, emphasis in the original)\n\nClosing out this section, we read:\n\nAn important difference between continuous-time hazard rates and discrete-time hazard probabilities is that rates are not bounded from above. Although neither can be negative, rates can easily exceed 1.0…. The possibility that continuous-time hazard rate can exceed 1 has serious consequences because it requires that we revise the statistical models that incorporate the effects of predictors. We cannot posit a model in terms of logit hazard (as in discrete time) because that transformation is defined only for values of hazard between 0 and 1. As a result, when we specify continuous-time hazard models in chapter 14, our specification will focus on the logarithm of hazard, a transformation that is defines for all values of hazard greater than 0. (p. 475, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#grouped-methods-for-estimating-continuous-time-survivor-and-hazard-functions",
    "href": "13.html#grouped-methods-for-estimating-continuous-time-survivor-and-hazard-functions",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "13.2 Grouped methods for estimating continuous-time survivor and hazard functions",
    "text": "13.2 Grouped methods for estimating continuous-time survivor and hazard functions\n\nIn principle, in continuous time, we would like to estimate a value for the survivor and hazard functions at every possible instant when an event could occur. In practice, we can do so only if we are willing to adopt constraining parametric assumptions about the distribution of event times. To support this approach, statisticians have identified dozens of different distributions–Weibull, Gompertz, gamma, and log-logistic, to name a few–that event times might follow, and in some fields—industrial product testing, for example–parametric estimation is the dominant mode of analysis (see, e.g., Lawless, 1982).\nIn many other fields, including most of the social, behavioral, and medical sciences, nonparametric methods are more popular. The fundamental advantage of nonparametric methods is that we need not make constraining assumptions about the distribution of event times. This flexibility is important because: (1) few researchers have a sound basis for preferring one distribution over another; and (2) adopting an incorrect assumption can lead to erroneous conclusions. With a nonparametric approach, you essentially trade the possibility of a minor increase in efficiency if a particular assumption holds for the guarantee of doing nearly as well for most data sets, regardless of its tenability.\nFor decades, in a kind of mathematic irony, statisticians obtained nonparametric estimates of the continuous-time survivor and hazard functions by grouping event times into a small number of intervals, constructing a life table, and applying the discrete-time strategies of chapter 10 (with some minor revisions noted below). In this section we describe two of the most popular of these grouped strategies: the discrete-time method (section 13.2.1) and the actuarial method (section 13.2.2). (pp. 475–476, emphasis in the original)\n\nAs we’ll see, brms supports parametric and nonparametric continuous-time survival models. In the sections and chapters to come, we will make extensive use of the Cox model, which is nonparametric. However, if you look through the Time-to-event models section of Bürkner’s (2021) vignette, Parameterization of response distributions in brms, you’ll see brms supports survival models with the exponential, inverse-Gaussian, gamma, log-normal, and Weibull likelihoods.\n\n13.2.1 Constructing a grouped life table\n\nGrouped estimation strategies begin with a life table that partitions continuous time into a manageable number of contiguous intervals. When choosing a partition, you should seek one that is: (1) substantively meaningful; (2) coarse enough to yield stable estimates; and (3) fine enough to reveal discernible patterns. (p. 476)\n\nFor the first step in making the life table of Table 13.2, we’ll make variables that partition the seconds column of the honking data into lower and upper bounds.\n\nhonking &lt;- honking %&gt;% \n  mutate(lb = case_when(\n    seconds &lt; 2 ~ 1,\n    seconds &lt; 3 ~ 2,\n    seconds &lt; 4 ~ 3,\n    seconds &lt; 5 ~ 4,\n    seconds &lt; 6 ~ 5,\n    seconds &lt; 7 ~ 6,\n    seconds &lt; 8 ~ 7,\n    seconds &gt;= 8 ~ 8\n  )) %&gt;% \n  mutate(ub = if_else(lb == 8, 18, lb + 1)) %&gt;% \n  mutate(time_interval = str_c(\"[\", lb, \", \", ub, \")\"))\n\nhonking %&gt;% head()\n\n# A tibble: 6 × 7\n     id seconds censor censor_1    lb    ub time_interval\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n1     1    2.88      0        1     2     3 [2, 3)       \n2     2    4.63      1        0     4     5 [4, 5)       \n3     3    2.36      1        0     2     3 [2, 3)       \n4     4    2.68      0        1     2     3 [2, 3)       \n5     5    2.5       0        1     2     3 [2, 3)       \n6     6    4.3       1        0     4     5 [4, 5)       \n\n\nNow we’ll transform the data into a life-table format, which we’ll save as honking_aggregated.\n\n honking_aggregated &lt;- honking %&gt;% \n  mutate(event = ifelse(censor == 0, \"n_events\", \"n_censored\")) %&gt;% \n  count(lb, event) %&gt;% \n  pivot_wider(names_from = event,\n              values_from = n) %&gt;% \n  mutate(ub = if_else(lb == 8, 18, lb + 1)) %&gt;% \n  mutate(time_interval = str_c(\"[\", lb, \", \", ub, \")\")) %&gt;% \n  mutate(n_censored = ifelse(is.na(n_censored), 0, n_censored)) %&gt;% \n  mutate(total = n_censored + n_events) %&gt;% \n  mutate(n_at_risk = sum(total) - cumsum(lag(total, default = 0))) %&gt;% \n  select(lb, ub, time_interval, n_at_risk, n_events, n_censored) %&gt;% \n  mutate(`p(t)` = n_events / n_at_risk)\n\nhonking_aggregated\n\n# A tibble: 8 × 7\n     lb    ub time_interval n_at_risk n_events n_censored `p(t)`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1     1     2 [1, 2)               57        5          1 0.0877\n2     2     3 [2, 3)               51       14          3 0.275 \n3     3     4 [3, 4)               34        9          2 0.265 \n4     4     5 [4, 5)               23        6          4 0.261 \n5     5     6 [5, 6)               13        2          2 0.154 \n6     6     7 [6, 7)                9        2          2 0.222 \n7     7     8 [7, 8)                5        1          0 0.2   \n8     8    18 [8, 18)               4        3          1 0.75  \n\n\n\n\n13.2.2 The discrete-time method\nHere we simply apply the discrete-time hazard model to our discretized continuous-time data. Before we fit the model, we’ll define a new term, \\(\\hat p(t_j)\\). Recall back to Section 10.2 where we defined the hazard function \\(\\hat h(t_{j})\\) as\n\\[\\hat h(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j}.\\]\nNow we’re working with continuous-time data (even if they’re momentarily discretized), we focus instead on \\(\\hat p(t_{j})\\). In words, \\(\\hat p(t_{j})\\) is the conditional probability that a member of the risk set at the beginning of the interval \\(j\\) will experience the target event during that interval. In discrete time we labeled this quantity “hazard,” but now we use the term “conditional probability” to distinguish it from a continuous time hazard rate. Our conditional probability follows the formula\n\\[\\hat p(t_{j}) = \\frac{n \\text{ events}_j}{n \\text{ at risk}_j},\\]\nwhere \\(n \\text{ events}_j\\) is the number of individuals who experienced the event in the \\(j^{th}\\) period and \\(n \\text{ at risk}_j\\) is the number of those at risk at the beginning of the interval \\(j\\).\nTime to fire up brms.\n\nlibrary(brms)\nlibrary(tidybayes)\n\nFor our first model, we will use the binomial likelihood with the aggregated version of the honking data, honking_aggregated. The main time variable in those data is time_interval, the lower and upper bounds for which are identified in the lb and ub columns, respectively. In anticipation of the upcoming plots, we’ll use the ub variable for time. But to make fitting the model easier with the brm() function, we’ll first save a factor version of the variable.\n\nhonking_aggregated &lt;- honking_aggregated %&gt;% \n  mutate(ub_f = factor(ub))\n\nIn the last chapter, we used the normal(0, 4) prior, which was permissive in the log-odds metric. Here we’ll be more conservative and use normal(0, 1.5), which is weakly-regularizing on the log-odds metric, but flat in the probability metric. A plot might help show this.\n\nset.seed(13)\n\ntibble(`log odds` = rnorm(1e6, mean = 0, sd = 1.5)) %&gt;% \n  mutate(probability = plogis(`log odds`)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"prior predictive distribution\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nOtherwise, this model is just like any of the other unconditional discrete-time models we’ve fit with brm().\n\nfit13.1 &lt;- brm(\n  data = honking_aggregated,\n  family = binomial,\n  n_events | trials(n_at_risk) ~ 0 + ub_f,\n  prior(normal(0, 1.5), class = b),\n  chains = 4, cores = 1, iter = 2000, warmup = 1000,\n  seed = 13,\n  file = \"fits/fit13.01\")\n\nCheck the parameter summary.\n\nprint(fit13.1)\n\n Family: binomial \n  Links: mu = logit \nFormula: n_events | trials(n_at_risk) ~ 0 + ub_f \n   Data: honking_aggregated (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nub_f2     -2.21      0.43    -3.13    -1.44 1.00     7509     2887\nub_f3     -0.95      0.31    -1.59    -0.38 1.00     7866     2857\nub_f4     -0.99      0.39    -1.78    -0.27 1.00     8305     2956\nub_f5     -1.00      0.47    -1.98    -0.12 1.00     6816     3079\nub_f6     -1.48      0.66    -2.89    -0.29 1.00     8041     2611\nub_f7     -1.08      0.69    -2.47     0.20 1.00     8507     3266\nub_f8     -1.02      0.84    -2.77     0.49 1.00     7275     3188\nub_f18     0.79      0.91    -0.87     2.71 1.00     7497     2610\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs will become apparent in a bit, our normal(0, 1.5) prior was not inconsequential. Our aggregated data were composed of the event/censoring information of 57 cases, spread across 8 time periods. This left little information in the likelihood, particularly for the later time periods. As a consequence, the prior left clear marks in the posterior.\nAs with the discrete-time model, we can formally define the survivor function for the continuous-time model as\n\\[\\hat S(t_j) =  \\big(1 - \\hat p(t_1)\\big) \\big(1 - \\hat p(t_2)\\big)... \\big(1 - \\hat p(t_j)\\big).\\]\nIt’ll take a little wrangling effort to transform the output from as_draws_df(fit13.1) into a useful form for plotting and summarizing \\(\\hat S(t_j)\\). We’ll save it as s.\n\ns &lt;- as_draws_df(fit13.1) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  mutate_all(plogis) %&gt;% \n  mutate(b_ub_f0 = 0) %&gt;% \n  select(b_ub_f0, everything()) %&gt;% \n  set_names(c(1:8, 18)) %&gt;% \n  mutate(iter = 1:n()) %&gt;% \n  pivot_longer(-iter,\n               names_to = \"time\",\n               values_to = \"p\") %&gt;% \n  mutate(time = time %&gt;% as.integer()) %&gt;% \n  group_by(iter) %&gt;% \n  mutate(survivor = cumprod(1 - p)) %&gt;% \n  ungroup()\n\nNow we can make the first 6 columns of Table 13.2 by combining a subset of the honking_aggregated data with a summary of our s.\n\nbind_cols(\n  # select the first 5 columns for Table 13.2\n  honking_aggregated %&gt;% \n    select(time_interval:`p(t)`),\n  # add the 6th column\n  s %&gt;% \n    filter(time &gt; 1) %&gt;% \n    group_by(time) %&gt;% \n    summarise(median = median(survivor),\n              sd     = sd(survivor)) %&gt;% \n    mutate_if(is.double, round, digits = 4) %&gt;% \n    transmute(`S(t)` = str_c(median, \" (\", sd, \")\"))\n)\n\n# A tibble: 8 × 6\n  time_interval n_at_risk n_events n_censored `p(t)` `S(t)`         \n  &lt;chr&gt;             &lt;dbl&gt;    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          \n1 [1, 2)               57        5          1 0.0877 0.8989 (0.0386)\n2 [2, 3)               51       14          3 0.275  0.6442 (0.0625)\n3 [3, 4)               34        9          2 0.265  0.4633 (0.0662)\n4 [4, 5)               23        6          4 0.261  0.3322 (0.0638)\n5 [5, 6)               13        2          2 0.154  0.262 (0.0611) \n6 [6, 7)                9        2          2 0.222  0.1892 (0.0555)\n7 [7, 8)                5        1          0 0.2    0.1308 (0.05)  \n8 [8, 18)               4        3          1 0.75   0.0405 (0.0311)\n\n\nYou’ll note that our posterior summary values in S(t) differ a little from those in the text. Remember, the likelihood was weak and we used a regularizing prior. If we had more cases spread across fewer discretized time periods, the likelihood would have done a better job updating the prior.\nNow let’s take a look at the posterior of our survivor function, \\(\\hat S(t_j)\\), in our version of the upper left panel of Figure 13.1.\n\ns %&gt;% \n  ggplot(aes(x = time, y = survivor)) +\n  geom_hline(yintercept = 0.5, color = \"white\") +\n  stat_lineribbon(alpha = 1/2) +\n  # add the ML-based survival estimates\n  geom_line(data = honking_aggregated %&gt;% mutate(s = cumprod(1 - `p(t)`)),\n            aes(x = ub, y = s),\n            color = \"red\") +\n  scale_fill_grey(\"CI level\", start = 0.7, end = 0.4) +\n  scale_x_continuous(\"seconds after light turns green\", limits = c(0, 20)) +\n  ylab(expression(widehat(italic(S(t[j]))))) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFor a little context, we superimposed the sample (ML) estimates of the survivor function in red. Based on the posterior median, our median lifetime appears to be between the time intervals of \\([2, 3)\\) and \\([3, 4)\\). Sticking with those medians, here’s the exact number using Miller’s (1981) interpolation approach from Section 10.2.2.\n\ns_medians &lt;- s %&gt;% \n  mutate(lb = time - 1,\n         ub = time) %&gt;% \n  mutate(time_interval = str_c(\"[\", lb, \", \", ub, \")\")) %&gt;% \n  filter(ub %in% c(3, 4)) %&gt;% \n  group_by(time_interval) %&gt;% \n  summarise(median = median(survivor)) %&gt;% \n  pull(median)\n\n3 + (s_medians[1] - .5) / (s_medians[1] - s_medians[2]) * (4 - 3)\n\n[1] 3.797289\n\n\nHere’s how we might convert the output of as_draws_df(fit13.1) into a useful format for our hazard function.\n\nh &lt;- as_draws_df(fit13.1) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  mutate_all(plogis) %&gt;% \n  set_names(c(2:8, 18)) %&gt;% \n  mutate(iter = 1:n()) %&gt;% \n  pivot_longer(-iter,\n               names_to = \"time\",\n               values_to = \"p\") %&gt;% \n  mutate(time = time %&gt;% as.integer())\n\nh\n\n# A tibble: 32,000 × 3\n    iter  time      p\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1     1     2 0.120 \n 2     1     3 0.331 \n 3     1     4 0.493 \n 4     1     5 0.113 \n 5     1     6 0.0781\n 6     1     7 0.437 \n 7     1     8 0.142 \n 8     1    18 0.900 \n 9     2     2 0.110 \n10     2     3 0.226 \n# ℹ 31,990 more rows\n\n\nFor continuous-time data, hazard is a rate, which is\n\nthe limit of the conditional probability of event occurrence in a (vanishingly small) interval divided by the interval’s width. A logical estimator is thus the ratio of the conditional probability of event occurrence in an interval to the interval’s width. (p. 479)\n\nThus, our new definition of hazard is\n\\[\\hat h(t_j) = \\frac{\\hat p(t_j)}{\\text{width}_j},\\]\nwhere \\(\\text{width}_j\\) denotes the width of the \\(j\\)th interval. The widths of most of our intervals were 1 (seconds). The final interval, \\([8, 18)\\), had a width of ten. Once we add that information to the h data, we can use the formula above to convert \\(\\hat p(t_j)\\) to \\(\\hat h(t_j)\\).\n\nh &lt;- h %&gt;% \n  mutate(width = if_else(time &lt;= 8, 1, 10)) %&gt;% \n  mutate(hazard = p / width)\n\nNow we can make the first 7 columns of Table 13.2 by adding an cleaned-up version of our h object to what we had before.\n\nbind_cols(\n  # select the first 5 columns for Table 13.2\n  honking_aggregated %&gt;% \n    select(time_interval:`p(t)`),\n  # add the 6th column\n  s %&gt;% \n    filter(time &gt; 1) %&gt;% \n    group_by(time) %&gt;% \n    summarise(median = median(survivor),\n              sd     = sd(survivor)) %&gt;% \n    mutate_if(is.double, round, digits = 4) %&gt;% \n    transmute(`S(t)` = str_c(median, \" (\", sd, \")\")),\n  # add the 7th column\n  h %&gt;% \n    group_by(time) %&gt;% \n    summarise(median = median(hazard),\n              sd     = sd(hazard)) %&gt;% \n    mutate_if(is.double, round, digits = 4) %&gt;% \n    transmute(`h(t)` = str_c(median, \" (\", sd, \")\"))\n)\n\n# A tibble: 8 × 7\n  time_interval n_at_risk n_events n_censored `p(t)` `S(t)`          `h(t)`         \n  &lt;chr&gt;             &lt;dbl&gt;    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          \n1 [1, 2)               57        5          1 0.0877 0.8989 (0.0386) 0.1011 (0.0386)\n2 [2, 3)               51       14          3 0.275  0.6442 (0.0625) 0.279 (0.0616) \n3 [3, 4)               34        9          2 0.265  0.4633 (0.0662) 0.2742 (0.0756)\n4 [4, 5)               23        6          4 0.261  0.3322 (0.0638) 0.2729 (0.0902)\n5 [5, 6)               13        2          2 0.154  0.262 (0.0611)  0.1924 (0.0978)\n6 [6, 7)                9        2          2 0.222  0.1892 (0.0555) 0.2569 (0.1245)\n7 [7, 8)                5        1          0 0.2    0.1308 (0.05)   0.2719 (0.1518)\n8 [8, 18)               4        3          1 0.75   0.0405 (0.0311) 0.0676 (0.0175)\n\n\nPerhaps even more so than with our estimates for the survivor function, our hazard estimates show the influence of our prior on the posterior. For example, note how our posterior standard deviations tend to be a bit smaller than the standard errors reported in the text. To my mind, plotting the marginal posteriors for the intervals of our hazard function really helps hit this home.\n\nh %&gt;% \n  mutate(time = factor(time)) %&gt;% \n  \n  ggplot(aes(x = hazard, y = time)) +\n  geom_vline(xintercept = 0.5, color = \"white\") +\n  stat_halfeye(.width = c(0.5, 0.95), normalize = \"xy\") +\n  xlim(0, 1) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAs wide and sloppy as those distributions look, they’re more precise than the estimates returned by Maximum Likelihood (ML). To finish this section out with the lower left panel of Figure 13.1, here’s what our hazard function looks like.\n\nh %&gt;% \n  ggplot(aes(x = time, y = hazard)) +\n  stat_lineribbon(alpha = 1/2) +\n  scale_fill_grey(\"CI level\", start = 0.7, end = 0.4) +\n  scale_x_continuous(\"seconds after light turns green\", limits = c(0, 20)) +\n  ylab(expression(widehat(italic(h(t[j]))))) +\n  coord_cartesian(ylim = c(0, 0.35)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n13.2.3 The actuarial method\nI’m not going to dive into a full explanation of the actuarial method. For that, read the book. However, the actuarial method presents a challenge for our brms paradigm. To appreciate the challenge, we’ll need a couple block quotes:\n\nFor the survivor function, we ask: What does it mean to be “at risk of surviving” past the end of an interval? Because a censored individual is no longer “at risk of surviving” once censoring occurs, we redefine each interval’s risk set to account for the censoring we assume to occur equally throughout. This implies that half the censored individuals would no longer be at risk half-way through, so we redefine the number of individuals “at risk of surviving past interval \\(j\\)” to be:\n\\[n' \\; at \\; risk_j = n \\; at \\; risk_j - \\frac{n \\; censored_j}{2}.\\]\nThe actuarial estimate of the survivor function is obtained by substituting \\(n' \\; at \\; risk_j\\) for \\(n \\; at \\; risk_j\\) in the discrete-time formulas just presented in section 13.2.2 (equations 13.3 and 13.4). (pp. 480–481, emphasis in the original)\n\nFurther:\n\nTo estimate the hazard function using the actuarial approach, we again redefine what it means to be “at risk.” Now, however, we ask about the “risk of event occurrence” during the interval, not the “risk of survival” past the interval. This change of definition suggests that each interval’s risk set should be diminished not just by censoring but also by event occurrence, because either eliminates the possibility of subsequent event occurrence. Because categorization continues to prevent us from knowing precisely when people leave the risk set, we assume that exits are scattered at random throughout the interval. This implies that half these individuals are no longer at risk of event occurrence halfway through, so we redefine the number of individuals “at risk of event occurrence” in interval \\(j\\) to be:\n\\[n'' \\; at \\; risk_j = n \\; at \\; risk_j - \\frac{n \\; censored_j}{2} - \\frac{n \\; events_j}{2}.\\]\nThe actuarial estimator of the continuous-time hazard function is then obtained by substituting \\(n'' \\; at \\; risk_j\\) for \\(n \\; at \\; risk_j\\) in discrete-time formulas of section 13.2.2 (equations 13.3 and 13.5). (pp. 481–481, emphasis in the original)\n\nHere’s how we might implement the equations for \\(n' \\; at \\; risk_j\\) and \\(n'' \\; at \\; risk_j\\) in our honking_aggregated data.\n\nhonking_aggregated &lt;- honking_aggregated %&gt;% \n  mutate(n_p_at_risk_a  = n_at_risk - (n_censored / 2),\n         n_pp_at_risk_a = n_at_risk - (n_censored / 2) - (n_events / 2))\n\nhonking_aggregated\n\n# A tibble: 8 × 10\n     lb    ub time_interval n_at_risk n_events n_censored `p(t)` ub_f  n_p_at_risk_a n_pp_at_risk_a\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1     1     2 [1, 2)               57        5          1 0.0877 2              56.5           54  \n2     2     3 [2, 3)               51       14          3 0.275  3              49.5           42.5\n3     3     4 [3, 4)               34        9          2 0.265  4              33             28.5\n4     4     5 [4, 5)               23        6          4 0.261  5              21             18  \n5     5     6 [5, 6)               13        2          2 0.154  6              12             11  \n6     6     7 [6, 7)                9        2          2 0.222  7               8              7  \n7     7     8 [7, 8)                5        1          0 0.2    8               5              4.5\n8     8    18 [8, 18)               4        3          1 0.75   18              3.5            2  \n\n\nThe essence of our problem is we’ve been using the binomial likelihood to fit discrete-time hazard functions with brms. In this paradigm, we feed in data composed of the number of successes and the corresponding number of trials to compute probability \\(p\\) of a success within a given trial. Although \\(p\\) is a continuous value ranging from 0 to 1, the binomial likelihood takes the numbers of successes and trials to be non-negative integers. If you try to feed our actuarial \\(n'' \\; at \\; risk_j\\) variable, n_at_risk_pp into the formula argument for brms::brm() (e.g., n_events | trials(n_at_risk_pp) ~ 0 + ub_f), brms will return the warning:\n\nError: Number of trials must be positive integers.\n\nWe can, however, follow along and compute the ML estimates by hand.\n\nhonking_aggregated &lt;- honking_aggregated %&gt;% \n  mutate(`S(t)_a` = cumprod(1 - n_events / n_p_at_risk_a),\n         `p(t)_a` = n_events / n_pp_at_risk_a,\n         width    = if_else(lb == 8, 10, 1)) %&gt;% \n  mutate(`h(t)_a` = `p(t)_a` / width)\n\nhonking_aggregated\n\n# A tibble: 8 × 14\n     lb    ub time_interval n_at_risk n_events n_censored `p(t)` ub_f  n_p_at_risk_a n_pp_at_risk_a `S(t)_a` `p(t)_a` width\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     1     2 [1, 2)               57        5          1 0.0877 2              56.5           54     0.912    0.0926     1\n2     2     3 [2, 3)               51       14          3 0.275  3              49.5           42.5   0.654    0.329      1\n3     3     4 [3, 4)               34        9          2 0.265  4              33             28.5   0.475    0.316      1\n4     4     5 [4, 5)               23        6          4 0.261  5              21             18     0.340    0.333      1\n5     5     6 [5, 6)               13        2          2 0.154  6              12             11     0.283    0.182      1\n6     6     7 [6, 7)                9        2          2 0.222  7               8              7     0.212    0.286      1\n7     7     8 [7, 8)                5        1          0 0.2    8               5              4.5   0.170    0.222      1\n8     8    18 [8, 18)               4        3          1 0.75   18              3.5            2     0.0243   1.5       10\n# ℹ 1 more variable: `h(t)_a` &lt;dbl&gt;\n\n\nBefore we make our version of the right-hand side of Figure 13.1, we’ll need to augment the data a little. Then geom_step() will do most of the magic.\n\n# add `S(t)_a` values for `lb = c(0, 18)`\np1 &lt;- honking_aggregated %&gt;% \n  select(lb, `S(t)_a`) %&gt;% \n  bind_rows(tibble(lb       = c(0, 18),\n                   `S(t)_a` = c(1, honking_aggregated$`S(t)_a`[8]))) %&gt;% \n  # reorder\n  arrange(lb) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = lb, y = `S(t)_a`)) +\n  geom_step() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(expression(widehat(italic(S(t[j])))), limits = c(0, 1))\n\n# add `h(t)_a` values for `lb = 18`\np2 &lt;- honking_aggregated %&gt;% \n  select(lb, `h(t)_a`) %&gt;% \n  bind_rows(tibble(lb       = 18, \n                   `h(t)_a` = honking_aggregated$`h(t)_a`[8])) %&gt;% \n  arrange(lb) %&gt;% \n  \n  ggplot(aes(x = lb, y = `h(t)_a`)) +\n  geom_step() +\n  scale_x_continuous(\"seconds after light turns green\", limits = c(0, 20)) +\n  scale_y_continuous(expression(widehat(italic(h(t[j])))), limits = c(0, 0.35))\n\nCombine the subplots and make our version of the right-hand side of Figure 13.1.\n\nlibrary(patchwork)\n\n(p1 / p2) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nI’m not going to bother with computing the ML standard errors for the actuarial survivor and hazard estimates. You can reference page 482 in the text for more on those.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#the-kaplan-meier-method-of-estimating-the-continuous-time-survivor-function",
    "href": "13.html#the-kaplan-meier-method-of-estimating-the-continuous-time-survivor-function",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "13.3 The Kaplan-Meier method of estimating the continuous-time survivor function",
    "text": "13.3 The Kaplan-Meier method of estimating the continuous-time survivor function\n\nA fundamental problem with grouped estimation methods is that they artificially categorize what is now, by definition, a continuous variable…. Shouldn’t it be possible to use the observed data–the actual event times–to describe the distribution of event occurrences? This compelling idea underlies the Kaplan-Meier method, named for the statisticians who demonstrated (in 1958) that the intuitive approach–also known as the product-limit method–has maximum likelihood properties as well. Below, we explain how this approach works and why it is preferable.\nThe Kaplan-Meier method is a simple extension of the discrete-time method with a fundamental change: instead of rounding event times to construct the intervals, capitalize on the raw event times and construct intervals so that each contains just one observed event time (as shown in table 13.3). Each Kaplan-Meier interval begins at one observed event time and ends just before the next. (p. 483, emphasis in the original)\n\nAs discussed in the prose in the middle of page 483, here are the first three event times.\n\nhonking %&gt;% \n  filter(censor == 0) %&gt;% \n  top_n(-3, seconds)\n\n# A tibble: 3 × 7\n     id seconds censor censor_1    lb    ub time_interval\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n1     9    1.41      0        1     1     2 [1, 2)       \n2    40    1.51      0        1     1     2 [1, 2)       \n3    50    1.67      0        1     1     2 [1, 2)       \n\n\n\nThe Kaplan-Meier estimate of the survivor function is obtained by applying the discrete-time estimator of section 13.2.2 to the data of these intervals. [Most] statistical packages include a routine for computing and plotting the estimates. Numerically, the process is simple: first compute the conditional probability of event occurrence (column 7) and then successively multiply the complements of these probabilities together to obtain the Kaplan-Meier estimate of the survivor function (column 8). Because the Kaplan-Meier estimator of the survivor function is identical to the discrete-time estimator of chapter 10, its standard errors (column 9) are estimated using the same formula (pp. 483–485).\n\nTo walk this out, we’ll first use the frequentist survival package.\n\nlibrary(survival)\n\nUse the survival::survfit() function to fit the unconditional model with the Kaplan-Meier estimator.\n\n fit13.2 &lt;- survfit(\n   data = honking,\n   Surv(seconds, censor_1) ~ 1)\n\nThe summary() returns a lot of output.\n\nsummary(fit13.2)\n\nCall: survfit(formula = Surv(seconds, censor_1) ~ 1, data = honking)\n\n  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  1.41     57       1   0.9825  0.0174      0.94896        1.000\n  1.51     55       1   0.9646  0.0246      0.91758        1.000\n  1.67     54       1   0.9467  0.0299      0.88985        1.000\n  1.68     53       1   0.9289  0.0343      0.86405        0.999\n  1.86     52       1   0.9110  0.0380      0.83950        0.989\n  2.12     51       1   0.8931  0.0412      0.81587        0.978\n  2.19     50       1   0.8753  0.0441      0.79296        0.966\n  2.48     48       1   0.8570  0.0468      0.77004        0.954\n  2.50     47       1   0.8388  0.0492      0.74765        0.941\n  2.53     46       1   0.8206  0.0514      0.72572        0.928\n  2.54     45       1   0.8023  0.0534      0.70418        0.914\n  2.56     44       1   0.7841  0.0552      0.68299        0.900\n  2.62     43       1   0.7659  0.0569      0.66212        0.886\n  2.68     42       1   0.7476  0.0584      0.64154        0.871\n  2.83     39       1   0.7285  0.0599      0.61996        0.856\n  2.88     38       1   0.7093  0.0614      0.59868        0.840\n  2.89     37       1   0.6901  0.0626      0.57769        0.824\n  2.92     36       1   0.6710  0.0637      0.55695        0.808\n  2.98     35       1   0.6518  0.0647      0.53648        0.792\n  3.14     33       1   0.6320  0.0657      0.51549        0.775\n  3.17     32       1   0.6123  0.0666      0.49477        0.758\n  3.21     31       1   0.5925  0.0673      0.47429        0.740\n  3.22     30       1   0.5728  0.0679      0.45405        0.723\n  3.24     29       1   0.5530  0.0684      0.43404        0.705\n  3.56     27       1   0.5325  0.0688      0.41338        0.686\n  3.57     26       1   0.5121  0.0692      0.39297        0.667\n  3.58     25       1   0.4916  0.0694      0.37282        0.648\n  3.78     24       1   0.4711  0.0694      0.35291        0.629\n  4.10     22       1   0.4497  0.0695      0.33217        0.609\n  4.18     21       1   0.4283  0.0694      0.31172        0.588\n  4.44     19       1   0.4057  0.0693      0.29028        0.567\n  4.51     18       1   0.3832  0.0690      0.26919        0.545\n  4.52     17       1   0.3606  0.0686      0.24847        0.523\n  4.96     14       1   0.3349  0.0683      0.22451        0.500\n  5.39     12       1   0.3070  0.0681      0.19875        0.474\n  5.73     11       1   0.2791  0.0674      0.17386        0.448\n  6.03      9       1   0.2481  0.0666      0.14651        0.420\n  6.30      7       1   0.2126  0.0659      0.11585        0.390\n  7.20      5       1   0.1701  0.0650      0.08044        0.360\n  9.59      4       1   0.1276  0.0611      0.04991        0.326\n 12.29      3       1   0.0851  0.0535      0.02478        0.292\n 13.18      2       1   0.0425  0.0403      0.00665        0.272\n\n\nTaking a cue from the good folks at IDRE, saving the summary results as an object will make it easy to subset and augment that information into our version of Table 13.1.\n\n# save the summary as t\nt &lt;- summary(fit13.2)\n\n# subset, augment, and save as honking_km\nhonking_km &lt;- tibble(seconds  = t$time,\n                     n_risk   = t$n.risk,\n                     n_events = t$n.event) %&gt;% \n  mutate(`p(t)`     = n_events / n_risk,\n         n_censored = n_risk - n_events - lead(n_risk, default = 0),\n         interval   = 1:n(),\n         interval_f = factor(1:n(), levels = 0:n()),\n         start      = seconds,\n         end        = lead(seconds, default = Inf)) %&gt;% \n  select(interval:interval_f, seconds, start:end, n_risk:n_events, n_censored, `p(t)`)\n\nhonking_km\n\n# A tibble: 42 × 9\n   interval interval_f seconds start   end n_risk n_events n_censored `p(t)`\n      &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1        1 1             1.41  1.41  1.51     57        1          1 0.0175\n 2        2 2             1.51  1.51  1.67     55        1          0 0.0182\n 3        3 3             1.67  1.67  1.68     54        1          0 0.0185\n 4        4 4             1.68  1.68  1.86     53        1          0 0.0189\n 5        5 5             1.86  1.86  2.12     52        1          0 0.0192\n 6        6 6             2.12  2.12  2.19     51        1          0 0.0196\n 7        7 7             2.19  2.19  2.48     50        1          1 0.02  \n 8        8 8             2.48  2.48  2.5      48        1          0 0.0208\n 9        9 9             2.5   2.5   2.53     47        1          0 0.0213\n10       10 10            2.53  2.53  2.54     46        1          0 0.0217\n# ℹ 32 more rows\n\n\nWe haven’t bothered adding the top interval == 0 row, but one could add that information with a little bind_rows() labor. Note how we slipped in a few extra columns (e.g., interval_f) because they’ll come in handy, later. We compute the Kaplan-Meier estimates for the survivor function by serially multiplying the compliments for the estimates of the conditional probability values, \\(\\hat p(t)\\). As in other examples, that’s just a little cumprod() code.\n\nhonking_km &lt;- honking_km %&gt;% \n  mutate(`S(t)` = cumprod(1 - `p(t)`))\n\nInstead of doing that by hand, we could have just subset the surv vector within t.\n\nt$surv\n\n [1] 0.98245614 0.96459330 0.94673046 0.92886762 0.91100478 0.89314195 0.87527911 0.85704413 0.83880914 0.82057416 0.80233918\n[12] 0.78410420 0.76586922 0.74763424 0.72846413 0.70929402 0.69012391 0.67095380 0.65178369 0.63203267 0.61228165 0.59253063\n[23] 0.57277961 0.55302859 0.53254605 0.51206351 0.49158097 0.47109843 0.44968486 0.42827130 0.40573070 0.38319011 0.36064951\n[34] 0.33488883 0.30698143 0.27907403 0.24806580 0.21262783 0.17010227 0.12757670 0.08505113 0.04252557\n\n\nHere we subset the standard errors for \\(\\hat S(t)\\).\n\nhonking_km &lt;- honking_km %&gt;% \n  mutate(`se[S(t)]` = t$std.err)\n\nhead(honking_km)\n\n# A tibble: 6 × 11\n  interval interval_f seconds start   end n_risk n_events n_censored `p(t)` `S(t)` `se[S(t)]`\n     &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1        1 1             1.41  1.41  1.51     57        1          1 0.0175  0.982     0.0174\n2        2 2             1.51  1.51  1.67     55        1          0 0.0182  0.965     0.0246\n3        3 3             1.67  1.67  1.68     54        1          0 0.0185  0.947     0.0299\n4        4 4             1.68  1.68  1.86     53        1          0 0.0189  0.929     0.0343\n5        5 5             1.86  1.86  2.12     52        1          0 0.0192  0.911     0.0380\n6        6 6             2.12  2.12  2.19     51        1          0 0.0196  0.893     0.0412\n\n\nWe can plot the fitted \\(\\hat S(t)\\) values with `geom_step() to make our version of the top half of Figure 13.2.\n\np1 &lt;- honking_km %&gt;% \n  select(seconds, `S(t)`) %&gt;% \n  bind_rows(tibble(seconds = 17.15, \n                   `S(t)`  = 0.04252557)) %&gt;% \n  \n  ggplot(aes(x = seconds, y = `S(t)`)) +\n  geom_step() +\n  scale_x_continuous(NULL, breaks = NULL, limits = c(0, 20))\n\nNow add the actuarial and discrete-time estimates to make our version of the lower panel of Figure 13.2.\n\narrow &lt;- tibble(\n  x    = c(5, 8.7, 2.7),\n  y    = c(0.8, 0.4, 0.23),\n  xend = c(2.6, 7, 3.9),\n  yend = c(0.875, 0.25, 0.33))\n\ntext &lt;- tibble(\n  x     = c(5, 8.7, 2.7),\n  y     = c(0.77, 0.43, 0.2),\n  label = c(\"Kaplan Meier\", \"Discrete-time\", \"Actuarial\"))\n\np2 &lt;- honking_km %&gt;% \n  select(seconds, `S(t)`) %&gt;% \n  bind_rows(tibble(seconds = 17.15, \n                   `S(t)`  = 0.04252557)) %&gt;% \n  \n  ggplot(aes(x = seconds, y = `S(t)`)) +\n  geom_step() +\n  geom_step(data = honking_aggregated,\n            aes(x = ub, y = `S(t)_a`), \n            linetype = 3, direction = \"vh\") +\n  geom_line(data = honking_aggregated %&gt;% mutate(s = cumprod(1 - `p(t)`)),\n            aes(x = ub, y = s),\n            linetype = 2) +\n  geom_segment(data = arrow,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3, arrow = arrow(length = unit(0.15,\"cm\"), type = \"closed\")) +\n  geom_text(data = text,\n            aes(x = x, y = y, label = label)) +\n  scale_x_continuous(\"seconds after light turns green\", limits = c(0, 20))\n\nCombine and plot.\n\n(p1 / p2) &\n  scale_y_continuous(expression(widehat(italic(S(t[j])))), limits = c(0, 1)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nDid you notice how in both subplots we used bind_rows() to add in an S(t) value for seconds = 17.15? In the text, we read:\n\nAs for actuarial estimates, we plot Kaplan-Meier estimates as a step function that associates the estimated probability with the entire interval. If the largest event time is censored, as it is here (17.15), we extend the step for the last estimate out to that largest censored value. (pp. 485–486)\n\nThose bind_rows() lines are what extended “the step for the last estimate.” Since that last estimate for \\(\\hat S(t_j)  = 0.04252557\\), we set S(t) = 0.04252557 in the plot data.\nWe can get the median lifetime by executing print(fit13.2).\n\nprint(fit13.2)\n\nCall: survfit(formula = Surv(seconds, censor_1) ~ 1, data = honking)\n\n      n events median 0.95LCL 0.95UCL\n[1,] 57     42   3.58    3.17    4.96\n\n\nLike in the text, it’s 3.58.\nEven though there is no Kaplan-Meier estimate for hazard, we can compute a Kaplan-Meier type hazard with the formula\n\\[\\hat h_\\text{KM} (t_j) = \\frac{\\hat p_\\text{KM} (t_j)}{\\text{width}_j}.\\]\nHere we compute both the \\(\\text{width}_j\\) and \\(\\hat h_\\text{KM} (t_j)\\) values by hand.\n\nhonking_km &lt;- honking_km %&gt;% \n  mutate(width = end - start) %&gt;% \n  mutate(width = if_else(end == Inf, 17.15 - start, width)) %&gt;%  # this might be wrong\n  mutate(`h[km](t)` = `p(t)` / width)\n\nhonking_km\n\n# A tibble: 42 × 13\n   interval interval_f seconds start   end n_risk n_events n_censored `p(t)` `S(t)` `se[S(t)]`  width `h[km](t)`\n      &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1        1 1             1.41  1.41  1.51     57        1          1 0.0175  0.982     0.0174 0.100      0.175 \n 2        2 2             1.51  1.51  1.67     55        1          0 0.0182  0.965     0.0246 0.16       0.114 \n 3        3 3             1.67  1.67  1.68     54        1          0 0.0185  0.947     0.0299 0.0100     1.85  \n 4        4 4             1.68  1.68  1.86     53        1          0 0.0189  0.929     0.0343 0.180      0.105 \n 5        5 5             1.86  1.86  2.12     52        1          0 0.0192  0.911     0.0380 0.26       0.0740\n 6        6 6             2.12  2.12  2.19     51        1          0 0.0196  0.893     0.0412 0.0700     0.280 \n 7        7 7             2.19  2.19  2.48     50        1          1 0.02    0.875     0.0441 0.29       0.0690\n 8        8 8             2.48  2.48  2.5      48        1          0 0.0208  0.857     0.0468 0.0200     1.04  \n 9        9 9             2.5   2.5   2.53     47        1          0 0.0213  0.839     0.0492 0.0300     0.709 \n10       10 10            2.53  2.53  2.54     46        1          0 0.0217  0.821     0.0514 0.0100     2.17  \n# ℹ 32 more rows\n\n\nSinger and Willett remarked that “because the interval width varies widely (and is itself a function of the distribution of event times), the resulting estimates vary from one interval to the next. Their values are usually so erratic that pattern identification is nearly impossible” (p. 487). That sounds fun. Let’s explore them in a plot!\n\nhonking_km %&gt;% \n  ggplot(aes(x = start, y = `h[km](t)`)) +\n  geom_path() +\n  geom_point() +\n  scale_x_continuous(\"seconds after light turns green\", limits = c(0, 15)) +\n  scale_y_continuous(expression(hat(italic(h))[KM](italic(t[j])))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nErratic, indeed.\n\n13.3.1 Fitting a Bayesian Kaplan-Meier model\nIt’s worth repeating one of the quotes from earlier:\n\nThe Kaplan-Meier method is a simple extension of the discrete-time method with a fundamental change: instead of rounding event times to construct the intervals, capitalize on the raw event times and construct intervals so that each contains just one observed event time (as shown in table 13.3). Each Kaplan-Meier interval begins at one observed event time and ends just before the next. (p. 483, emphasis in the original)\n\nWhether we’re working with one event occurrence at a time or binning them in intervals, the basic product of the model is a conditional probability. The binomial likelihood served us well when we binned the event occurrences into largish time intervals, and it will work just the same when we work them in serial fashion. The biggest obstacle is properly setting up the data. After some experimentation, the easiest way to format the data properly is to just use the summary results from survfit(data = honking, Surv(seconds, censor_1) ~ 1), which we saved as honking_km. Take another look.\n\nglimpse(honking_km)\n\nRows: 42\nColumns: 13\n$ interval   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30…\n$ interval_f &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30…\n$ seconds    &lt;dbl&gt; 1.41, 1.51, 1.67, 1.68, 1.86, 2.12, 2.19, 2.48, 2.50, 2.53, 2.54, 2.56, 2.62, 2.68, 2.83, 2.88, 2.89, 2.92, 2…\n$ start      &lt;dbl&gt; 1.41, 1.51, 1.67, 1.68, 1.86, 2.12, 2.19, 2.48, 2.50, 2.53, 2.54, 2.56, 2.62, 2.68, 2.83, 2.88, 2.89, 2.92, 2…\n$ end        &lt;dbl&gt; 1.51, 1.67, 1.68, 1.86, 2.12, 2.19, 2.48, 2.50, 2.53, 2.54, 2.56, 2.62, 2.68, 2.83, 2.88, 2.89, 2.92, 2.98, 3…\n$ n_risk     &lt;dbl&gt; 57, 55, 54, 53, 52, 51, 50, 48, 47, 46, 45, 44, 43, 42, 39, 38, 37, 36, 35, 33, 32, 31, 30, 29, 27, 26, 25, 2…\n$ n_events   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ n_censored &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 1, 1…\n$ `p(t)`     &lt;dbl&gt; 0.01754386, 0.01818182, 0.01851852, 0.01886792, 0.01923077, 0.01960784, 0.02000000, 0.02083333, 0.02127660, 0…\n$ `S(t)`     &lt;dbl&gt; 0.98245614, 0.96459330, 0.94673046, 0.92886762, 0.91100478, 0.89314195, 0.87527911, 0.85704413, 0.83880914, 0…\n$ `se[S(t)]` &lt;dbl&gt; 0.01738929, 0.02459209, 0.02992911, 0.03428307, 0.03799347, 0.04123439, 0.04410945, 0.04680819, 0.04923621, 0…\n$ width      &lt;dbl&gt; 0.10, 0.16, 0.01, 0.18, 0.26, 0.07, 0.29, 0.02, 0.03, 0.01, 0.02, 0.06, 0.06, 0.15, 0.05, 0.01, 0.03, 0.06, 0…\n$ `h[km](t)` &lt;dbl&gt; 0.17543860, 0.11363636, 1.85185185, 0.10482180, 0.07396450, 0.28011204, 0.06896552, 1.04166667, 0.70921986, 2…\n\n\nIn the n_events column we have the number of cases that experienced the event in a given moment in continuous time. In the n_risk column we have the number of possible cases that could have experienced the event at that moment. In the interval_f column we’ve saved each moment as a factor, conveniently named \\(1, 2,..., 42\\). Thus we can fit a simple Bayesian Kaplan-Meier model with brms::brm() by specifying formula = n_events | trials(n_risk) ~ 0 + interval_f.\nConsider the implications for our priors. Because we’re treating each instance in time as a factor, that means the number of cases experiencing the event in one of those factors will always be 1 or some other small number in the unusual case of a tie. But the number in the denominator, n_risk, will tend to be relatively large, which means the probabilities will tend to be small. The weakly regularizing prior approach centered on zero might not make sense in this context.\nIn earlier models, we used normal(0, 4) and normal(0, 1.5). Here’s what those look like when we convert them to the probability metric.\n\nset.seed(13)\n\ntibble(sd = c(4, 1.5)) %&gt;% \n  mutate(prior    = factor(str_c(\"normal(0, \", sd, \")\"),\n                           levels = str_c(\"normal(0, \", c(1, 1.5, 4), \")\")),\n         log_odds = map(sd, rnorm, n = 1e5, mean = 0)) %&gt;% \n  unnest(log_odds) %&gt;% \n  mutate(p  = plogis(log_odds)) %&gt;% \n  \n  ggplot(aes(x = p, y = prior)) +\n  stat_histinterval(.width = c(0.5, 0.95), normalize = \"xy\") +\n  labs(x = expression(italic(p)),\n       y = \"prior (log-odds scale)\") +\n  coord_cartesian(ylim = c(1.5, 2.5)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe normal(0, 4) prior does not reflect our expectation that the probabilities will tend to be small. Interestingly, normal(0, 4) pushes a lot of the prior mass to the edges. What we want is a prior that pushed the mass to the left. Here are some options:\n\ncrossing(mean = -4:-1,\n         sd   = 1:4) %&gt;% \n  mutate(log_odds = map2(mean, sd, rnorm, n = 1e5)) %&gt;% \n  unnest(log_odds) %&gt;% \n  mutate(p    = plogis(log_odds),\n         mean = factor(str_c(\"mean = \", mean),\n                       levels = str_c(\"mean = \", -4:-1)),\n         sd   = str_c(\"sd = \", sd)) %&gt;% \n  \n  ggplot(aes(x = p)) +\n  stat_histinterval(.width = c(0.5, 0.95), normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(italic(p))) +\n  theme(panel.grid = element_blank()) +\n  facet_grid(sd ~ mean)\n\n\n\n\n\n\n\n\nAs you decrease the prior mean, the mass in the probability metric heads to zero. Increasing or shrinking the prior standard deviation accelerates or attenuates that leftward concentration. To my way of thinking, we want a prior that, while concentrating the mass toward zero, still offers a good spread toward the middle. Let’s try normal(-4, 3).\n\nfit13.3 &lt;- brm(\n  data = honking_km,\n  family = binomial,\n  n_events | trials(n_risk) ~ 0 + interval_f,\n  prior(normal(-4, 3), class = b),\n  chains = 4, cores = 1, iter = 2000, warmup = 1000,\n  seed = 13,\n  file = \"fits/fit13.03\")\n\nCheck the summary.\n\nprint(fit13.3)\n\n Family: binomial \n  Links: mu = logit \nFormula: n_events | trials(n_risk) ~ 0 + interval_f \n   Data: honking_km (Number of observations: 42) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ninterval_f1     -4.43      1.06    -6.87    -2.73 1.00     4702     2535\ninterval_f2     -4.40      1.09    -7.00    -2.66 1.00     4766     2223\ninterval_f3     -4.41      1.12    -6.98    -2.62 1.00     4743     2019\ninterval_f4     -4.38      1.11    -7.01    -2.60 1.00     4670     2208\ninterval_f5     -4.34      1.08    -6.92    -2.65 1.00     5038     2567\ninterval_f6     -4.31      1.08    -6.79    -2.55 1.00     4539     2323\ninterval_f7     -4.34      1.12    -6.92    -2.57 1.00     4474     2234\ninterval_f8     -4.28      1.10    -6.83    -2.55 1.00     5000     2210\ninterval_f9     -4.20      1.07    -6.67    -2.47 1.00     4409     2205\ninterval_f10    -4.22      1.09    -6.73    -2.53 1.00     4663     2672\ninterval_f11    -4.22      1.08    -6.72    -2.50 1.00     4237     2001\ninterval_f12    -4.18      1.05    -6.56    -2.50 1.00     4414     2528\ninterval_f13    -4.19      1.13    -6.89    -2.44 1.00     3817     1869\ninterval_f14    -4.15      1.14    -6.81    -2.35 1.00     4949     2442\ninterval_f15    -4.09      1.10    -6.60    -2.31 1.00     4734     2400\ninterval_f16    -4.09      1.10    -6.54    -2.34 1.00     4090     2072\ninterval_f17    -4.03      1.09    -6.56    -2.30 1.00     4762     2707\ninterval_f18    -4.01      1.10    -6.66    -2.26 1.00     4102     2326\ninterval_f19    -3.99      1.13    -6.50    -2.20 1.00     4819     2441\ninterval_f20    -3.93      1.13    -6.46    -2.11 1.00     4249     2175\ninterval_f21    -3.92      1.12    -6.46    -2.11 1.00     4958     2248\ninterval_f22    -3.87      1.10    -6.30    -2.13 1.00     4583     2740\ninterval_f23    -3.84      1.08    -6.31    -2.11 1.00     4713     2566\ninterval_f24    -3.83      1.14    -6.45    -2.06 1.00     4189     2520\ninterval_f25    -3.76      1.14    -6.31    -1.91 1.00     5006     2450\ninterval_f26    -3.75      1.18    -6.56    -1.88 1.00     4858     2151\ninterval_f27    -3.67      1.15    -6.31    -1.84 1.00     4339     2513\ninterval_f28    -3.66      1.15    -6.24    -1.79 1.00     4569     2145\ninterval_f29    -3.58      1.17    -6.34    -1.72 1.00     4551     2312\ninterval_f30    -3.55      1.17    -6.33    -1.65 1.00     4747     2605\ninterval_f31    -3.43      1.16    -6.13    -1.58 1.00     4962     2645\ninterval_f32    -3.37      1.16    -6.10    -1.53 1.00     4005     2352\ninterval_f33    -3.32      1.15    -6.06    -1.47 1.00     5168     2233\ninterval_f34    -3.14      1.19    -5.90    -1.22 1.00     4470     1738\ninterval_f35    -3.02      1.20    -5.83    -1.08 1.00     4831     2127\ninterval_f36    -2.91      1.19    -5.69    -1.00 1.00     4191     2105\ninterval_f37    -2.75      1.25    -5.65    -0.69 1.00     5563     2597\ninterval_f38    -2.48      1.28    -5.35    -0.40 1.00     5155     2789\ninterval_f39    -2.13      1.33    -5.12     0.07 1.00     5380     2287\ninterval_f40    -1.87      1.34    -4.85     0.42 1.00     4906     2834\ninterval_f41    -1.55      1.42    -4.74     0.94 1.00     4980     2405\ninterval_f42    -0.99      1.55    -4.27     1.83 1.00     5196     2581\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur parameter diagnostics look excellent. It might be helpful to inspect their posteriors in a plot. Here we show them in both the log-odds and probability metrics.\n\ndraws &lt;- as_draws_df(fit13.3) %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  set_names(1:42) \n\ndraws %&gt;% \n  pivot_longer(everything(),\n               names_to = \"parameter\",\n               values_to = \"log_odds\") %&gt;% \n  mutate(parameter   = factor(parameter, levels = 1:42),\n         probability = plogis(log_odds)) %&gt;% \n  pivot_longer(probability:log_odds) %&gt;% \n  \n  ggplot(aes(x = value, y = parameter)) +\n  stat_halfeye(.width = 0.95, normalize = \"xy\", size = 1/2) +\n  labs(x = expression(hat(italic(p))[Bayes](italic(t[j]))),\n       y = expression(italic(j))) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_x\")\n\n\n\n\n\n\n\n\nLet’s wrangle our draws object a little to put it in a more useful format.\n\ndraws &lt;- draws %&gt;% \n  mutate_all(plogis) %&gt;% \n  mutate(`0` = 0) %&gt;% \n  mutate(iter = 1:n()) %&gt;% \n  pivot_longer(-iter,\n               names_to = \"interval\",\n               values_to = \"p\") %&gt;% \n  mutate(interval = interval %&gt;% as.double()) %&gt;% \n  arrange(interval) %&gt;% \n  group_by(iter) %&gt;% \n  mutate(survivor = cumprod(1 - p)) %&gt;% \n  ungroup() \n\nglimpse(draws)\n\nRows: 172,000\nColumns: 4\n$ iter     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, …\n$ interval &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ p        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ survivor &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\nWe might want to compare our Bayesian \\(\\hat p(t_j)\\) estimates with their Maximum Likelihood counterparts. Since the Bayesian marginal posteriors are rather asymmetrical, we’ll summarize \\(\\hat p_\\text{Bayes} (t_j)\\) with means, medians, and modes.\n\ndraws %&gt;% \n  mutate(interval = factor(interval, levels = 0:42)) %&gt;% \n  group_by(interval) %&gt;% \n  summarise(mean   = mean(survivor),\n            median = median(survivor),\n            mode   = Mode(survivor)) %&gt;% \n  pivot_longer(-interval,\n               names_to = \"posterior estimate\") %&gt;% \n  \n  ggplot(aes(x = interval, y = value)) +\n  geom_point(aes(color = `posterior estimate`),\n             size = 3, shape = 1) +\n  geom_point(data = honking_km %&gt;% \n               mutate(interval = factor(interval, levels = 0:42)),\n             aes(y = `S(t)`)) +\n  scale_color_viridis_d(option = \"D\", begin = 0.1, end = 0.8) +\n  ylab(expression(hat(italic(p))(italic(t[j])))) +\n  theme(legend.background = element_blank(),\n        legend.key = element_rect(fill = \"grey92\"),\n        legend.position = c(0.85, 0.8),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe black dots were the ML estimates and the colored circles were the Bayesian counterparts. Overall, it looks like they matched up pretty well! Building on those sensibilities, here’s an alternative version of the top panel from Figure 13.2, this time comparing the frequentist \\(\\hat S (t_j)\\) with our Bayesian counterpart.\n\ndraws %&gt;% \n  group_by(interval) %&gt;% \n  summarise(mean   = mean(survivor),\n            median = median(survivor),\n            mode   = Mode(survivor)) %&gt;% \n  pivot_longer(-interval,\n               names_to = \"posterior estimate\") %&gt;% \n  left_join(honking_km %&gt;% \n              distinct(interval, seconds),\n            by = \"interval\") %&gt;% \n  mutate(seconds = if_else(is.na(seconds), 0, seconds)) %&gt;% \n  \n  ggplot(aes(x = seconds, y = value)) +\n  geom_step(aes(color = `posterior estimate`),\n            linewidth = 1) +\n  geom_step(data = honking_km,\n            aes(y = `S(t)`)) +\n  scale_color_viridis_d(option = \"D\", begin = 0.1, end = 0.8) +\n  scale_x_continuous(\"time\", limits = c(0, 20)) +\n  ylab(expression(widehat(italic(S(t[j]))))) +\n  theme(legend.background = element_blank(),\n        legend.key = element_rect(fill = \"grey92\"),\n        legend.position = c(0.85, 0.8),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAgain, we showed the ML \\(\\hat S (t_j)\\) in black and the Bayesian counterpart as summarized by three alternative measures of central tendency in color. Overall, the results were very similar across methods. What this plot makes clear is that it’s the last few estimates for that where our Bayesian estimates diverge from ML. If you compare this plot with the previous one, it appears that the nature of the divergence is our Bayesian estimates are shrunk a bit toward \\(p = .5\\), though the modes shrank less than the medians and means. Also recall how uncertain our posteriors were for the last few intervals. This is because the likelihoods for those intervals were incredibly weak. Take a glance at the last few rows in the data.\n\ntail(honking_km)\n\n# A tibble: 6 × 13\n  interval interval_f seconds start    end n_risk n_events n_censored `p(t)` `S(t)` `se[S(t)]` width `h[km](t)`\n     &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1       37 37            6.03  6.03   6.3       9        1          1  0.111 0.248      0.0666 0.270     0.412 \n2       38 38            6.3   6.3    7.2       7        1          1  0.143 0.213      0.0659 0.9       0.159 \n3       39 39            7.2   7.2    9.59      5        1          0  0.2   0.170      0.0650 2.39      0.0837\n4       40 40            9.59  9.59  12.3       4        1          0  0.25  0.128      0.0611 2.7       0.0926\n5       41 41           12.3  12.3   13.2       3        1          0  0.333 0.0851     0.0535 0.890     0.375 \n6       42 42           13.2  13.2  Inf         2        1          1  0.5   0.0425     0.0403 3.97      0.126 \n\n\nBased on the n_risk column, the number of trials ranged from \\(n = 9\\) at interval == 37 to \\(n = 2\\) for interval == 42. With so little information informing the likelihood, you largely get back the prior. Moving forward, here’s another version of the survivor function of Figure 13.2, but this time using posterior intervals to highlight uncertainty in the estimates.\n\n# augment draws\ndraws &lt;- draws %&gt;% \n  bind_rows(\n    draws %&gt;% \n      filter(interval == 42) %&gt;% \n      mutate(interval = 43)\n    )\n\ndraws %&gt;% \n  left_join(\n    bind_rows(\n      distinct(honking_km, interval, seconds),\n      tibble(interval = 43, seconds = 17.15)),\n    by = \"interval\") %&gt;% \n  mutate(seconds = if_else(is.na(seconds), 0, seconds)) %&gt;% \n  \n  ggplot(aes(x = seconds, y = survivor)) + \n  stat_lineribbon(step = \"hv\", .width = c(0.5, 0.95), linewidth = 3/4) +\n  annotate(geom = \"text\",\n           x = 5.3, y = 0.54, hjust = 0, size = 3.5,\n           label = \"This time the black line is the Bayesian posterior median,\\nwhich is the `stat_lineribbon()` default.\") +\n  geom_segment(x = 5.3, xend = 4.2,\n               y = 0.55, yend = 0.475,\n               arrow = arrow(length = unit(0.15,\"cm\"), type = \"closed\"),\n               linewidth = 1/5) +\n  scale_fill_grey(\"CI\", start = 0.8, end = 0.6,\n                  labels = c(\"95%\", \"50%\")) +\n  scale_x_continuous(\"time\", limits = c(0, 20)) +\n  ylab(expression(widehat(italic(S(t[j]))))) +\n  theme(legend.background = element_blank(),\n        legend.key = element_rect(fill = \"grey92\"),\n        legend.position = c(0.925, 0.85),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nJust for giggles, here’s how you might depict our \\(\\hat S_\\text{Bayes} (t_j)\\) with more of a 3D approach.\n\ndraws %&gt;% \n  left_join(\n    bind_rows(\n      distinct(honking_km, interval, seconds),\n      tibble(interval = 43, seconds = 17.15)),\n    by = \"interval\") %&gt;% \n  mutate(seconds = if_else(is.na(seconds), 0, seconds)) %&gt;% \n  \n  ggplot(aes(x = seconds, y = survivor)) + \n  stat_lineribbon(.width = seq(from = 0.01, to = 0.99, by = 0.01),\n                  step = \"hv\", linewidth = 0, show.legend = F) +\n  scale_fill_grey(start = 0.89, end = 0) +\n  scale_x_continuous(\"time\", limits = c(0, 20)) +\n  ylab(expression(widehat(italic(S(t[j]))))) +\n  theme(panel.grid = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#the-cumulative-hazard-function",
    "href": "13.html#the-cumulative-hazard-function",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "13.4 The cumulative hazard function",
    "text": "13.4 The cumulative hazard function\n\nKaplan-Meier type estimates of hazard are simply too erratic to be meaningful.\nThis is where the cumulative hazard function comes in. Denoted \\(H (t_{ij})\\), the cumulative hazard function assesses, at each point in time, the total amount of accumulated risk that individual \\(i\\) has faced from the beginning of time until the present. (p. 488, emphasis in the original)\n\nThe cumulative hazard function follows the equation\n\\[H (t_{ij}) = \\underset{\\text{between } t_0 \\text{ and } t_j}{\\text{cumulation}} [h(t_{ij})],\\]\n“where the phrase ‘cumulation between \\(t_0\\) and \\(t_j\\)’ indicates that cumulative hazard totals the infinite number of specific values of \\(h(t_{ij})\\) that exist between \\(t_0\\) and \\(t_j\\)” (p. 488).\n\n13.4.1 Understanding the meaning of cumulative hazard\nAlthough the absolute values of the cumulative hazard function aren’t particularly illuminating, the overall shape is. Figure 13.3 gives several examples. We don’t have the data or the exact specifications for the functions expressed in Figure 13.3. But if you’re okay with a little imprecision, we can make a few good guesses. Before diving in, it’ll help simplify our subplot code if we make two custom geoms. We’ll call them geom_h() and geom_H().\n\ngeom_h &lt;- function(subtitle, ...) {\n  list(\n    geom_line(...),\n    scale_x_continuous(NULL, breaks = NULL),\n    scale_y_continuous(expression(italic(h)(italic(t[ij]))),\n                       breaks = 0:5 * 0.02),\n    labs(subtitle = subtitle),\n    coord_cartesian(ylim = c(0, 0.1))\n  )\n}\n\ngeom_H &lt;- function(y_ul, ...) {\n  list(\n    geom_line(...),\n    ylab(expression(italic(H)(italic(t[ij])))),\n    coord_cartesian(ylim = c(0, y_ul))\n  )\n}\n\nNow we have our custom geoms, here’s the code to make Figure 13.3.\n\n# a: constant hazard\nd &lt;- tibble(time = seq(from = 0, to = 100, by = 1)) %&gt;% \n  mutate(h = 0.05) %&gt;%  \n  mutate(H = cumsum(h))\n\np1 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = h)) +\n  geom_h(subtitle = \"A: Constant hazard\")\n\np2 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(y_ul = 6)\n\n# b: increasing hazard\nd &lt;- tibble(time = seq(from = 0, to = 100, by = 1)) %&gt;% \n  mutate(h = 0.001 * time) %&gt;%  \n  mutate(H = cumsum(h))\n\np3 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = h)) + \n  geom_h(subtitle = \"B: Increasing hazard\")\n  \np4 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(y_ul = 5)\n\n# decreasing hazard\nd &lt;- tibble(time = seq(from = 0.2, to = 100, by = 0.1)) %&gt;% \n  # note out use of the gamma distribution (see )\n  mutate(h = dgamma(time, shape = 0.02, rate = 0.001)) %&gt;%  \n  mutate(H = cumsum(h))\n\np5 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = h)) +\n  geom_h(subtitle = \"C: Decreasing hazard\")\n\np6 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(y_ul = 1.2)\n\n# increasing & decreasing hazard\nd &lt;- tibble(time = seq(from = 1, to = 100, by = 1)) %&gt;% \n  # note our use of the Fréchet distribution\n  mutate(h = dfrechet(time, loc = 0, scale = 250, shape = 0.5) * 25) %&gt;%  \n  mutate(H = cumsum(h))\n\np7 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = h)) +\n  geom_h(subtitle = \"D: Increasing &\\n decreasing hazard\")\n\np8 &lt;- d %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(y_ul = 5)\n\n# combine with patchwork and plot!\n((p1 / p2) | (p3 / p4) | (p5 / p6) | (p7 / p8)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nDid you notice our use of the gamma and Fréchet distributions? Both are supported for continuous-time survival models in brms (see Bürkner’s vignette, Parameterization of response distributions in brms).\n\n\n13.4.2 Estimating the cumulative hazard function\nThe two methods to estimate the cumulative hazard function are the Nelson-Aalen method and the negative log survivor function method. The Nelson-Aalen method used Kaplan-Meier-type hazard estimates as follows:\n\\[\\hat H_\\text{NA} (t_j) = \\hat h_\\text{KM} (t_1) \\text{width}_1 + \\hat h_\\text{KM} (t_2) \\text{width}_2 + \\dots + \\hat h_\\text{KM} (t_j) \\text{width}_j,\\]\nwhere \\(\\hat h_\\text{KM} (t_j) \\text{width}_j\\) is the total hazard during the \\(j\\)th interval. The negative log survivor function method makes use of the estimates of the Kaplan-Meier survivor function with the formula\n\\[\\hat H_{- \\text{LS}} (t_j) = - \\log \\hat S_\\text{KM} (t_{ij}),\\]\nwhere \\(\\hat S_\\text{KM} (t_{ij})\\), recall, is the cumulative survivor function for the Kaplan-Meier estimator. Here we put both formulas to use and make our version of Figure 13.4.\n\n# for annotation\ntext &lt;- tibble(\n  seconds = c(13.15, 13.4),\n  y       = c(3.275, 2.65),\n  label   = c(\n    \"Negative~log~survivor*','*~hat(italic(H))[-LS](italic(t[j]))\", \n    \"Nelson-Aalen*','*~hat(italic(H))[N][A](italic(t[j]))\"))\n\n# top plot\np1 &lt;- honking_km %&gt;% \n  mutate(width = if_else(width == Inf, 17.15 - start, width)) %&gt;% \n  mutate(`H[na](t)` = cumsum(`h[km](t)` * width),\n         `H[ls](t)` = - log(`S(t)`)) %&gt;% \n  select(seconds, `H[na](t)`, `H[ls](t)`) %&gt;% \n  bind_rows(tibble(seconds    = 17.15, \n                   `H[na](t)` = 2.78499694, \n                   `H[ls](t)` = 3.15764982)) %&gt;% \n  # add a line index\n  mutate(line = rep(letters[1:4], times = c(12, 21, 7, 3))) %&gt;% \n  \n  \n  ggplot(aes(x = seconds)) +\n  geom_step(aes(y = `H[ls](t)`),\n            color = \"grey50\") +\n  geom_step(aes(y = `H[na](t)`),\n            linetype = 2) +\n  geom_text(data = text,\n            aes(y = y, label = label),\n            hjust = 0, size = 3, parse = T) +\n  scale_x_continuous(\"time\", limits = c(0, 20)) +\n  scale_y_continuous(expression(widehat(italic(H)(italic(t[j])))), \n                     breaks = seq(from = 0, to = 3.5, by = 0.5), limits = c(0, 3.5)) +\n  theme(panel.grid = element_blank())\n\n# bottom plot\np2 &lt;- p1 +\n  stat_smooth(data = . %&gt;% filter(line != \"d\"),\n              aes(y = (`H[ls](t)` + `H[na](t)`) / 2, color = line, fill = line),\n              method = \"lm\", show.legend = F) +\n  scale_fill_viridis_d(option = \"A\", begin = 0.2, end = 0.8) +\n  scale_color_viridis_d(option = \"A\", begin = 0.2, end = 0.8)\n\n# combine\n(p1 + scale_x_continuous(NULL, breaks = NULL, limits = c(0, 20))) / p2\n\n\n\n\n\n\n\n\nVisualizing the cumulative hazard function from our Bayesian fit13.3 is a minor extension to the approach we used for the survivor function, above. As an example, here’s our plot for \\(\\hat H_{- \\text{LS}} (t_j)\\). The biggest change in the code is the last line in the second mutate() statement, hazard = -log(survivor).\n\ndraws %&gt;% \n  left_join(\n    bind_rows(\n      distinct(honking_km, interval, seconds),\n      tibble(interval = 43, seconds = 17.15)),\n    by = \"interval\") %&gt;% \n  mutate(seconds = if_else(is.na(seconds), 0, seconds),\n         # convert S to H\n         hazard = -log(survivor)) %&gt;% \n  \n  ggplot(aes(x = seconds, y = hazard)) + \n  stat_lineribbon(step = \"hv\", .width = c(0.5, 0.8, 0.95), linewidth = 3/4) +\n  scale_fill_grey(\"CI\", start = 0.85, end = 0.6,\n                  labels = c(\"95%\", \"80%\", \"50%\")) +\n  scale_x_continuous(\"time\", limits = c(0, 20)) +\n  ylab(expression(widehat(italic(H)[-LS](italic(t[j]))))) +\n  theme(legend.background = element_blank(),\n        legend.key = element_rect(fill = \"grey92\"),\n        legend.position = c(0.925, 0.73),\n        panel.grid = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#kernel-smoothed-estimates-of-the-hazard-function",
    "href": "13.html#kernel-smoothed-estimates-of-the-hazard-function",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "13.5 Kernel-smoothed estimates of the hazard function",
    "text": "13.5 Kernel-smoothed estimates of the hazard function\n\nThe idea behind kernel smoothing is simple. At each of many distinct points in time, estimate a function’s average value by aggregating together all the point estimates available within the focal time’s temporal vicinity. Conceptually, kernel-smoothed estimates are a type of moving average. They do not identify precise values of hazard at each point in time but rather approximate values based on the estimates nearby. Even though each smoothed value only approximates the underlying true value, a plot over time can help reveal the underlying function’s shape.\nKernel smoothing requires a set of point estimates to smooth. For the hazard function, one way of obtaining these point estimates is by computing successive differences in the estimated cumulative hazard function from each observed even time until the next. Each difference acts as a pseudo-slope, a measure of the local rate of change in cumulative hazard during that period. Either Nelson-Aalen estimates or negative log survivor function estimates of cumulative hazard can be used. (pp. 495–496)\n\nSinger and Willett showed examples of this smoothing in Figure 13.5, in which they applied a smoothing algorithm to the \\(H_{- \\text{LS}} (t_j)\\) estimates. The good folks at IDRE have already worked out the code to reproduce Singer and Willette’s smoothing algorithm. The IDRE folks called their custom function smooth().\n\nmy_smooth &lt;- function(width, time, survive) { \n  n   &lt;- length(time)\n  lo  &lt;- time[1] + width\n  hi  &lt;- time[n] - width\n  npt &lt;- 50\n  inc &lt;- (hi - lo) / npt\n  \n  s &lt;- t(lo + t(c(1:npt)) * inc)\n  \n  slag &lt;- c(1, survive[1:n - 1])\n  h    &lt;- 1 - survive / slag\n  x1   &lt;- as.vector(rep(1, npt)) %*% (t(time))\n  x2   &lt;- s %*% as.vector(rep(1, n))\n  x    &lt;- (x1 - x2) / width\n  k    &lt;- .75 * (1 - x * x) * (abs(x) &lt;= 1)\n  \n  lambda &lt;- (k %*% h) / width\n  \n  data.frame(x = s, y = lambda)\n}\n\nWe’ve renamed the function my_smooth() to avoid overwriting the already existing smooth() function. The code for my_smooth() is a mild update from the one in the link above in that it returns its results in a data frame. Let’s give it a quick whirl.\n\nmy_smooth(width = 1, \n          time = honking_km$seconds,\n          survive = honking_km$`S(t)`) %&gt;% \n  glimpse()\n\nRows: 50\nColumns: 2\n$ x &lt;dbl&gt; 2.6054, 2.8008, 2.9962, 3.1916, 3.3870, 3.5824, 3.7778, 3.9732, 4.1686, 4.3640, 4.5594, 4.7548, 4.9502, 5.1456, 5.3410…\n$ y &lt;dbl&gt; 0.32686176, 0.36745825, 0.38958042, 0.38516865, 0.37093732, 0.34810198, 0.33956166, 0.31166618, 0.29193431, 0.27646483…\n\n\nThe time and survive arguments take seconds and S(t) values, respectively. The width argument determines over how many values we would like to smooth over on a given point. For example, width = 1 would, for a given point on the time axis, aggregate over all values \\(\\pm1\\) that point. Larger width values result in more aggressive smoothing. Also notice my_smooth() returned a data frame containing 50 rows for two columns, x and y. Those columns are the coordinates for the TIME and smoothed hazard, respectively. Here we put my_smooth() to work and make our version of Figure 13.5.\n\ntibble(width = 1:3) %&gt;% \n  mutate(xy = map(width, ~my_smooth(width = ., \n                                    time = honking_km$seconds, \n                                    survive = honking_km$`S(t)`)),\n         width = str_c(\"width = \", width)) %&gt;% \n  unnest(xy) %&gt;% \n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(\"seconds after light turns green\", limits = c(0, 20)) +\n  ylab(\"smoothed hazard\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ width, nrow = 3)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#developing-an-intuition-about-continuous-time-survivor-cumulative-hazard-and-kernel-smoothed-hazard-functions",
    "href": "13.html#developing-an-intuition-about-continuous-time-survivor-cumulative-hazard-and-kernel-smoothed-hazard-functions",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "13.6 Developing an intuition about continuous-time survivor, cumulative hazard, and kernel-smoothed hazard functions",
    "text": "13.6 Developing an intuition about continuous-time survivor, cumulative hazard, and kernel-smoothed hazard functions\nBuckle up and load the relapse data from Cooney and colleagues (1991); the US Supreme Court tenure data from Zorn and van Winkle (2000); the first depressive episode data from Sorenson, Rutter, and Aneshensel (1991); and the health-workers employment data from Singer and colleagues (1998).\n\nalcohol_relapse  &lt;- read_csv(\"data/alcohol_relapse.csv\") %&gt;% rename_all(str_to_lower)\njudges           &lt;- read_csv(\"data/judges.csv\")\nfirst_depression &lt;- read_csv(\"data/firstdepression.csv\")\nhealth_workers   &lt;- read_csv(\"data/healthworkers.csv\")\n\nglimpse(alcohol_relapse)\n\nRows: 89\nColumns: 3\n$ weeks  &lt;dbl&gt; 0.7142857, 0.7142857, 1.1428571, 1.4285714, 1.7142857, 1.7142857, 2.1428571, 2.7142857, 3.8571429, 4.1428571, 4.5…\n$ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31…\n\nglimpse(judges)\n\nRows: 109\nColumns: 7\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31…\n$ tenure &lt;dbl&gt; 1, 6, 6, 9, 21, 9, 1, 13, 4, 15, 31, 4, 34, 30, 16, 19, 23, 34, 20, 2, 32, 14, 32, 5, 28, 15, 28, 19, 27, 5, 23, …\n$ dead   &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,…\n$ retire &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,…\n$ leave  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age    &lt;dbl&gt; 50, 57, 44, 47, 57, 38, 59, 47, 51, 55, 36, 44, 45, 32, 49, 42, 58, 32, 55, 49, 44, 50, 45, 52, 59, 57, 51, 57, 5…\n$ year   &lt;dbl&gt; 1789, 1789, 1789, 1789, 1789, 1790, 1791, 1793, 1796, 1796, 1798, 1799, 1801, 1804, 1806, 1807, 1811, 1811, 1823,…\n\nglimpse(first_depression)\n\nRows: 2,974\nColumns: 3\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31…\n$ age    &lt;dbl&gt; 4, 6, 8, 8, 9, 9, 10, 10, 11, 12, 12, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ censor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\nglimpse(health_workers)\n\nRows: 2,074\nColumns: 3\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31…\n$ weeks  &lt;dbl&gt; 0.14, 0.42, 1.00, 1.28, 1.28, 1.71, 1.85, 1.85, 1.85, 2.00, 2.14, 2.14, 2.14, 2.28, 2.28, 2.28, 3.00, 3.00, 3.14,…\n$ censor &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\nFor our first go, just fit the models with survfit().\n\nfit13.4 &lt;- survfit(\n  data = alcohol_relapse,\n  Surv(weeks, abs(censor - 1)) ~ 1)\n\nfit13.5 &lt;- survfit(\n  data = judges,\n  Surv(tenure, leave) ~ 1)\n\nfit13.6 &lt;- survfit(\n  data = first_depression,\n  Surv(age, abs(censor - 1)) ~ 1)\n\nfit13.7 &lt;- survfit(\n  data = health_workers,\n  Surv(weeks, abs(censor - 1)) ~ 1)\n\nWith the model from Section 13.3, we organized the Kaplan-Meier output in a tibble following the outline of Table 13.3 (p. 484). That layout was useful for plotting the frequentist results and for fitting the Bayesian version of the model with brms. Since we’re juggling four models, let’s make a convenience function to do that with a single line of code. Call it km_tibble().\n\nkm_tibble &lt;- function(surv_fit) {\n  t &lt;- summary(surv_fit)\n  \n  length &lt;- length(t$time)\n  \n  d &lt;- tibble(time     = c(0, t$time),\n              n_risk   = c(t$n.risk[1], t$n.risk),\n              n_events = c(0, t$n.event)) %&gt;% \n    mutate(p        = n_events / n_risk,\n           n_censored = n_risk - n_events - lead(n_risk, default = 0),\n           interval   = 0:length,\n           interval_f = factor(0:length, levels = 0:c(length + 1))) %&gt;% \n    select(interval:interval_f, time, n_risk:n_events, n_censored, p) %&gt;% \n    mutate(S = cumprod(1 - p)) %&gt;% \n    mutate(H = ifelse(interval == 0, NA, \n                      ifelse(S == 0, NA,\n                             -log(S))),\n           p = ifelse(interval == 0, NA, p))\n  \n  bind_rows(\n    d, \n    d %&gt;% \n      slice(n()) %&gt;%\n      mutate(interval   = length + 1,\n             interval_f = factor(length + 1,\n                                 levels = 0:c(length + 1)),\n             time       = surv_fit$time %&gt;% max()))\n}\n\nHere’s an example of our custom km_tibble() function works based on our earlier model of the honking data, fit13.2.\n\nkm_tibble(fit13.2)\n\n# A tibble: 44 × 9\n   interval interval_f  time n_risk n_events n_censored       p     S       H\n      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1        0 0           0        57        0          0 NA      1     NA     \n 2        1 1           1.41     57        1          1  0.0175 0.982  0.0177\n 3        2 2           1.51     55        1          0  0.0182 0.965  0.0360\n 4        3 3           1.67     54        1          0  0.0185 0.947  0.0547\n 5        4 4           1.68     53        1          0  0.0189 0.929  0.0738\n 6        5 5           1.86     52        1          0  0.0192 0.911  0.0932\n 7        6 6           2.12     51        1          0  0.0196 0.893  0.113 \n 8        7 7           2.19     50        1          1  0.02   0.875  0.133 \n 9        8 8           2.48     48        1          0  0.0208 0.857  0.154 \n10        9 9           2.5      47        1          0  0.0213 0.839  0.176 \n# ℹ 34 more rows\n\n\nNow apply km_tibble() to our four new fits.\n\nkm &lt;- tibble(ml_fit = str_c(\"fit13.\", 4:7)) %&gt;% \n  mutate(d = map(ml_fit, ~ get(.) %&gt;% km_tibble())) %&gt;% \n  unnest(d)\n\nIf we want the settings in our \\(x\\)- and \\(y\\)-axes to differ across subplots, good old facet_wrap() and facet_grid() aren’t going to cut it for our version of Figure 13.6. To avoid needless repetition in the settings across subplot code, we’ll make a few custom geoms.\n\ngeom_S &lt;- function(x_ul, y_lab = NULL, ...) {\n  list(\n    geom_hline(yintercept = 0.5, color = \"white\"),\n    geom_step(...),\n    scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)),\n    scale_y_continuous(y_lab, breaks = c(0, 0.5, 1), labels = c(\"0\", \".5\", \"1\"), limits = c(0, 1))\n  )\n}\n\ngeom_H &lt;- function(x_ul, y_lab = NULL, ...) {\n  list(\n    geom_step(...),\n    scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)),\n    scale_y_continuous(y_lab, limits = c(0, NA))\n  )\n}\n\ngeom_h &lt;- function(x_lab, x_ul, y_lab = NULL, ...) {\n  list(\n    geom_line(...),\n    scale_x_continuous(x_lab, limits = c(0, x_ul)),\n    scale_y_continuous(y_lab, limits = c(0, NA))\n  )\n}\n\nUse geom_S() to make and save the top row, the \\(\\widehat{S (t_j)}\\) plots.\n\np1 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.4\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 110, y_lab = expression(widehat(italic(S(t[j]))))) +\n  labs(subtitle = \"Cooney et al (1991)\")\n\np2 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.5\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 36, y_lab = NULL) +\n  labs(subtitle = \"Zorn &  Van Winkle (2000)\")\n\np3 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.6\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 102, y_lab = NULL) +\n  labs(subtitle = \"Sorenson et al (1991)\")\n\np4 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.7\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 150, y_lab = NULL) +\n  labs(subtitle = \"Singer et al (1998)\")\n\nUse geom_H() to make and save the middle row, the \\(\\widehat{H (t_j)}\\) plots.\n\np5 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.4\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 110, y_lab = expression(widehat(italic(H(t[j])))))\n\np6 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.5\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 36, y_lab = NULL)\n\np7 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.6\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 102, y_lab = NULL)\n\np8 &lt;- km %&gt;% \n  filter(ml_fit == \"fit13.7\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 150, y_lab = NULL)\n\nUse geom_h() to make and save the bottom row, the \\(\\widehat{h (t_j)}\\) plots.\n\np9 &lt;- my_smooth(width = 12, time = fit13.4$time, survive = fit13.4$surv) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"weeks after discharge\", x_ul = 110, y_lab = expression(widehat(italic(h(t[j])))))\n\np10 &lt;- my_smooth(width = 5, time = fit13.5$time, survive = fit13.5$surv) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"years on court\", x_ul = 36, y_lab = NULL)\n\np11 &lt;- my_smooth(width = 7, time = fit13.6$time, survive = fit13.6$surv) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"age (in years)\", x_ul = 102, y_lab = NULL)\n\np12 &lt;- my_smooth(width = 12, time = fit13.7$time, survive = fit13.7$surv) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"weeks since hired\", x_ul = 150, y_lab = NULL)\n\nFinally, combine the subplots and behold our version of Figure 13.6!\n\n((p1 / p5 / p9) | (p2 / p6 / p10) | (p3 / p7 / p11) | (p4 / p8 / p12)) &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n13.6.1 Bonus: Bayesians can compare continuous-time survivor, cumulative hazard, and kernel-smoothed hazard functions, too\nNow let’s repeat that process as Bayesians. Here we fit those last four models with brm(). Note the data statements. Filtering by ml_fit allowed us to select the correct subset of data saved in km. The two filtering statements by interval allowed us to focus on the actual data instead of including the two rows we added for plotting conveniences. Otherwise the brm() code is just like what we used before.\n\nfit13.8 &lt;- brm(\n  data = filter(km, ml_fit == \"fit13.4\" & interval &gt; 0 & interval &lt; 61),\n  family = binomial,\n  n_events | trials(n_risk) ~ 0 + interval_f,\n  prior(normal(-4, 3), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 13,\n  file = \"fits/fit13.08\")\n\nfit13.9 &lt;- brm(\n  data = filter(km, ml_fit == \"fit13.5\" & interval &gt; 0 & interval &lt; 34),\n  family = binomial,\n  n_events | trials(n_risk) ~ 0 + interval_f,\n  prior(normal(-4, 3), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 13,\n  file = \"fits/fit13.09\")\n\nfit13.10 &lt;- brm(\n  data = filter(km, ml_fit == \"fit13.6\" & interval &gt; 0 & interval &lt; 44),\n  family = binomial,\n  n_events | trials(n_risk) ~ 0 + interval_f,\n  prior(normal(-4, 3), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 13,\n  file = \"fits/fit13.10\")\n\nfit13.11 &lt;- brm(\n  data = filter(km, ml_fit == \"fit13.7\" & interval &gt; 0 & interval &lt; 306),\n  family = binomial,\n  n_events | trials(n_risk) ~ 0 + interval_f,\n  prior(normal(-4, 3), class = b),\n  chains = 4, cores = 4, iter = 2000, warmup = 1000,\n  seed = 13,\n  file = \"fits/fit13.11\")\n\nFor the sake of space, I’m not going to show all the summary output. If you’re following along, I still recommend you give them a look. Spoiler alert: the parameter diagnostics look great.\n\nprint(fit13.8)\nprint(fit13.9)\nprint(fit13.10)\nprint(fit13.11)\n\nSince we’re working with four brm() fits, it might make sense to bundle the steps and keep the results in one place. Here we make something of a super function. With wrangle_draws(), we’ll extract the posterior draws from each model; add a couple interval columns; convert the results to the \\(\\widehat{p (t_j)}\\), \\(\\widehat{S (t_j)}\\), and \\(\\widehat{H (t_j)}\\) metrics; and join the results to the data stored in km. If the steps seem overwhelming, just flip back to the ends of Sections 13.3 and 13.4. This is a small extension of the data wrangling steps we took to make the \\(\\widehat{S (t_j)}\\) and \\(\\widehat{H (t_j)}\\) plots for our brms model fit13.3.\n\nwrangle_draws &lt;- function(brms, survfit) {\n  \n  # extract the draws\n  draws &lt;- get(brms) %&gt;% \n    as_draws_df() %&gt;% \n    select(starts_with(\"b_\"))\n  \n  # how many columns?\n  n_col &lt;- ncol(draws)  \n  \n  # transform to the p metric, add a 0 interval, make it long, and add S\n  draws &lt;- draws %&gt;% \n    set_names(1:n_col) %&gt;% \n    mutate_all(plogis) %&gt;% \n    mutate(`0` = 0) %&gt;% \n    mutate(draw = 1:n()) %&gt;% \n    pivot_longer(-draw,\n                 names_to = \"interval\",\n                 values_to = \"p\") %&gt;% \n    mutate(interval = interval %&gt;% as.double()) %&gt;% \n    arrange(interval) %&gt;% \n    group_by(draw) %&gt;% \n    mutate(S = cumprod(1 - p)) %&gt;% \n    ungroup() %&gt;% \n    mutate(H = -log(S))\n  \n  # add the final interval, join the data, and return()\n  bind_rows(draws,\n            draws %&gt;% filter(interval == n_col) %&gt;% mutate(interval = n_col + 1)) %&gt;% \n    left_join(km %&gt;% filter(ml_fit == survfit) %&gt;% select(interval:n_censored),\n              by = \"interval\")\n}\n\nOur wrangle_draws() function takes two arguments, brms and survfit, which indicate the desired brms model and the corresponding index within km that contains the associated survival data. Let’s put it to work.\n\ndraws &lt;- tibble(\n  brms    = str_c(\"fit13.\", 8:11),\n  survfit = str_c(\"fit13.\", 4:7)) %&gt;% \n  mutate(draw = map2(brms, survfit, wrangle_draws)) %&gt;% \n  unnest(draw)\n\ndraws\n\n# A tibble: 1,796,000 × 12\n   brms    survfit  draw interval     p     S     H interval_f  time n_risk n_events n_censored\n   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 fit13.8 fit13.4     1        0     0     1     0 0              0     89        0          0\n 2 fit13.8 fit13.4     2        0     0     1     0 0              0     89        0          0\n 3 fit13.8 fit13.4     3        0     0     1     0 0              0     89        0          0\n 4 fit13.8 fit13.4     4        0     0     1     0 0              0     89        0          0\n 5 fit13.8 fit13.4     5        0     0     1     0 0              0     89        0          0\n 6 fit13.8 fit13.4     6        0     0     1     0 0              0     89        0          0\n 7 fit13.8 fit13.4     7        0     0     1     0 0              0     89        0          0\n 8 fit13.8 fit13.4     8        0     0     1     0 0              0     89        0          0\n 9 fit13.8 fit13.4     9        0     0     1     0 0              0     89        0          0\n10 fit13.8 fit13.4    10        0     0     1     0 0              0     89        0          0\n# ℹ 1,795,990 more rows\n\n\nLike before, we have 12 subplots to make and we can reduce redundancies in the code by working with custom geoms. To accommodate our Bayesian fits, we’ll redefine geom_S() and geom_H() to depict the step functions with tidybayes::stat_lineribbon(). Happily, our geom_h() is good to go as is.\n\ngeom_S &lt;- function(x_ul, y_lab = NULL, ...) {\n  list(\n    geom_hline(yintercept = 0.5, color = \"white\"),\n    stat_lineribbon(step = \"hv\", .width = c(0.5, 0.95), \n                    linewidth = 1/2, show.legend = F, ...),\n    scale_fill_grey(start = 0.8, end = 0.6),\n    scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)),\n    scale_y_continuous(y_lab, breaks = c(0, 0.5, 1), labels = c(\"0\", \".5\", \"1\"), limits = c(0, 1))\n  )\n}\n\ngeom_H &lt;- function(x_ul, y_lab = NULL, ...) {\n  list(\n    stat_lineribbon(step = \"hv\", .width = c(0.5, 0.95), \n                    linewidth = 1/2, show.legend = F, ...),\n    scale_fill_grey(start = 0.8, end = 0.6),\n    scale_x_continuous(NULL, breaks = NULL, limits = c(0, x_ul)),\n    scale_y_continuous(y_lab, limits = c(0, NA))\n  )\n}\n\nMake and save the subplots.\n\n# use `geom_S()` to make and save the top row\np1 &lt;- draws %&gt;% \n  filter(brms == \"fit13.8\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 110, y_lab = expression(widehat(italic(S(t[j]))))) +\n  labs(subtitle = \"Cooney et al (1991)\")\n\np2 &lt;- draws %&gt;% \n  filter(brms == \"fit13.9\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 36, y_lab = NULL) +\n  labs(subtitle = \"Zorn &  Van Winkle (2000)\")\n\np3 &lt;- draws %&gt;% \n  filter(brms == \"fit13.10\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 102, y_lab = NULL) +\n  labs(subtitle = \"Sorenson et al (1991)\")\n\np4 &lt;- draws %&gt;% \n  filter(brms == \"fit13.11\") %&gt;% \n  ggplot(aes(x = time, y = S)) +\n  geom_S(x_ul = 150, y_lab = NULL) +\n  labs(subtitle = \"Singer et al (1998)\")\n  \n# use `geom_H()` to make and save the middle row\np5 &lt;- draws %&gt;% \n  filter(brms == \"fit13.8\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 110, y_lab = expression(widehat(italic(H(t[j])))))\n\np6 &lt;- draws %&gt;% \n  filter(brms == \"fit13.9\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 36, y_lab = NULL)\n\np7 &lt;- draws %&gt;% \n  filter(brms == \"fit13.10\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 102, y_lab = NULL)\n\np8 &lt;- draws %&gt;% \n  filter(brms == \"fit13.11\") %&gt;% \n  ggplot(aes(x = time, y = H)) +\n  geom_H(x_ul = 150, y_lab = NULL)\n  \n# use `geom_h()` to make and save the bottom row\np9 &lt;- draws %&gt;% \n  filter(brms == \"fit13.8\") %&gt;% \n  group_by(interval, time) %&gt;% \n  summarize(median = median(S)) %&gt;% \n  nest(data = everything()) %&gt;% \n  mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% \n  unnest(smooth) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"weeks after discharge\", x_ul = 110, y_lab = expression(widehat(italic(h(t[j])))))\n\np10 &lt;- draws %&gt;% \n  filter(brms == \"fit13.9\") %&gt;% \n  group_by(interval, time) %&gt;% \n  summarize(median = median(S)) %&gt;% \n  nest(data = everything()) %&gt;% \n  mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% \n  unnest(smooth) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"years on court\", x_ul = 36, y_lab = NULL)\n\np11 &lt;- draws %&gt;% \n  filter(brms == \"fit13.10\") %&gt;% \n  group_by(interval, time) %&gt;% \n  summarize(median = median(S)) %&gt;% \n  nest(data = everything()) %&gt;% \n  mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% \n  unnest(smooth) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"age (in years)\", x_ul = 102, y_lab = NULL)\n\np12 &lt;- draws %&gt;% \n  filter(brms == \"fit13.11\") %&gt;% \n  group_by(interval, time) %&gt;% \n  summarize(median = median(S)) %&gt;% \n  nest(data = everything()) %&gt;% \n  mutate(smooth = map(data, ~ my_smooth(width = 12, time = .$time, survive = .$median))) %&gt;% \n  unnest(smooth) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_h(x_lab = \"weeks since hired\", x_ul = 150, y_lab = NULL)\n\nFinally, combine the subplots and behold our Bayesian alternative version of Figure 13.6!\n\n((p1 / p5 / p9) | (p2 / p6 / p10) | (p3 / p7 / p11) | (p4 / p8 / p12)) &\n  theme(panel.grid = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#session-info",
    "href": "13.html#session-info",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] survival_3.8-3  patchwork_1.3.2 tidybayes_3.0.7 brms_2.23.0     Rcpp_1.1.0      lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        svUnit_1.0.8            viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] timechange_0.3.0        estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 magrittr_2.0.4         \n[16] posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7             tools_4.5.1             utf8_1.2.6             \n[21] knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1    htmlwidgets_1.6.4       curl_7.0.0             \n[26] pkgbuild_1.4.8          bit_4.6.0               plyr_1.8.9              RColorBrewer_1.1-3      abind_1.4-8            \n[31] multcomp_1.4-29         withr_3.0.2             grid_4.5.1              stats4_4.5.1            inline_0.3.21          \n[36] xtable_1.8-4            emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65             cli_3.6.5              \n[41] mvtnorm_1.3-3           rmarkdown_2.30          crayon_1.5.3            generics_0.1.4          RcppParallel_5.1.11-1  \n[46] rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000       splines_4.5.1          \n[51] bayesplot_1.15.0.9000   parallel_4.5.1          matrixStats_1.5.0       vctrs_0.6.5             V8_8.0.1               \n[56] Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          hms_1.1.4               arrayhelpers_1.1-0     \n[61] bit64_4.6.0-1           ggdist_3.3.3            glue_1.8.0              codetools_0.2-20        distributional_0.5.0   \n[66] stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1           htmltools_0.5.9        \n[71] Brobdingnag_1.2-9       R6_2.6.1                vroom_1.6.6             evaluate_1.0.5          lattice_0.22-7         \n[76] backports_1.5.0         rstantools_2.5.0.9000   gridExtra_2.3           coda_0.19-4.1           nlme_3.1-168           \n[81] checkmate_2.3.3         mgcv_1.9-3              xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "13.html#comments",
    "href": "13.html#comments",
    "title": "13  Describing Continuous-Time Event Occurrence Data",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBürkner, P.-C. (2021). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html\n\n\nCooney, N. L., Kadden, R. M., Litt, M. D., & Getter, H. (1991). Matching alcoholics to coping skills or interactional therapies: Two-year follow-up results. Journal of Consulting and Clinical Psychology, 59(4), 598. https://doi.org/10.1037/0022-006X.59.4.598\n\n\nDiekmann, A., Jungbauer-Gans, M., Krassnig, H., & Lorenz, S. (1996). Social status and aggression: A field study analyzed by survival analysis. The Journal of Social Psychology, 136(6), 761–768. https://doi.org/10.1080/00224545.1996.9712252\n\n\nKaplan, E. L., & Meier, P. (1958). Nonparametric estimation from incomplete observations. Journal of the American Statistical Association, 53(282), 457–481. https://doi.org/10.1080/01621459.1958.10501452\n\n\nLawless, J. F. (1982). Statistical models and methods for lifetime data. John Wiley & Sons.\n\n\nMiller, R. G. (1981). Survival analysis. John Wiley & Sons.\n\n\nSinger, J. D., Davidson, S. M., Graham, S., & Davidson, H. S. (1998). Physician retention in community and migrant health centers: Who stays and for how long? Medical Care, 1198–1213. http://www.jstor.org/stable/3766886\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nSorenson, S. B., Rutter, C. M., & Aneshensel, C. S. (1991). Depression in the community: An investigation into age of onset. Journal of Consulting and Clinical Psychology, 59(4), 541. https://doi.org/10.1037/0022-006X.59.4.541\n\n\nZorn, C. J., & Van Winkle, S. R. (2000). A competing risks model of Supreme Court vacancies, 1789–1992. Political Behavior, 22(2), 145–166. https://doi.org/10.1023/A:1006667601289",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Describing Continuous-Time Event Occurrence Data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aalen, O. O. (1988). Heterogeneity in survival analysis. Statistics\nin Medicine, 7(11), 1121–1137. https://doi.org/10.1002/sim.4780071105\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4.\nJournal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates, D., Maechler, M., Bolker, B., & Steven Walker. (2022).\nlme4: Linear mixed-effects\nmodels using Eigen’ and S4. https://CRAN.R-project.org/package=lme4\n\n\nBeck, N. (1999). Modelling space and time: The event\nhistory approach. In E. Scarbrough & E. Tanenbaum (Eds.),\nResearch strategies in social science: A guide to new\napproaches. Oxford University Press. https://doi.org/10.1093/0198292376.001.0001\n\n\nBeck, Nathaniel, Katz, J. N., & Tucker, R. (1998). Taking time\nseriously: Time-series-cross-section\nanalysis with a binary dependent variable. American Journal of\nPolitical Science, 42(4), 1260–1288. https://doi.org/10.2307/2991857\n\n\nBrennan, R. L. (2001). Generalizability Theory.\nSpringer-Verlag. https://doi.org/10.1007/978-1-4757-3456-0\n\n\nBrilleman, S. (2019). Estimating survival (time-to-event) models\nwith rstanarm. https://github.com/stan-dev/rstanarm/blob/feature/frailty-models/vignettes/surv.Rmd\n\n\nBrilleman, S. L., Elci, E. M., Novik, J. B., & Wolfe, R. (2020).\nBayesian survival analysis using the rstanarm\nR package. https://arxiv.org/abs/2002.09633\n\n\nBrown, D. R., & Gary, L. E. (1985). Predictors of depressive\nsymptoms among unemployed Black adults. Journal of\nSociology and Social Welfare, 12, 736. https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=1721&amp=&context=jssw&amp=&sei-redir=1&referer=https%253A%252F%252Fscholar.google.com%252Fscholar%253Fq%253D%252522CES-D%252522%252Bunemployment%2526hl%253Den%2526as_sdt%253D0%25252C44%2526as_ylo%253D1977%2526as_yhi%253D2000#search=%22CES-D%20unemployment%22\n\n\nBryk, A. S., & Raudenbush, S. W. (1987). Application of hierarchical\nlinear models to assessing change. Psychological Bulletin,\n101(1), 147. https://doi.org/10.1037/0033-2909.101.1.147\n\n\nBürkner, P.-C. (2017). brms: An\nR package for Bayesian multilevel models using\nStan. Journal of Statistical Software,\n80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel\nmodeling with the R package brms. The R Journal,\n10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2020). Bayesian item response modeling in\nR with brms and Stan. http://arxiv.org/abs/1905.09501\n\n\nBürkner, P.-C. (2021a). brms reference\nmanual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2021b). Estimating non-linear models with brms.\nhttps://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nBürkner, P.-C. (2021c). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html\n\n\nBürkner, P.-C. (2021d). Parameterization of response distributions\nin brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html\n\n\nBürkner, P.-C. (2022a). brms:\nBayesian regression models using ’Stan’.\nhttps://CRAN.R-project.org/package=brms\n\n\nBürkner, P.-C. (2022b). Estimating multivariate models with\nbrms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nBürkner, P.-C., Gabry, J., Kay, M., & Vehtari, A. (2022). posterior: Tools for working with\nposterior distributions. https://CRAN.R-project.org/package=posterior\n\n\nCapaldi, D. M., Crosby, L., & Stoolmiller, M. (1996). Predicting the\ntiming of first sexual intercourse for at-risk adolescent males.\nChild Development, 67(2), 344–359. https://doi.org/10.2307/1131818\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B.,\nBetancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017).\nStan: A probabilistic programming language. Journal of\nStatistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01\n\n\nChung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., & Liu, J.\n(2013). A nondegenerate penalized likelihood estimator for variance\nparameters in multilevel models. Psychometrika, 78(4),\n685–709. https://doi.org/10.1007/s11336-013-9328-2\n\n\nCooney, N. L., Kadden, R. M., Litt, M. D., & Getter, H. (1991).\nMatching alcoholics to coping skills or interactional therapies: Two-year follow-up results. Journal of\nConsulting and Clinical Psychology, 59(4), 598. https://doi.org/10.1037/0022-006X.59.4.598\n\n\nCox, David R. (1972). Regression models and life-tables. Journal of\nthe Royal Statistical Society: Series B (Methodological),\n34(2), 187–202. https://doi.org/10.1111/j.2517-6161.1972.tb00899.x\n\n\nCox, David Roxbee, & Oakes, D. (1984). Analysis of survival\ndata (Vol. 21). CRC Press. https://www.routledge.com/Analysis-of-Survival-Data/Cox-Oakes/p/book/9780412244902\n\n\nCranford, J. A., Shrout, P. E., Iida, M., Rafaeli, E., Yip, T., &\nBolger, N. (2006). A procedure for evaluating sensitivity to\nwithin-person change: Can mood measures in diary studies\ndetect change reliably? Personality and Social Psychology\nBulletin, 32(7), 917–929. https://doi.org/10.1177/0146167206287721\n\n\nCronbach, L. J., Gleser, G. C., Nanda, H., & Rajaratnam, N. (1972).\nThe dependability of behavioral measurements: Theory of\ngeneralizability for scores and profiles. John Wiley & Sons. https://www.amazon.com/Dependability-Behavioral-Measurements-Generalizability-Profiles/dp/0471188506\n\n\nDiekmann, A., Jungbauer-Gans, M., Krassnig, H., & Lorenz, S. (1996).\nSocial status and aggression: A field study analyzed by\nsurvival analysis. The Journal of Social Psychology,\n136(6), 761–768. https://doi.org/10.1080/00224545.1996.9712252\n\n\nEnders, C. K. (2010). Applied missing data analysis. Guilford\nPress. http://www.appliedmissingdata.com/\n\n\nFlinn, C. J., & Heckman, J. J. (1982). New methods for analyzing\nindividual event histories. Sociological Methodology,\n13, 99–140. https://doi.org/10.2307/270719\n\n\nFrank, A. R., & Keith, T. Z. (1984). Academic abilities of persons\nentering and remaining in special education. Exceptional\nChildren, 51(1), 76–77. https://eric.ed.gov/?id=EJ306852\n\n\nGabry, J. (2020). loo reference manual,\nVersion 2.4.1. https://CRAN.R-project.org/package=loo/loo.pdf\n\n\nGabry, J., & Mahr, T. (2022). bayesplot: Plotting for\nBayesian models. https://CRAN.R-project.org/package=bayesplot\n\n\nGabry, J., & Modrák, M. (2020). Visual MCMC\ndiagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A.\n(2019). Visualization in Bayesian workflow. Journal of\nthe Royal Statistical Society: Series A (Statistics in Society),\n182(2), 389–402. https://doi.org/10.1111/rssa.12378\n\n\nGamse, B. C., & Conger, D. (1997). An evaluation of the\nSpencer post-doctoral dissertation program. Abt\nAssociates.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A.,\n& Rubin, D. B. (2013). Bayesian data analysis (Third\nEdition). CRC press. https://stat.columbia.edu/~gelman/book/\n\n\nGelman, A., Goodrich, B., Gabry, J., & Vehtari, A. (2019). R-squared\nfor Bayesian regression models. The American\nStatistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression\nand multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other\nstories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGilks, W. R., Richardson, S., & Spiegelhalter, D. (1995). Markov\nchain Monte Carlo in practice. Chapman and\nHall/CRC. https://www.routledge.com/Markov-Chain-Monte-Carlo-in-Practice/Gilks-Richardson-Spiegelhalter/p/book/9780412055515\n\n\nGinexi, E. M., Howe, G. W., & Caplan, R. D. (2000). Depression and\ncontrol beliefs in relation to reemployment: What are the\ndirections of effect? Journal of Occupational Health\nPsychology, 5(3), 323–336. https://doi.org/10.1037/1076-8998.5.3.323\n\n\nGraham, S. E. (1997). The exodus from mathematics: When\nand why? [PhD thesis]. Harvard Graduate School of Education.\n\n\nGreenwood, M. (1926). The natural duration of cancer. Reports on\nPublic Health and Medical Subjects, 33, 1–26.\n\n\nHead, R., & Pike, D. (1975). A review of response surface\nmethodology from a biometric point of view. Biometrics. Journal of\nthe International Biometric Society, 31, 803–851.\n\n\nHeckman, J., & Singer, B. S. (Eds.). (1984). Longitudinal\nanalysis of labor market data. Cambridge University Press. https://doi.org/10.1017/CCOL0521304539\n\n\nHu, X. J., & Lawless, J. F. (1996). Estimation from truncated\nlifetime data with supplementary information on covariates and censoring\ntimes. Biometrika, 83(4), 747–761. https://doi.org/10.1093/biomet/83.4.747\n\n\nJaeger, B. C., Edwards, L. J., Das, K., & Sen, P. K. (2017). An\nR2 statistic for fixed effects in the generalized linear\nmixed model. Journal of Applied Statistics, 44(6),\n1086–1105. https://doi.org/10.1080/02664763.2016.1193725\n\n\nKaplan, E. L., & Meier, P. (1958). Nonparametric estimation from\nincomplete observations. Journal of the American Statistical\nAssociation, 53(282), 457–481. https://doi.org/10.1080/01621459.1958.10501452\n\n\nKay, M. (2021). ggdist:\nVisualizations of distributions and uncertainty\n[Manual]. https://CRAN.R-project.org/package=ggdist\n\n\nKay, M. (2023). tidybayes:\nTidy data and ’geoms’ for Bayesian\nmodels. https://CRAN.R-project.org/package=tidybayes\n\n\nKeiley, Margaret Kraatz, Bates, J. E., Dodge, K. A., & Pettit, G. S.\n(2000). A cross-domain growth analysis: Externalizing and\ninternalizing behaviors during 8 years of childhood. Journal of\nAbnormal Child Psychology, 28(2), 161–179. https://doi.org/10.1023/A:1005122814723\n\n\nKeiley, M. K., & Martin, N. C. (2002). Child abuse, neglect, and\njuvenile delinquency: How “new” statistical\napproaches can inform our understanding of “old”\nquestions—A reanalysis of Widom, 1989.\nManuscript Submitted for Publication.\n\n\nKreft, I. G. G., & de Leeuw, J. (1990). Comparing four different\nstatistical packages for hierarchical linear regression:\nGENMOD, HLM, ML2, and\nVARCL. CSE Dissemination Office, UCLA Graduate School\nof Education, 405 Hilgard Avenue, Los Angeles, CA 90024-1521. https://files.eric.ed.gov/fulltext/ED340731.pdf\n\n\nKreft, I. G., & de Leeuw, J. (1998). Introducing multilevel\nmodeling. SAGE Publications, Inc. https://doi.org/https://dx.doi.org/10.4135/9781849209366\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis:\nA tutorial with R, JAGS, and\nStan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKruschke, J. K., & Liddell, T. M. (2018). The Bayesian New\nStatistics: Hypothesis testing, estimation,\nmeta-analysis, and power analysis from a Bayesian\nperspective. Psychonomic Bulletin & Review, 25(1),\n178–206. https://doi.org/10.3758/s13423-016-1221-4\n\n\nKuhn, M., Jackson, S., & Cimentada, J. (2020). corrr: Correlations in\nR [Manual]. https://CRAN.R-project.org/package=corrr\n\n\nKurz, A. S. (2026a). Statistical rethinking 2 with brms and the\ntidyverse (version 0.5.0). https://solomon.quarto.pub/sr2/\n\n\nKurz, A. S. (2026b). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0).\nhttps://solomon.quarto.pub/sr/\n\n\nLambert, B. (2018). A student’s guide to Bayesian\nstatistics. SAGE Publications, Inc. https://ben-lambert.com/a-students-guide-to-bayesian-statistics/\n\n\nLawless, J. F. (1982). Statistical models and methods for lifetime\ndata. John Wiley & Sons.\n\n\nLi, H., & Lahiri, P. (2010). An adjusted maximum likelihood method\nfor solving small area estimation problems. Journal of Multivariate\nAnalysis, 101(4), 882–892. https://doi.org/10.1016/j.jmva.2009.10.009\n\n\nLittle, R. J. (1995). Modeling the drop-out mechanism in\nrepeated-measures studies. Journal of the American Statistical\nAssociation, 90(431), 1112–1121. https://doi.org/10.1080/01621459.1995.10476615\n\n\nLittle, R. J. A., & Rubin, D., B. (1987). Statistical analysis\nwith missing data. Wiley.\n\n\nLittle, R. J., & Rubin, D. B. (2019). Statistical analysis with\nmissing data (3rd ed., Vol. 793). John Wiley & Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798\n\n\nLoPilato, A. C., Carter, N. T., & Wang, M. (2015). Updating\ngeneralizability theory in management research: Bayesian\nestimation of variance components. Journal of Management,\n41(2), 692–717. https://doi.org/10.1177/0149206314554215\n\n\nMallinckrod, C. H., Lane, P. W., Schnell, D., Peng, Y., & Mancuso,\nJ. P. (2008). Recommendations for the primary analysis of continuous\nendpoints in longitudinal clinical trials. Drug Information\nJournal, 42(4), 303–319. https://doi.org/10.1177/009286150804200402\n\n\nMare, R. D. (1994). Discrete-time bivariate hazards with unobserved\nheterogeneity: A partially observed contingency table\napproach. Sociological Methodology, 341–383. https://doi.org/10.2307/270987\n\n\nMcElreath, R. (2015). Statistical rethinking: A\nBayesian course with examples in R and\nStan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020a). rethinking\nR package. https://xcelab.net/rm/software/\n\n\nMcElreath, R. (2020b). Statistical rethinking: A\nBayesian course with examples in R and\nStan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcNeish, D., Stapleton, L. M., & Silverman, R. D. (2017). On the\nunnecessary ubiquity of hierarchical linear modeling. Psychological\nMethods, 22(1), 114. https://doi.org/10.1037/met0000078\n\n\nMiller, R. G. (1981). Survival analysis. John Wiley & Sons.\n\n\nMorris, C., & Tang, R. (2011). Estimating random effects via\nadjustment for density maximization. Statistical Science,\n26(2), 271–287. https://doi.org/10.1214/10-STS349\n\n\nNewsom, J. T. (2015). Longitudinal structural equation modeling:\nA comprehensive introduction. Routledge. http://www.longitudinalsem.com/\n\n\nNezlek, J. B. (2007). A multilevel framework for understanding\nrelationships among traits, states, situations and behaviours.\nEuropean Journal of Personality, 21(6), 789–810. https://doi.org/10.1002/per.640\n\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T.\n(2018). The preregistration revolution. Proceedings of the National\nAcademy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114\n\n\nPearl, J., Glymour, M., & Jewell, N. P. (2016). Causal\nInference in Statistics - A\nPrimer (1st Edition). Wiley. https://www.wiley.com/en-us/Causal+Inference+in+Statistics%3A+A+Primer-p-9781119186847\n\n\nPedersen, T. L. (2022). patchwork:\nThe composer of plots. https://CRAN.R-project.org/package=patchwork\n\n\nPeng, R. D. (2019). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n\n\nPinheiro, J., Bates, D., & R-core. (2021). nlme: Linear and nonlinear mixed\neffects models [Manual]. https://CRAN.R-project.org/package=nlme\n\n\nPlummer, M. (2003). JAGS: A program for\nanalysis of Bayesian graphical models using\nGibbs sampling. Proceedings of the 3rd International\nWorkshop on Distributed Statistical Computing, 124, 1–10.\nhttp://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf\n\n\nPlummer, M. (2012). JAGS Version 3.3.0 user\nmanual. http://www.stat.cmu.edu/~brian/463-663/week10/articles,%20manuals/jags_user_manual.pdf\n\n\nR Core Team. (2022). R: A language and environment for\nstatistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRadloff, L. S. (1977). The CES-D Scale: A\nself-report depression scale for research in the general population.\nApplied Psychological Measurement, 1(3), 385–401. https://doi.org/10.1177/014662167700100306\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear\nmodels: Applications and data analysis methods (Second\nEdition). SAGE Publications, Inc. https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230\n\n\nRaudenbush, S. W., & Chan, W.-S. (2016). Growth curve analysis in\naccelerated longitudinal designs. Journal of Research in Crime and\nDelinquency, 29(4), 387–411. https://doi.org/10.1177/0022427892029004001\n\n\nRevelle, W. (2022). psych:\nProcedures for psychological, psychometric, and personality\nresearch. https://CRAN.R-project.org/package=psych\n\n\nRights, Jason D., & Cole, D. A. (2018). Effect size measures for\nmultilevel models in clinical child and adolescent research: New R-squared methods and recommendations.\nJournal of Clinical Child & Adolescent Psychology,\n47(6), 863–873. https://doi.org/10.1080/15374416.2018.1528550\n\n\nRights, Jason D., & Sterba, S. K. (2020). New recommendations on the\nuse of R-squared differences in multilevel\nmodel comparisons. Multivariate Behavioral Research,\n55(4), 568–599. https://doi.org/10.1080/00273171.2019.1660605\n\n\nRipley, B. (2022). MASS: Support functions\nand datasets for venables and Ripley’s\nMASS. https://CRAN.R-project.org/package=MASS\n\n\nRobinson, D., Hayes, A., & Couch, S. (2022). broom: Convert statistical objects\ninto tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom\n\n\nRogosa, D. R., & Willett, J. B. (1985). Understanding correlates of\nchange by modeling individual differences in growth.\nPsychometrika, 50(2), 203–228. https://doi.org/10.1007/BF02294247\n\n\nRogosa, D., Brandt, D., & Zimowski, M. (1982). A growth curve\napproach to the measurement of change. Psychological Bulletin,\n92(3), 726–748. https://doi.org/10.1037/0033-2909.92.3.726\n\n\nRupert G. Miller, Jr. (1997). Beyond ANOVA:\nBasics of applied statistics. Chapman and\nHall/CRC. https://www.routledge.com/Beyond-ANOVA-Basics-of-Applied-Statistics/Jr/p/book/9780412070112\n\n\nSandberg, D. E., Meyer-Bahlburg, H. F. L., & Yager, T. J. (1991).\nThe Child Behavior Checklist nonclinical standardization\nsamples: Should they be utilized as norms? Journal of\nthe American Academy of Child & Adolescent Psychiatry,\n30(1), 124–134. https://doi.org/10.1097/00004583-199101000-00019\n\n\nSchafer, J. L. (1997). Analysis of incomplete multivariate\ndata. CRC press. https://www.routledge.com/Analysis-of-Incomplete-Multivariate-Data/Schafer/p/book/9780412040610\n\n\nScheike, T. H., & Jensen, T. K. (1997). A discrete survival model\nwith random effects: An application to time to pregnancy.\nBiometrics. Journal of the International Biometric Society,\n318–329. https://doi.org/10.2307/2533117\n\n\nSchloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen,\nE., Elberg, A., & Larmarange, J. (2021). GGally:\nExtension to ’ggplot2’. https://CRAN.R-project.org/package=GGally\n\n\nShrout, P. E., & Lane, S. P. (2012). Psychometrics. In M. R. Mehl\n& T. S. Conner (Eds.), Handbook of research methods for studying\ndaily life (pp. 302–320). The Guilford Press. https://www.guilford.com/books/Handbook-of-Research-Methods-for-Studying-Daily-Life/Mehl-Conner/9781462513055\n\n\nSinger, J. D. (1992). Are special educators’ career paths special?\nResults from a 13-year longitudinal study. Exceptional\nChildren, 59(3), 262–279. https://doi.org/10.1177/001440299305900309\n\n\nSinger, J. D., Davidson, S. M., Graham, S., & Davidson, H. S.\n(1998). Physician retention in community and migrant health centers:\nWho stays and for how long? Medical Care,\n1198–1213. http://www.jstor.org/stable/3766886\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal\ndata analysis: Modeling change and event occurrence.\nOxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nSnijders, T. A. B., & Bosker, R. J. (1994). Modeled variance in\ntwo-level models. Sociological Methods & Research,\n22(3), 342–363. https://doi.org/10.1177/0049124194022003004\n\n\nSorenson, S. B., Rutter, C. M., & Aneshensel, C. S. (1991).\nDepression in the community: An investigation into age of\nonset. Journal of Consulting and Clinical Psychology,\n59(4), 541. https://doi.org/10.1037/0022-006X.59.4.541\n\n\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. V. D.\n(2002). Bayesian measures of model complexity and fit. Journal of\nthe Royal Statistical Society: Series B (Statistical Methodology),\n64(4), 583–639. https://doi.org/10.1111/1467-9868.00353\n\n\nStan Development Team. (2021a). Stan reference manual,\nVersion 2.27. https://mc-stan.org/docs/2_27/reference-manual/\n\n\nStan Development Team. (2021b). Stan user’s guide,\nVersion 2.26. https://mc-stan.org/docs/2_26/stan-users-guide/index.html\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016).\nIncreasing transparency through a multiverse analysis. Perspectives\non Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637\n\n\nSueyoshi, G. T. (1995). A class of binary response models for grouped\nduration data. Journal of Applied Econometrics, 10(4),\n411–431. https://doi.org/10.1002/jae.3950100406\n\n\nTherneau, Terry M. (2021a). survival\nreference manual, Version 3.2-10. https://CRAN.R-project.org/package=survival/survival.pdf\n\n\nTherneau, Terry M. (2021b). survival:\nSurvival analysis [Manual]. https://github.com/therneau/survival\n\n\nTherneau, Terry M. (2021c). A package for survival analysis in\nR. https://CRAN.R-project.org/package=survival/vignettes/survival.pdf\n\n\nTherneau, Terry M., & Grambsch, P. M. (2000). Modeling survival\ndata: Extending the Cox model. Springer.\nhttps://link.springer.com/book/10.1007/978-1-4757-3294-8\n\n\nTomarken, A., Shelton, R., Elkins, L., & Anderson, T. (1997). Sleep\ndeprivation and anti-depressant medication: Unique effects\non positive and negative affect. American Psychological Society\nMeeting, Washington, DC.\n\n\nTurnbull, B. W. (1974). Nonparametric estimation of a survivorship\nfunction with doubly censored data. Journal of the American\nStatistical Association, 69(345), 169–173. https://doi.org/10.1080/01621459.1974.10480146\n\n\nvan Buuren, S. (2018). Flexible imputation of missing data\n(Second Edition). CRC Press. https://stefvanbuuren.name/fimd/\n\n\nVaupel, J. W., Manton, K. G., & Stallard, E. (1979). The impact of\nheterogeneity in individual frailty on the dynamics of mortality.\nDemography, 16(3), 439–454. https://doi.org/10.2307/2061224\n\n\nVaupel, J. W., & Yashin, A. I. (1985). Heterogeneity’s ruses:\nSome surprising effects of selection on population\ndynamics. The American Statistician, 39(3), 176–185.\nhttps://doi.org/10.1080/00031305.1985.10479424\n\n\nVehtari, A., & Gabry, J. (2020). Using the loo package (version\n&gt;= 2.0.0). https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html\n\n\nVehtari, A., Gabry, J., Magnusson, M., Yao, Y., & Gelman, A. (2022).\nloo: Efficient\nleave-one-out cross-validation and WAIC for bayesian\nmodels. https://CRAN.R-project.org/package=loo/\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical\nBayesian model evaluation using leave-one-out\ncross-validation and WAIC. Statistics and\nComputing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner,\nP.-C. (2019). Rank-normalization, folding, and localization:\nAn improved for assessing convergence of\nMCMC. https://arxiv.org/abs/1903.08008?\n\n\nVehtari, A., Simpson, D., Gelman, A., Yao, Y., & Gabry, J. (2021).\nPareto smoothed importance sampling. https://arxiv.org/abs/1507.02646\n\n\nVenables, W. N., & Ripley, B. D. (2002). Modern applied\nstatistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4\n\n\nWatanabe, S. (2010). Asymptotic equivalence of Bayes cross\nvalidation and widely applicable information criterion in singular\nlearning theory. Journal of Machine Learning Research,\n11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html\n\n\nWheaton, B., Roszell, P., & Hall, K. (1997). The impact of twenty\nchildhood and adult traumatic stressors on the risk of psychiatric\ndisorder. In I. H. Gotlib & B. Wheaton (Eds.), Stress and\nadversity over the life course: Trajectories and turning\npoints (pp. 50–72). Cambridge University Press. https://doi.org/10.1017/CBO9780511527623\n\n\nWickham, H. (2022). tidyverse:\nEasily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D.,\nFrançois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M.,\nPedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J.,\nRobinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to\nthe tidyverse. Journal of Open Source Software, 4(43),\n1686. https://doi.org/10.21105/joss.01686\n\n\nWillett, J. B. (1988). Chapter 9: Questions and answers in\nthe measurement of change. Review of Research in Education,\n15, 345–422. https://doi.org/10.2307/1167368\n\n\nWillett, J. B. (1989). Some results on reliability for the longitudinal\nmeasurement of change: Implications for the design of\nstudies of individual growth. Educational and Psychological\nMeasurement, 49(3), 587–602. https://doi.org/10.1177/001316448904900309\n\n\nWilliams, D. R., Rouder, J., & Rast, P. (2019). Beneath the\nsurface: Unearthing within-Person variability\nand mean relations with Bayesian mixed models. https://doi.org/10.31234/osf.io/gwatq\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using\nstacking to average Bayesian predictive distributions (with\ndiscussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091\n\n\nZorn, C. J., & Van Winkle, S. R. (2000). A competing risks model of\nSupreme Court vacancies, 1789–1992. Political\nBehavior, 22(2), 145–166. https://doi.org/10.1023/A:1006667601289",
    "crumbs": [
      "References"
    ]
  }
]